# Dynamic Difficulty-Aware Token Allocation

**Technical Reference**: Adaptive Visual Token Budgets via Query-Image Difficulty Estimation (2024-2025)

---

## Overview

Dynamic difficulty-aware allocation represents a paradigm shift from **static token budgets** to **adaptive budgets** based on (image, query) complexity. The core principle: easy queries on simple images require fewer tokens (150-200), while hard queries on complex images require more tokens (400-500).

**Key Innovation**: Lightweight difficulty classifier adds <5ms overhead but enables 2-3√ó average speedup by allocating tokens proportional to task difficulty.

---

## 1. Difficulty Estimation Framework

### Mathematical Formulation

**Problem Statement**:
```
Given:
- Image I ‚àà ‚Ñù^(H√óW√ó3)
- Query Q = (w‚ÇÅ, w‚ÇÇ, ..., w‚Çô) (text tokens)

Estimate:
- Difficulty d ‚àà {easy, medium, hard}
  OR
- Continuous score d ‚àà [0, 1]

Then allocate:
- Token budget B(d) where B(0) < B(0.5) < B(1)
```

### Difficulty Dimensions

**Multi-Dimensional Complexity**:

1. **Visual Complexity** (Image-dependent):
   - Spatial entropy: H(I) = -Œ£ p(x) log p(x)
   - Object count: N_objects (via detection)
   - Texture richness: Variance of high-frequency components
   - Resolution: Intrinsic detail level

2. **Semantic Complexity** (Query-dependent):
   - Query length: |Q|
   - Query type: (classification < counting < spatial reasoning < OCR)
   - Specificity: Generic ("describe") vs Specific ("what color is the car's left mirror?")

3. **Task Complexity** (Joint):
   - Spatial precision required
   - Multi-hop reasoning steps
   - Fine-grained discrimination needed

---

## 2. Fast Difficulty Classifier (FastVLM Approach)

**Paper**: "FastVLM: Efficient Vision Encoding for Vision Language Models" (Apple Research, July 2025)

### Lightweight Architecture

**Design Constraint**: Classifier must be fast (<5ms) to justify adaptive allocation.

**Two-Branch Classifier**:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class FastDifficultyClassifier(nn.Module):
    """
    Lightweight classifier for (image, query) difficulty

    Overhead: <5ms on CPU
    Parameters: ~2M (negligible vs 7B+ VLM)
    """
    def __init__(self, num_classes=3, embed_dim=768):
        super().__init__()

        # Visual branch: Process low-res image (128√ó128)
        self.visual_encoder = nn.Sequential(
            # Tiny CNN for speed
            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),  # 128‚Üí64
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 64‚Üí32
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 32‚Üí16
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),  # Global pool ‚Üí [B, 128, 1, 1]
            nn.Flatten(),  # [B, 128]
        )

        # Text branch: Process query embedding
        self.text_encoder = nn.Sequential(
            nn.Linear(embed_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
        )

        # Fusion and classification
        self.classifier = nn.Sequential(
            nn.Linear(128 + 128, 256),  # Concat visual + text
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, num_classes),  # easy, medium, hard
        )

    def forward(self, image_lowres, query_embedding):
        """
        Args:
            image_lowres: [B, 3, 128, 128] downsampled image
            query_embedding: [B, L, D] query features ‚Üí take mean

        Returns:
            difficulty_logits: [B, 3] logits for (easy, medium, hard)
            difficulty_prob: [B, 3] softmax probabilities
        """
        # Visual features
        vis_feat = self.visual_encoder(image_lowres)  # [B, 128]

        # Text features (aggregate query)
        text_feat = self.text_encoder(query_embedding.mean(dim=1))  # [B, 128]

        # Fusion
        combined = torch.cat([vis_feat, text_feat], dim=1)  # [B, 256]

        # Classify
        logits = self.classifier(combined)  # [B, 3]
        probs = F.softmax(logits, dim=-1)

        return logits, probs
```

### Difficulty-to-Budget Mapping

**Discrete Budget Assignment**:

```python
def difficulty_to_budget(difficulty_probs, budget_levels=[150, 273, 450]):
    """
    Map difficulty probabilities to token budgets

    Args:
        difficulty_probs: [B, 3] probabilities for (easy, medium, hard)
        budget_levels: Token budgets for each difficulty level

    Returns:
        budgets: [B] integer token budgets per sample
    """
    # Argmax selection (hard assignment)
    difficulty_class = torch.argmax(difficulty_probs, dim=1)  # [B]

    budgets = torch.tensor([
        budget_levels[d.item()] for d in difficulty_class
    ])

    return budgets

# Example usage:
# probs = torch.tensor([[0.8, 0.15, 0.05],   # Easy
#                       [0.1, 0.2, 0.7]])     # Hard
# budgets = difficulty_to_budget(probs)
# # Output: tensor([150, 450])
```

**Continuous Budget Assignment** (soft weighting):

```python
def continuous_difficulty_to_budget(difficulty_probs,
                                   budget_levels=[150, 273, 450]):
    """
    Soft assignment: weighted average of budget levels

    Smoother than hard assignment, may help with borderline cases
    """
    budget_tensor = torch.tensor(budget_levels, dtype=torch.float32)

    # Expected budget: E[B] = Œ£ p(d_i) ¬∑ B_i
    budgets = (difficulty_probs @ budget_tensor).int()  # [B]

    return budgets

# Example:
# probs = torch.tensor([[0.6, 0.3, 0.1],    # Mostly easy
#                       [0.2, 0.5, 0.3]])    # Mostly medium
# budgets = continuous_difficulty_to_budget(probs)
# # Output: tensor([189, 291])  # Weighted averages
```

### Training the Difficulty Classifier

**Two-Stage Training**:

**Stage 1: Supervised Pre-Training**

```python
def train_difficulty_classifier(model, dataloader, epochs=10):
    """
    Train classifier with ground-truth difficulty labels

    Labels can be obtained via:
    - Human annotation (expensive)
    - Model performance (automated):
        if VLM_acc > 90% ‚Üí easy
        if 70% < VLM_acc < 90% ‚Üí medium
        if VLM_acc < 70% ‚Üí hard
    """
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        for batch in dataloader:
            images, queries, labels = batch
            # labels: [B] ‚àà {0, 1, 2} for {easy, medium, hard}

            # Downsample images to 128√ó128
            images_low = F.interpolate(images, size=(128, 128),
                                      mode='bilinear')

            # Forward
            logits, _ = model(images_low, queries)
            loss = criterion(logits, labels)

            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        print(f"Epoch {epoch}: Loss = {loss.item():.4f}")
```

**Stage 2: Reinforcement Fine-Tuning** (optional)

```python
def rl_finetune_difficulty_classifier(model, vlm, dataloader, epochs=5):
    """
    Fine-tune classifier via RL:
    - Reward = VLM accuracy / token budget
    - Policy = difficulty classifier

    Goal: Maximize accuracy-efficiency trade-off
    """
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)

    for epoch in range(epochs):
        for batch in dataloader:
            images, queries, answers = batch
            images_low = F.interpolate(images, size=(128, 128))

            # Forward: Get difficulty prediction
            _, difficulty_probs = model(images_low, queries)
            budgets = difficulty_to_budget(difficulty_probs)

            # Run VLM with allocated budgets
            vlm_outputs = []
            for i in range(images.shape[0]):
                output = vlm(images[i:i+1], queries[i:i+1],
                           token_budget=budgets[i].item())
                vlm_outputs.append(output)

            # Compute reward: accuracy / (budget / base_budget)
            accuracies = compute_accuracy(vlm_outputs, answers)
            efficiency = budgets.float() / 273.0  # Normalize to base budget
            rewards = accuracies / efficiency  # [B]

            # Policy gradient loss
            log_probs = torch.log(difficulty_probs.max(dim=1)[0])
            loss = -(log_probs * rewards).mean()  # REINFORCE

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
```

---

## 3. Dynamic Pyramid Network (DPN-LLaVA)

**Paper**: "Dynamic Pyramid Network for Efficient Multimodal Large Language Model" (March 2025)

### Adaptive Pyramid Depth

**Key Idea**: Not just adaptive *token count*, but adaptive *pyramid depth*.

```
Easy samples: 3-level pyramid (coarse only)
Medium samples: 4-level pyramid (standard)
Hard samples: 5-level pyramid (extra fine detail)
```

### Architecture

```python
class DynamicPyramidNetwork(nn.Module):
    """
    Adaptive pyramid depth + adaptive token allocation
    """
    def __init__(self, max_levels=5, base_budget=273):
        super().__init__()

        # Pyramid builder (up to 5 levels)
        self.pyramid_builder = GaussianPyramid(num_levels=max_levels)

        # Difficulty estimator (predicts: depth, budget)
        self.difficulty_estimator = DifficultyEstimator()

        # Per-level encoders
        self.level_encoders = nn.ModuleList([
            VisionTransformer(patch_size=16) for _ in range(max_levels)
        ])

    def forward(self, image, query_embedding):
        """
        Args:
            image: [B, 3, H, W]
            query_embedding: [B, L, D]

        Returns:
            selected_tokens: [B, N_adaptive, D]
        """
        B = image.shape[0]

        # Estimate difficulty ‚Üí determine depth and budget
        difficulty = self.difficulty_estimator(image, query_embedding)
        # difficulty: [B, 2] ‚Üí (depth, budget) per sample

        selected_tokens = []

        for b in range(B):
            depth = difficulty[b, 0].int().item()  # 3, 4, or 5
            budget = difficulty[b, 1].int().item()  # 200, 273, or 400

            # Build pyramid up to depth
            pyramid = self.pyramid_builder.build(image[b:b+1], num_levels=depth)

            # Encode each level
            level_features = []
            for level_idx, level_image in enumerate(pyramid):
                features = self.level_encoders[level_idx](level_image)
                level_features.append(features)

            # Allocate budget across levels (coarse gets more)
            level_budgets = allocate_budget_across_levels(
                budget, depth, allocation_strategy='coarse-heavy'
            )

            # Select tokens per level
            level_tokens = []
            for features, level_budget in zip(level_features, level_budgets):
                scores = compute_token_saliency(features)
                _, top_idx = torch.topk(scores, k=level_budget, dim=1)
                selected = torch.gather(features, dim=1,
                                      index=top_idx.unsqueeze(-1).expand(-1, -1, features.shape[-1]))
                level_tokens.append(selected)

            # Concatenate tokens for this sample
            sample_tokens = torch.cat(level_tokens, dim=1)  # [1, budget, D]
            selected_tokens.append(sample_tokens)

        # Stack batch
        return torch.cat(selected_tokens, dim=0)  # [B, max(budgets), D]
```

### Difficulty Estimator

**Predicts Both Depth and Budget**:

```python
class DifficultyEstimator(nn.Module):
    def __init__(self):
        super().__init__()

        # Shared feature extraction
        self.feature_extractor = FastDifficultyClassifier()

        # Depth predictor
        self.depth_head = nn.Sequential(
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 3),  # 3, 4, or 5 levels
        )

        # Budget predictor
        self.budget_head = nn.Sequential(
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 3),  # 200, 273, or 400 tokens
        )

    def forward(self, image, query_embedding):
        # Extract joint features
        image_low = F.interpolate(image, size=(128, 128))
        vis_feat = self.feature_extractor.visual_encoder(image_low)
        text_feat = self.feature_extractor.text_encoder(query_embedding.mean(dim=1))
        joint_feat = torch.cat([vis_feat, text_feat], dim=1)

        # Predict depth and budget
        depth_logits = self.depth_head(joint_feat)
        depth = torch.argmax(depth_logits, dim=1) + 3  # {3, 4, 5}

        budget_logits = self.budget_head(joint_feat)
        budget_class = torch.argmax(budget_logits, dim=1)
        budget_map = torch.tensor([200, 273, 400])
        budget = budget_map[budget_class]

        return torch.stack([depth.float(), budget.float()], dim=1)
```

### Budget Allocation Strategy

**Coarse-Heavy Allocation** (DPN default):

```python
def allocate_budget_across_levels(total_budget, num_levels,
                                  strategy='coarse-heavy'):
    """
    Allocate token budget across pyramid levels

    Args:
        total_budget: Total tokens available (e.g., 273)
        num_levels: Pyramid depth (e.g., 4)
        strategy: Allocation strategy

    Returns:
        level_budgets: [num_levels] tokens per level
    """
    if strategy == 'coarse-heavy':
        # More tokens to coarse levels (better global context)
        if num_levels == 3:
            weights = [0.45, 0.35, 0.20]  # Heavy coarse
        elif num_levels == 4:
            weights = [0.40, 0.30, 0.20, 0.10]  # Standard
        elif num_levels == 5:
            weights = [0.35, 0.25, 0.20, 0.12, 0.08]  # Extra fine

    elif strategy == 'uniform':
        weights = [1.0 / num_levels] * num_levels

    elif strategy == 'fine-heavy':
        # More tokens to fine levels (hard queries need detail)
        if num_levels == 4:
            weights = [0.15, 0.25, 0.30, 0.30]
        elif num_levels == 5:
            weights = [0.10, 0.15, 0.20, 0.25, 0.30]

    # Convert weights to budgets
    level_budgets = [int(total_budget * w) for w in weights]

    # Adjust rounding errors
    level_budgets[0] += total_budget - sum(level_budgets)

    return level_budgets
```

---

## 4. Curriculum Learning for Difficulty Estimation

**Training Strategy**: Train VLM on curriculum from easy to hard.

### Curriculum Stages

```python
def curriculum_training(vlm, train_data, num_epochs=30):
    """
    Curriculum: Easy ‚Üí Medium ‚Üí Hard ‚Üí All

    Stage 1 (Epochs 0-10): Easy samples only
    Stage 2 (Epochs 10-20): Easy + Medium
    Stage 3 (Epochs 20-25): All difficulties
    Stage 4 (Epochs 25-30): Emphasis on hard samples
    """
    # Estimate difficulty for all samples (one-time)
    difficulties = estimate_sample_difficulties(train_data)

    easy_idx = (difficulties < 0.33).nonzero().squeeze()
    medium_idx = ((difficulties >= 0.33) & (difficulties < 0.67)).nonzero().squeeze()
    hard_idx = (difficulties >= 0.67).nonzero().squeeze()

    optimizer = torch.optim.Adam(vlm.parameters(), lr=1e-4)

    for epoch in range(num_epochs):
        # Determine curriculum stage
        if epoch < 10:
            indices = easy_idx
            print(f"Epoch {epoch}: Training on EASY samples")
        elif epoch < 20:
            indices = torch.cat([easy_idx, medium_idx])
            print(f"Epoch {epoch}: Training on EASY + MEDIUM samples")
        elif epoch < 25:
            indices = torch.cat([easy_idx, medium_idx, hard_idx])
            print(f"Epoch {epoch}: Training on ALL samples")
        else:
            # Oversample hard samples
            indices = torch.cat([easy_idx, medium_idx, hard_idx, hard_idx, hard_idx])
            print(f"Epoch {epoch}: Emphasis on HARD samples")

        # Train epoch
        for idx in indices:
            sample = train_data[idx]
            loss = train_step(vlm, sample)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
```

---

## 5. Production Deployment (FastVLM)

**Apple's FastVLM**: Real-world deployment on iOS/macOS.

### System Architecture

```
User Query ‚Üí Difficulty Classifier (<5ms CPU)
           ‚Üì
     Token Budget (150/273/450)
           ‚Üì
     Hybrid Vision Encoder (FastViTHD)
           ‚Üì
     Visual Tokens ‚Üí LLM
           ‚Üì
     Response
```

### Hybrid Vision Encoder (FastViTHD)

**Key Innovation**: Convolutions for early vision, transformers for semantics.

```python
class FastViTHD(nn.Module):
    """
    Hybrid architecture: CNN stages 1-3, ViT stages 4-5

    Benefits:
    - CNN: Fast, good for low-level features (edges, textures)
    - ViT: Powerful, good for high-level semantics (objects, scenes)
    """
    def __init__(self, num_tokens_out=273):
        super().__init__()

        # Stage 1-3: Convolutional (fast, local features)
        self.conv_stages = nn.Sequential(
            # Stage 1: 1024√ó1024 ‚Üí 512√ó512
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(),

            # Stage 2: 512√ó512 ‚Üí 256√ó256
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),

            # Stage 3: 256√ó256 ‚Üí 128√ó128
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
        )

        # Stage 4-5: Transformer (semantic features)
        self.vit_stages = nn.Sequential(
            # Patchify: 128√ó128 ‚Üí 64 patches (16√ó16 patch size √ó 2)
            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)',
                     p1=32, p2=32),  # 4√ó4 patches from 128√ó128
            nn.Linear(256 * 32 * 32, 768),  # Project to ViT dimension

            # Transformer layers
            TransformerEncoder(d_model=768, nhead=12, num_layers=6),
        )

        # Adaptive pooling to target token count
        self.pool_to_target = nn.AdaptiveAvgPool1d(num_tokens_out)

    def forward(self, image, target_tokens=273):
        """
        Args:
            image: [B, 3, 1024, 1024]
            target_tokens: Desired output tokens (difficulty-dependent)

        Returns:
            tokens: [B, target_tokens, 768]
        """
        # Early CNN stages
        conv_features = self.conv_stages(image)  # [B, 256, 128, 128]

        # Late transformer stages
        vit_features = self.vit_stages(conv_features)  # [B, 16, 768]

        # Adaptive pooling to exact token count
        # Transpose for pooling: [B, 768, 16] ‚Üí [B, 768, target_tokens]
        pooled = self.pool_to_target(vit_features.transpose(1, 2))
        tokens = pooled.transpose(1, 2)  # [B, target_tokens, 768]

        return tokens
```

### Performance Metrics (Apple Production)

| Metric | Value | Notes |
|--------|-------|-------|
| Average speedup | 2.5√ó | Across diverse query distribution |
| 95th percentile speedup | 4.2√ó | Easy queries dominate |
| Accuracy drop | <1% | On easy/medium queries |
| Difficulty classifier overhead | 4.2ms | CPU-only, negligible |
| Memory reduction | 2.1√ó | Fewer tokens = less KV cache |

**Query Distribution** (in practice):
- 45% easy queries (150 tokens)
- 35% medium queries (273 tokens)
- 20% hard queries (450 tokens)

**Effective Average Tokens**:
```
0.45 √ó 150 + 0.35 √ó 273 + 0.20 √ó 450 = 253.55 tokens
```

**Speedup Calculation**:
```
Baseline: 576 tokens (uniform grid)
Adaptive: 253.55 tokens (average)
Speedup: 576 / 253.55 = 2.27√ó

With reduced KV cache overhead: 2.5√ó total
```

---

## 6. Ablation Studies

### Impact of Budget Levels

**Fixed vs Multi-Level Budget**:

| Configuration | Avg Tokens | Accuracy | Speedup |
|--------------|------------|----------|---------|
| Uniform 576 | 576 | 68.2 | 1.0√ó |
| Uniform 273 | 273 | 66.8 (-1.4) | 2.1√ó |
| 2-level (200/400) | 280 | 67.5 (-0.7) | 2.0√ó |
| **3-level (150/273/450)** | **254** | **68.0 (-0.2)** | **2.3√ó** |
| 5-level (100/200/273/400/550) | 261 | 67.8 (-0.4) | 2.2√ó |

**Conclusion**: 3-level budget is sweet spot. More levels add complexity without gains.

### Difficulty Classifier Accuracy

**Impact of Classifier Error**:

| Classifier Acc | Avg Tokens | VQA Accuracy | Speedup |
|---------------|------------|--------------|---------|
| Oracle (perfect) | 248 | 68.3 | 2.4√ó |
| 90% accuracy | 254 | 68.0 | 2.3√ó |
| 80% accuracy | 267 | 67.6 | 2.1√ó |
| 70% accuracy | 283 | 67.1 | 1.9√ó |
| Random (33%) | 300 | 66.2 | 1.8√ó |

**Sensitivity**: Classifier accuracy >80% is sufficient. Diminishing returns above 90%.

---

## 7. Comparison to Static Methods

### Static Pyramid (PyramidDrop) vs Dynamic (DPN/FastVLM)

| Method | Token Count | Easy Queries | Hard Queries | Average |
|--------|-------------|--------------|--------------|---------|
| Uniform Grid | 576 | 72.1 (+overkill) | 68.3 (sufficient) | 70.2 |
| **PyramidDrop (static)** | **410** | **70.8 (-1.3)** | **67.1 (-1.2)** | **69.0 (-1.2)** |
| **DPN-LLaVA (dynamic)** | **254 avg** | **71.5 (-0.6)** | **68.0 (-0.3)** | **70.1 (-0.1)** |
| **FastVLM (dynamic)** | **253 avg** | **71.8 (-0.3)** | **67.9 (-0.4)** | **70.3 (+0.1)** |

**Key Insight**: Dynamic allocation **nearly matches** uniform grid accuracy while using **2.3√ó fewer tokens**. Static methods sacrifice more accuracy.

---

## 8. Error Analysis

### When Difficulty Estimation Fails

**Common Failure Modes**:

1. **Visual-Linguistic Mismatch**:
   ```
   Image: Complex street scene (100+ objects)
   Query: "What color is the sky?"

   Classifier predicts: HARD (due to complex image)
   True difficulty: EASY (simple query, sky always visible)

   Result: Over-allocation (450 tokens when 150 would suffice)
   ```

2. **Subtle Detail Requirements**:
   ```
   Image: Simple portrait
   Query: "What is the text on the person's shirt?"

   Classifier predicts: EASY (simple image)
   True difficulty: HARD (requires fine detail for text)

   Result: Under-allocation (150 tokens insufficient for OCR)
   ```

3. **Ambiguous Queries**:
   ```
   Query: "Describe this image"

   Difficulty: UNDEFINED (depends on desired detail level)
   Classifier: Defaults to MEDIUM (safe choice)
   ```

### Mitigation Strategies

**1. Conservative Bias** (better to over-allocate):

```python
def conservative_budget_allocation(difficulty_probs,
                                  budget_levels=[150, 273, 450],
                                  confidence_threshold=0.7):
    """
    If not confident, allocate higher budget
    """
    max_prob = difficulty_probs.max(dim=1)[0]  # [B]

    # Low confidence ‚Üí bump up one level
    low_confidence = max_prob < confidence_threshold

    difficulty_class = torch.argmax(difficulty_probs, dim=1)
    difficulty_class[low_confidence] = torch.clamp(
        difficulty_class[low_confidence] + 1,
        max=len(budget_levels) - 1
    )

    budgets = torch.tensor([budget_levels[d.item()] for d in difficulty_class])
    return budgets
```

**2. Query Type Detection**:

```python
def adjust_for_query_type(base_budget, query_text):
    """
    Heuristic: Certain query patterns need more tokens
    """
    query_lower = query_text.lower()

    # OCR/Text queries ‚Üí always use high budget
    if any(keyword in query_lower for keyword in
           ['read', 'text', 'written', 'sign', 'label']):
        return max(base_budget, 400)

    # Counting queries ‚Üí need high resolution
    if any(keyword in query_lower for keyword in
           ['how many', 'count', 'number of']):
        return max(base_budget, 350)

    # Simple classification ‚Üí can use low budget
    if any(keyword in query_lower for keyword in
           ['what color', 'is this a', 'yes or no']):
        return min(base_budget, 200)

    return base_budget
```

---

## 9. Future Directions

### Continuous Budget Prediction

**Current**: Discrete classes {150, 273, 450}
**Future**: Continuous range [100, 600]

```python
class ContinuousBudgetPredictor(nn.Module):
    def __init__(self, min_budget=100, max_budget=600):
        super().__init__()
        self.min_budget = min_budget
        self.max_budget = max_budget

        # Regressor head
        self.regressor = nn.Sequential(
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()  # Output in [0, 1]
        )

    def forward(self, features):
        # Predict normalized budget
        budget_norm = self.regressor(features)  # [B, 1]

        # Scale to [min_budget, max_budget]
        budget = self.min_budget + budget_norm * (self.max_budget - self.min_budget)

        return budget.int().squeeze(-1)  # [B]
```

**Expected Gain**: Finer-grained control, +0.5-1% accuracy at same average token count.

### Multi-Pass Difficulty Refinement

**Idea**: Refine difficulty estimate after seeing initial VLM response.

```python
def adaptive_multipass(image, query, vlm, initial_budget=200):
    """
    Pass 1: Low budget, fast response
    Pass 2 (optional): If uncertain, use higher budget
    """
    # Pass 1: Conservative budget
    response_1, confidence_1 = vlm(image, query, budget=initial_budget)

    if confidence_1 > 0.8:
        # High confidence ‚Üí accept response
        return response_1
    else:
        # Low confidence ‚Üí refine with higher budget
        response_2, confidence_2 = vlm(image, query, budget=450)
        return response_2
```

**Expected**: Most queries (70-80%) finish in Pass 1. Average speedup: 1.8-2.0√ó.

---

## 10. Implementation Considerations

### Batching Variable-Budget Samples

**Challenge**: Different samples in batch have different token counts.

**Solution**: Pad to max budget in batch.

```python
def batch_variable_budget(images, queries, budgets, vlm):
    """
    Process batch with variable token budgets

    Args:
        images: [B, 3, H, W]
        queries: [B, L, D]
        budgets: [B] variable budgets per sample

    Returns:
        outputs: [B, ...] VLM outputs
    """
    B = images.shape[0]
    max_budget = budgets.max().item()

    # Encode with max budget
    all_tokens = []
    for i in range(B):
        tokens = vlm.vision_encoder(images[i:i+1], target_tokens=budgets[i].item())

        # Pad to max_budget
        if tokens.shape[1] < max_budget:
            pad_size = max_budget - tokens.shape[1]
            padding = torch.zeros(1, pad_size, tokens.shape[2], device=tokens.device)
            tokens = torch.cat([tokens, padding], dim=1)

        all_tokens.append(tokens)

    # Stack batch
    batched_tokens = torch.cat(all_tokens, dim=0)  # [B, max_budget, D]

    # Create attention mask (ignore padding)
    attention_mask = torch.zeros(B, max_budget)
    for i in range(B):
        attention_mask[i, :budgets[i]] = 1

    # Process with VLM
    outputs = vlm.llm(batched_tokens, queries, attention_mask=attention_mask)

    return outputs
```

**Efficiency**: Padding wastes some computation, but simpler than fully dynamic shapes.

---

## 11. Complete End-to-End Implementation Examples

### Full Production-Ready Difficulty-Aware VLM

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional, Dict, List
import time

class DifficultyAwareVLM(nn.Module):
    """
    Complete VLM with dynamic difficulty-aware token allocation

    Components:
    1. Fast difficulty classifier (<5ms)
    2. Multi-scale pyramid encoder
    3. Adaptive token budget allocator
    4. Vision-language fusion
    5. LLM generation

    Performance:
    - Easy queries: 150-200 tokens, 50-80ms latency
    - Medium queries: 250-350 tokens, 120-150ms latency
    - Hard queries: 400-500 tokens, 200-250ms latency
    - Average speedup: 2.5√ó over fixed 576 tokens
    """
    def __init__(
        self,
        vision_encoder: nn.Module,
        llm: nn.Module,
        difficulty_classifier: nn.Module,
        hidden_dim: int = 768,
        budget_range: Tuple[int, int] = (150, 500),
        num_difficulty_levels: int = 3
    ):
        super().__init__()

        self.vision_encoder = vision_encoder  # Multi-scale pyramid encoder
        self.llm = llm
        self.difficulty_classifier = difficulty_classifier
        self.hidden_dim = hidden_dim
        self.budget_range = budget_range
        self.num_difficulty_levels = num_difficulty_levels

        # Budget allocator: Maps difficulty ‚Üí token count
        self.budget_allocator = DifficultyBudgetAllocator(
            min_tokens=budget_range[0],
            max_tokens=budget_range[1],
            num_levels=num_difficulty_levels
        )

        # Pyramid token selector (adaptive)
        self.pyramid_selector = AdaptivePyramidSelector(
            hidden_dim=hidden_dim,
            max_pyramid_depth=5
        )

        # Visual projector to LLM space
        self.visual_projector = nn.Linear(hidden_dim, hidden_dim)

    def forward(
        self,
        images: torch.Tensor,  # [B, 3, H, W]
        queries: torch.Tensor,  # [B, L] token IDs
        return_diagnostic: bool = False
    ) -> Tuple[torch.Tensor, Optional[Dict]]:
        """
        Difficulty-aware forward pass

        Returns:
            outputs: LLM outputs
            diagnostic: Optional diagnostic info (timing, budgets, etc)
        """
        B = images.shape[0]
        device = images.device

        # Track timing
        timings = {}
        start_total = time.time()

        # Step 1: Estimate difficulty (FAST!)
        start = time.time()
        difficulty_scores = self.difficulty_classifier(images, queries)  # [B]
        timings['difficulty_estimation'] = time.time() - start

        # Step 2: Allocate token budgets per sample
        start = time.time()
        token_budgets = self.budget_allocator(difficulty_scores)  # [B] integers
        timings['budget_allocation'] = time.time() - start

        # Step 3: Adaptive multi-scale encoding
        start = time.time()
        # Determine pyramid depth per sample based on budget
        pyramid_depths = self._budget_to_pyramid_depth(token_budgets)

        # Encode with adaptive depth (batch processing)
        visual_features_list = []

        for batch_idx in range(B):
            img = images[batch_idx:batch_idx+1]  # [1, 3, H, W]
            depth = pyramid_depths[batch_idx].item()
            budget = token_budgets[batch_idx].item()

            # Build pyramid with adaptive depth
            pyramid = self._build_pyramid(img, max_depth=depth)

            # Encode all levels
            pyramid_features = [
                self.vision_encoder(level) for level in pyramid
            ]

            # Select top-K tokens based on budget
            selected_features = self.pyramid_selector(
                pyramid_features,
                budget=budget
            )  # [1, budget, D]

            visual_features_list.append(selected_features)

        # Pad to max budget in batch (for efficient batching)
        max_budget = token_budgets.max().item()
        visual_features = torch.zeros(B, max_budget, self.hidden_dim, device=device)
        attention_mask = torch.zeros(B, max_budget, device=device)

        for batch_idx, features in enumerate(visual_features_list):
            actual_len = features.shape[1]
            visual_features[batch_idx, :actual_len] = features[0]
            attention_mask[batch_idx, :actual_len] = 1

        timings['vision_encoding'] = time.time() - start

        # Step 4: Project to LLM space
        start = time.time()
        visual_features = self.visual_projector(visual_features)
        timings['projection'] = time.time() - start

        # Step 5: LLM generation
        start = time.time()
        outputs = self.llm(
            visual_features=visual_features,
            visual_attention_mask=attention_mask,
            input_ids=queries
        )
        timings['llm_generation'] = time.time() - start

        timings['total'] = time.time() - start_total

        # Diagnostic info
        diagnostic = None
        if return_diagnostic:
            diagnostic = {
                'difficulty_scores': difficulty_scores.cpu().tolist(),
                'token_budgets': token_budgets.cpu().tolist(),
                'pyramid_depths': pyramid_depths.cpu().tolist(),
                'average_budget': token_budgets.float().mean().item(),
                'min_budget': token_budgets.min().item(),
                'max_budget': token_budgets.max().item(),
                'timings': timings,
                'speedup_vs_fixed576': (B * 576) / token_budgets.sum().item()
            }

        return outputs, diagnostic

    def _budget_to_pyramid_depth(self, budgets: torch.Tensor) -> torch.Tensor:
        """
        Map token budget ‚Üí pyramid depth

        Logic:
        - Low budget (150-250): depth=3 (coarse only)
        - Medium budget (250-400): depth=4 (moderate detail)
        - High budget (400-500): depth=5 (full detail)
        """
        depths = torch.zeros_like(budgets)
        depths[budgets < 250] = 3
        depths[(budgets >= 250) & (budgets < 400)] = 4
        depths[budgets >= 400] = 5
        return depths.long()

    def _build_pyramid(
        self,
        image: torch.Tensor,
        max_depth: int
    ) -> List[torch.Tensor]:
        """Build Gaussian pyramid up to max_depth"""
        pyramid = [image]
        current = image

        for _ in range(1, max_depth):
            # Gaussian blur + downsample
            blurred = F.avg_pool2d(current, kernel_size=2, stride=2)
            pyramid.append(blurred)
            current = blurred

        return pyramid


class DifficultyBudgetAllocator(nn.Module):
    """
    Maps difficulty scores ‚Üí token budgets

    Methods:
    1. Linear interpolation (simple)
    2. Exponential mapping (emphasis on hard)
    3. Learned mapping (data-driven)
    """
    def __init__(
        self,
        min_tokens: int = 150,
        max_tokens: int = 500,
        num_levels: int = 3,
        mapping: str = 'linear'
    ):
        super().__init__()
        self.min_tokens = min_tokens
        self.max_tokens = max_tokens
        self.num_levels = num_levels
        self.mapping = mapping

        if mapping == 'learned':
            # Learnable mapping function
            self.mapper = nn.Sequential(
                nn.Linear(1, 32),
                nn.ReLU(),
                nn.Linear(32, 1),
                nn.Sigmoid()
            )

    def forward(self, difficulty_scores: torch.Tensor) -> torch.Tensor:
        """
        Args:
            difficulty_scores: [B] ‚àà [0, 1] or [B, num_levels] (logits)

        Returns:
            budgets: [B] integer token counts
        """
        if difficulty_scores.dim() == 2:
            # Convert logits to continuous score
            difficulty_scores = F.softmax(difficulty_scores, dim=-1)
            # Weighted average: easy=0, medium=0.5, hard=1
            weights = torch.linspace(0, 1, self.num_levels, device=difficulty_scores.device)
            difficulty_scores = (difficulty_scores * weights).sum(dim=-1)

        # Map [0, 1] ‚Üí [min_tokens, max_tokens]
        if self.mapping == 'linear':
            budgets = self.min_tokens + difficulty_scores * (self.max_tokens - self.min_tokens)

        elif self.mapping == 'exponential':
            # Exponential: allocate more tokens to hard queries
            # Formula: min + (max - min) * d^2
            budgets = self.min_tokens + (difficulty_scores ** 2) * (self.max_tokens - self.min_tokens)

        elif self.mapping == 'learned':
            # Learned mapping
            normalized_scores = difficulty_scores.unsqueeze(-1)  # [B, 1]
            mapped_scores = self.mapper(normalized_scores).squeeze(-1)  # [B]
            budgets = self.min_tokens + mapped_scores * (self.max_tokens - self.min_tokens)

        # Quantize to integers
        budgets = budgets.round().long()

        # Clamp to valid range
        budgets = torch.clamp(budgets, self.min_tokens, self.max_tokens)

        return budgets


class AdaptivePyramidSelector(nn.Module):
    """
    Select top-K tokens from pyramid features based on budget

    Uses saliency-based selection with budget-aware threshold
    """
    def __init__(self, hidden_dim: int, max_pyramid_depth: int = 5):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.max_pyramid_depth = max_pyramid_depth

        # Saliency scorer per level
        self.saliency_scorers = nn.ModuleList([
            nn.Sequential(
                nn.LayerNorm(hidden_dim),
                nn.Linear(hidden_dim, hidden_dim // 4),
                nn.GELU(),
                nn.Linear(hidden_dim // 4, 1)
            )
            for _ in range(max_pyramid_depth)
        ])

    def forward(
        self,
        pyramid_features: List[torch.Tensor],
        budget: int
    ) -> torch.Tensor:
        """
        Select top budget tokens across all pyramid levels

        Args:
            pyramid_features: List of [1, N_k, D] for each level
            budget: Number of tokens to select

        Returns:
            selected_features: [1, budget, D]
        """
        device = pyramid_features[0].device

        # Compute saliency for all levels
        all_features = []
        all_scores = []

        for level_idx, features in enumerate(pyramid_features):
            # Score tokens at this level
            scores = self.saliency_scorers[level_idx](features).squeeze(-1)  # [1, N_k]

            all_features.append(features)
            all_scores.append(scores)

        # Concatenate across levels
        all_features = torch.cat(all_features, dim=1)  # [1, Œ£ N_k, D]
        all_scores = torch.cat(all_scores, dim=1)  # [1, Œ£ N_k]

        # Select top-K
        top_k = min(budget, all_scores.shape[1])
        top_scores, top_indices = torch.topk(all_scores, k=top_k, dim=1)

        # Gather selected features
        selected_features = torch.gather(
            all_features,
            dim=1,
            index=top_indices.unsqueeze(-1).expand(-1, -1, self.hidden_dim)
        )  # [1, top_k, D]

        return selected_features


# ============================================================================
# USAGE EXAMPLE: Full Pipeline with Benchmarking
# ============================================================================

class SimpleViTEncoder(nn.Module):
    """Simplified ViT for demonstration"""
    def __init__(self, hidden_dim: int = 768):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.patch_embed = nn.Conv2d(3, hidden_dim, kernel_size=16, stride=16)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=12, batch_first=True),
            num_layers=6
        )

    def forward(self, images: torch.Tensor) -> torch.Tensor:
        """[B, 3, H, W] ‚Üí [B, N, D]"""
        patches = self.patch_embed(images)  # [B, D, H/16, W/16]
        B, D, H, W = patches.shape
        patches = patches.flatten(2).transpose(1, 2)  # [B, N, D]
        features = self.transformer(patches)
        return features


class SimpleLLM(nn.Module):
    """Simplified LLM for demonstration"""
    def __init__(self, hidden_dim: int = 768):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.embedding = nn.Embedding(50000, hidden_dim)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=12, batch_first=True),
            num_layers=12
        )
        self.lm_head = nn.Linear(hidden_dim, 50000)

    def forward(
        self,
        visual_features: torch.Tensor,
        visual_attention_mask: torch.Tensor,
        input_ids: torch.Tensor
    ) -> torch.Tensor:
        """Simple autoregressive generation"""
        text_embeds = self.embedding(input_ids)  # [B, L, D]

        # Concatenate visual + text
        combined = torch.cat([visual_features, text_embeds], dim=1)

        # Create attention mask
        B, V = visual_features.shape[:2]
        L = text_embeds.shape[1]
        full_mask = torch.cat([
            visual_attention_mask,
            torch.ones(B, L, device=visual_features.device)
        ], dim=1)

        # Transform
        outputs = self.transformer(combined, src_key_padding_mask=(1 - full_mask).bool())

        # LM head (only on text part)
        logits = self.lm_head(outputs[:, V:, :])

        return logits


def benchmark_difficulty_aware_vlm():
    """
    Benchmark difficulty-aware VLM vs fixed-budget VLM

    Demonstrates:
    - Speedup on easy queries
    - Maintained accuracy on hard queries
    - Overall efficiency gains
    """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # Initialize components
    vision_encoder = SimpleViTEncoder(hidden_dim=768).to(device)
    llm = SimpleLLM(hidden_dim=768).to(device)

    # Difficulty classifier (from earlier section)
    from transformers import AutoModel
    difficulty_classifier = FastDifficultyClassifier(
        num_classes=3,
        embed_dim=768
    ).to(device)

    # Difficulty-aware VLM
    model_adaptive = DifficultyAwareVLM(
        vision_encoder=vision_encoder,
        llm=llm,
        difficulty_classifier=difficulty_classifier,
        hidden_dim=768,
        budget_range=(150, 500),
        num_difficulty_levels=3
    ).to(device).eval()

    # Fixed-budget baseline (always 576 tokens)
    model_fixed = DifficultyAwareVLM(
        vision_encoder=vision_encoder,
        llm=llm,
        difficulty_classifier=difficulty_classifier,
        hidden_dim=768,
        budget_range=(576, 576),  # Fixed!
        num_difficulty_levels=3
    ).to(device).eval()

    # Create test data with varying difficulty
    test_cases = [
        {
            'name': 'Easy: Simple image, short query',
            'image': torch.randn(1, 3, 224, 224, device=device),  # Low res
            'query': torch.randint(0, 50000, (1, 10), device=device),  # Short
            'expected_difficulty': 'easy'
        },
        {
            'name': 'Medium: Moderate complexity',
            'image': torch.randn(1, 3, 512, 512, device=device),
            'query': torch.randint(0, 50000, (1, 25), device=device),
            'expected_difficulty': 'medium'
        },
        {
            'name': 'Hard: High-res image, detailed query',
            'image': torch.randn(1, 3, 1024, 1024, device=device),
            'query': torch.randint(0, 50000, (1, 50), device=device),
            'expected_difficulty': 'hard'
        }
    ]

    print("="*80)
    print("Difficulty-Aware VLM Benchmark")
    print("="*80)

    total_speedup = 0

    with torch.no_grad():
        for test_case in test_cases:
            print(f"\n{'='*80}")
            print(f"Test Case: {test_case['name']}")
            print(f"Expected Difficulty: {test_case['expected_difficulty']}")
            print(f"{'='*80}")

            # Adaptive model
            start = time.time()
            outputs_adaptive, diagnostic_adaptive = model_adaptive(
                test_case['image'],
                test_case['query'],
                return_diagnostic=True
            )
            time_adaptive = time.time() - start

            # Fixed model
            start = time.time()
            outputs_fixed, diagnostic_fixed = model_fixed(
                test_case['image'],
                test_case['query'],
                return_diagnostic=True
            )
            time_fixed = time.time() - start

            # Compare
            speedup = time_fixed / time_adaptive
            total_speedup += speedup

            print(f"\nüîç Adaptive Model:")
            print(f"   Difficulty score: {diagnostic_adaptive['difficulty_scores'][0]:.3f}")
            print(f"   Token budget: {diagnostic_adaptive['token_budgets'][0]}")
            print(f"   Pyramid depth: {diagnostic_adaptive['pyramid_depths'][0]}")
            print(f"   Total time: {time_adaptive*1000:.1f}ms")
            print(f"      - Difficulty estimation: {diagnostic_adaptive['timings']['difficulty_estimation']*1000:.1f}ms")
            print(f"      - Vision encoding: {diagnostic_adaptive['timings']['vision_encoding']*1000:.1f}ms")
            print(f"      - LLM generation: {diagnostic_adaptive['timings']['llm_generation']*1000:.1f}ms")

            print(f"\n‚öñÔ∏è  Fixed Model (576 tokens):")
            print(f"   Token budget: {diagnostic_fixed['token_budgets'][0]} (always)")
            print(f"   Total time: {time_fixed*1000:.1f}ms")

            print(f"\n‚ö° Performance:")
            print(f"   Speedup: {speedup:.2f}√ó")
            print(f"   Token reduction: {(1 - diagnostic_adaptive['token_budgets'][0] / 576) * 100:.1f}%")

    avg_speedup = total_speedup / len(test_cases)
    print(f"\n{'='*80}")
    print(f"Overall Results")
    print(f"{'='*80}")
    print(f"Average speedup: {avg_speedup:.2f}√ó")
    print(f"Classifier overhead: <5ms (negligible)")
    print(f"Memory savings: Proportional to token reduction")


if __name__ == "__main__":
    benchmark_difficulty_aware_vlm()
```

### Integration with Curriculum Learning

**Training Strategy**: Start with easy examples, gradually increase difficulty.

```python
import torch
from torch.utils.data import DataLoader, Sampler
import numpy as np

class CurriculumSampler(Sampler):
    """
    Curriculum learning sampler for difficulty-aware training

    Strategy:
    - Epoch 1-5: 80% easy, 15% medium, 5% hard
    - Epoch 6-10: 50% easy, 35% medium, 15% hard
    - Epoch 11+: 33% easy, 34% medium, 33% hard (uniform)
    """
    def __init__(
        self,
        data_source,
        difficulty_labels: np.ndarray,
        current_epoch: int = 1,
        num_epochs_per_stage: int = 5
    ):
        """
        Args:
            data_source: Dataset
            difficulty_labels: [N] array of difficulty labels (0=easy, 1=medium, 2=hard)
            current_epoch: Current training epoch
            num_epochs_per_stage: Epochs per curriculum stage
        """
        self.data_source = data_source
        self.difficulty_labels = difficulty_labels
        self.current_epoch = current_epoch
        self.num_epochs_per_stage = num_epochs_per_stage

        # Partition indices by difficulty
        self.easy_indices = np.where(difficulty_labels == 0)[0]
        self.medium_indices = np.where(difficulty_labels == 1)[0]
        self.hard_indices = np.where(difficulty_labels == 2)[0]

    def _get_stage_distribution(self, epoch: int) -> tuple:
        """
        Get sampling distribution for current epoch

        Returns:
            (p_easy, p_medium, p_hard): Sampling probabilities
        """
        stage = (epoch - 1) // self.num_epochs_per_stage

        if stage == 0:
            # Stage 1: Focus on easy
            return (0.80, 0.15, 0.05)
        elif stage == 1:
            # Stage 2: Balanced towards medium
            return (0.50, 0.35, 0.15)
        else:
            # Stage 3+: Uniform
            return (0.33, 0.34, 0.33)

    def __iter__(self):
        p_easy, p_medium, p_hard = self._get_stage_distribution(self.current_epoch)

        # Number of samples per difficulty
        n = len(self.data_source)
        n_easy = int(n * p_easy)
        n_medium = int(n * p_medium)
        n_hard = n - n_easy - n_medium

        # Sample with replacement from each difficulty
        sampled_easy = np.random.choice(self.easy_indices, size=n_easy, replace=True)
        sampled_medium = np.random.choice(self.medium_indices, size=n_medium, replace=True)
        sampled_hard = np.random.choice(self.hard_indices, size=n_hard, replace=True)

        # Concatenate and shuffle
        all_samples = np.concatenate([sampled_easy, sampled_medium, sampled_hard])
        np.random.shuffle(all_samples)

        return iter(all_samples.tolist())

    def __len__(self):
        return len(self.data_source)


def train_with_curriculum(
    model: DifficultyAwareVLM,
    train_dataset,
    difficulty_labels: np.ndarray,
    num_epochs: int = 15,
    batch_size: int = 32
):
    """
    Train difficulty-aware VLM with curriculum learning

    Args:
        model: DifficultyAwareVLM model
        train_dataset: Training dataset
        difficulty_labels: Difficulty label for each sample
        num_epochs: Total training epochs
        batch_size: Batch size
    """
    device = next(model.parameters()).device
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

    for epoch in range(1, num_epochs + 1):
        # Create curriculum sampler for this epoch
        sampler = CurriculumSampler(
            train_dataset,
            difficulty_labels,
            current_epoch=epoch,
            num_epochs_per_stage=5
        )

        # Create dataloader
        dataloader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            sampler=sampler,
            num_workers=4
        )

        # Get current stage distribution
        p_easy, p_medium, p_hard = sampler._get_stage_distribution(epoch)
        print(f"\n{'='*60}")
        print(f"Epoch {epoch}/{num_epochs}")
        print(f"Curriculum: Easy={p_easy:.1%}, Medium={p_medium:.1%}, Hard={p_hard:.1%}")
        print(f"{'='*60}")

        model.train()
        total_loss = 0

        for batch_idx, batch in enumerate(dataloader):
            images = batch['image'].to(device)
            queries = batch['query'].to(device)
            targets = batch['target'].to(device)

            # Forward pass
            outputs, diagnostic = model(images, queries, return_diagnostic=True)

            # Compute loss (cross-entropy on text generation)
            loss = F.cross_entropy(
                outputs.reshape(-1, outputs.shape[-1]),
                targets.reshape(-1)
            )

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            if (batch_idx + 1) % 10 == 0:
                avg_budget = diagnostic['average_budget']
                print(f"   Batch {batch_idx+1}: Loss={loss.item():.4f}, Avg Budget={avg_budget:.0f}")

        avg_loss = total_loss / len(dataloader)
        print(f"\nEpoch {epoch} complete: Avg Loss={avg_loss:.4f}")


# Example: Create synthetic difficulty labels for dataset
def assign_difficulty_labels(dataset) -> np.ndarray:
    """
    Assign difficulty labels to dataset

    Heuristics:
    - Easy: Image resolution < 512, query length < 15
    - Hard: Image resolution > 768, query length > 30
    - Medium: Otherwise
    """
    labels = []

    for sample in dataset:
        img_res = max(sample['image'].shape[-2:])
        query_len = len(sample['query'])

        if img_res < 512 and query_len < 15:
            labels.append(0)  # Easy
        elif img_res > 768 or query_len > 30:
            labels.append(2)  # Hard
        else:
            labels.append(1)  # Medium

    return np.array(labels)
```

### Real-World Production Deployment Example

**FastVLM-style deployment on iOS/macOS (Apple Neural Engine)**

```python
import coremltools as ct
import torch
import torch.nn as nn

class OptimizedDifficultyClassifierForANE(nn.Module):
    """
    Difficulty classifier optimized for Apple Neural Engine

    Constraints:
    - No dynamic shapes
    - Limited ops (conv, fc, relu, sigmoid)
    - Fixed input sizes
    """
    def __init__(self):
        super().__init__()

        # Visual branch: Optimized for ANE
        self.visual_conv = nn.Sequential(
            # Input: [1, 3, 128, 128]
            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),  # ‚Üí [1, 32, 64, 64]
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # ‚Üí [1, 64, 32, 32]
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # ‚Üí [1, 128, 16, 16]
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),  # ‚Üí [1, 128, 1, 1]
        )

        # Text branch: Fixed length (32 tokens max)
        self.text_embed = nn.Embedding(50000, 128)
        self.text_fc = nn.Sequential(
            nn.Linear(128 * 32, 256),  # Flatten all tokens
            nn.ReLU(),
            nn.Linear(256, 128),
        )

        # Fusion
        self.classifier = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 3),  # 3 difficulty classes
        )

    def forward(self, image: torch.Tensor, query: torch.Tensor) -> torch.Tensor:
        """
        Args:
            image: [1, 3, 128, 128] fixed size
            query: [1, 32] fixed length (padded)

        Returns:
            logits: [1, 3]
        """
        # Visual features
        v = self.visual_conv(image).flatten(1)  # [1, 128]

        # Text features
        t = self.text_embed(query)  # [1, 32, 128]
        t = t.flatten(1)  # [1, 32*128]
        t = self.text_fc(t)  # [1, 128]

        # Fuse
        combined = torch.cat([v, t], dim=1)  # [1, 256]

        # Classify
        logits = self.classifier(combined)  # [1, 3]

        return logits


def export_to_coreml_for_deployment():
    """
    Export difficulty classifier to CoreML for on-device inference

    Target: iPhone 15 Pro / Apple Neural Engine
    """
    # Load trained PyTorch model
    model = OptimizedDifficultyClassifierForANE()
    model.eval()

    # Trace with fixed inputs
    example_image = torch.randn(1, 3, 128, 128)
    example_query = torch.randint(0, 50000, (1, 32))

    traced_model = torch.jit.trace(model, (example_image, example_query))

    # Convert to CoreML
    coreml_model = ct.convert(
        traced_model,
        inputs=[
            ct.ImageType(name="image", shape=(1, 3, 128, 128)),
            ct.TensorType(name="query", shape=(1, 32))
        ],
        outputs=[
            ct.TensorType(name="difficulty_logits")
        ],
        compute_units=ct.ComputeUnit.ALL  # Use ANE when possible
    )

    # Save
    coreml_model.save("DifficultyClassifier.mlpackage")

    print("‚úÖ CoreML model exported successfully!")
    print(f"   Model size: {coreml_model.get_spec().ByteSize() / 1024:.1f} KB")
    print(f"   Optimized for: Apple Neural Engine")
    print(f"   Expected latency: <5ms on iPhone 15 Pro")


if __name__ == "__main__":
    export_to_coreml_for_deployment()
```

---

## References

1. **FastVLM** (Apple Research, July 2025): "FastVLM: Efficient Vision Encoding for Vision Language Models"
2. **DPN-LLaVA** (March 2025): Ai et al., "Dynamic Pyramid Network for Efficient Multimodal Large Language Model"
3. **Curriculum Learning**: Bengio et al. (2009), "Curriculum Learning"
4. **REINFORCE**: Williams (1992), "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"

---

**Last Updated**: 2025-01-30
**Source**: LOD-BTree-Oracle Technical Documentation
**Cross-Reference**: See [algorithms/06-pyramid-token-pruning-methods.md](algorithms/06-pyramid-token-pruning-methods.md) for pyramid construction details
