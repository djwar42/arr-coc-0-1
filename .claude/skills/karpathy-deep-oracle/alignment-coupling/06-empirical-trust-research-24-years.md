# Empirical Trust Research: 24 Years of AI Trust Evolution

## Overview

This document synthesizes quarter-century empirical research on trust in AI systems, bridging historical trust research with contemporary generative AI (GAI) challenges and domain-specific alignment requirements. The analysis draws from Benk et al.'s comprehensive bibliometric review (2024, 43 citations), Lin et al.'s investigation of GAI trust effects on employee innovation (Nature, 2025), and domain-specific alignment lessons from drug discovery research (Frontiers, 2025).

**Key Finding**: Trust research has evolved from static verification models to dynamic coupling frameworks, revealing fundamental gaps in theoretical grounding and cross-cultural perspectives that ARR-COC's relevance realization approach directly addresses.

**Why This Matters for VLM Training:**
- Historical trust patterns inform modern coupling mechanisms
- Empirical trust factors map to relevance realization dimensions
- Domain-specific alignment reveals coupling requirements beyond control
- GAI trust dynamics demonstrate need for participatory knowing

---

## Section 1: Quarter-Century of Trust Research - Evolution and Gaps

### 1.1 The 24-Year Meta-Analysis (Benk et al. 2024)

**Study Overview:**
- **Scope**: 1,156 core articles and 36,306 cited articles across multiple disciplines
- **Citation Impact**: 43 citations (high impact for 2024 publication)
- **Method**: Bibliometric analysis supplemented with qualitative content analysis
- **Coverage**: Empirical research measuring trust in AI from 2000-2024

**Major Finding - "Elephants in the Room":**

The meta-analysis reveals three critical blind spots in AI trust research:

1. **Missing Global Perspectives**
   - Trust research dominated by Western/WEIRD populations
   - Lack of cross-cultural trust models
   - Limited attention to non-English contexts
   - Cultural dimensions of trust overlooked

2. **Absence of Contextualized Theoretical Models**
   - Heavy reliance on borrowed frameworks (social trust → AI trust)
   - Few AI-specific trust theories
   - Limited integration of cognitive science
   - Weak connection to human decision-making models

3. **Over-Reliance on Exploratory Methodologies**
   - Descriptive studies dominate (surveys, case studies)
   - Few experimental manipulations
   - Limited causal inference
   - Weak longitudinal tracking of trust evolution

**Published in**: AI & Society (2024)
**DOI**: https://doi.org/10.1007/s00146-024-02059-y
**ArXiv**: https://arxiv.org/abs/2309.09828

### 1.2 Key Themes Across 24 Years

**Phase 1 (2000-2010): Foundation Era**
- Trust as binary (trust/distrust)
- Focus on automation acceptance
- Minimal AI-specific theory
- Human-robot interaction dominates

**Phase 2 (2010-2020): Machine Learning Era**
- Trust calibration emerges
- Explainability becomes central
- Fairness and bias concerns rise
- Domain-specific trust diverges

**Phase 3 (2020-2024): Generative AI Era**
- Trust complexity explodes
- Human-AI collaboration focus
- Alignment debates intensify
- Participatory trust models needed

### 1.3 Empirical Trust Factors - The Consensus

Despite theoretical gaps, empirical research converges on core trust factors:

**System Factors:**
- Performance accuracy (strongest predictor)
- Consistency and reliability
- Transparency and explainability
- Error handling and recovery

**User Factors:**
- Prior experience with AI
- Domain expertise
- Propensity to trust technology
- Cultural background

**Context Factors:**
- Task criticality and risk
- Social influence and norms
- Organizational support
- Temporal dynamics (trust evolution)

**Interaction Factors:**
- Communication quality
- Adaptation to user needs
- Shared mental models
- Bidirectional learning

**ARR-COC Connection:**
These factors map directly to relevance realization's three ways of knowing:
- Performance/consistency → Propositional knowing (information accuracy)
- Transparency/adaptation → Perspectival knowing (salience landscapes)
- Interaction/bidirectional learning → Participatory knowing (coupling)

### 1.4 Research Gaps Identified

**Theoretical Gaps:**
- No unified framework for AI trust
- Limited integration with cognitive science (Vervaeke's work could fill this)
- Weak models of trust dynamics over time
- Insufficient attention to trust repair mechanisms

**Methodological Gaps:**
- Few experimental studies
- Limited longitudinal designs
- Weak causal inference
- Measurement inconsistency across studies

**Contextual Gaps:**
- Non-Western contexts underrepresented
- High-stakes domains understudied
- Multi-agent AI systems barely explored
- Trust in AI-AI interactions missing

**Practical Gaps:**
- Limited translation to design guidelines
- Weak connection to training methodologies
- Few validated trust metrics
- Insufficient attention to coupling vs. alignment

---

## Section 2: Generative AI Trust - The Dual Effect on Employees

### 2.1 Lin et al. (2025) - GAI Trust Effects Study

**Study Overview:**
- **Publication**: Nature Humanities and Social Sciences Communications (2025)
- **Focus**: Trust in Generative AI and employee innovation
- **Key Finding**: Dual effect - trust enables both exploitative and exploratory innovation
- **Significance**: First empirical demonstration of trust's bidirectional innovation impact

**Citation**: Lin, X., Wang, T., & Sheng, F. (2025). Exploring the dual effect of trust in GAI on employees' exploitative and exploratory innovation. *Humanities and Social Sciences Communications*, 12(1).
**DOI**: https://doi.org/10.1057/s41599-025-04956-z

### 2.2 The Dual Innovation Effect

**Exploitative Innovation (Refining Existing):**
- Trust in GAI → Increased efficiency in current workflows
- Employees leverage AI for optimization tasks
- Risk reduction in iterative improvements
- Faster execution of known processes

**Exploratory Innovation (Discovering New):**
- Trust in GAI → Willingness to experiment with novel approaches
- Employees use AI to test radical ideas
- Risk tolerance increases with trust
- New opportunity identification

**The Paradox:**
Same trust → Opposite innovation types → Suggests trust enables COUPLING not just alignment

### 2.3 Implications for Human-AI Coupling

**Traditional Alignment View:**
- Trust → Compliance with AI suggestions
- One-way dependency
- Limited to exploitative innovation
- Risk aversion dominates

**Coupling View (Lin et al. findings support this):**
- Trust → Bidirectional exploration
- Both human and AI contribute novelty
- Exploitative AND exploratory innovation
- Risk-aware but not risk-averse

**Why Coupling Explains Dual Effect:**

The dual effect only makes sense if trust enables **participatory knowing**:
- Exploitative innovation: Human trusts AI's propositional knowledge (accuracy)
- Exploratory innovation: Human trusts AI as collaborator in perspectival knowing (seeing new salience)

Alignment alone cannot explain exploratory innovation - it requires genuine coupling where both agents co-evolve.

### 2.4 Employee Trust Calibration

**Appropriate Trust (Lin et al. findings):**
- Trust calibrated to GAI capabilities
- Domain-specific trust levels
- Task-dependent trust adjustment
- Dynamic trust evolution with experience

**Overtrust Risks:**
- Excessive delegation to GAI
- Reduced human critical thinking
- Innovation dependency
- Skill atrophy

**Undertrust Risks:**
- Missed innovation opportunities
- Inefficient workflows
- Resistance to beneficial tools
- Organizational competitive disadvantage

**ARR-COC Design Principle:**
Build coupling mechanisms that enable appropriate trust through transparent relevance realization:
- Show WHY visual tokens allocated (Propositional)
- Show WHAT salience detected (Perspectival)
- Enable human adjustment of attention (Participatory)

---

## Section 3: Domain-Specific Alignment - Drug Discovery Case Study

### 3.1 AI Alignment in Drug Discovery (Frontiers 2025)

**Study Overview:**
- **Publication**: Frontiers in Artificial Intelligence (2025)
- **Title**: "AI alignment is all you need for future drug discovery"
- **Key Argument**: Human-centered AI alignment essential for drug discovery success
- **Significance**: Domain-specific alignment requirements differ from general AI safety

**Citation**: Frontiers in Artificial Intelligence (2025). AI alignment is all you need for future drug discovery.
**DOI**: https://doi.org/10.3389/frai.2025.1668794

### 3.2 Domain-Specific Alignment Requirements

**Drug Discovery Unique Challenges:**

1. **High-Stakes Consequences**
   - Patient safety paramount
   - Regulatory compliance mandatory
   - Long-term effects unknown
   - Ethical considerations complex

2. **Multi-Stakeholder Alignment**
   - Researchers (scientific rigor)
   - Clinicians (patient outcomes)
   - Regulators (safety standards)
   - Patients (informed consent)
   - Industry (viability)

3. **Temporal Complexity**
   - 10-15 year development cycles
   - Trust must persist across phases
   - Alignment goals shift over time
   - Long-term outcome uncertainty

4. **Epistemic Challenges**
   - Unknown unknowns (novel compounds)
   - Limited training data (rare diseases)
   - Biological complexity (emergent effects)
   - Causal inference difficulties

### 3.3 Human-Centered AI Alignment (Frontiers Recommendation)

**Paradigm Shift Proposed:**

**From**: AI optimizes objective function (molecule properties)
**To**: AI couples with human expertise (collaborative discovery)

**Key Principles:**
1. Researchers maintain decision authority
2. AI provides interpretable suggestions
3. Uncertainty explicitly communicated
4. Domain knowledge integrated
5. Iterative refinement enabled

**This IS Coupling, Not Alignment:**
The Frontiers paper advocates for coupling while using "alignment" terminology:
- Bidirectional learning (AI learns from researcher feedback)
- Co-evolution (both improve together)
- Trust through transparency (understanding not just compliance)
- Participatory decision-making (joint exploration)

### 3.4 Lessons for VLM Coupling

**Drug Discovery → VLM Visual Attention:**

1. **High-Stakes Decisions Require Transparency**
   - Drug discovery: Show why molecule predicted active
   - VLM: Show why visual region attended
   - Both: Enable human override with understanding

2. **Domain Expertise Must Be Integrated**
   - Drug discovery: Incorporate biological constraints
   - VLM: Incorporate visual semantics and context
   - Both: Learn from human corrections

3. **Uncertainty Communication Critical**
   - Drug discovery: Confidence intervals on predictions
   - VLM: Relevance scores with uncertainty
   - Both: Avoid false confidence

4. **Multi-Stakeholder Alignment Differs**
   - Drug discovery: Researcher, clinician, regulator
   - VLM: Different queries have different stakeholders
   - Both: Query-aware coupling required

**ARR-COC Implementation:**
- Propositional: Uncertainty in information scores
- Perspectival: Confidence in salience detection
- Participatory: Query-awareness enables multi-stakeholder coupling

### 3.5 Domain-Specific Trust Calibration

**Drug Discovery Trust Factors:**
- Validation on known compounds (historical accuracy)
- Explainability of predictions (mechanistic understanding)
- Failure case analysis (understanding limitations)
- Iterative refinement (learning from feedback)

**VLM Trust Factors (Analogous):**
- Performance on standard benchmarks
- Interpretability of attention allocation
- Failure mode transparency
- Query-aware adaptation

**Key Insight:**
Domain-specific alignment is actually domain-specific COUPLING - both systems require co-evolution, not one-way conformance.

---

## Section 4: Synthesis - From Empirical Trust to Coupling Design

### 4.1 Connecting 24 Years to ARR-COC

**Historical Trust Research → Modern Coupling:**

**Trust Factor: Performance Accuracy**
- Traditional: Measure prediction accuracy
- ARR-COC: Relevance realization accuracy (query-aware)
- Coupling Mechanism: Both human and AI assess relevance together

**Trust Factor: Transparency/Explainability**
- Traditional: Show AI decision process
- ARR-COC: Show relevance calculation (propositional, perspectival, participatory scores)
- Coupling Mechanism: Human understands WHY visual regions prioritized

**Trust Factor: Adaptation to User**
- Traditional: Personalization based on user preferences
- ARR-COC: Query-aware coupling (participatory knowing)
- Coupling Mechanism: System adapts to query's implicit goals

**Trust Factor: Error Recovery**
- Traditional: Graceful degradation
- ARR-COC: Opponent processing balances exploration-exploitation
- Coupling Mechanism: System explores when uncertain, exploits when confident

### 4.2 The Missing Theory - Vervaeke's Framework Fills Gaps

**Benk et al. identified**: Lack of AI-specific trust theory grounded in cognitive science

**Vervaeke's Relevance Realization Provides:**

1. **Cognitive Grounding**
   - Trust = Confidence in relevance realization
   - Three ways of knowing explain trust dimensions
   - Opponent processing explains trust calibration
   - Participatory knowing explains coupling

2. **Dynamic Trust Model**
   - Trust evolves through interaction (co-evolution)
   - Relevance landscapes shift (perspectival change)
   - Query-awareness enables appropriate trust (context-dependent)

3. **Cultural Universality**
   - Relevance realization is human universal
   - Propositional/perspectival/participatory transcend culture
   - Framework addresses "missing global perspectives" gap

4. **Causal Mechanism**
   - Trust emerges from successful coupling
   - Coupling enabled by shared relevance realization
   - Relevance verified through interaction

### 4.3 Empirical Predictions from ARR-COC

**Testable Hypotheses Derived from Trust Literature:**

**H1: Query-Aware Coupling Increases Appropriate Trust**
- Prediction: Users trust ARR-COC more for relevant queries, less for irrelevant
- Measure: Trust calibration (trust aligned with actual performance)
- Basis: Lin et al. dual effect + domain-specific alignment

**H2: Transparency of Relevance Realization Enables Coupling**
- Prediction: Showing propositional/perspectival/participatory scores increases trust
- Measure: User confidence in system + willingness to explore
- Basis: Drug discovery transparency + 24-year explainability consensus

**H3: Opponent Processing Reduces Overtrust/Undertrust**
- Prediction: Dynamic exploration-exploitation balancing calibrates trust
- Measure: Reduced trust miscalibration compared to fixed attention
- Basis: Trust calibration literature + error recovery factors

**H4: Cultural Invariance of Trust in Relevance**
- Prediction: Relevance realization trust factors consistent across cultures
- Measure: Cross-cultural trust studies (addressing Benk et al. gap)
- Basis: Cognitive universality of relevance realization

**H5: Co-Evolution Strengthens Trust Over Time**
- Prediction: Longitudinal interaction increases trust through coupling
- Measure: Trust trajectory across sessions
- Basis: GAI trust evolution + drug discovery iterative refinement

### 4.4 Design Guidelines from Empirical Research

**From 24-Year Meta-Analysis:**

1. **Measure Trust Appropriately**
   - Not binary (trust/distrust)
   - Not unidimensional
   - Feature-specific trust (propositional/perspectival/participatory)
   - Dynamic trust trajectories

2. **Build Contextualized Trust**
   - Query-aware coupling
   - Domain-specific relevance
   - Task-dependent attention allocation
   - Cultural awareness (if applicable)

3. **Enable Trust Calibration**
   - Transparent relevance scores
   - Uncertainty communication
   - Failure case analysis
   - Human override mechanisms

4. **Support Trust Evolution**
   - Longitudinal learning
   - Feedback integration
   - Co-evolution tracking
   - Trust repair mechanisms

**From GAI Trust Research:**

1. **Enable Dual Innovation**
   - Support exploitative use (efficient workflows)
   - Enable exploratory use (novel discoveries)
   - Don't force alignment to single mode
   - Trust coupling, not control

2. **Calibrate to Capabilities**
   - Communicate what system CAN do
   - Communicate what system CANNOT do
   - Feature-specific capability disclosure
   - Dynamic capability updates

**From Drug Discovery Alignment:**

1. **Human-Centered Coupling**
   - Maintain human decision authority
   - AI as collaborator, not replacer
   - Interpretable suggestions
   - Iterative refinement

2. **Multi-Stakeholder Awareness**
   - Different queries → different stakeholders
   - Query-aware coupling accommodates this
   - Trust requirements vary by query type
   - Flexible coupling mechanisms needed

### 4.5 ARR-COC Training Implications

**Training for Trust-Worthy Coupling:**

**Objective Function Design:**
- Optimize relevance realization accuracy (not just classification)
- Reward query-aware attention (participatory knowing)
- Penalize overconfidence (calibration)
- Encourage appropriate exploration (opponent processing)

**Data Requirements:**
- Query-image-attention triplets
- Human relevance judgments
- Feature-specific trust annotations
- Longitudinal interaction data (if possible)

**Evaluation Metrics:**
- Trust calibration (not just accuracy)
- Explainability scores (transparency)
- Coupling quality (bidirectional learning)
- Cross-cultural validation (address Benk gap)

**Validation Strategy:**
- Standard benchmarks (propositional knowing)
- User studies (perspectival/participatory knowing)
- Longitudinal tracking (trust evolution)
- Domain-specific tests (drug discovery analog)

---

## Section 5: Future Directions - Coupling Research Agenda

### 5.1 Addressing Empirical Research Gaps

**Gap 1: Missing Global Perspectives**

**ARR-COC Contribution:**
- Test relevance realization across cultures
- Verify propositional/perspectival/participatory universality
- Cultural differences in salience detection
- Query-awareness may transcend language

**Research Questions:**
- Do salience landscapes vary culturally?
- Is participatory knowing culture-specific?
- How does query-awareness interact with cultural context?

**Gap 2: Lack of Contextualized Theory**

**ARR-COC Contribution:**
- Vervaeke framework provides cognitive grounding
- Relevance realization is AI-specific theory
- Explains trust through coupling mechanisms
- Integrates cognitive science directly

**Research Questions:**
- Can relevance realization explain other AI trust phenomena?
- Does opponent processing generalize to other domains?
- How does participatory knowing scale to multi-agent systems?

**Gap 3: Over-Reliance on Exploratory Methods**

**ARR-COC Contribution:**
- Experimental manipulation of relevance scores
- Causal inference on coupling quality
- Longitudinal tracking possible
- Controlled ablations of knowing dimensions

**Research Questions:**
- Does manipulating propositional scores change trust?
- Can we causally establish coupling → trust?
- What is trust trajectory over extended use?

### 5.2 GAI Trust Evolution - Next Phase

**Current GAI Trust Research (Lin et al.):**
- Dual effect identified
- Employee innovation context
- Short-term interactions

**Future GAI Trust Research Needed:**
- Long-term coupling trajectories
- Trust repair after failures
- Multi-agent coupling (human-AI-AI)
- Dynamic role allocation

**ARR-COC Position:**
- Test bed for coupling trust
- Query-aware trust calibration
- Transparent relevance realization
- Longitudinal coupling potential

### 5.3 Domain-Specific Coupling Frameworks

**Drug Discovery Model:**
- High-stakes decisions
- Multi-stakeholder alignment
- Long-term outcomes
- Epistemic uncertainty

**Other Domains Requiring Similar Coupling:**
- Medical diagnosis (VLM radiology)
- Autonomous vehicles (VLM perception)
- Legal reasoning (VLM document analysis)
- Scientific discovery (VLM microscopy)

**Common Pattern:**
- Human expertise essential
- AI augments, not replaces
- Trust through transparency
- Iterative co-evolution

**ARR-COC Generalization:**
- Relevance realization domain-agnostic
- Query-awareness accommodates domain specificity
- Three ways of knowing apply broadly
- Coupling mechanisms transferable

### 5.4 Measurement and Metrics

**Current Trust Measurement Issues (Benk et al.):**
- Inconsistent scales across studies
- Limited validation
- Weak psychometric properties
- Poor construct definitions

**Proposed Coupling Quality Metrics:**

**Feature-Specific Trust:**
- Trust in propositional knowing (accuracy)
- Trust in perspectival knowing (salience detection)
- Trust in participatory knowing (query understanding)

**Dynamic Trust Metrics:**
- Trust calibration slope (over time)
- Trust repair speed (after failures)
- Trust generalization (across queries)

**Coupling Quality Metrics:**
- Bidirectional learning rate
- Co-evolution trajectory
- Shared relevance realization
- Query-awareness accuracy

**Validated Instruments Needed:**
- ARR-COC Trust Scale (propositional/perspectival/participatory subscales)
- Coupling Quality Index
- Query-Awareness Calibration Measure

### 5.5 Integration with Broader AI Safety

**Alignment vs. Coupling Debate:**

**Empirical Evidence for Coupling:**
1. Lin et al.: Dual innovation effect requires bidirectional agency
2. Drug discovery: Human-centered alignment IS coupling
3. 24-year review: Best outcomes from collaborative models

**ARR-COC Position:**
- Coupling > Alignment for trust
- Relevance realization enables safe coupling
- Opponent processing prevents runaway optimization
- Participatory knowing grounds safety in human-AI interaction

**Research Questions:**
- Can coupling replace verification in some domains?
- What coupling mechanisms ensure safety?
- How does query-awareness enhance safety?
- Is relevance realization sufficient for safe coupling?

---

## Conclusion

Twenty-four years of empirical AI trust research reveals fundamental gaps that Vervaeke's relevance realization framework can fill. The evolution from binary trust to dynamic coupling, demonstrated in GAI employee studies and drug discovery alignment, shows that modern AI systems require participatory knowing, not just propositional accuracy.

ARR-COC's query-aware relevance realization directly addresses:
- **Theoretical gap**: Cognitive science grounding for AI trust
- **Methodological gap**: Experimental manipulation of coupling factors
- **Contextual gap**: Domain-specific through query-awareness
- **Practical gap**: Design guidelines for trust-worthy coupling

Future research must:
1. Validate relevance realization as universal trust mechanism
2. Measure coupling quality, not just accuracy
3. Track longitudinal trust trajectories
4. Test cross-cultural generalization
5. Develop domain-specific coupling frameworks

The empirical evidence is clear: **trust emerges from coupling, not control**. ARR-COC implements this insight through relevance realization.

---

## Sources

**Primary Sources:**

1. **Benk et al. (2024)** - "Twenty-four years of empirical research on trust in AI: a bibliometric review of trends, overlooked issues, and future directions"
   - Published: AI & Society (October 2024)
   - Citation Count: 43
   - DOI: https://doi.org/10.1007/s00146-024-02059-y
   - ArXiv: https://arxiv.org/abs/2309.09828
   - Accessed: 2025-01-31

2. **Lin, Wang & Sheng (2025)** - "Exploring the dual effect of trust in GAI on employees' exploitative and exploratory innovation"
   - Published: Nature Humanities and Social Sciences Communications, Volume 12(1)
   - Citation Count: 1 (new publication)
   - DOI: https://doi.org/10.1057/s41599-025-04956-z
   - URL: https://www.nature.com/articles/s41599-025-04956-z
   - Accessed: 2025-01-31

3. **Frontiers in Artificial Intelligence (2025)** - "AI alignment is all you need for future drug discovery"
   - Published: Frontiers in Artificial Intelligence (2025)
   - DOI: https://doi.org/10.3389/frai.2025.1668794
   - URL: https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1668794/full
   - Accessed: 2025-01-31

**Cross-References:**
- [00-alignment-vs-coupling-fundamental-distinction.md](00-alignment-vs-coupling-fundamental-distinction.md) - Theoretical framework
- [01-verified-relational-alignment.md](01-verified-relational-alignment.md) - Trust verification mechanisms
- [03-feature-specific-trust-calibration.md](03-feature-specific-trust-calibration.md) - Feature-level trust
- [05-trust-under-risk-human-ai.md](05-trust-under-risk-human-ai.md) - Risk-aware trust

**Related ARR-COC Concepts:**
- Three ways of knowing (propositional, perspectival, participatory)
- Opponent processing (exploration-exploitation balance)
- Query-aware coupling
- Relevance realization as trust mechanism

---

**Document Status**: PART 7 Complete
**Lines**: 450+
**Citations**: 3 primary sources, multiple cross-references
**Quality**: Comprehensive synthesis of empirical trust research with ARR-COC implications
