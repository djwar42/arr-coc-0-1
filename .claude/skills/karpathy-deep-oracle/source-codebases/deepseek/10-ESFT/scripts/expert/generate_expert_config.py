# <claudes_code_comments>
# ** Function List **
# parse_line(line) - parses tab-delimited expert IDs and weights from profiling log line
# get_summary(files) - aggregates expert activation statistics across all ranks and layers
#
# ** Technical Review **
# Expert selection script - converts profiling logs into training configuration.
# Second step in 3-stage ESFT pipeline: Profile → **Select** → Train
#
# INPUT DATA STRUCTURE:
# Reads expert_scores_dir containing profiling logs from get_expert_scores.py:
#   expert_scores_dir/
#     rank_0/
#       layer_1.txt (each line: "expert_ids\t\texpert_weights")
#       layer_2.txt
#       ...
#       layer_26.txt
#     rank_1/
#       ...
#     summary.json (generated by this script)
#
# PARSE_LINE ALGORITHM:
# Input: "8\t9\t15\t23\t47\t51\t\t0.25\t0.20\t0.18\t0.15\t0.12\t0.10"
# Split on "\t\t" → expert_ids_str="8\t9\t15\t23\t47\t51", expert_weights_str="0.25\t0.20\t0.18\t0.15\t0.12\t0.10"
# Split each on "\t", convert to int/float lists
# Output: expert_ids=[8,9,15,23,47,51], expert_weights=[0.25,0.20,0.18,0.15,0.12,0.10]
#
# GET_SUMMARY AGGREGATION:
# Two statistics tracked per expert in each layer:
#
# 1. gate_scores[layer][expert]: Sum of gating weights across all tokens
#    Formula: gate_scores[l][e] = Σ(weight when expert e selected in layer l)
#    Interpretation: Measures total "activation strength" of expert
#    Example: Expert 8 in layer 1 selected 1000 times with avg weight 0.20 → gate_score = 200
#
# 2. token_scores[layer][expert]: Fraction of tokens routed to expert
#    Formula: token_scores[l][e] = (count of tokens routed to expert e) / TOP_K
#    Why divide by TOP_K=6? Each token routes to 6 experts, so raw count would be 6x inflated
#    Example: 10000 tokens processed, expert 8 selected 2000 times → token_score = 2000/6 = 333.3
#
# Accumulation logic (lines 24-32):
# for each line in layer_X.txt:
#   expert_ids, expert_weights = parse_line(line)
#   np.add.at(gate_scores[layer_id], expert_ids, expert_weights)  # Accumulate weights
#   np.add.at(token_scores[layer_id], expert_ids, ones_like(weights)/TOP_K)  # Count tokens
#
# Normalization (line 34-35):
# total_tokens = sum(token_scores[0])  # All tokens routed through layer 0 (total tokens processed)
# gate_scores /= total_tokens  # Per-token average gating weight
# token_scores /= total_tokens  # Per-token routing fraction
#
# CONSTANTS:
# TOP_K = 6  # DeepSeek-V2 top-6 routing
# N_EXPERTS = 64  # Number of experts per layer
# N_LAYERS = 26  # Layers 1-26 are MoE (layer 0 is dense)
#
# EXPERT SELECTION ALGORITHM:
# For each layer l:
#   1. scores = summary[score_function + "_scores"][l]  # Either gate_scores or token_scores
#   2. Sort experts by score descending: [(expert_id, score), ...]
#   3. Greedily select experts until cumulative score >= top_p threshold
#   4. Store selected expert IDs in expert_cfg["experts"][layer]
#
# Example with top_p=0.2, token_scores for layer 1:
#   Expert scores (sorted): [(8, 0.08), (9, 0.06), (15, 0.04), (23, 0.03), ...]
#   Cumulative: 0.08 → 0.14 → 0.18 → 0.21 (stop, exceeded 0.20)
#   Selected experts: [8, 9, 15, 23]
#   These 4 experts handle 21% of tokens, but we train them (rest frozen)
#
# TOP_P THRESHOLD INTERPRETATION:
# top_p=0.2: Train experts handling 20% of routing decisions
# Typical ESFT: top_p=0.15 to 0.30
#   - Lower (0.15): Fewer experts trained, more memory savings, risk missing important experts
#   - Higher (0.30): More experts trained, better coverage, higher memory usage
#
# SCORE FUNCTION CHOICES:
# --score_function=token: Select by token frequency (how often expert chosen)
#   Pro: Captures which experts are "most used"
#   Con: Ignores experts with high gating weight but lower frequency
#
# --score_function=gate: Select by cumulative gating weight (activation strength)
#   Pro: Captures experts model relies on most heavily
#   Con: High-weight but rare experts might dominate
#
# Empirically, token_scores works better for most tasks (used in paper)
#
# ADDITIONAL CONFIG OPTIONS:
# --train_shared_experts: If True, shared experts (present in all layers) also trainable
#   DeepSeek-V2 has shared experts + routed experts
#   Default False: Only train selected routed experts
#
# --train_non_expert_modules: If True, train attention/embeddings/norms too
#   Default False: Only train experts (maximum memory savings)
#   Use True for tasks requiring adaptation beyond MLP (e.g., new token types)
#
# OUTPUT FORMAT (expert_cfg.json):
# {
#   "experts": {
#     "1": [8, 9, 15, 23],  # Layer 1: Train experts 8,9,15,23
#     "2": [0, 1, 2, 3, 4, 5, 6, 7],  # Layer 2: Train experts 0-7
#     ...
#     "26": [10, 11, 20, 21, 30, 31]  # Layer 26: Train experts 10,11,20,21,30,31
#   },
#   "shared_experts": false,  # Don't train shared experts
#   "non_expert_modules": false  # Don't train non-expert params
# }
#
# This config drives to_esft() in esft.py:
# - For each layer, experts in list → to_param() (trainable)
# - Experts not in list → to_buffer() (frozen)
#
# SUMMARY.JSON OUTPUT:
# Also saves summary.json with full statistics:
# {
#   "token_scores": {"1": {"0": 0.015, "1": 0.012, ..., "63": 0.008}, "2": {...}, ...},
#   "gate_scores": {"1": {"0": 0.018, "1": 0.014, ..., "63": 0.010}, "2": {...}, ...}
# }
# Useful for post-hoc analysis and visualization
#
# TYPICAL USAGE:
# python scripts/expert/generate_expert_config.py \
#   --eval_dataset=translation \
#   --expert_scores_dir=results/expert_scores/translation \
#   --output_path=results/expert_configs/translation.json \
#   --score_function=token \
#   --top_p=0.2
#
# Output: translation.json config ready for training
# Next step: python train.py --expert_config=results/expert_configs/translation.json ...
# </claudes_code_comments>

import argparse
import json
import os
from multiprocessing import Pool
import numpy as np


def parse_line(line):
    expert_ids, expert_weights = line.split("\t\t")
    expert_ids = [int(i) for i in expert_ids.split("\t")]
    expert_weights = [float(i) for i in expert_weights.split("\t")]
    return expert_ids, expert_weights


def get_summary(files):
    TOP_K=6
    N_EXPERTS=64
    N_LAYERS=26 # 27 layers totally, the first layer is not MoE

    gate_scores = np.zeros((N_LAYERS, N_EXPERTS))
    token_scores = np.zeros((N_LAYERS, N_EXPERTS))

    print("loading files")
    for rank, file in files:
        layer_id = int(file.split(".")[0].split("_")[2]) - 1

        with open(os.path.join(args.expert_scores_dir, rank, file)) as f:
            data = f.readlines()
            for line in data:
                expert_ids, expert_weights = parse_line(line)
                np.add.at(gate_scores[layer_id], expert_ids, expert_weights)
                np.add.at(token_scores[layer_id], expert_ids, np.ones_like(expert_weights) / TOP_K)
    
    total = sum(token_scores[0])
    gate_scores = gate_scores / total
    token_scores = token_scores / total

    summary = {"token_scores": token_scores, "gate_scores": gate_scores}
    summary = {k: {str(i+1): {str(j): round(v, 4) for j, v in enumerate(l)} for i, l in enumerate(v)} for k, v in summary.items()}

    return summary



if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--eval_dataset", type=str, required=True)
    parser.add_argument("--expert_scores_dir", type=str, required=True)
    parser.add_argument("--output_path", type=str, required=True)
    parser.add_argument("--score_function", type=str, required=True)
    parser.add_argument("--top_p", type=float, required=True)
    parser.add_argument("--train_shared_experts", action="store_true")
    parser.add_argument("--train_non_expert_modules", action="store_true")

    args = parser.parse_args()

    expert_cfg = { # initialize expert config
        "experts": {},
        "shared_experts": args.train_shared_experts,
        "non_expert_modules": args.train_non_expert_modules
    }

    # let's walk inside args.expert_scores_dir and get abs file names
    file_names = []
    for rank in [i for i in os.listdir(args.expert_scores_dir) if 'rank' in i]:
        for file in os.listdir(os.path.join(args.expert_scores_dir, rank)):
            file_names.append([rank, file])

    summary_file = os.path.join(args.expert_scores_dir, "summary.json")
    summary = get_summary(file_names)

    with open(summary_file, "w") as f:
        f.write(json.dumps(summary))


    scores = summary[f"{args.score_function}_scores"]
    for layer, l_score in scores.items():
        l_score = [(int(k), v) for k,v in l_score.items()]
        l_score = sorted(l_score, key=lambda x: x[1], reverse=True)
        selected_experts = []
        current_score = 0
        for expert, score in l_score:
            if current_score >= args.top_p:
                break
            selected_experts.append(expert)
            current_score += score
        expert_cfg["experts"][layer] = selected_experts

    top_p = args.top_p
    train_shared_experts = args.train_shared_experts
    train_non_expert_modules = args.train_non_expert_modules



    os.makedirs(os.path.dirname(args.output_path), exist_ok=True)
    with open(args.output_path, "w") as f:
        json.dump(expert_cfg, f)
