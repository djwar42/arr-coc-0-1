# Attention as Resource Allocation

## Overview

Attention operates as a limited cognitive resource that must be strategically allocated across competing demands. This framework conceptualizes attention as a finite capacity bottleneck that constrains information processing, requiring organisms to selectively prioritize task-relevant stimuli while filtering irrelevant information. Understanding attention through the lens of resource allocation provides critical insights into how cognitive systems—biological and artificial—manage the fundamental tradeoff between processing capacity and environmental complexity.

**Core principle**: Attention is not unlimited; it functions as a constrained resource requiring strategic allocation mechanisms to optimize performance under capacity limits.

## Section 1: Attention as Limited Resource (Capacity Constraints, Bottleneck Theories)

### The Attentional Bottleneck

The attentional bottleneck refers to a fundamental limitation in processing capacity that occurs when the brain must focus on multiple stimuli or tasks simultaneously. This bottleneck places hard constraints on:

- **Perceptual throughput**: How much information can be processed at once
- **Concurrent task performance**: Ability to maintain multiple goals
- **Working memory access**: What can be actively maintained
- **Response selection**: Speed and accuracy of behavioral outputs

From [Capacity Limits Lead to Information Bottlenecks in Ongoing Perception](https://pmc.ncbi.nlm.nih.gov/articles/PMC10012325/) (Moulton et al., 2023):
> "Capacity limits lead to information bottlenecks and processes like attention help reduce the effects that these bottlenecks have on cognitive performance."

### Unified Attentional Bottleneck

Research by Tombu and colleagues (2011) demonstrates a unified attentional bottleneck in the human brain that constrains both:
- **Perceptual processing**: What we can consciously perceive
- **Response selection**: What actions we can initiate

This bottleneck is not modality-specific but represents a central limitation affecting all cognitive domains.

### Capacity Theory Fundamentals

**Limited capacity models** propose that:

1. **Total pool of resources**: Fixed amount of cognitive "fuel" available
2. **Flexible allocation**: Resources can be distributed variably across tasks
3. **Performance degradation**: Adding tasks reduces resources per task
4. **Resource depletion**: Sustained attention depletes available capacity

From [The Sustained Attention Paradox](https://pmc.ncbi.nlm.nih.gov/articles/PMC11975262/) (Sharpe et al., 2025):
> "This framework conceptualizes attention as a finite cognitive resource that must be allocated across competing demands. When sustained attention is required, this continuous allocation leads to resource depletion and deteriorating performance over time."

### The Vigilance Decrement

Sustained attention tasks reveal the cost of continuous resource allocation:

- **Vigilance decrement**: Performance declines over time during monitoring tasks
- **Resource depletion**: Finite capacity is exhausted by continuous demands
- **Strategic allocation**: Observers must decide when/where to allocate limited resources

The Strategic Allocation Theory of vigilance (Murray, 2024) proposes that:
> "The allocation of attention and other cognitive resources is a function of over-arching meta-cognitive processes that continuously evaluate task demands and available capacity."

### Historical Development

**Classical view** (pre-1990s):
- Pre-attentive stage: Unlimited parallel processing
- Attentive stage: Serial, focused processing
- **Problem**: Failed to account for capacity limits in "pre-attentive" stage

**Modern view** (1990s-present):
- No truly unlimited stage exists
- **All processing** is capacity-limited
- Selectivity mechanisms required throughout

### Bottleneck Location Debate

Where does the bottleneck occur?

1. **Early selection** (Broadbent): Bottleneck at perceptual encoding
2. **Late selection** (Deutsch & Deutsch): Bottleneck at response selection
3. **Flexible selection** (Lavie): Bottleneck location depends on perceptual load

**Current consensus**: Multiple bottlenecks exist at different processing stages, with the most severe constraint occurring at the transition from perception to working memory.

### Empirical Evidence

**Dual-task interference**:
- Performance on Task A + Task B < Performance on Task A alone + Task B alone
- Interference increases with task similarity (shared resources)
- Greater overlap in processing mechanisms → greater competition

**Inattentional blindness**:
- Observers fail to notice clearly visible stimuli when attention is engaged elsewhere
- Demonstrates hard limit: unattended information doesn't reach awareness
- Classic example: Gorilla in basketball passing task (Simons & Chabris, 1999)

**Attentional blink**:
- After detecting Target 1, detection of Target 2 is impaired for ~200-500ms
- Shows temporal bottleneck in visual awareness
- Limited capacity to consolidate information into working memory

## Section 2: Biased Competition Model (Neural Competition for Representation)

### Core Principles

Biased competition theory (Desimone & Duncan, 1995) proposes that objects in the visual field compete for neural representation due to limited cortical processing capacity. This competition can be **biased** by:

- **Bottom-up factors**: Stimulus salience (brightness, motion, contrast)
- **Top-down factors**: Task goals, expectations, working memory contents

From [Wikipedia: Biased Competition Theory](https://en.wikipedia.org/wiki/Biased_competition_theory):
> "Each object in the visual field competes for cortical representation and cognitive processing. This competition can be biased toward the object that is currently attended in the visual field, or alternatively toward the object most relevant to behavior."

### Five Core Tenets (Desimone, 1998)

**1. Simultaneous competition**:
Objects presented simultaneously compete for cell responses in visual cortex. When two stimuli activate neurons in the same receptive field, their neural responses interact in a mutually suppressive manner rather than being processed independently.

**2. Spatial proximity amplifies competition**:
Two stimuli activating cells in the same cortical area provide the strongest competitive interactions. Competition is strongest when stimuli fall within the same receptive field.

**3. Feedback bias**:
Competitive interactions can be biased in favor of one stimulus through:
- **Top-down feedback**: Task relevance, behavioral goals
- **Bottom-up influences**: Novelty, salience, "pop-out" features

**4. Feature-based bias**:
Biasing is not purely spatial. Competition can be biased toward stimuli with specific task-relevant features (color, shape, texture, motion) regardless of location.

**5. Prefrontal sources**:
A main source of top-down biasing derives from structures involved in working memory, particularly the **prefrontal cortex**.

### Neural Mechanisms

**Receptive field (RF) studies** (Kastner & Ungerleider, 2001):

- **Single stimulus**: Low firing rate when one effective stimulus presented alone in RF
- **Paired stimuli**: Reduced response compared to sum of individual responses
- **Mutual suppression**: Two stimuli presented together compete for neural representation

**Key finding**: When two stimuli are presented within a neuron's receptive field, the response is **not** the sum of individual responses but shows **mutual suppression**.

**Suppression mechanism**:
```
Response(Stim A + Stim B) < Response(Stim A alone) + Response(Stim B alone)
```

This suppression occurs because stimuli compete for limited neural resources (computational capacity, metabolic resources, cortical real estate).

### Bottom-Up Bias: "Pop-Out" Effects

Bottom-up processes are stimulus-driven and require no conscious direction:

**Pop-out features** that bias competition:
- **High contrast**: Bright objects vs. dim background
- **Motion**: Moving objects among static items
- **Size**: Large objects among small
- **Color**: Red apple among green apples
- **Orientation**: Tilted line among vertical lines

From Proulx & Egeth (2008):
> "When irrelevant stimuli were large or bright, attention was biased towards the irrelevant objects, prioritizing them for cognitive processing."

**Mechanism**: Salient features automatically increase the "weight" of their neural representation, biasing competition in their favor without conscious intent.

### Top-Down Bias: Goal-Driven Control

Top-down processes use prior knowledge and task goals to bias competition:

**Working memory template**:
- Hold target features in mind (e.g., "red and circular")
- Bias competition toward objects matching template
- Suppress non-matching objects

**Semantic knowledge**:
- Search for "coffee cup" biases toward table/kitchen locations
- Contextual expectations guide attention
- Categorical information modulates competition

**Task set**:
- "Find faces" enhances fusiform face area responses
- "Detect motion" enhances MT/MST area responses
- Task-relevant cortical areas receive top-down bias

From Beck & Kastner (2009):
> "Observers have top-down control over which locations benefit from biased competition in spatial selection tasks. Evidence supports that observers can make voluntary decisions about which locations are selected or features that capture attention."

### Integration Across Systems

Competition is not isolated to visual cortex but integrates across:

**Visual hierarchy**:
- V1 (orientation, edges) ↔ V4 (color, shape) ↔ IT (objects)
- Competition at each level, with feedback from higher areas

**Attention networks**:
- Dorsal attention network (fronto-parietal): Top-down control
- Ventral attention network: Bottom-up capture
- Integration through reciprocal connections

**Working memory**:
- Prefrontal cortex maintains task-relevant features
- Sends top-down bias signals to sensory cortex
- Updates based on task success/failure

From fMRI evidence (Reddy, Kanwisher, & VanRullen, 2009):
> "Attention effects bias the internal weight (strengthens connections) of task-relevant features toward the attended object. This was shown by increased oxygenated blood to specific neurons following a locational cue."

### Computational Models

**Normalization models**:
- Each neuron's response is divided by pooled activity of neighboring neurons
- Implements divisive suppression (competition)
- Attention increases numerator (target signal) and/or decreases denominator (suppression pool)

**Winner-take-all networks** (Koch & Ullman, 1985):
- Lateral inhibition between neurons representing different objects
- Strongest representation suppresses competitors
- Attention provides "starting advantage" to task-relevant items

**Neural field models**:
- Continuous attractor dynamics
- Stable peaks represent attended locations/features
- Competition implemented through lateral inhibition

## Section 3: Endogenous vs Exogenous Attention (Goal-Driven vs Stimulus-Driven)

### Fundamental Distinction

**Endogenous attention** (top-down, voluntary, goal-driven):
- **Definition**: Attention directed by internal goals and intentions
- **Control**: Conscious, deliberate
- **Speed**: Slow to deploy (~300-500ms)
- **Duration**: Sustained as long as goal remains active
- **Example**: Searching for friend in crowded room, following driving instructions

**Exogenous attention** (bottom-up, involuntary, stimulus-driven):
- **Definition**: Attention automatically captured by external stimuli
- **Control**: Automatic, involuntary
- **Speed**: Rapid (~120ms peak effect)
- **Duration**: Transient, fleeting
- **Example**: Turning toward loud bang, bright flash of light

From [Interactions Between Endogenous and Exogenous Attention](https://pmc.ncbi.nlm.nih.gov/articles/PMC3539749/) (MacLean et al., 2009):
> "Goal-driven attention is referred to as top-down or endogenous attention, whereas stimulus-driven attention is referred to as bottom-up or exogenous attention."

### Key Differences

| Feature | Endogenous | Exogenous |
|---------|-----------|-----------|
| **Control** | Voluntary, top-down | Involuntary, bottom-up |
| **Driven by** | Internal goals | External stimuli |
| **Speed** | Slower (300-500ms) | Faster (~120ms) |
| **Duration** | Sustained | Transient |
| **Flexibility** | Adaptable to task | Automatic response |
| **Neural source** | Prefrontal cortex, fronto-parietal network | Sensory cortex, temporo-parietal junction |

### Temporal Dynamics

**Endogenous attention**:
- **Gradual buildup**: Takes 300-500ms to fully deploy
- **Sustained maintenance**: Can be held for extended periods
- **Voluntary disengagement**: Can be redirected at will
- **Fatigue**: Sustained endogenous attention is effortful and depletes resources

**Exogenous attention**:
- **Rapid onset**: Peaks around 120ms after stimulus
- **Brief duration**: Effect dissipates within ~200-300ms
- **Automatic capture**: Cannot be prevented by will alone
- **Inhibition of return (IOR)**: After ~300ms, attention is biased *away* from previously cued location

From Fernández et al. (2022):
> "Endogenous attention is slow to engage a target stimulus, whereas exogenous attention occurs rapidly. Compared with endogenous attention, exogenous attention had a stronger orientation gain enhancement but a weaker overall spatial frequency gain enhancement."

### Neural Substrates

**Endogenous attention networks**:
- **Dorsal frontoparietal network**: Intraparietal sulcus (IPS), frontal eye fields (FEF)
- **Function**: Voluntary orienting, goal maintenance, top-down biasing
- **Activation**: Sustained during expectation period before target

**Exogenous attention networks**:
- **Ventral frontoparietal network**: Temporoparietal junction (TPJ), ventral frontal cortex (VFC)
- **Function**: Circuit-breaking, involuntary orienting
- **Activation**: Phasic response to unexpected stimuli

From Dugué et al. (2020):
> "These findings reveal that endogenous and exogenous attention distinctly modulate activity in visuo-occipital areas during orienting and reorienting."

### Interaction Effects

**Contingent capture**:
- Exogenous capture is modulated by endogenous set
- Irrelevant but target-similar stimuli capture attention
- Completely irrelevant stimuli may not capture

**Endogenous override**:
- Strong endogenous focus can suppress exogenous capture
- Task difficulty affects this: harder tasks → less override
- Individual differences in executive control

From Meyer et al. (2018):
> "Guiding attention in service of goals requires voluntary, or endogenous, shifts of attention, whereas involuntary, or exogenous, attention refers to automatic responses to things that 'grab our attention.'"

**Complementary roles**:
- Endogenous: Proactive, anticipatory (top-down guidance)
- Exogenous: Reactive, adaptive (bottom-up safety mechanism)
- Optimal performance requires balance

### Experimental Paradigms

**Posner cueing task**:
- **Endogenous**: Central arrow cue (→) points to likely target location
- **Exogenous**: Peripheral brightening at target or opposite location
- Measures: RT benefits at cued location, RT costs at uncued location

**Visual search**:
- **Endogenous**: Template-guided search for specific color/shape
- **Exogenous**: Singleton detection (unique item pops out)
- Measures: Search slope (ms per item added)

**Attentional capture**:
- Irrelevant singleton (distractor) appears during search
- Exogenous capture measured by RT increase
- Endogenous filtering measured by ability to ignore

## Section 4: Feature-Based vs Spatial Attention (What vs Where)

### Dual Systems

Visual attention operates through two dissociable systems:

**Spatial attention** (where):
- Selects information based on **location** in visual field
- "Spotlight" or "zoom lens" metaphor
- Enhances processing within attended region
- Suppresses processing outside attended region

**Feature-based attention** (what):
- Selects information based on **features** (color, motion, orientation)
- Operates **globally** across visual field
- Enhances processing of attended feature everywhere
- Can operate without spatial selection

### Neural Substrates: Dorsal vs Ventral Streams

**Dorsal stream** ("where" pathway):
- V1 → V2 → V3 → MT/MST → Posterior parietal cortex
- Processes spatial location, motion, spatial relationships
- Critical for spatial attention
- Damage → spatial neglect, impaired spatial orienting

**Ventral stream** ("what" pathway):
- V1 → V2 → V4 → Inferior temporal cortex
- Processes object identity, color, shape, texture
- Critical for feature-based attention
- Damage → agnosia, impaired object recognition

From Ungerleider & Haxby (1994):
> "'What' and 'where' in the human brain correspond to distinct processing streams with the ventral stream important for object recognition and the dorsal stream important for spatial perception."

### Spatial Attention Mechanisms

**Spotlight metaphor**:
- Attention is like a beam of light
- Illuminates attended region
- Enhances contrast/signal within spotlight
- Serial movement from location to location

**Zoom lens metaphor**:
- Spotlight can expand or contract
- **Zoomed in**: High resolution, small area
- **Zoomed out**: Low resolution, large area
- Tradeoff: Resolution vs coverage

**Gradient model**:
- Attention not "all-or-none" but graded
- Strongest at focus, falls off with distance
- Continuous allocation across space

**Priority map**:
- Topographic representation of "priority" for attention
- Combines bottom-up salience + top-down relevance
- Peak = next saccade/attention target
- Implemented in superior colliculus, FEF, LIP

### Feature-Based Attention Mechanisms

**Global enhancement**:
- Attending to "red" enhances red everywhere in visual field
- Not limited to spotlight location
- Measured by improved sensitivity to unattended-location probes

**Feature similarity gain**:
- Enhancement proportional to similarity to attended feature
- Attending to "vertical" → most gain for vertical, less for oblique
- Tuning curve sharpening: attended feature becomes more distinct

From [Allocation of Space-Based Attention is Guided by Efficient Coding Principles](https://journalofcognition.org/articles/325/) (Barnas et al., 2024):
> "Efficiently extracting spatial directions from a visually cluttered environment can make navigation safer and less error prone. Feature-based attention helps extract task-relevant information based on similarity to target features."

**Object-based attention**:
- Features grouped into objects compete as units
- Attending to one feature of object → enhances all features
- Object boundaries constrain attention
- Evidence from same-object advantage in cueing tasks

### Integration: Spatial × Feature Interaction

**Additive combination**:
- Spatial attention provides baseline enhancement in region
- Feature attention adds further enhancement for attended feature
- Combined: Spatial + Feature > Either alone

**Multiplicative combination**:
- Feature attention modulates effectiveness of spatial attention
- Strongest when both spatial and feature attention align
- Implemented via gain modulation in cortex

**Hierarchical selection**:
1. Spatial attention selects region
2. Feature attention selects relevant features within region
3. Object attention selects grouped features as units

From Baldauf & Desimone (2014):
> "Neural mechanisms of object-based attention reveal integration across spatial and feature dimensions, with attention operating on perceptual objects rather than independent features."

### Computational Models

**Guided Search** (Wolfe):
- Feature maps (color, orientation, size) activated in parallel
- Maps combined into activation map
- Spatial attention serially scans peaks in activation map
- Top-down feature weights bias which features contribute

**Dimension-weighting account**:
- Attention allocated to relevant feature dimensions (color, motion, etc.)
- Within-dimension search for specific feature value
- Cross-dimension costs: switching between dimensions is slow

**Integrated competition model**:
- Features compete within dimension (red vs green)
- Dimensions compete for limited resources
- Spatial locations compete for selection
- Attention resolves competition at all levels

## Section 5: Attention Control Networks (Fronto-Parietal, Dorsal/Ventral Attention)

### Dorsal Attention Network (DAN)

**Anatomy**:
- **Intraparietal sulcus (IPS)**: Posterior parietal cortex
- **Frontal eye fields (FEF)**: Precentral sulcus
- **Superior parietal lobule (SPL)**

**Function**:
- **Top-down, goal-directed attention**
- Voluntary orienting to locations or features
- Maintains attentional set (what to attend)
- Generates priority map for eye movements and covert attention

**Activation pattern**:
- Sustained during expectation/preparation period
- Before target appearance
- Reflects endogenous control signals

From [Editorial: The Operationalization of Cognitive Systems](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1501636/full) (Winter et al., 2024):
> "This research provides insights into how internal attention shifts affect visual working memory, advancing understanding of cognitive resource allocation during complex visual tasks. The fronto-parietal network implements top-down control."

### Ventral Attention Network (VAN)

**Anatomy**:
- **Temporoparietal junction (TPJ)**: Right-lateralized
- **Ventral frontal cortex (VFC)**: Inferior frontal gyrus
- **Middle frontal gyrus (MFG)**

**Function**:
- **Bottom-up, stimulus-driven attention**
- Circuit-breaker for unexpected events
- Reorienting to task-relevant stimuli outside current focus
- Interaction with default mode network

**Activation pattern**:
- Phasic response to salient/unexpected stimuli
- Target detection (even when unattended)
- Reflects exogenous capture

### Network Interactions

**Competition**:
- DAN active during focused attention
- VAN suppressed during focused attention (to avoid distraction)
- VAN activation interrupts DAN when relevant stimulus detected

**Cooperation**:
- DAN sets attentional goals
- VAN detects relevant events matching goals
- Feedback from VAN updates DAN priorities

**Hemispheric asymmetry**:
- Left DAN: Contralateral (right) space
- Right DAN: Bilateral space
- Right VAN: Bilateral detection (left VAN less critical)
- Result: Right hemisphere lesions → left spatial neglect

### Prefrontal Control

**Dorsolateral prefrontal cortex (dlPFC)**:
- Maintains task rules and goals
- Provides top-down bias to DAN
- Working memory maintenance
- Cognitive control over attention

**Anterior cingulate cortex (ACC)**:
- Conflict monitoring
- Error detection
- Signals need for control adjustment
- Modulates attention intensity

**Orbitofrontal cortex (OFC)**:
- Value-based attention
- Reward prediction
- Guides attention to high-value stimuli
- Integrates motivation with attention

### Subcortical Structures

**Superior colliculus**:
- Midbrain structure
- Generates saccadic eye movements
- Spatial priority map
- Receives input from cortical attention networks

**Pulvinar (thalamus)**:
- Thalamic nucleus with visual connections
- Synchronizes cortical activity
- Filters information reaching cortex
- Attention-related modulation

**Basal ganglia**:
- Selection of actions/goals
- Habit vs goal-directed control
- Dopaminergic modulation of attention
- Reward-based learning of what to attend

### Neuromodulators

**Norepinephrine (NE)** - Locus coeruleus:
- Arousal and alertness
- Phasic NE → target detection (VAN)
- Tonic NE → sustained attention (DAN)
- Inverted-U: Optimal at moderate levels

**Acetylcholine (ACh)** - Basal forebrain:
- Enhances sensory processing
- Increases signal-to-noise ratio
- Attention-dependent release in cortex
- Critical for perceptual learning

**Dopamine (DA)** - VTA/SNc:
- Reward prediction and motivation
- Modulates attentional priority
- Value-based attention
- Executive control functions

## Section 6: Attention and Working Memory (Resource Sharing, Trade-Offs)

### Shared Resource Framework

Attention and working memory (WM) are not independent but share:

**Common neural substrates**:
- Prefrontal cortex (dlPFC, vlPFC)
- Posterior parietal cortex
- Fronto-parietal network

**Common functions**:
- Selective maintenance of task-relevant information
- Filtering of irrelevant information
- Top-down biasing of sensory processing

From Hollingworth & Luck (2009):
> "Visual working memory (VWM) plays a critical role in the control of gaze during visual search. The contents of VWM bias attentional selection toward matching items in the environment."

### Capacity Constraints

**Working memory capacity limits**:
- ~3-4 objects or chunks can be held simultaneously
- Individual differences (low: 2, high: 5)
- Related to fluid intelligence

**Attention capacity limits**:
- Limited number of locations/objects that can be attended
- Similar to WM capacity (~3-4)
- Shared limit or separate but correlated?

**Shared vs separate debate**:
- **Shared**: Single pool of resources for attention + WM
- **Separate**: Independent but interacting systems
- **Current view**: Overlapping resources with some independence

### Bidirectional Interactions

**WM → Attention** (top-down guidance):
- WM contents bias attention toward matching features
- Template-matching: Hold target in WM → guides search
- Automatic capture by WM contents (even when irrelevant)

**Attention → WM** (selective encoding):
- Attended information prioritized for WM storage
- Unattended information rarely enters WM
- Attention acts as gatekeeper to WM

From Gazzaley & Nobre (2012):
> "Top-down modulation bridges selective attention and working memory. Attention determines what enters WM, while WM contents guide attentional selection in a continuous interactive loop."

### Trade-Offs

**Attention load affects WM**:
- High attentional demands → reduced WM capacity
- Dividing attention → poorer WM encoding
- Dual-task costs: Attention task + WM task < Sum of individual

**WM load affects attention**:
- High WM load → reduced attentional control
- Harder to ignore distractors when WM is full
- Diminished top-down biasing under load

From [Serial Attentional Resource Allocation During Parallel Multi-Feature Tracking](https://elifesciences.org/articles/91183) (Merkel et al., 2023):
> "This study presents important findings on how human observers keep track of continuously changing feature values. Resource allocation shifts serially even when tracking parallel streams, demonstrating fundamental capacity limits."

### Neural Mechanisms

**Sustained activity**:
- Delay-period firing in PFC during WM maintenance
- Same neurons show attention-related enhancement
- Activity level correlates with WM precision and attentional strength

**Oscillatory coupling**:
- **Theta oscillations** (4-8 Hz): WM maintenance, attention control
- **Gamma oscillations** (30-100 Hz): Feature binding, attentional selection
- **Theta-gamma coupling**: WM items represented by gamma nested in theta cycles

**Synaptic facilitation**:
- Attention and WM both enhance synaptic efficacy
- Shared mechanism: Increased gain on sensory responses
- Competing demands reduce gain for each function

### Individual Differences

**Working memory capacity predicts**:
- Attentional control ability
- Resistance to distraction
- Task-switching efficiency
- Fluid intelligence (Gf)

From Lu et al. (2022):
> "High Gf individuals tend to control their attention state in tasks with diverse demands, allowing them to dynamically optimize the use of cognitive resources and flexibly adapt to changing conditions."

**Attention training effects**:
- Training on attention tasks → improved WM capacity
- Training on WM tasks → improved attentional control
- Suggests shared trainable substrate

## Section 7: Computational Models (Normalization Models, Priority Maps)

### Normalization Model

**Core equation**:
```
R_i = (E_i × A_i) / (σ + Σ_j E_j)
```

Where:
- R_i = Response of neuron i
- E_i = Excitatory drive to neuron i
- A_i = Attention field (gain on neuron i)
- σ = Semi-saturation constant
- Σ_j E_j = Normalization pool (sum of activity)

**Key properties**:
- **Divisive normalization**: Response divided by pooled activity
- **Attention as gain**: Multiplication in numerator (increases signal)
- **Suppression**: Competition in denominator (divisive suppression)

**Predictions**:
- Attention increases response to attended stimulus
- Attention decreases response to unattended stimuli in same RF
- Explains contrast-dependent and feature-similarity-dependent effects

### Priority Map Framework

**Concept**:
- Topographic map encoding "priority" for attention/eye movements
- Each location has priority value = bottom-up salience + top-down relevance
- Peak in map = next target for selection

**Implementation sites**:
- **Superior colliculus**: Oculomotor priority map
- **Frontal eye fields (FEF)**: Cortical priority map
- **Lateral intraparietal area (LIP)**: Parietal priority map
- **V4**: Feature-weighted priority

**Computation**:
```
Priority(x,y) = w_salience × Salience(x,y) + w_relevance × Relevance(x,y)
```

Where:
- x,y = Spatial location
- Salience = Bottom-up features (contrast, motion, color pop-out)
- Relevance = Top-down task goals (target features, semantic expectations)
- w = Weights adjusted by task demands

**Dynamics**:
- Winner-take-all: Strongest priority location selected
- Inhibition of return: Previously attended locations suppressed
- Build-up activity: Priority accumulates before saccade
- Threshold crossing: Saccade triggered when priority exceeds threshold

### Neural Field Models

**Continuous attractor dynamics**:
- Spatially organized neurons with local excitation, surround inhibition
- Self-sustained activity bumps represent attended locations
- Movement of bump = attention shifting
- Stability maintained by recurrent connections

**Field equations**:
```
τ ∂u/∂t = -u + ∫ w(x-x') f(u(x')) dx' + I(x)
```

Where:
- u = Neural activity field
- w = Connectivity kernel (Mexican hat: local excite, surround inhibit)
- f = Activation function
- I = External input (sensory, top-down)

**Emergent properties**:
- Stable peaks without input (working memory)
- Winner-take-all competition
- Smooth attentional movements
- Capacity limits (limited number of stable peaks)

### Guided Search Model (Wolfe)

**Architecture**:
1. **Feature maps**: Parallel processing of color, orientation, size, etc.
2. **Activation map**: Weighted sum of feature maps
3. **Attentional guidance**: Peaks in activation map guide serial search

**Equations**:
```
Activation(x,y) = Σ_features w_feature × FeatureMap_feature(x,y)
```

**Top-down weights**:
- User sets weights based on target features
- "Find red vertical" → increase w_red, w_vertical
- Weights learned from experience/feedback

**Search process**:
- Attention serially visits peaks in activation map
- High activation = high priority
- Threshold criterion: "Is this the target?"
- Reject → inhibit location → move to next peak

**Predictions**:
- Search efficiency depends on feature discriminability
- Parallel features → flat search slopes (<10ms/item)
- Serial features → steep search slopes (>30ms/item)
- Hybrid: Most searches show moderate slopes (guidance helps but doesn't eliminate serial component)

### Dimension-Weighting Models

**Core idea**:
- Attention allocated to feature dimensions (color, motion, orientation)
- Within-dimension search for target value
- Cross-dimension switching is costly

**Intertrial priming**:
- Repeating target dimension → faster RT
- Switching dimensions → slower RT (switch cost)
- Weight adjustment takes time

**Dimensional imbalance**:
- If target defined in Color → weight Color dimension high
- Non-color distractors have less impact
- Weight optimization improves efficiency

### Integrated Competition Models

**Multi-level competition**:
- **Feature level**: Red vs green, vertical vs horizontal
- **Dimension level**: Color vs orientation vs motion
- **Location level**: Left vs right, near vs far
- **Object level**: Object A vs Object B

**Attention resolves competition**:
- Biases toward task-relevant features/dimensions/locations
- Suppresses irrelevant information
- Hierarchical: Higher-level decisions constrain lower levels

**Neural implementation**:
- Lateral inhibition within level
- Feedback between levels
- Winner-take-all dynamics
- Biasing signals from prefrontal cortex

## Section 8: ARR-COC-0-1 Token Allocation (Attention Budget 64-400, Relevance-Driven)

### Core Architecture: Attention as Resource Allocation

ARR-COC-0-1 implements attention as a **dynamic token budget allocation** problem. Each image patch receives 64-400 tokens based on realized relevance, directly instantiating the cognitive neuroscience principle of limited attentional capacity requiring strategic allocation.

**Parallel to cognitive attention**:
- **Limited capacity**: Fixed total token budget across all patches
- **Variable allocation**: High-relevance patches get more tokens (400), low-relevance get fewer (64)
- **Competition**: Patches compete for representation in the token budget
- **Biased competition**: Query + visual features bias which patches receive resources

From [ARR-COC-VIS Application Guide](../john-vervaeke-oracle/ARR-COC-VIS-Application-Guide.md):
> "Compression ↔ Particularization is fully implemented. Low entropy + low relevance → compress (64 tokens). High entropy + high relevance → particularize (400 tokens). Continuous spectrum between extremes."

### Attention Bottleneck in Vision-Language Models

Standard VLMs process all visual tokens equally:
- **Problem**: Uniform token allocation wastes computation on irrelevant regions
- **Bottleneck**: Transformer attention is O(n²) in sequence length
- **Cost**: Processing 1024 image tokens = 1,048,576 pairwise attention operations

ARR-COC-0-1 addresses this bottleneck:
- **Dynamic allocation**: Only high-relevance regions get detailed encoding
- **Compression**: Background regions encoded at low resolution (64 tokens)
- **Efficiency gain**: Average ~150 tokens/patch instead of uniform 256

**Computational savings**:
```
Standard: 100 patches × 256 tokens = 25,600 total tokens
ARR-COC: 100 patches × 150 avg tokens = 15,000 total tokens
Savings: 41% reduction in sequence length → ~64% reduction in attention cost
```

### Biased Competition Implementation

**Bottom-up bias** (propositional + perspectival knowing):
```python
# knowing.py - InformationScorer (propositional)
entropy = shannon_entropy(patch)  # High entropy → more information → needs detail

# knowing.py - PerspectivalScorer
salience = archetypal_patterns(patch)  # Visual salience → perceptual importance
```

**Top-down bias** (participatory knowing):
```python
# knowing.py - ParticipatoryScorer
relevance = cross_attention(patch, query)  # Query-content coupling → task relevance
```

**Competition resolution** (balancing.py):
```python
# Combine all sources of bias
total_score = w1*entropy + w2*salience + w3*relevance

# Navigate compression ↔ particularization tension
if total_score > threshold_high:
    tokens = 400  # Particularize: high detail
elif total_score < threshold_low:
    tokens = 64   # Compress: low detail
else:
    tokens = interpolate(total_score)  # Continuous allocation
```

This directly implements the biased competition model:
- **Patches compete** for limited token budget
- **Competition biased** by bottom-up (entropy, salience) and top-down (query relevance)
- **Winner allocation**: High-scoring patches get more resources

### Endogenous vs Exogenous Attention

**Endogenous (query-driven)**:
- User query = top-down attentional goal
- "Find the cat" → enhances cat-relevant patches
- Sustained: Query maintained throughout processing
- Implemented via cross-attention scores (ParticipatoryScorer)

**Exogenous (stimulus-driven)**:
- High-salience regions automatically prioritized
- Bright colors, strong edges, motion (if video)
- Automatic: Computed from image statistics
- Implemented via entropy and salience scores

**Integration**:
```python
# attending.py - AttentionAllocator
endogenous_weight = query_specificity(query)  # Strong query → high weight
exogenous_weight = 1 - endogenous_weight      # Weak query → rely on salience

attention_score = (endogenous_weight * participatory_score +
                   exogenous_weight * (information_score + perspectival_score))
```

### Feature-Based vs Spatial Attention

**Spatial attention** (where):
- Patch location determines competition
- Neighboring patches compete more strongly
- Spatial priority map emerges from relevance scores
- Implementation: Patches are spatially organized grid

**Feature-based attention** (what):
- Query specifies target features ("red car")
- All patches matching features get boost, regardless of location
- Global enhancement of query-relevant features
- Implementation: Cross-attention operates globally across patches

**Integrated selection**:
1. **Query analysis**: Extract feature preferences ("red", "car", "left side")
2. **Spatial guidance**: Priority to query-mentioned locations
3. **Feature matching**: Enhance patches with target features
4. **Combined score**: Spatial × Feature relevance

### Priority Map in ARR-COC-0-1

The relevance scores form a **priority map** over image patches:

```python
# Computed in knowing.py, realized in attending.py
priority_map[patch_i] = relevance_realization(
    information=entropy[patch_i],
    perspectival=salience[patch_i],
    participatory=query_relevance[patch_i]
)
```

**Properties**:
- **Topographic**: Organized by spatial location (patch grid)
- **Multi-factor**: Combines bottom-up + top-down
- **Dynamic**: Changes with different queries
- **Graded**: Continuous values, not binary on/off

**Attention allocation**:
- Peak in priority map → maximum tokens (400)
- Minimum in priority map → minimum tokens (64)
- Gradient allocation between extremes

### Normalization and Competition

**Divisive normalization** analogue:
```python
# balancing.py - TensionBalancer
normalized_score = relevance_score / (sigma + total_relevance_across_patches)
```

This implements:
- **Mutual suppression**: Adding more high-relevance patches reduces allocation to each
- **Gain control**: Normalization prevents saturation
- **Contrast enhancement**: Relative differences amplified

**Token budget constraint**:
```python
total_tokens = sum(tokens_per_patch)  # Must not exceed model capacity
if total_tokens > max_tokens:
    # Reduce allocation to lower-priority patches (competitive suppression)
    scale_factor = max_tokens / total_tokens
    tokens_per_patch *= scale_factor
```

### Working Memory Analogue: Quality Adapter

The **quality adapter** (adapter.py) implements procedural knowing—learned compression skills—analogous to how working memory maintains attentional templates:

**Function**:
- Learns optimal token allocation strategies from experience
- Stores "templates" for effective compression patterns
- Guides future allocation based on past success

**Working memory parallel**:
- WM holds target features → guides attention
- Quality adapter holds compression patterns → guides token allocation
- Both provide top-down bias from learned knowledge

From [ARR-COC-VIS Application Guide](../john-vervaeke-oracle/ARR-COC-VIS-Application-Guide.md):
> "The quality adapter IS the 4th P (procedural knowing). It develops automatic competence through training, learning which compression strategies work best."

### Resource Trade-Offs

**Attention-WM trade-off** in ARR-COC-0-1:
- **High token allocation** (400) = detailed encoding = high WM load
- **Low token allocation** (64) = compressed encoding = low WM load
- **Total budget constraint** = cannot maximize detail everywhere

**Strategic allocation**:
- Allocate tokens where information is most needed (query-relevant)
- Compress irrelevant regions to free capacity
- Continuous optimization based on relevance realization

**Computational analogy**:
- Attention capacity ≈ Total token budget
- WM maintenance ≈ Token-per-patch allocation
- Resource sharing ≈ Budget constraint across patches

### Comparison to Standard VLM Attention

| Aspect | Standard VLM | ARR-COC-0-1 |
|--------|-------------|--------------|
| **Token allocation** | Uniform (all patches equal) | Dynamic (64-400 based on relevance) |
| **Attention type** | Fixed transformer attention | Biased competition (top-down + bottom-up) |
| **Resource model** | No capacity constraint | Explicit token budget |
| **Efficiency** | Wasteful (detail on irrelevant regions) | Strategic (detail where needed) |
| **Biological parallel** | No clear analogue | Direct implementation of attentional resource allocation |

### Empirical Validation

**Required experiments** (from research-methodology knowledge):

1. **Gaze comparison**: Compare ARR-COC-0-1 token allocation to human eye fixations
   - Hypothesis: High-token patches should correlate with longer fixation durations
   - Measure: Correlation between tokens and dwell time

2. **Ablation study**: Fixed vs dynamic allocation
   - Baseline: All patches 256 tokens (standard)
   - ARR-COC: Dynamic 64-400 allocation
   - Measure: Task accuracy, computational cost

3. **Relevance manipulation**: Vary query specificity
   - Specific query ("red car on left") → strong endogenous bias
   - General query ("describe scene") → rely on exogenous features
   - Measure: Allocation patterns shift appropriately

4. **Capacity limits**: Total budget constraint
   - Reduce total budget → must compress more patches
   - Measure: Graceful degradation vs catastrophic failure

### Future Enhancements: Missing Dimensions

From [ARR-COC-VIS Application Guide](../john-vervaeke-oracle/ARR-COC-VIS-Application-Guide.md):

**Exploit-Explore tension** (currently missing):
```python
# Proposed addition
class CognitiveTemperingDimension:
    def allocate_with_tempering(self, patch, query, history):
        # Exploit: Use learned allocation pattern
        exploit_tokens = quality_adapter.predict(patch, query)

        # Explore: Random perturbation to discover better strategies
        explore_tokens = exploit_tokens + random_perturbation()

        # Balance based on confidence/uncertainty
        if confidence_high(history):
            return 0.9 * exploit_tokens + 0.1 * explore_tokens
        else:
            return 0.5 * exploit_tokens + 0.5 * explore_tokens
```

**Focus-Diversify tension** (partially implemented):
- Currently: Implicit (high tokens to query-relevant, minimum to all)
- Missing: Explicit dynamic shifting between narrow focus and broad monitoring
- Needed: Context-dependent adjustment (threat detection → diversify, search → focus)

## Sources

### Source Documents

**Vervaeke Framework**:
- [john-vervaeke-oracle/](../john-vervaeke-oracle/) - Relevance realization, opponent processing, attending.py concepts
- [ARR-COC-VIS Application Guide](../john-vervaeke-oracle/ARR-COC-VIS-Application-Guide.md) - Token allocation as relevance realization

### Web Research

**Attention as Limited Resource**:
- Sharpe, B.T. et al. (2025). "The Sustained Attention Paradox: A Critical Commentary." PMC11975262. (accessed 2025-11-14) - Attention as finite resource, vigilance decrement
- Moulton, R.H. et al. (2023). "Capacity Limits Lead to Information Bottlenecks in Ongoing Perception." PMC10012325. (accessed 2025-11-14) - Bottleneck theory
- Tombu, M.N. et al. (2011). "A Unified Attentional Bottleneck in the Human Brain." PNAS. (accessed 2025-11-14) - Unified bottleneck across perception and response
- Murray, S. (2024). "The Strategic Allocation Theory of Vigilance." Wiley Interdisciplinary Reviews: Cognitive Science. (accessed 2025-11-14) - Meta-cognitive resource allocation

**Biased Competition**:
- Desimone, R., & Duncan, J. (1995). "Neural Mechanisms of Selective Visual Attention." Annual Review of Neuroscience, 18, 193-222.
- Wikipedia: Biased Competition Theory. https://en.wikipedia.org/wiki/Biased_competition_theory (accessed 2025-11-14) - Comprehensive overview
- Beck, D.M., & Kastner, S. (2009). "Top-down and Bottom-up Mechanisms in Biasing Competition in the Human Brain." Vision Research, 49, 1154-1165.
- Kastner, S., & Ungerleider, L.G. (2001). "The Neural Basis of Biased Competition in Human Visual Cortex." Neuropsychologia, 39, 1263-1276.
- Reddy, L., Kanwisher, N.G., & VanRullen, R. (2009). "Attention and Biased Competition in Multi-voxel Object Representations." PNAS, 106, 50, 21447-21452.

**Endogenous vs Exogenous**:
- MacLean, K.A. et al. (2009). "Interactions Between Endogenous and Exogenous Attention During Vigilance." PMC3539749. (accessed 2025-11-14) - Fundamental distinction
- Fernández, A. et al. (2022). "Differential Effects of Endogenous and Exogenous Attention." Journal of Neuroscience, 42(7), 1316. (accessed 2025-11-14) - Temporal dynamics
- Dugué, L. et al. (2020). "Differential Impact of Endogenous and Exogenous Attention on Visuo-Occipital Areas." Nature Scientific Reports. (accessed 2025-11-14) - Neural substrates
- Meyer, K.N. et al. (2018). "Exogenous vs. Endogenous Attention: Shifting the Balance of Fronto-Cortical Activity." Neuropsychologia. (accessed 2025-11-14) - Network interactions

**Attention Control Networks**:
- Winter, M. et al. (2024). "Editorial: The Operationalization of Cognitive Systems in Neuroscience." Frontiers in Neuroscience. (accessed 2025-11-14) - Fronto-parietal control

**Attention and Working Memory**:
- Hollingworth, A., & Luck, S.J. (2009). "The Role of Visual Working Memory in the Control of Gaze During Visual Search." Attention, Perception, & Psychophysics, 71, 4, 936-949.
- Gazzaley, A., & Nobre, A.C. (2012). "Top-down Modulation: Bridging Selective Attention and Working Memory." Trends in Cognitive Sciences, 16, 2, 129-135.
- Merkel, C. et al. (2023). "Serial Attentional Resource Allocation During Parallel Multi-Feature Tracking." eLife, 91183. (accessed 2025-11-14) - Resource trade-offs
- Lu, R. et al. (2022). "Attentional Resource Allocation Among Individuals with High Fluid Intelligence." Neuropsychologia. (accessed 2025-11-14) - Individual differences

**Computational Models**:
- Barnas, A.J. et al. (2024). "Allocation of Space-Based Attention is Guided by Efficient Coding Principles." Journal of Cognition, 325. (accessed 2025-11-14) - Priority maps, feature-based attention
- Ungerleider, L.G., & Haxby, J.V. (1994). "'What' and 'Where' in the Human Brain." Current Opinion in Neurobiology, 4, 157-165.
- Baldauf, D., & Desimone, R. (2014). "Neural Mechanisms of Object-Based Attention." Science, 344(6182), 424-427.

### Additional References

- Koch, C., & Ullman, S. (1985). "Shifts in Selective Visual Attention: Towards the Underlying Neural Circuitry." Human Neurobiology, 4, 219-227.
- Wolfe, J.M. (2006). "The Role of Attentional Bottlenecks and Limited Visual Memory." PMC2574522. (accessed 2025-11-14)
- Proulx, M.J., & Egeth, H.E. (2008). "Biased Competition and Visual Search: The Role of Luminance and Size Contrast." Psychological Research, 72, 106-113.
