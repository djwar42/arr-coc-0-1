# GCP Compliance Auditing

Comprehensive guide to Google Cloud Platform audit logging, compliance automation, and analysis tools for regulatory and operational requirements.

---

## Section 1: Audit Log Types (~150 lines)

### Overview

GCP provides **4 types of Cloud Audit Logs** to track "who did what, where, and when" across all Google Cloud resources.

From [Cloud Audit Logs Overview](https://docs.cloud.google.com/logging/docs/audit) (accessed 2025-02-03):
- All audit logs are automatically generated (except Data Access logs require explicit enablement)
- Logs capture operations performed by users, service accounts, and Google Cloud services
- Each log entry includes: principal identity, resource accessed, operation performed, timestamp, request/response details

### Log Type 1: Admin Activity Logs

**Purpose**: Track administrative operations and configuration changes

**Characteristics**:
- **Always enabled** - cannot be disabled
- **400-day retention** at no charge
- Captures API calls that modify resource configuration or metadata

**What's Logged**:
- Creating/deleting/modifying cloud resources (VMs, buckets, datasets)
- Changing IAM policies and permissions
- Enabling/disabling APIs and services
- Quota modifications
- Network configuration changes

**Example Operations**:
```
compute.instances.insert        // Creating a VM
storage.buckets.create         // Creating a GCS bucket
iam.serviceAccounts.create     // Creating a service account
bigquery.datasets.update       // Modifying BigQuery dataset
```

**Use Cases**:
- Security audits (who changed what permissions)
- Troubleshooting configuration issues
- Compliance reporting (SOC 2, ISO 27001)
- Change tracking for incident response

From [Resource Manager Audit Logging](https://cloud.google.com/resource-manager/docs/audit-logging) (accessed 2025-02-03):
- Admin Activity logs include the full request/response payloads
- Log entries contain `protoPayload.methodName` field identifying the operation
- `protoPayload.authenticationInfo.principalEmail` identifies who performed the action

### Log Type 2: Data Access Logs

**Purpose**: Track reading/writing of user-provided data (not system metadata)

**Characteristics**:
- **Disabled by default** (except BigQuery Data Access logs)
- Must be explicitly enabled per-service
- Can generate high volume → cost considerations
- Separate from Admin Activity logs for security/privacy

**Categories**:
- **ADMIN_READ**: Read operations on metadata (e.g., list resources)
- **DATA_READ**: Read operations on user data (e.g., download blob, query table)
- **DATA_WRITE**: Write operations on user data (e.g., upload blob, insert row)

**Example Operations**:
```
storage.objects.get           // Reading a GCS object (DATA_READ)
bigquery.jobs.insert          // Running a query (DATA_WRITE)
compute.instances.get         // Getting VM details (ADMIN_READ)
```

**Cost Implications**:
From [Best Practices for Cloud Audit Logs](https://docs.cloud.google.com/logging/docs/audit/best-practices) (accessed 2025-02-03):
- Data Access logs can be 10-100x volume of Admin Activity
- BigQuery generates logs for every query, scan, export
- GCS generates logs for every object access
- **Recommendation**: Enable selectively for sensitive resources only

**Configuration Example** (Organization-level via IAM policy):
```json
{
  "auditConfigs": [
    {
      "service": "storage.googleapis.com",
      "auditLogConfigs": [
        { "logType": "DATA_READ" },
        { "logType": "DATA_WRITE" }
      ]
    }
  ]
}
```

From [Enable Data Access Audit Logs](https://docs.cloud.google.com/logging/docs/audit/configure-data-access) (accessed 2025-02-03):
- Can enable at organization, folder, or project level
- More specific configurations override broader ones
- Exemptions can exclude specific users/service accounts

### Log Type 3: System Event Logs

**Purpose**: Track Google Cloud system operations (not user-initiated)

**Characteristics**:
- **Always enabled** - cannot be disabled
- **400-day retention** at no charge
- Automatically generated by GCP systems

**What's Logged**:
- Compute Engine live migration events
- Preemptible VM terminations
- Scheduled resource operations
- Automatic scaling events
- Certificate rotations
- Quota enforcement actions

**Example Events**:
```
compute.instances.migrateOnHostMaintenance  // VM live migration
compute.instances.preempted                 // Spot VM termination
iam.serviceAccounts.autoRotate             // Service account key rotation
```

**Use Cases**:
- Understanding VM availability issues (preemptions, migrations)
- Tracking automatic system actions
- Capacity planning (scaling events)
- SLA/SLO monitoring

From [Understanding Audit Logs](https://medium.com/google-cloud/understanding-stackdriver-audit-logs-3879473de82a) (accessed 2025-02-03):
- System Event logs distinguish between user actions vs. system actions
- Critical for troubleshooting "why did my VM restart?"
- No configuration required - always active

### Log Type 4: Policy Denied Logs

**Purpose**: Track operations denied by IAM permissions or VPC Service Controls

**Characteristics**:
- **Always enabled** - cannot be disabled
- **30-day retention** by default
- Captures security policy violations

**What's Logged**:
- IAM permission denials (user lacks required permission)
- VPC Service Controls violations (perimeter restrictions)
- Organization Policy constraint violations
- Conditional IAM binding denials

**Example Denials**:
```
Permission: compute.instances.create
Principal: user@example.com
Reason: PERMISSION_DENIED (user lacks compute.instanceAdmin role)

VPC-SC Violation:
Resource: storage.googleapis.com/bucket-name
Perimeter: production-perimeter
Reason: ACCESS_FROM_DISALLOWED_IP
```

**Use Cases**:
- Security monitoring (detect lateral movement attempts)
- Access troubleshooting (why can't I access this resource?)
- Zero-trust policy validation
- Insider threat detection

From [Policy Denied Audit Logs](https://cloud.google.com/logging/docs/audit) (accessed 2025-02-03):
- Includes full context: who, what, why denied
- `status.code = 7` indicates PERMISSION_DENIED
- VPC-SC violations include perimeter details

---

## Section 2: Compliance Automation (~150 lines)

### Automated Audit Configuration

**Organization-Level Automation** (Terraform):
```hcl
# Enable Data Access logs across organization
resource "google_organization_iam_audit_config" "org_audit" {
  org_id  = "123456789"
  service = "allServices"

  audit_log_config {
    log_type = "ADMIN_READ"
  }
  audit_log_config {
    log_type = "DATA_READ"
    exempted_members = [
      "serviceAccount:monitoring@project.iam.gserviceaccount.com"
    ]
  }
  audit_log_config {
    log_type = "DATA_WRITE"
  }
}
```

From [GCP Compliance Automation Tools](https://www.dspmguides.com/guides/detect-audit-logs-gcp/) (accessed 2025-02-03):
- Terraform allows declarative audit config management
- Organization-level policies propagate to all projects
- Version control for compliance configurations

### Continuous Compliance Monitoring

**Automated Alert Policies** (Cloud Monitoring):
```yaml
# Alert on unauthorized IAM changes
displayName: "Unauthorized IAM Policy Change"
conditions:
  - displayName: "IAM change by non-admin"
    conditionThreshold:
      filter: |
        resource.type="project"
        protoPayload.methodName="SetIamPolicy"
        protoPayload.authenticationInfo.principalEmail!~"admin@.*"
      comparison: COMPARISON_GT
      thresholdValue: 0
      duration: 0s
notificationChannels:
  - "projects/PROJECT_ID/notificationChannels/CHANNEL_ID"
```

**Python Automation for Policy Compliance**:
```python
from google.cloud import logging_v2
from google.cloud import monitoring_v3

def check_audit_log_compliance(project_id):
    """Verify all projects have required audit logging enabled"""
    client = logging_v2.services.config_service_v2.ConfigServiceV2Client()

    # Check for required log sinks
    sinks = client.list_sinks(parent=f"projects/{project_id}")

    required_sinks = {
        "audit-to-bigquery": "bigquery.googleapis.com/projects/{}/datasets/audit_logs",
        "security-to-pubsub": "pubsub.googleapis.com/projects/{}/topics/security-alerts"
    }

    missing = []
    for name, destination in required_sinks.items():
        if not any(s.name == name for s in sinks):
            missing.append(name)

    return {
        "compliant": len(missing) == 0,
        "missing_sinks": missing
    }

def enforce_audit_retention(project_id, retention_days=365):
    """Ensure audit log buckets meet retention requirements"""
    client = logging_v2.services.config_service_v2.ConfigServiceV2Client()

    bucket_name = f"projects/{project_id}/locations/global/buckets/_Default"
    bucket = client.get_bucket(name=bucket_name)

    if bucket.retention_days < retention_days:
        bucket.retention_days = retention_days
        client.update_bucket(bucket=bucket)
        return f"Updated retention to {retention_days} days"

    return f"Retention compliant: {bucket.retention_days} days"
```

From [Automating BigQuery Audit Log Collection](https://www.cloudkeeper.com/insights/blog/automating-bigquery-audit-log-collection-centralized-bigquery-dataset) (accessed 2025-02-03):
- Centralized audit log collection enables cross-project compliance checks
- Automated enforcement prevents configuration drift
- Python scripts can run in Cloud Functions for continuous monitoring

### Compliance Reporting Automation

**Scheduled Compliance Reports** (Cloud Scheduler + BigQuery):
```sql
-- Daily compliance report: Export to BigQuery
CREATE OR REPLACE TABLE compliance_reports.daily_audit_summary AS
SELECT
  DATE(timestamp) as report_date,
  COUNT(*) as total_operations,
  COUNTIF(severity = 'ERROR') as errors,
  COUNTIF(protoPayload.methodName LIKE '%SetIamPolicy%') as iam_changes,
  COUNTIF(protoPayload.methodName LIKE '%delete%') as deletions,
  COUNT(DISTINCT protoPayload.authenticationInfo.principalEmail) as unique_users
FROM
  `project.dataset.cloudaudit_googleapis_com_activity_*`
WHERE
  _TABLE_SUFFIX = FORMAT_DATE('%Y%m%d', CURRENT_DATE())
GROUP BY report_date;
```

**Automated Anomaly Detection**:
```python
from google.cloud import bigquery
from google.cloud import monitoring_v3

def detect_audit_anomalies(project_id):
    """Detect unusual patterns in audit logs"""
    client = bigquery.Client(project=project_id)

    # Find unusual API call patterns
    query = """
    WITH daily_stats AS (
      SELECT
        DATE(timestamp) as date,
        protoPayload.methodName as method,
        COUNT(*) as call_count
      FROM `audit_logs.cloudaudit_googleapis_com_activity_*`
      WHERE _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY))
      GROUP BY date, method
    ),
    avg_stats AS (
      SELECT
        method,
        AVG(call_count) as avg_count,
        STDDEV(call_count) as stddev_count
      FROM daily_stats
      GROUP BY method
    )
    SELECT
      d.date,
      d.method,
      d.call_count,
      a.avg_count,
      (d.call_count - a.avg_count) / a.stddev_count as z_score
    FROM daily_stats d
    JOIN avg_stats a ON d.method = a.method
    WHERE d.date = CURRENT_DATE()
      AND ABS((d.call_count - a.avg_count) / a.stddev_count) > 3
    ORDER BY ABS(z_score) DESC
    """

    anomalies = client.query(query).to_dataframe()
    return anomalies
```

From [Best Practices for Monitoring GCP Audit Logs](https://www.datadoghq.com/blog/monitoring-gcp-audit-logs/) (accessed 2025-02-03):
- Anomaly detection identifies potential security incidents
- Baseline normal behavior over time (7-30 days)
- Alert on 3+ standard deviations from baseline

### Infrastructure as Code for Compliance

**Audit Config Management** (Python API):
```python
from google.cloud import logging_v2

def configure_audit_sink(project_id, sink_name, destination_dataset):
    """Create standardized audit log sink"""
    client = logging_v2.services.config_service_v2.ConfigServiceV2Client()

    parent = f"projects/{project_id}"
    sink = logging_v2.types.LogSink(
        name=sink_name,
        destination=f"bigquery.googleapis.com/projects/{project_id}/datasets/{destination_dataset}",
        filter='logName:"cloudaudit.googleapis.com"',
        include_children=True  # Include all child resources
    )

    response = client.create_sink(parent=parent, sink=sink)
    return response.name
```

---

## Section 3: Analysis Tools (~100 lines)

### BigQuery Log Analytics

**Purpose**: SQL-based analysis of audit logs at scale

From [BigQuery Audit Logs Overview](https://docs.cloud.google.com/bigquery/docs/reference/auditlogs) (accessed 2025-02-03):
- BigQuery exports all audit logs to Cloud Logging automatically
- Can route to BigQuery dataset via log sink for analysis
- Enables joining audit logs with other datasets

**Setup: Export Audit Logs to BigQuery**:
```bash
# Create BigQuery dataset
gcloud logging sinks create audit-to-bq \
  bigquery.googleapis.com/projects/PROJECT_ID/datasets/audit_logs \
  --log-filter='logName:"cloudaudit.googleapis.com"'

# Grant sink writer permissions
gcloud projects add-iam-policy-binding PROJECT_ID \
  --member="serviceAccount:SINK_SERVICE_ACCOUNT" \
  --role="roles/bigquery.dataEditor"
```

**Analysis Queries**:

1. **IAM Changes by User**:
```sql
SELECT
  timestamp,
  protoPayload.authenticationInfo.principalEmail as user,
  protoPayload.methodName as operation,
  protoPayload.resourceName as resource,
  protoPayload.response.bindings as new_permissions
FROM
  `project.audit_logs.cloudaudit_googleapis_com_activity_*`
WHERE
  protoPayload.methodName LIKE '%SetIamPolicy%'
  AND _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY))
ORDER BY timestamp DESC;
```

2. **Top 10 Most Active Users**:
```sql
SELECT
  protoPayload.authenticationInfo.principalEmail as user,
  COUNT(*) as operation_count,
  COUNT(DISTINCT protoPayload.methodName) as unique_operations
FROM
  `project.audit_logs.cloudaudit_googleapis_com_activity_*`
WHERE
  _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY))
GROUP BY user
ORDER BY operation_count DESC
LIMIT 10;
```

3. **Resource Deletions**:
```sql
SELECT
  timestamp,
  protoPayload.authenticationInfo.principalEmail as deleted_by,
  resource.type as resource_type,
  protoPayload.resourceName as resource_name,
  protoPayload.methodName as operation
FROM
  `project.audit_logs.cloudaudit_googleapis_com_activity_*`
WHERE
  protoPayload.methodName LIKE '%delete%'
  AND _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY))
ORDER BY timestamp DESC;
```

From [BigQuery Audit Logs Pipelines & Analysis](https://cloud.google.com/blog/products/data-analytics/bigquery-audit-logs-pipelines-analysis) (accessed 2025-02-03):
- Use table wildcards (`_*`) for querying across date-partitioned tables
- Filter by `_TABLE_SUFFIX` to limit scan cost
- Create materialized views for frequently accessed patterns

### Log Analytics (Built-in Tool)

**Purpose**: Dedicated analysis tool for Cloud Logging data

From [Cloud Logging Overview](https://cloud.google.com/logging) (accessed 2025-02-03):
- Log Analytics provides SQL interface to logs without export
- Automatically indexes log fields for fast queries
- Supports Log Analytics on _Required bucket (400-day retention)

**Features**:
- SQL-compatible query language
- Field-level indexing (faster than BigQuery for log queries)
- Histogram visualizations
- Built-in compliance templates

**Example Query** (Log Analytics):
```sql
SELECT
  timestamp,
  jsonPayload.user,
  jsonPayload.action,
  resource.labels.project_id
FROM
  `_Required`
WHERE
  timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)
  AND jsonPayload.action = "SetIamPolicy"
ORDER BY timestamp DESC;
```

### Security Command Center Integration

**Purpose**: Centralized security and compliance dashboard

From [GCP Security Command Center](https://cloud.google.com/security-command-center) (accessed 2025-02-03):
- Built-in audit log analysis for security findings
- Automatically detects policy violations
- Integrates with Cloud Audit Logs for context

**Key Features**:
- **Asset Inventory**: Track all resources across organization
- **Security Findings**: Detect misconfigurations, vulnerabilities
- **Compliance Reporting**: Pre-built reports for SOC 2, PCI-DSS, HIPAA
- **Event Threat Detection**: Analyze audit logs for anomalies

**Automation** (Python SDK):
```python
from google.cloud import securitycenter

def get_compliance_findings(organization_id):
    """Retrieve compliance findings from Security Command Center"""
    client = securitycenter.SecurityCenterClient()

    org_name = f"organizations/{organization_id}"
    findings = client.list_findings(
        parent=f"{org_name}/sources/-",
        filter='category="AUDIT_LOGGING_DISABLED" OR category="IAM_POLICY_VIOLATION"'
    )

    return [
        {
            "finding": f.finding.name,
            "category": f.finding.category,
            "severity": f.finding.severity,
            "resource": f.finding.resource_name
        }
        for f in findings
    ]
```

### Third-Party Analysis Tools

From [Datadog GCP Audit Logs Integration](https://docs.datadoghq.com/integrations/google-cloud-audit-logs/) (accessed 2025-02-03):
- Datadog, Splunk, Elastic can ingest GCP audit logs
- Pub/Sub → Third-party SIEM for real-time analysis
- Cross-cloud correlation (AWS CloudTrail + GCP Audit Logs)

**Export to Pub/Sub** (for SIEM integration):
```bash
gcloud logging sinks create audit-to-siem \
  pubsub.googleapis.com/projects/PROJECT_ID/topics/audit-logs-topic \
  --log-filter='logName:"cloudaudit.googleapis.com"'
```

---

## Section 4: Retention Policies (~100 lines)

### Default Retention Periods

From [Cloud Audit Logs Overview](https://docs.cloud.google.com/logging/docs/audit) (accessed 2025-02-03):

| Log Type | Default Retention | Can Configure? | Cost |
|----------|------------------|----------------|------|
| Admin Activity | 400 days | Yes (1-3650 days) | Free up to 400 days |
| Data Access | 30 days | Yes (1-3650 days) | Charged after 30 days |
| System Event | 400 days | Yes (1-3650 days) | Free up to 400 days |
| Policy Denied | 30 days | Yes (1-3650 days) | Charged after 30 days |

### Configuring Custom Retention

**Via Console**:
1. Go to Cloud Logging → Logs Storage
2. Select bucket (_Default or _Required)
3. Edit retention settings (1-3650 days)

**Via gcloud**:
```bash
# Set 2-year retention for _Required bucket
gcloud logging buckets update _Required \
  --location=global \
  --retention-days=730

# Set 1-year retention for _Default bucket
gcloud logging buckets update _Default \
  --location=global \
  --retention-days=365
```

**Via Terraform**:
```hcl
resource "google_logging_project_bucket_config" "audit_retention" {
  project        = "my-project"
  location       = "global"
  retention_days = 365
  bucket_id      = "_Required"
}
```

From [Best Practices for Cloud Audit Logs](https://docs.cloud.google.com/logging/docs/audit/best-practices) (accessed 2025-02-03):
- **Compliance requirements** dictate retention:
  - SOC 2: 1 year minimum
  - PCI-DSS: 1 year online, 3 years archive
  - HIPAA: 6 years
  - SEC 17a-4: 7 years immutable
- Set retention to meet **strictest** applicable regulation

### Long-Term Archival Strategies

**Strategy 1: Export to Cloud Storage** (lowest cost):
```bash
# Create GCS bucket with lifecycle policy
gsutil mb -c ARCHIVE -l us-central1 gs://audit-logs-archive

# Set lifecycle to move to Coldline after 90 days
cat > lifecycle.json <<EOF
{
  "lifecycle": {
    "rule": [
      {
        "action": {"type": "SetStorageClass", "storageClass": "COLDLINE"},
        "condition": {"age": 90}
      }
    ]
  }
}
EOF
gsutil lifecycle set lifecycle.json gs://audit-logs-archive

# Create log sink
gcloud logging sinks create audit-archive \
  storage.googleapis.com/audit-logs-archive \
  --log-filter='logName:"cloudaudit.googleapis.com"'
```

**Cost Comparison** (per GB/month):
- Cloud Logging storage (30-400 days): $0.50/GB
- BigQuery storage: $0.02/GB (active), $0.01/GB (long-term)
- GCS Standard: $0.020/GB
- GCS Nearline: $0.010/GB
- GCS Coldline: $0.004/GB
- GCS Archive: $0.0012/GB

**Strategy 2: BigQuery with Table Expiration**:
```sql
-- Create dataset with default table expiration
CREATE SCHEMA audit_logs_hot
OPTIONS (
  default_table_expiration_ms = 31536000000  -- 1 year in milliseconds
);

-- Archive old logs to cold dataset
INSERT INTO audit_logs_cold.cloudaudit_googleapis_com_activity
SELECT * FROM audit_logs_hot.cloudaudit_googleapis_com_activity_*
WHERE _TABLE_SUFFIX < FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 365 DAY));
```

From [View Logs Routed to BigQuery](https://docs.cloud.google.com/logging/docs/export/bigquery) (accessed 2025-02-03):
- BigQuery automatic 90-day long-term storage pricing (50% discount)
- Use table expiration to automatically delete old data
- Archive to GCS for >2 year retention

### Immutable Log Storage (SEC 17a-4 Compliance)

**GCS Bucket Lock** (cannot delete/modify for retention period):
```bash
# Create bucket with retention policy
gsutil mb gs://immutable-audit-logs

# Set 7-year retention (SEC 17a-4)
gsutil retention set 7y gs://immutable-audit-logs

# Lock the retention policy (CANNOT BE REVERSED)
gsutil retention lock gs://immutable-audit-logs
```

**Verification**:
```bash
gsutil retention get gs://immutable-audit-logs
# Output:
# Retention Policy (LOCKED):
#   Duration: 7 Year(s)
#   Effective Time: 2025-02-03T00:00:00Z
```

From [GCS Retention Policies](https://cloud.google.com/storage/docs/bucket-lock) (accessed 2025-02-03):
- Bucket Lock prevents deletion until retention period expires
- Complies with SEC Rule 17a-4(f) for financial records
- Cannot unlock even with admin/owner permissions

### Retention Compliance Monitoring

**Automated Checks** (Python):
```python
from google.cloud import storage
from google.cloud import logging_v2
from datetime import datetime, timedelta

def verify_retention_compliance(project_id, required_days=365):
    """Check all log buckets meet minimum retention"""
    client = logging_v2.services.config_service_v2.ConfigServiceV2Client()

    parent = f"projects/{project_id}/locations/global"
    buckets = client.list_buckets(parent=parent)

    non_compliant = []
    for bucket in buckets:
        if bucket.retention_days < required_days:
            non_compliant.append({
                "bucket": bucket.name,
                "current_retention": bucket.retention_days,
                "required_retention": required_days
            })

    return {
        "compliant": len(non_compliant) == 0,
        "violations": non_compliant
    }

def verify_archive_bucket_lock(bucket_name):
    """Check GCS bucket has locked retention policy"""
    client = storage.Client()
    bucket = client.get_bucket(bucket_name)

    if not bucket.retention_policy_locked:
        return {
            "compliant": False,
            "reason": "Retention policy not locked"
        }

    retention_years = bucket.retention_period / (365.25 * 24 * 3600)
    if retention_years < 7:
        return {
            "compliant": False,
            "reason": f"Retention only {retention_years:.1f} years (need 7)"
        }

    return {"compliant": True}
```

---

## Section 5: Export Strategies (~50 lines)

### Export Destinations

From [Cloud Logging Sinks](https://cloud.google.com/logging/docs/export) (accessed 2025-02-03):

**Supported Destinations**:
1. **BigQuery**: SQL analysis, dashboards, joins with other data
2. **Cloud Storage**: Long-term archive, lowest cost
3. **Pub/Sub**: Real-time streaming to SIEM, alerting
4. **Another Cloud Logging bucket**: Cross-project aggregation

**Destination Selection Criteria**:
- **BigQuery**: Active analysis, compliance reporting, dashboards
- **GCS**: >2 year retention, immutable audit trail, regulatory archive
- **Pub/Sub**: Real-time monitoring, SIEM integration, security automation
- **Logging bucket**: Centralized multi-project logs, unified retention

### Creating Export Sinks

**Example: Multi-Destination Strategy**:
```bash
# 1. Hot analysis (BigQuery) - 1 year retention
gcloud logging sinks create audit-hot \
  bigquery.googleapis.com/projects/PROJECT_ID/datasets/audit_logs_hot \
  --log-filter='logName:"cloudaudit.googleapis.com"'

# 2. Long-term archive (GCS) - 7 year retention
gcloud logging sinks create audit-archive \
  storage.googleapis.com/audit-logs-archive-7y \
  --log-filter='logName:"cloudaudit.googleapis.com"'

# 3. Real-time security (Pub/Sub) - SIEM integration
gcloud logging sinks create audit-siem \
  pubsub.googleapis.com/projects/PROJECT_ID/topics/security-events \
  --log-filter='logName:"cloudaudit.googleapis.com" AND (protoPayload.methodName:"SetIamPolicy" OR severity="ERROR")'
```

### Sink Filters for Selective Export

**Cost Optimization**: Export only necessary logs

```bash
# Export only high-risk operations
--log-filter='
  logName:"cloudaudit.googleapis.com" AND (
    protoPayload.methodName:"delete" OR
    protoPayload.methodName:"SetIamPolicy" OR
    severity="ERROR" OR
    protoPayload.status.code!=0
  )
'

# Export only specific services
--log-filter='
  logName:"cloudaudit.googleapis.com" AND
  resource.type=("gce_instance" OR "bigquery_dataset" OR "gcs_bucket")
'

# Exclude service accounts (reduce noise)
--log-filter='
  logName:"cloudaudit.googleapis.com" AND
  NOT protoPayload.authenticationInfo.principalEmail=~".*@.*\.iam\.gserviceaccount\.com"
'
```

From [How to Export Google Cloud Logs](https://medium.com/google-cloud/how-to-export-google-cloud-logs-c0441aa60d45) (accessed 2025-02-03):
- Filters reduce export volume and cost
- Test filters in Logs Explorer before creating sink
- Use `NOT` to exclude service account noise

### Centralized Multi-Project Logging

**Organization-Level Sink** (aggregate all projects):
```bash
gcloud logging sinks create org-audit-central \
  bigquery.googleapis.com/projects/CENTRAL_PROJECT/datasets/org_audit_logs \
  --organization=ORG_ID \
  --include-children \
  --log-filter='logName:"cloudaudit.googleapis.com"'

# Grant necessary permissions
gcloud projects add-iam-policy-binding CENTRAL_PROJECT \
  --member="serviceAccount:SINK_SERVICE_ACCOUNT" \
  --role="roles/bigquery.dataEditor"
```

**Benefits**:
- Single dataset for cross-project analysis
- Unified retention policy enforcement
- Simplified compliance reporting
- Reduced operational overhead

From [Automating BigQuery Audit Log Collection to a Centralized Dataset](https://www.cloudkeeper.com/insights/blog/automating-bigquery-audit-log-collection-centralized-bigquery-dataset) (accessed 2025-02-03):
- Organization-level sinks propagate to all current and future projects
- Centralized analysis enables detecting cross-project attack patterns
- Single compliance view across entire organization

---

## Sources

### GCP Official Documentation

1. [Cloud Audit Logs Overview](https://docs.cloud.google.com/logging/docs/audit) - Google Cloud Documentation (accessed 2025-02-03)
2. [Best Practices for Cloud Audit Logs](https://docs.cloud.google.com/logging/docs/audit/best-practices) - Google Cloud Documentation (accessed 2025-02-03)
3. [Enable Data Access Audit Logs](https://docs.cloud.google.com/logging/docs/audit/configure-data-access) - Google Cloud Documentation (accessed 2025-02-03)
4. [Resource Manager Audit Logging](https://cloud.google.com/resource-manager/docs/audit-logging) - Google Cloud Documentation (accessed 2025-02-03)
5. [BigQuery Audit Logs Overview](https://docs.cloud.google.com/bigquery/docs/reference/auditlogs) - Google Cloud Documentation (accessed 2025-02-03)
6. [View Logs Routed to BigQuery](https://docs.cloud.google.com/logging/docs/export/bigquery) - Google Cloud Documentation (accessed 2025-02-03)
7. [Cloud Logging Overview](https://cloud.google.com/logging) - Google Cloud Product Page (accessed 2025-02-03)
8. [GCS Retention Policies and Bucket Lock](https://cloud.google.com/storage/docs/bucket-lock) - Google Cloud Documentation (accessed 2025-02-03)
9. [Cloud Logging Sinks](https://cloud.google.com/logging/docs/export) - Google Cloud Documentation (accessed 2025-02-03)

### Technical Blogs & Guides

10. [Understanding Stackdriver Audit Logs](https://medium.com/google-cloud/understanding-stackdriver-audit-logs-3879473de82a) - Medium/Google Cloud, Yuri Grinshteyn (accessed 2025-02-03)
11. [BigQuery Audit Logs Pipelines & Analysis](https://cloud.google.com/blog/products/data-analytics/bigquery-audit-logs-pipelines-analysis) - Google Cloud Blog (accessed 2025-02-03)
12. [Best Practices for Monitoring GCP Audit Logs](https://www.datadoghq.com/blog/monitoring-gcp-audit-logs/) - Datadog Blog (accessed 2025-02-03)
13. [How to Export Google Cloud Logs](https://medium.com/google-cloud/how-to-export-google-cloud-logs-c0441aa60d45) - Medium/Google Cloud, minherz (accessed 2025-02-03)
14. [How to Save Google Cloud Logs in BigQuery](https://python.plainenglish.io/how-to-save-google-cloud-logs-in-bigquery-b4e2cbd4dd58) - Python in Plain English (accessed 2025-02-03)

### Compliance & Security Resources

15. [GCP Audit Logs Detection - DSPM Guides](https://www.dspmguides.com/guides/detect-audit-logs-gcp/) - DSPM Guides (accessed 2025-02-03)
16. [Automating BigQuery Audit Log Collection to a Centralized Dataset](https://www.cloudkeeper.com/insights/blog/automating-bigquery-audit-log-collection-centralized-bigquery-dataset) - CloudKeeper (accessed 2025-02-03)
17. [Google Cloud Platform (GCP) Audit Log](https://searchinform.com/articles/cybersecurity/measures/log-management/gcp-audit-log/) - SearchInform (accessed 2025-02-03)
18. [Datadog GCP Audit Logs Integration](https://docs.datadoghq.com/integrations/google-cloud-audit-logs/) - Datadog Documentation (accessed 2025-02-03)

### Additional References

19. [Cloud Security Audit: A Complete Guide in 2025](https://qualysec.com/cloud-security-audit/) - Qualysec (accessed 2025-02-03)
20. [Why You Need to Enable Audit Logs in Google Cloud](https://medium.com/google-cloud/why-you-need-to-enable-audit-logs-in-google-cloud-0c31578cd4f1) - Medium/Google Cloud, Allan Alfonso (accessed 2025-02-03)
