---
sourceFile: "Exploiting DeepSeek-R1: Breaking Down Chain of Thought Security"
exportedBy: "Kortex"
exportDate: "2025-10-29T02:36:27.561Z"
---

# Exploiting DeepSeek-R1: Breaking Down Chain of Thought Security

b400bf54-bb45-46ba-9a49-d7cff5eeb413

Exploiting DeepSeek-R1: Breaking Down Chain of Thought Security

ea2ebd20-009a-457e-a6b5-150a8649ef1f

https://cache.pressmailing.net/content/1bef853b-f910-499d-bf90-82f032f40234/TrendMicro_Exploitin~hain%20of%20Thought.pdf

Exploiting DeepSeek-R1: Breaking Down Chain of Thought Security

 DeepSeek-R1 uses Chain of Thought (CoT) reasoning, explicitly sharing its step-by-step thought process, which we found was exploitable for prompt attacks.

 Prompt attacks can exploit the transparency of CoT reasoning to achieve malicious objectives, similar to phishing tactics, and can vary in impact depending on the context.

 We used tools like NVIDIA’s Garak to test various attack techniques on DeepSeek-R1, where we discovered that insecure output generation and sensitive data theft had higher success rates due to the CoT exposure.

 To mitigate the risk of prompt attacks, it is recommended to filter out <think> tags from LLM responses in chatbot applications and employ red teaming strategies for ongoing vulnerability assessments and defenses.

Welcome to the inaugural article in a series dedicated to evaluating AI models. In this entry, we’ll examine the release of deepseek-r1. The growing usage of chain-of-thought reasoning marks a new era for large language models. Chain-of-thought reasoning encourages the model to think through its answer before the final response. A distinctive feature of deepseek-r1 is that this chain of thought is shared directly with you. We perform a series of prompt attacks against 671 billion parameter deepseek-r1. We found that we can leverage  this additional information to increase our attack success rate.

## Chain of Thought reasoning

CoT reasoning encourages a model to take a series of intermediate steps before arriving at a final response. This approach has been shown to enhance the performance of large models on math-focused benchmarks, such as the GSM8K dataset for word problems.

CoT has become a cornerstone for state-of-the-art reasoning models, including OpenAI’s O1 and O3-mini plus DeepSeek-R1, all of which are trained to employ CoT reasoning.

A notable characteristic of the Deepseek-R1 model is that it explicitly shows its reasoning process within the <think> </think> tags included in response to a prompt.

https://lh3.googleusercontent.com/notebooklm/AG60hOrm8nDgTDYSO8Cdq3y6D-bTuPEvo44Dzj_zJ9Mfu3uhb-gWsy5-yIqkl7AuQ-ihCTsdsDvJNwyEImprA7-tUb_3qfrXxibtS9zivGibMJMG7s6KS-91cjvPMlblAVOWEM68Fbxmyg=w1952-h988-v0

37636041-6da9-442e-b2ec-03da6fdc3606

https://lh3.googleusercontent.com/notebooklm/AG60hOpm-2r9y6govQG7KQboUr9pdO2W0vHKS3QXADKZwkqlTzOeeiMeYQVfIgLR6KHsRihFas4uSagDCkIRaMCcXXMEC8CzeLjIYZkCkXdWBAfgs52Y_vULfSC0lVy2lfNg1RrqFa87=w776-h209-v0

28e95562-ba15-4558-97fd-9730e218bb3e

Figure 1. Deepseek-R1 providing its reasoning process

Prompt attacks  A prompt attack is when an attacker crafts and sends prompts to an LLM to achieve a malicious objective. These prompt attacks can be broken down into two parts, the attack technique, and the attack objective.

Figure 2. Tricking the LLM into revealing its system prompt

In the example above, the attack is attempting to trick the LLM into revealing its system prompt, which are a set of overall instructions that define how the model should behave. Depending on the system context, the impact of revealing the system prompt can vary. For example, within an agent-based AI system, the attacker can use this technique to discover all the tools available to the agent.

https://lh3.googleusercontent.com/notebooklm/AG60hOqMghaCaFho2rZ2ibVOUCmtKB5OMWkj2RmT9GLC42X5cwjIsehq9OXOApdekc_hfUMuW0KlJ7t_fDd6SDXgvWJw_KQH96dwRhLu8_qfgEkfP2tfDMYHpaipYgnIjwqPSjIYc-Oh=w879-h432-v0

e531a256-a4c9-4d96-8ab0-6afe35b492e6

Figure 3. A sample AI model’s system prompt

The process of developing these techniques mirrors that of an attacker searching for ways to trick users into clicking on phishing links. Attackers identify methods that bypass system guardrails and exploit them until defenses catch up—creating an ongoing cycle of adaptation and countermeasures.

Given the expected growth of agent-based AI systems, prompt attack techniques are expected to continue to evolve, posing an increasing risk to organizations. A notable example occurred with Google’s Gemini integrations, where researchers discovered that indirect prompt injection could lead the model to generate phishing links.

Red-teaming DeepSeek-R1

We used open-source red team tools such as NVIDIA’s Garak —designed to identify vulnerabilities in LLMs by sending automated prompt attacks—along with specially crafted prompt attacks to analyze DeepSeek-R1’s responses to various attack techniques and objectives.

https://lh3.googleusercontent.com/notebooklm/AG60hOrEarYdXDGScnDd69GIcsEP_uMdWFJ3nD--FNBKQEQVnl7M3ntYXAoiBLHyaBngUbirJ7UWlxTWJL5a1rkQGYVwXbhKgoVLhtjN_xKrGz9-uOfciPJD5m1zwaCwkCOF1zNfGjP6WQ=w1629-h454-v0

b1149ecd-909c-45d6-9d2e-99644aa66c4e

Figure 4. Attack objectives and the techniques performed against DeepSeek-R1

Prompt AttacksThe following tables show the attack techniques and objectives we used during our investigation. We also included their IDs based on OWASP’s 2025 Top 10 Risk & Mitigations for LLMs and Gen AI Apps and MITRE ATLAS.

## Name OWASP ID MITRE ATLAS ID

Prompt injection LLM01:2025 – Prompt Injection AML.T0051 – LLM Prompt Injection

Jailbreak LLM01:2025 – Prompt Injection AML.T0054 – LLM Jailbreak

Table 1. Attack techniques and their corresponding risk classifications under the OWASP and MITRE ATLAS indices

## Name OWASP ID MITRE ATLAS ID

Jailbreak LLM01:2025 – Prompt Injection AML.T0054 – LLM Jailbreak

Model theft  AML.T0048.004 – External Harms: ML Intellectual Property Theft

Package hallucination LLM09:2025 – Misinformation AML.T0062 – Discover LLM Hallucinations

Sensitive data theft LLM02:2025 – Sensitive Information Disclosure

AML.T0057 – LLM Data Leakage

Insecure output generation LLM05:2025 – Improper Output Handling

AML.T0050 – Command and Scripting Interpreter

Toxicity  AML.T0048 – External Harms

Table 2. Attack objectives and their corresponding risk classifications under the OWASP and MITRE ATLAS indices

https://lh3.googleusercontent.com/notebooklm/AG60hOqq8ceA3trT447pG7a0Sq2OLtaWSaI23rfadXcJVsD1Q7HHnZxBW1W3JlJplQOAxJNejb78UhFaDSz7yfPwOtcMm0CFHS2TW2TR_fAncXRK7j0BRbrhf9kc3zF-b0noU_CHekUmSg=w879-h551-v0

cb169bb9-bce6-4e1f-82c4-60f0d56e9dea

## Stealing secrets

Sensitive information should never be included in system prompts. However, a lack of security awareness can lead to their unintentional exposure. In this example, the system prompt contains a secret, but a prompt hardening defense technique is used to instruct the model not to disclose it.

As seen below, the final response from the LLM does not contain the secret. However, the secret is clearly disclosed within the <think> tags, even though the user prompt does not ask for it.  To answer the question the model searches for context in all its available information in an attempt to interpret the user prompt successfully. Consequently, this results in the model using the API speci ication to craft the HTTP request required to answer the user's question. This inadvertently results in the API key from the system prompt being included in its chain-of-thought.

https://lh3.googleusercontent.com/notebooklm/AG60hOqU68ka-OJnMxNMRcnJzl1JHdjKarXlW4m_7S8S_mkA9KVyOdJ80DIX4luJg7_FtK1PZBhr3uBNRpJAUkvbC5psan2K7_VnlPCYQMPZnHvPKwAu2MpqE2fBrp8-QAvFiGHXRId_=w899-h680-v0

a82280b5-3670-495a-8b77-2ba267f0ef2c

Figure 5. A secret being exposedin DeepSeek-R1's CoT

## Discovering attack methods using CoT

In this section, we demonstrate an example of how to exploit the exposed CoT through a discovery process. First, we attempted to directly ask the model to achieve our goal:

https://lh3.googleusercontent.com/notebooklm/AG60hOoJiGcC29MOzhwIT97QbhuqZbTWYtYdkuIE9Kg0eoeXtu4LCfSiqdn6u_iNFvZgtPZoAyxntT_MqMGW1fim6ZwFxE9n3my9QTQspgw0BCUeM7q0CWybm79LEQhiyKR4E0spjPtJiA=w1080-h575-v0

487316e0-e73f-4fd3-a4a2-45620d69fc5b

https://lh3.googleusercontent.com/notebooklm/AG60hOr7hlaMm39Y-UeJ7MS-BWH-ZK-HShaenziReY473tpncIF5hmXuRzLaRgMFT5h0M49eIcrItbOVM4IunslBFQf65dOD13VlStiGtILDS073ldbWDrsGdQ0iTPKk0niTHgjTg6DXjw=w857-h284-v0

887ebd64-74d2-47b3-bf1f-8681063c1837

Figure 6. Directly asking the model for sensitive information

When the model denied our request, we then explored its guardrails by directly inquiring about them.

Figure 7. Asking the model about its guardrails

The model appears to have been trained to reject impersonation requests. We can further inquire about its thought process regarding impersonation.

https://lh3.googleusercontent.com/notebooklm/AG60hOojS3dSj_zRKpX5TlxswqU-lpKvjew124pBMYv9qNOmK604d6iuzTdxucP223qluaksEqL33VUUfaUbaDxl0rSXhiJ56dE5WOALJNNA3_f4M1dMeiYPj7kpl58irPNPouMM3CyA=w1003-h505-v0

cb313af8-b863-4630-98d7-5e254fdbcbb7

Figure 8. Finding a loophole in the model’s reasoning

With these exceptions noted in the <think> tag, we can now craft an attack to bypass the guardrails to achieve our goal (using payload splitting).

https://lh3.googleusercontent.com/notebooklm/AG60hOpiriBAZKK79a4su1zt1xI2cLDvOgpf5c8F6kFBxGkcAlJl0ngD3LVNDV4Hp6rL4LEvUT0i1l5jdxPIXp1yOvKJaszLkzFBsvd2c0LLo8W4or8klaxJbATl37vpLXCHhTGqUGwh=w1178-h721-v0

648de57c-9a17-48a0-b5c4-816cd5901931

https://lh3.googleusercontent.com/notebooklm/AG60hOpXeNv2kKe2UfoUOoy_jtLl2EeXbZ8kLdOE9_5IdJ3yAXowuZVU5NuD9wEXEmjiB98mu8HPlbV7yBJneYVI0NILDez8x2SephWQHxd8n9t7TStXzjsQo_JXV_padWK9rZnDv34k=w1178-h721-v0

e6026ad8-4248-4b39-8e00-432ed3de79d0

https://lh3.googleusercontent.com/notebooklm/AG60hOqibViWfgOrGNjch2MdT8uUH0GUdM-tS5eogY9aTLOMb8YXGIri2jEdSWiTH9xcaPbo-tcXhCjlqtOmkNBNL1s3QewA1mgCbJaThaQ7dntDq0yf1f0sfeCsh2jhTd9W0-bmR9O7kw=w1178-h721-v0

2ec5cc8c-b654-4ff9-8685-2d6c374e6ff0

Figure 9. The attack scenario

## Attack success rate

We used NVIDIA Garak to assess how di erent attack objectives perform against DeepSeek-R1. Our findings indicate a higher attack success rate in the categories of insecure output generation and sensitive data theft compared to toxicity, jailbreak, model theft, and package hallucination. We suspect this discrepancy may be influenced by the presence of <think> tags in the model's responses. However, further research is needed to confirm this, and we plan to share our findings in the future.

https://lh3.googleusercontent.com/notebooklm/AG60hOrkefJ5uQp18PXv9nx4cNHNunCONPOMuvOkCl-e5PUKsfCl6kzFgcWttgbVA3ptmlAWj0PgoI_q16mJyypUjoe0jS5UiRowXCV-CxKi5WkJp7aVKDYgEKtv-MmL-pSrLKLQ54aE=w1952-h976-v0

b0214427-2c48-49a9-8b8f-331c183af18a

https://lh3.googleusercontent.com/notebooklm/AG60hOomE0Rh4hQ0rsdo3Vc_m-N9wmOSXhJ_VbzvPFCsnUzZlsgs_ANdru0HsXNCTpZfRnkqALmH7kYzOB4rSmVu0JwyRusdlHZN_VjS0JNanWXC8hgZ4yydOJnCEVLqbafK47jo7Nu84A=w1952-h976-v0

c390522d-1cbc-4e2e-bf2e-f84e92dc5ba3

Figure 10. Garak attack success rate broken down per attack objective

Defending against prompt attacks Our research indicates that the content within <think> tags in model responses can contain valuable information for attackers. Exposing the model’s CoT increases the risk of threat actors discovering and refining prompt attacks to achieve malicious objectives. To mitigate this, we recommend filtering <think> tags from model responses in chatbot applications.

Additionally, red teaming is a crucial risk mitigation strategy for LLM-based applications. In this article, we demonstrated an example of adversarial testing and highlighted how tools like NVIDIA’s Garak can help reduce the attack surface of LLMs. We are excited to continue sharing our research as the threat landscape evolves. In the coming months, we plan to evaluate a wider range of models, techniques, and objectives to provide deeper insights.

