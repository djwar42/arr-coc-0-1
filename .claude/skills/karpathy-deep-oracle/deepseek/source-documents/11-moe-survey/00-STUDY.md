# MoE Survey - Study

**Source**: arXiv (A Survey on Mixture of Experts in Large Language Models)
**Date Processed**: 2025-10-28
**Category**: Mixture of Experts (Survey Paper)

---

## üìù Summary

Comprehensive survey of MoE techniques in LLMs.

**Covers**:
- Historical evolution of MoE architectures
- Routing mechanisms (top-K, learned routing)
- Load balancing strategies (auxiliary loss, dynamic routing)
- Expert specialization patterns
- Training and inference optimizations
- Comparison of major MoE models (Switch, GLaM, GShard, DeepSeek)

**Value**: Broad context for understanding DeepSeek's MoE choices

**Complements**: [DeepSeekMoE Paper](../03-deepseek-moe-paper/00-STUDY.md) with landscape view

---

## üîó Cross-References

- [DeepSeekMoE Paper](../03-deepseek-moe-paper/00-STUDY.md) - DeepSeek's specific approach
- [Aux-Loss-Free](../08-aux-loss-free-balancing/00-STUDY.md) - V3's balancing innovation
- `knowledge-categories/model-architectures/` - MoE in DeepSeek context

---

**Last Updated**: 2025-10-28
**Status**: Reference survey
