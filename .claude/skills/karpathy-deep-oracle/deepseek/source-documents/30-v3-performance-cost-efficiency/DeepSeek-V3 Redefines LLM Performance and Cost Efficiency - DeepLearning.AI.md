---
sourceFile: "DeepSeek-V3 Redefines LLM Performance and Cost Efficiency - DeepLearning.AI"
exportedBy: "Kortex"
exportDate: "2025-10-29T02:36:21.157Z"
---

# DeepSeek-V3 Redefines LLM Performance and Cost Efficiency - DeepLearning.AI

04114d13-b3f8-4e9c-be4d-9b664743936c

DeepSeek-V3 Redefines LLM Performance and Cost Efficiency - DeepLearning.AI

a0e4e131-e0fc-4685-b371-7a17c24ff5fe

https://www.deeplearning.ai/the-batch/deepseek-v3-redefines-llm-performance-and-cost-efficiency/

✨ New course! Enroll in

Fine-tuning and Reinforcement Learning for LLMs: Intro to Post-Training

https://bit.ly/4ofRjka

## Start Learning

https://bit.ly/4ofRjka

## Weekly Issues

https://bit.ly/4ofRjka

Andrew's Letters

https://bit.ly/4ofRjka

## Data Points

https://bit.ly/4ofRjka

## ML Research

https://bit.ly/4ofRjka

https://bit.ly/4ofRjka

https://bit.ly/4ofRjka

https://bit.ly/4ofRjka

https://bit.ly/4ofRjka

https://bit.ly/4ofRjka

https://bit.ly/4ofRjka

DeepSeek Ups the Open Weights Ante  DeepSeek-V3 redefines LLM performance and cost efficiency

## Machine Learning Research

https://bit.ly/4ofRjka

Large Language Models (LLMs)

https://bit.ly/4ofRjka

Jan 15, 2025

https://www.deeplearning.ai/the-batch/tag/jan-15-2025/

Reading time 3  min read Share

A new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.

What’s new:

DeepSeek-V3

https://www.deeplearning.ai/the-batch/tag/jan-15-2025/

is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights are

https://huggingface.co/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9k2ZCBDjArTAqDDbVQ8kUKR4VL6qLhcv55srL7EFI_zDr0s_AJ-odFdqhfOtqDLCXKVBeP

except for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download them

https://huggingface.co/deepseek-ai/DeepSeek-V3/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9k2ZCBDjArTAqDDbVQ8kUKR4VL6qLhcv55srL7EFI_zDr0s_AJ-odFdqhfOtqDLCXKVBeP

Mixture of experts (MoE) basics:

The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.

How it works:

DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours — less than 1/10 the

time required to train Llama 3.1 405B

https://build.nvidia.com/meta/llama-3_1-405b-instruct/modelcard?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9k2ZCBDjArTAqDDbVQ8kUKR4VL6qLhcv55srL7EFI_zDr0s_AJ-odFdqhfOtqDLCXKVBeP

, which DeepSeek-V3 substantially outperforms — at an extraordinarily low cost of $5.6 million.

The developers trained it on roughly 15 trillion tokens, including a larger percentage of coding and math data relative to DeepSeek-V2. They fine-tuned it on a wide variety of tasks using output generated by

DeepSeek-R1

https://www.deeplearning.ai/the-batch/deepseek-r1-a-transparent-challenger-to-openai-o1/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9k2ZCBDjArTAqDDbVQ8kUKR4VL6qLhcv55srL7EFI_zDr0s_AJ-odFdqhfOtqDLCXKVBeP

and DeepSeek-V2.5. They further sharpened its performance across diverse domains using the reinforcement learning algorithm known as

group relative policy optimization

https://arxiv.org/abs/2402.03300?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9k2ZCBDjArTAqDDbVQ8kUKR4VL6qLhcv55srL7EFI_zDr0s_AJ-odFdqhfOtqDLCXKVBeP

https://arxiv.org/abs/2404.19737?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9k2ZCBDjArTAqDDbVQ8kUKR4VL6qLhcv55srL7EFI_zDr0s_AJ-odFdqhfOtqDLCXKVBeP

showed that training to predict the next two tokens would improve performance over learning to predict just one. The authors implemented this procedure. The model learned to predict the first token as usual and used an additional set of layers to learn to predict the second token. The additional layers aren’t used at inference.

DeepSeek-V2

https://arxiv.org/abs/2405.04434?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9k2ZCBDjArTAqDDbVQ8kUKR4VL6qLhcv55srL7EFI_zDr0s_AJ-odFdqhfOtqDLCXKVBeP

, DeepSeek-V3 uses multi-head latent attention, which saves memory during execution relative to other variants of attention.

Also like DeepSeek-V2, the new model combines dedicated (routed) and shared experts. The model chooses eight of 256 experts for a particular input, but it also uses a shared expert that processes all inputs.

In DeepSeek’s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.

DeepSeek-V3 showed exceptional performance in coding and math tasks. In coding, DeepSeek-V3 dominated in five of the seven benchmarks tested. However, DeepSeek-V3 lost to o1 on one of the five, according to a public leaderboard. Specifically, on

https://aider.chat/docs/leaderboards/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9k2ZCBDjArTAqDDbVQ8kUKR4VL6qLhcv55srL7EFI_zDr0s_AJ-odFdqhfOtqDLCXKVBeP

, which tests a model’s ability to generate code in response to difficult requests in multiple programming languages, DeepSeek-V3 (48.5 percent accuracy) beat Claude Sonnet 3.5 (45.3 percent accuracy), though it lost to o1 (61.7 percent accuracy).

In language tasks, it performed neck-and-neck with Claude 3.5 Sonnet, achieving higher scores in some tasks and lower in others.

Behind the news:

OpenAI’s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.

Why it matters:

Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3’s shockingly low training cost.

The team doesn’t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits “meticulous engineering optimizations.”) But it’s likely that DeepSeek’s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn’t employ MoE. In 2022,

https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9k2ZCBDjArTAqDDbVQ8kUKR4VL6qLhcv55srL7EFI_zDr0s_AJ-odFdqhfOtqDLCXKVBeP

found that MoE cost five times less in training for equal performance compared to a dense model, and

https://arxiv.org/pdf/2112.06905?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9k2ZCBDjArTAqDDbVQ8kUKR4VL6qLhcv55srL7EFI_zDr0s_AJ-odFdqhfOtqDLCXKVBeP

https://arxiv.org/pdf/2112.10684?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9k2ZCBDjArTAqDDbVQ8kUKR4VL6qLhcv55srL7EFI_zDr0s_AJ-odFdqhfOtqDLCXKVBeP

reported that MoE achieved better performance than dense models trained on the same numbers of tokens.

We’re thinking:

If they can be replicated, DeepSeek’s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically.

## Subscribe to The Batch

## Stay updated with weekly AI News and Insights delivered to your inbox

