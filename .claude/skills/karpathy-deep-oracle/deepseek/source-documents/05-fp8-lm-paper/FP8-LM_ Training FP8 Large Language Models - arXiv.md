---
sourceFile: "FP8-LM: Training FP8 Large Language Models - arXiv"
exportedBy: "Kortex"
exportDate: "2025-10-29T02:36:28.337Z"
---

# FP8-LM: Training FP8 Large Language Models - arXiv

6e4e4b81-1662-4bd0-af40-92f1be475efd

FP8-LM: Training FP8 Large Language Models - arXiv

da4defeb-3a61-4397-a066-a303051bc7f4

https://arxiv.org/pdf/2310.18313

FP8-LM: Training FP8 Large Language Models

Houwen Peng ∗ Kan Wu ∗ Yixuan Wei ∗ Guoshuai Zhao Yuxiang Yang Ze Liu Yifan Xiong Ziyue Yang

Bolin Ni Jingcheng Hu Ruihang Li Miaosen Zhang Chen Li Jia Ning Ruizhe Wang Zheng Zhang Shuguang Liu Joe Chau Han Hu † Peng Cheng †

## Microsoft Azure and Microsoft Research

In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 39% reduction in real memory usage but also ran 75% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 37%. This largely reduces the training costs for large foundation models. Fur-thermore, our FP8 mixed-precision training methodology is generic. It can be seamlessly applied to other tasks such as LLM instruction tuning and reinforcement learning with human feedback, offering savings in fine-tuning expenses. Our FP8 low-precision training framework is open-sourced at aka.ms/MS.AMP.

1 Introduction

Large language models (LLMs) (Brown et al., 2020; Smith et al., 2022; Chowdhery et al., 2022; Zhang et al., 2022) have demonstrated unprecedented capabilities in language comprehension and generation, leading to breakthroughs in reasoning, math, science, and many other tasks (OpenAI, 2023; Anil et al., 2023). However, training LLMs is extremely costly. For example, PaLM takes 6,144 TPUv4 chips to train a 540B model, while GPT-3 175B consumes several thousand petaflop/s-days of compute for pre-training (Chowdhery et al., 2022; Brown et al., 2020). This motivates the needs of reducing the training costs of LLMs, especially for the scaling of next-generation super-intelligent models. Low-precision training is one of the most promising directions to reduce the costs, as it can provide high speed, small memory footprint, and low communication overhead. Most existing training systems, e.g., Megatron-LM (Shoeybi et al., 2019), MetaSeq (Zhang et al., 2022), and Colossal-AI (Li et al., 2023a), train LLMs with either FP32 full-precision or FP16/BF16 mixed-precision by default. This is not essential, however,

Contributions for all the authors can be found in Section 5. * equal work † contact: {hanhu | pengc}@microsoft.com

0 200 400 600 800 1000 Model Size (B)

s BF16 Our FP8

Figure 1: An analysis of comparing the maximum model sizes attainable through the utilization of either the prevalent BF16 or our FP8 mixed-precision training approach on a cluster of Nvidia H100 GPUs with 80GB memory.

to achieve full accuracy for large models. With the release of Nvidia H100 GPU, FP8 is becoming the next-generation datatype for low-precision representation (Nvidia, 2022a; Micikevicius et al., 2022). Theoretically, FP8 can achieve 2× speed-up, 50% - 75% memory cost savings, and 50% - 75% communication savings compared with current 16-bit and 32-bit floating point mixed-precision training, which is very promising for scaling-up next-generation foundation models. Unfortunately, the current support for FP8 training is rare and limited. The only usable framework is the Nvidia Transformer Engine (TE) (Nvidia, 2022b), but it applies FP8 solely for GEMM computation and still retains master weights and gradients using high precision, e.g., FP16 or FP32. As a result, the end-to-end speed-up, memory and communication cost savings are very limited, which does not fully unveil the power of FP8. To address this issue, we propose an extremely optimized FP8 mixed-precision framework for LLM training. The core idea is to infiltrate FP8 compute, storage, and communication into the whole progress of large model training, making the forward and backward pass all used the low-precision FP8, thus largely reducing system workloads compared to previous frameworks (Micikevicius et al., 2017; Nvidia, 2022b; Micikevicius et al., 2022). Specifically, we design three optimization levels that utilize FP8 to streamlinemixed-precision and distributed training. The three levels gradually incorporate 8-bit collective communication, optimizer, and distributed parallel training in an incremental manner. The higher optimization level indicates using more FP8 during LLM training. Moreover, for large-scale training, such as GPT-175B trained on thousand of GPUs, our framework provides FP8 low-bit parallelism, including tensor, pipeline, and sequence parallelism, paving the way to next-generation low-precision parallel training. Training LLMs with FP8 is non-trivial. The challenges stem from issues such as data underflow or overflow, coupled with quantization errors arising from the narrower dynamic range and reduced precision inherent in FP8 data formats. These challenges cause numerical instabilities and irreversible divergences throughout the training process. To tackle them, we propose two techniques: precision decoupling and automatic scaling for preventing the loss of critical information. The former one involves decoupling the influence of data precision on parameters such as weights, gradients, optimizer states, and assigning reduced precision to components that are not precision sensitive. The latter one is to preserve gradient values within the representation range of FP8 data formats through the dynamic adjustment of tensor scaling factors, thereby alleviating underflow and overflow occurrences during all-reduce communication. To validate the proposed FP8 low-precision framework, we apply it to GPT-stylemodel training, encompassing both pre-training and supervised fine-tuning (SFT). The experimental results demonstrate the effectiveness of our FP8 methodology, yielding substantial benefits including a 29% to 39% reduction in real memory usage (e.g., 29% reduction for GPT-7B while 39% for GPT-175B ) and a notable 63% to 65% decrease in weight-related communication overhead compared to the prevalent BF16 mixed-precision training approach. Without changes to any hyper-parameters, such as learning rate and weight decay, the models trained using FP8 exhibit performance equivalency to those employing BF16 high precision, both in pre-training and

downstream tasks. It is noteworthy that during the training of GPT-175B model, our FP8 mix-precision framework reduces training time by 37% compared to TE (Nvidia, 2022b), while consuming 42% less memory on H100 GPU platform. More importantly, the reduction in costs achieved through the utilization of low-precision FP8 can be further increased, as the scale of models continues to expand, which is presented in Fig. 1. For fine-tuning, we employ FP8 mixed-precision for instruction tuning and reinforcement learning with human feedback (RLHF) to better align pre-trained LLMs with end tasks and user preferences. Specifically, we fine-tune pre-trained models on publicly user-shared instruction-following data (ShareGPT, 2023). The models tuned with our FP8 mixed-precision demonstrate comparable performance to those utilizing the half-precision BF16 (Zheng et al., 2023) on the AlpacaEval (Li et al., 2023b) and MT-Bench (Zheng et al., 2023) benchmarks, while achieving 27% improvements in training speed. Moreover, FP8 mixed-precision exhibits considerable potentials in RLHF, a process that necessitates loading multiple models during training. Through the utilization of FP8 in training, the prevalent RLHF framework AlpacaFarm (Dubois et al., 2023) can yield a 32% reduction in model weights and a 62% reduction in optimizer states’ memory consumption. This further demonstrates the versatility and adaptability of our FP8 low-precision training framework. We are making the following contributions to drive the design of next-generation FP8 low-precision training for LLMs.

A new FP8 mixed-precision training framework. It unlocks 8-bit weights, gradients, optimizer, and distributed training gradually in an add-on fashion, which is convenient in use. This 8-bit framework can be used as a simple drop-in replacement for existing 16/32-bit mixed-precision counterparts, without requiring any changes to the hyper-parameters and training receipts. Additionally, we provide a Pytorch implementation that enables 8-bit low-precision training in a few lines of code.

A new family of GPT-style models trained with FP8. We apply the proposed FP8 scheme to GPT pre-training and fine-tuning (i.e., SFT and RLHF), and demonstrate its potentials on a variety of model scales ranging from 7B to 175B parameters. We equip prevalent parallel computation paradigms with FP8 supports, including tensor, pipeline, and sequence parallelisms, enabling the utilization of FP8 to train large foundation models. We open-source the first FP8 GPT training codebase based upon Megatron-LM (Shoeybi et al., 2019) implementation.

We expect the release of our FP8 framework will establish a new paradigm for next-generation low-precision training system dedicated to large foundation models.

2 FP8 LLMs

Mixed-precision (Micikevicius et al., 2017) has been widely used in LLM training to improve compute and memory efficiency. The most popular mixed-precision schemes are FP16-FP32 and BF16-FP32. Because of the restricted numerical range of FP16, FP16-FP32 scheme has been known instabilities for training large models (Rae et al., 2021; Zeng et al., 2022). Consequently, the community now commonly adopts BF16-FP32 for training LLMs, such as Megatron-Turing NLG-530B (Smith et al., 2022), Bloom-175B (Scao et al., 2022) and Gopher (Rae et al., 2021). The underlying reason is that BF16 has a wide dynamic range to maintain numerical stability while matching the performance of the full-precision FP32. Moreover, BF16 employs half the number of bits as compared to FP32, thus reducing considerable memory footprints while improving compute efficiency. FP8 is a natural evolution from 16-bit data formats to further reducing computing costs. However, training LLMs with reduced-precision FP8 poses new challenges. The dynamic range and representation precision of FP81 are much lower than BF16 and FP16, which inevitably induces more training collapses, such as loss spikes or even NaNs. To address the issues, tensor scaling techniques are proposed (Sun et al., 2019; Micikevicius et al., 2022). The core idea is multiplying higher precision values with a scaling factor prior to their casting to FP8 in order to move them into a range that better overlaps with the representable range of

1The details of FP8 data formats are presented in Appendix A.1.

a corresponding FP8 format2 (Micikevicius et al., 2022). Such a per-tensor scaling technique reduces data quantization errors while improving numerical stability and accuracy, thus enabling the utilization of the lower-precision FP8 for training large models. Unfortunately, the current support for FP8 low-precision training is restricted. Nvidia TE (Nvidia, 2022b) only supports FP8 compute for linear layers in Transformer (Vaswani et al., 2017), while leaving all other operations, such as weight update and gradient synchronization, still using higher precision. In this work, we present an extremely optimized FP8 mixed-precision strategy for LLM training. The new FP8 optimization includes three key perspectives: FP8 communication, FP8 optimizer, and FP8 distributed training. By integrating these aspects, the training of LLMs such as the 175B GPT-3 model can fully harness the advantages of FP8 low-precision and improve training efficiency.

2.1 FP8 Gradient and All-Reduce Communication

Existingmixed-precision trainingmethodologies (Micikevicius et al., 2017;Nvidia, 2022b) typically employ 16-bit or 32-bit datatype for the computation and storage of gradients, resulting in a high bandwidth requirement for collective communication throughout the training process. We found that directly applying FP8 to gradients leads to a decrease in accuracy. The fundamental issue lies in the underflow and overflow problems arising from the low-bit all-reduce operation. Specifically, there are two standard methods aggregating gradients across GPUs during all-reduce: pre-scaling and post-scaling. Pre-scaling divides the gradient gi calculated on the i-th GPU by the total number of GPUs (i.e., N) before being summed, which is formulated as:

g = g1/N + g2/N + · · ·+ gN/N. (1) When N is large, this division can cause data underflow, especially for FP8 low-precision representation of gradients. To mitigate this issue, post-scaling performs the gradient summation first, followed by the division scaling during the gradient collection process:

g = (g1 + g2 + · · ·+ gN )/N. (2) This post-scaling approach keeps the gradients close to the maximum value of the FP8 datatype, effectively alleviating the underflow issue. However, this approach encounters overflow issues when aggregating gradients. In contrast, we propose an automatic scaling technique to resolve both the underflow and overflow issues in the pre-scaling and post-scaling approaches. To be specific, we introduce an auto-scaling factor µ, that changes on the fly during the training, to reduce the occurrences of overflow and underflow in gradients:

g′i = µ · gi. (3) A statistical analysis is conducted on the gradient values of g′i, with the objective of quantifying the proportion of values that attains the maximum feasible value within the FP8 representation range. If the ratio of the maximum value exceeds a specified threshold, i.e., 0.001%, µ is set to 1/2 in the subsequent training step, thereby mitigating the risk of overflow. Conversely, when the ratio consistently remains the threshold, we opt to exponentially increase µ to 2 over the span of 1,000 training steps, thereby effectively mitigating the risk of underflow occurrences. Another key obstacle of FP8 collective communication lies in devising an effective strategy to manage the tensor-wise scaling factors that are associated with each gradient tensor. The current NCCL implementation (Nvidia, 2020) lacks the capability of performing all-reduce operation considering the additional tensor-wise scaling factors. Meanwhile, efficient implementation is also very challenging, especially considering that the NCCL gradient summation operates at sub-tensor level. This complexity increases significantly when incorporating updates for tensor-wise scaling factors. To overcome this issue, we propose a new mechanism that scales FP8 gradients across GPUs using a single shared scalar. To be specific, let (g′i, s′i) denote a scaling tensor which stores the weight gradient in the i-th GPU, where g′i is a FP8 tensor and s′i is the corresponding scaling factor. The actual weight gradient is g′i/s′i. Prior to the all-reduce operation over gradient tensors,

2The details of FP8 tensor scaling are introduced in Appendix A.2.

we first gather the scaling factors s′i of each gradient tensor on all GPUs and calculate the global minimum scaling factor s′g as:

s′g = min (s′1, s′2, . . . , s′N ) , (4) where the global minimum scaling factor s′g is shared across GPUs. We use this shared scaling factor s′g to unify the rescaling of the gradient tensors across GPUs. In this way, all gradient tensors associated with the same weight use the same shared scaling factor to quantize the tensors into FP8 format on all GPUs:

g′′i = FP8 (s′g · (g′i/s′i)) . (5) This approach reduces communication overhead by transmitting only a single scalar s′g , making the additional synchronization step highly efficient. As the input tensors share the same scaling factor, it eliminates the need of considering all-reduce the scaling factors in parallel and allows for standard NCCL all-reduce operation to be performed. The final collected gradient is obtained as follows:

g = g′′1 + g′′2 + · · ·+ g′′N , s = N · s′g, (6) where g is the final aggregated gradient and s is the corresponding scaling factor. Rescaling the scaling factor for the summed gradient g is equivalent to dividing g by N in theory. By implementing the aforementioned dual strategies of distributed and automated scaling, we can successfully realize FP8 low-bit gradient communication while preserving model accuracy. Furthermore, this approach entails storing gradients in FP8 and conducting communication in FP8 as well, thereby yielding reductions in GPU memory usage and communication bandwidth consumption.

2.2 FP8 Optimizer

In the training of LLMs, Adam and its variants (Kingma and Ba, 2015; Loshchilov and Hutter, 2018) are the most frequently-used optimization methods, that maintain copies of model weights, gradients, first-order and second-order gradient moments for model updates. Mixed-precision training (Micikevicius et al., 2017) with Adam optimizer typically stores master weights, gradients and gradient moments in 32-bit float format for numerical stability (Shoeybi et al., 2019; Rajbhandari et al., 2020; Zhang et al., 2022; Scao et al., 2022). Consequently, the Adam optimizer consumes 16 bytes of memory per parameter during training:

4︸︷︷︸ master weights

+ 4︸︷︷︸ gradients

+ 4 + 4︸ ︷︷ ︸ Adam states

= 16 bytes. (7)

When model size is large, the memory consumption of the variables in Adam will become a bottleneck. Previous work (Rae et al., 2021; Zeng et al., 2022; Liu et al., 2022) has revealed that reducing precision of the variables in optimizer to 16-bit leads to accuracy degradation when training billion-scale models3. This prompts an evaluation of which variables in the optimizer should be allocated high precision and which can be accommodated with low-precision. To clarify, we decouple the influence of data precision on the variables in the optimizer and investigate which one can be assigned lower precision, i.e., precision decoupling. We find a guiding principle: the gradient statistics can use lower precision, while the master weights necessitate high precision. More concretely, the first-order gradient moment can tolerate a high quantization error and can be assigned with low-precision FP8, while the second-order moment requires a higher precision, as analyzed in Sec. 3.3. This stems from the fact that, during model updates in Adam, the direction of the gradient holds greater significance than its magnitude. FP8 with tensor scaling can effectively preserve the distribution of the first-order moment as the high-precision tensor, though it introduces precision degradation to some extend. Calculating the square of gradients for the second-order gradient moment might lead to data underflow due to the typically small gradient values. Therefore, allocating a 16-bit higher precision is necessary to preserve numerical accuracy. On the other hand, we find that it is crucial to keep the master weights using high precision. The underlying reason is that weight updates can sometimes become extremely small or large during training, higher

3BF16 lacks the precision needed for accuracy, while FP16 has a restricted dynamic range. Given these challenges, prevalent mixed-precision training methodologies rely on utilizing FP32 full-precision for master weights, gradients, and gradient moments.

## LayerN orm

## LayerN orm

## Sequence Parallel Tensor Parallel Sequence Parallel

## LayerN orm

## LayerN orm

and   : FP8 Weight      and   : Activation

Figure 2: Transformer layer with FP8 tensor and sequence parallelism. The FP8 low-bit operation is high-lighted with orange. g is all-gather in forward pass and reduce-scatter in backward pass, while ḡ is reduce-scatter in forward pass and all-gather in backward pass. The gather-reduce operation g between sequence parallel and tensor parallel is executed utilizing FP8 low-precision activation, thus saving half communication costs.

precision for the master weights helps prevent loss of information when updating weights, ensuring more stable and accurate training. In the implementation, the master weights have two viable options: utilizing either FP32 full-precision or FP16 with tensor scaling. FP16 with tensor scaling offers the advantage of conserving memory without compromising accuracy. Consequently, our default choice is to employ FP16 with tensor scaling for storing master weights in the optimizer. Our FP8 mixed-precision optimizer consumes 6 bytes of memory per parameter during training:

2︸︷︷︸ master weights

+ 1︸︷︷︸ gradients

+ 1 + 2︸ ︷︷ ︸ Adam states

= 6 bytes. (8)

This new low-bit optimizer reduces memory footprints by 2.6x in comparison to the previous solution, as exemplified in Eq. (7). Noteworthily, this is the first FP8 optimizer for LLM training. The experiments in Sec. 3.2 show that FP8 optimizer can preserve model accuracy at various scales, ranging from 125M to 175B parameters.

2.3 FP8 Distributed Parallel Training

Training LLMs like GPT-3 requires distributed learning strategies for parallelizing across GPUs. The frequently-used strategies include data parallelism, tensor parallelism, pipeline parallelism, and sequence parallelism. Each parallelism has its own merits and has been used in a complementary fashion in existing systems (Smith et al., 2022; Shoeybi et al., 2019; Zhang et al., 2022; Scao et al., 2022; Li et al., 2023a). For FP8 supports of these strategies, neither data parallelism nor pipeline parallelism necessitates any specific modifications, because they do not involve additional FP8 compute and communication when splitting data batches or model layers into segments across devices. Tensor parallelism partitions individual layers of a model across multiple devices, such that the shards of weight, gradient and activation tensors are placed on separate GPUs, instead of a single one. To equip tensor parallelism with FP8, we convert the sharded weight and activation tensors to FP8 format for linear layer computation, enabling the forward compute and backward gradient collective communication all using FP8. On the other hand, sequence parallelism splits input sequences into multiple chunks and the sub-sequences are fed to different devices to save activation memory. As shown in Fig. 2, sequence and tensor parallelism are performed in parallel to different parts of a Transformer model to make the best use of the available memory and improve training efficiency. There is a converter g between sequence and tensor parallel regions to all-gather sequence partitions in the forward pass (or reduce-scatter tensor segments in the backward pass). We add an FP8 datatype conversion prior to g, such that the all-gather (or reduce-scatter) operation uses FP8 low-bit activation to save communication cost across GPUs.

Figure 3: ZeRO tensor partitioning with and without scaling factors. Left: the original high-precision ZeRO method, which splits a single tensor into multiple partitions and distributes them to different devices. Right: the proposed FP8 ZeRO, which distributes each tensor in its entirety across devices while taking tensor scaling into account.

In addition, Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., 2020) is another frequently-used distributed learning technique in large model training. The core idea of ZeRO is to shade model states over devices, such that each device only hold a fraction of data (e.g., master weights, gradients, and optimizer states) required for a training step. To reduce memory consumption, ZeRO method generally splits a single tensor into multiple partitions and distributes them to different devices. Directly applying FP8 to ZeRO is infeasible, because it is difficult to handle the scaling factors associated with the FP8 partitions. The per-tensor scaling factors should be distributed along with FP8 partitions. To address this issue, we implement a new FP8 distribution scheme that distributes each tensor as a whole across devices, rather than partitioning it into multiple sub-tensors as in ZeRO. The distribution of FP8 tensors is processed in a greedy manner, as outlined in Alg. 1. Specifically, our method first sorts the tensors of model states according to their sizes, and then distributes the tensors to different GPUs based upon the remaining memory size of each GPU. The distribution follows the principle that the GPUs with larger remaining memory get a higher priority in receiving new distributed tensors. In this way, the tensor scaling factors can be distributed along with the tensors smoothly, while reducing communication and compute complexity. Figure 3 presents a visual illustration of the difference in ZeRO tensor partitioning between scenarios with and without scaling factors.

3 Experiment

In this section, we assess the effectiveness of the proposed FP8mixed-precision training approach onGPT-style LLMs, including a wide range of model scales, from 125 million to 175 billion parameters. For performance ablation, we compare GPT models trained with FP8 against those trained with half-precision BF16 and

Algorithm 1 Greedy Distribution Algorithm for ZeRO Input: FP8 tensors with their corresponding scaling factors: T = {(s1, t1), (s2, t2), . . . , (sn, tn)}, where s denotes scaling

factors while t represents 8-bit tensors. The size of each tensor: C = {c1, c2, . . . , cn}. Output: Partitions representing scaling tensors assigned to each GPU. 1: Sort T in descending order of their sizes to get T ′ = {(s′1, t′1), (s′2, t′2), . . . , (s′n, t′n)} and C′ = {c′1, c′2, . . . , c′n}, where

c′1 ⩾ c′2 ⩾ · · · ⩾ c′n. 2: Initialize memory usage uj = 0 and partition pj = ∅ for each GPU Gj . 3: for i = 1 to n do 4: j ← argminj uj ▷ Find the GPU j ∈ [1,m]with the least memory usage. 5: pj ← pj ∪ {(s′i, t′i)} ▷ Assign (s′i, t

′ i) to Gj .

6: uj ← uj + c′i ▷ Update the memory usage of Gj . 7: end for 8: return Partitions P = {p1, p2, . . . , pm}

params dimension n heads n layers TP PP SP learning rate batch size n tokens 125M 768 12 12 1 1 ✓ 6.0e−4 1M 100B 7B 4096 32 32 1 1 ✓ 3.0e−4 4M 100B 13B 5120 40 40 2 1 ✓ 3.0e−4 4M 100B 175B 12288 96 96 8 4 ✓ 3.0e−5 1M 40B

Table 1: Model sizes, architectures, and training hyper-parameters. TP, PP, and SP indicate tensor, pipeline, and sequence parallelism, respectively. To mitigate carbon emissions and save cost, we restrict the training of the 175B model to a dataset comprising only 40B tokens, which has proven to be sufficient for evaluating system performance. full-precision FP32. For generality evaluation, we conduct experiments encompassing both FP8 low-bit pre-training and fine-tuning, considering instruction tuning and human preference alignment.

3.1 Experimental Setup

3.1.1 Training Dataset

Our pre-training data is constructed using open-sourced language collections from several sources, including CommonCrawl4, The Pile (Gao et al., 2020), C4 (Raffel et al., 2020), OpenWebText (Radford et al., 2019; Gokaslan and Cohen, 2019), CC-NEWS (Liu et al., 2019), CC-Stories (Trinh and Le, 2018), Redpajama (Redpajama, 2023), and Wikipedia5. We apply fuzzy deduplication (Lee et al., 2022) across CommonCrawl snapshots to enhance data quality. Tab. 10 in Appendix A.3 provides details of our pre-training data, including information such as the number of tokens from each source and associated sampling weights. For a more comprehensive understanding of the data and its cleaning pipeline, readers are encouraged to refer to Appendix A.3. Moreover, for instruction tuning, we follow the same settings as Vicuna-v1.1(VicunaTeam, 2023), which uses a publicly user-shared instruction following data (ShareGPT, 2023). For reinforcement learning with human feedback, the training data we used is a combination of the Anthropic’s Helpful and Harmless dataset (Bai et al., 2022) and Open-Assistant dataset (Köpf et al., 2023). The training framework and associated configurations align with the publicly available AlpacaFarm (Dubois et al., 2023).

3.1.2 Model Configuration

The model architecture we used is a decoder-only Transformer (Brown et al., 2020), which has been widely-used in recent generative LLMs like PaLM (Chowdhery et al., 2022), OPT (Zhang et al., 2022), and LLaMA (Touvron et al., 2023). In addition to the base architecture, we integrate several modifications proposed recently to improve model efficiency and effectiveness. 1) Rotary Positional Embedding: Drawing inspiration from recent successful experiments (Black et al., 2022; Touvron et al., 2023), we incorporate rotary positional embeddings (RoPE) (Su et al., 2021) into our approach. This addition enables us to capture both absolute and relative positions information, enhancing performance especially when extrapolating to larger context windows. 2) Flash Attention: The standard attention implementation is bottlenecked by memory access (Ivanov et al., 2021). Flash Attention (Dao et al., 2022) proposed an IO-aware exact attention algorithm which uses tiling to reduce the amount of HBM accesses, achieving substantial acceleration. We train the models using the proposed FP8 optimizer, which is built upon Adam (Kingma and Ba, 2015) with decoupled weight decay (Loshchilov and Hutter, 2018), following the common practise with the decay rates β1 = 0.9, β2 = 0.95, and weight decay = 0.1. The learning rate schedule is cosine-like, and the final learning rate is 10% of the maximal learning rate. We train the models for 100B tokens in total with a batch size of 4M tokens, and the input sequence length is set to 2048. The model warm-up is conducted for 1,000 iterations. Tab. 1 presents the details of model configurations and the corresponding training settings. The training is conducted on Azure NDv5 H100 GPU platform (Microsoft, 2023).

4https://commoncrawl.org 5https://wikipedia.org

https://lh3.googleusercontent.com/notebooklm/AG60hOph8-zl_0_nFWbmncyfdOA7IpPMpJMIa3fNiwP-JgT34exwOg2Lftm5Er2giTxClcg5nkpLhnmU9qm-93vOQMW8xLEoTbwLkb-osuni65_FfDGekDS-eXymdGPHb0SybnSrbXklQg=w472-h304-v0

986b0757-41e1-4437-ad23-9e6091824dd9

https://lh3.googleusercontent.com/notebooklm/AG60hOrYp6TsItRzOVGL--jJVhjoGgNPXn76zWuPePnpSoLQyGj74lQYZbmMz8hwzYu2TvR-wpnTCAEyTsdqJHHBvfHOs0SypmFPifr26ey9fCsmtUPJL4uPFApeyjuMFS8MYCcwfxnxxA=w472-h304-v0

ec35931c-1b25-466a-bfe9-9c8a94a284b9

https://lh3.googleusercontent.com/notebooklm/AG60hOqQcg3VJU9J3jB-Em0xMrFSD9C3wmz3mReq-v3kKSvdYLURq-EhQ_gehD8u18fR-4PePRmF65TYM3P-OSpfiUfQTLxZ4yIcfP-V7wCWLQfvQ50GmoyN1WRmbZTtNijrLN5L8qJR=w1000-h600-v0

cb6f23ae-b671-41e5-a936-3c8d52119a7c

(a) GPT-7B (b) GPT-13B (c) GPT-175B

Figure 4: A comparison between FP8 andBF16: Analyzing the training loss ofGPTmodelswith the parameters ranging from 7 billion to 175 billion.

HS Lambada BoolQ PIQA COPA Winogrande Arc-C Arc-E ObQA Avg GPT-7B model zero-shot performance

BF16 61.3 61.4 61.2 75.0 79.0 58.5 32.9 59.7 36.4 58.4 FP8 60.0 61.8 62.0 74.2 78.0 59.8 32.9 58.7 34.6 58.0 GPT-13B model zero-shot performance

BF16 64.8 64.9 63.4 75.9 82.0 61.0 35.2 61.5 40.6 61.0 FP8 64.1 63.4 63.9 76.2 81.0 61.6 34.9 61.3 36.8 60.4

Table 2: Zero-shot performance on downstream tasks. The models are trained with either the standard BF16 mixed-precision scheme (Shoeybi et al., 2019) or the proposed FP8 low-precision scheme.

3.2 Main Results

3.2.1 Model Performance

We first compare the performance of models trained using FP8mixed-precision with those trained using BF16. In Fig. 4, the pre-training loss over tokens is displayed for GPT models of 7B, 13B, and 175B parameters. The training configurations and hyper-parameters remain consistent across models trained with FP8 and BF16. The only difference lies in the mixed-precision schemes utilized. As shown in Fig. 4, the loss curves almost overlap with each other. The results unequivocally demonstrate that the proposed FP8 mixed-precision scheme can achieve equivalent performance to the prevalent higher-precision BF16 scheme (Shoeybi et al., 2019; Rae et al., 2021; Hoffmann et al., 2022) across a diverse array of model scales. Also, we evaluate the pre-trained models on a wide range of downstream tasks, including HellaSwag (HS) (Zellers et al., 2019), Lambada (Paperno et al., 2016) BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), COPA (Roemmele et al., 2011), Winogrande (Sakaguchi et al., 2021), Arc (Clark et al., 2018), and OpenbookQA (ObQA) (Mihaylov et al., 2018). As reported in Tab. 2, the FP8 pre-trained models exhibit comparable zero-shot performance in comparison to their BF16 counterparts. This result provides further validation that models pre-trained with FP8 low-precision maintain both accuracy and intrinsic in-context learning capabilities at a level comparable to their high-precision counterparts. Furthermore, we leverage the proposed FP8 mixed-precision approach for fine-tuning LLMs in instruction following. For a fair comparison, we follow the same instruction tuning settings as Vicuna-v1.1 (VicunaTeam, 2023), which adopts the open-sourced LLaMA-7B (Touvron et al., 2023) as the base model for fine-tuning. Fig. 5 presents the fine-tuning loss, where the curves corresponding to BF16 and FP8 display a notable degree of overlap. Meanwhile, the win-rate of our FP8 fine-tuned models against Davinci-003 (OpenAI, 2022) is also comparable to that of Vicuna-v1.1, which is fine-tuned using BF16 half-precision, as reported in Tab. 3. This indicates that our FP8 low-bit training scheme is versatile, as it is applicable not only to pre-training phase but also to downstream fine-tuning tasks. In addition, we further apply the proposed FP8 mixed-precision scheme to reinforcement learning from human feedback (RLHF), a more complex process to align LLMs with user preferences. Following the

https://lh3.googleusercontent.com/notebooklm/AG60hOqD_1G8IcAw7ha90GMfV0aaE2cSYgyk1Ihgrz8dZ3sFgiCmiJVD5MR-0NZl5PjhChIQV4tFVSO8uw2AOP6wCmG02u7iXoNsS4Yeue67d4qoqBL0NlkvDuOPwZXvqXc8Q3gnnTPP=w481-h299-v0

db261c5f-eefb-41f8-8939-2327b8c19b9e

https://lh3.googleusercontent.com/notebooklm/AG60hOp75dLWKAREWLRJc0MU1ltfxWwDDHMs81m-I0BDiYh5CWS_WkoUaTKj_Ng4jyfMq5Sp3_VjPvPWx8sEL4AeL3ix0I5Dmw3430TWEYmpE762l-GwhVuXi4YcbT7A0SIyWYGQ3EfM=w459-h299-v0

c780a59c-4a34-481b-82cb-937269091a63

Figure 5: SFT training loss.

Mixed-precision System Performance Model Performance GPU Mem. (GB) Throughput AlpacaEval MT-Bench

BF16 51.1 103 66.15 5.75 FP8 44.0(-14%) 131(+27%) 67.20 5.70

Table 3: A comparison between FP8 and BF16 for SFT. For sys-tem performance, we report results of GPU memory usage and training throughput. For model performance, we present the win-rate against Davinci-003 on AlpacaEval and GPT-4 judged scores on MT-Bench.

Figure 6: RLHF training loss.

Mixed-precision Memory Usage (MB) Model Performance Weights Optimizer States AlpacaEval MT-Bench

BF16 15,082 15,116 72.05 6.16 FP8 10,292(-32%) 5,669(-62%) 72.42 6.04

Table 4: A comparison of FP8 and BF16 RLHF alignment. Mem-ory usage is assessed with a focus on weights and optimizer states, while model performance is evaluated on AlpacaEval considering win-rate against Davinci-003, and MT-Bench using GPT-4 judged scores.

same training setting as AlpacaFarm (Dubois et al., 2023), a recent RL framework for LLM alignment, we optimize policy models with PPO algorithm (Schulman et al., 2017). The solely difference lies in the choice of mixed-precision training schemes, i.e., BF16 v.s. FP8. From the results reported in Fig. 6 and Tab. 4, we observe a notable reduction in memory utilization, for instance, a 32% memory reduction concerning model weights and a 62% reduction concerning optimizer states. Consequently, it can be inferred that FP8 is capable of replicating the BF16 mixed-precision for RLHF training. This underscores the broader applicability and versatility of our FP8 low-bit training solution. 3.2.2 System Performance

In this section, we evaluate system-level performance of FP8 mixed-precision, considering communication efficiency, memory utilization, and the overall speed, with an emphasis on cost savings. Our method employs 8-bit gradients for all-reduce collective communication among GPUs. Theoretically, this results in a 75% reduction in communication costs when compared to the mainstream 32-bit scheme (Despite BF16 mixed-precision computing gradients using 16-bit precision, it still employs 32-bit precision for all-reduce communication (Shoeybi et al., 2019)). Due to the impact of system transmission loss, the observed practical reduction during GPT model training falls within the range of 63% to 65%, as indicated in Table 5. Furthermore, it is worth noting that the recent Nvidia Transformer Engine (TE) (Nvidia, 2022b) still relies on full-precision FP32 for collective communication, resulting in the same level of reduction for our FP8 solution. When training GPTmodels with identical batch sizes, FP8 mixed-precision can lead to a reduction in memory footprint ranging from 28% to 39% when compared to BF16, as reported in Tab. 5. These reductions in memory consumption are attributed to the FP8 gradient and FP8 optimizer techniques we have introduced. Moreover, compared with TE (Nvidia, 2022b), our solution is also very competitive, obtaining 36.1%, 36.0%, and 42.1% additional memory reductions for different model sizes, i.e., GPT-7B, 13B, and 175B. Although TE employs FP8 for compute, it still uses high-precision optimizer and gradients, which consumes much more memory than our solution. In addition, the saved memory in our method can be used to train larger batch size or longer sequence. For example, when employing 32 H100 GPUs with a memory capacity of 80GB, our approach enables the training of models with a context of 4,096 tokens, accommodating up to 175 billion parameters. In contrast, TE can only accommodate models with a context of 2,048 tokens. This showcases the potential of integrating our FP8 mixed-precision training into existing LLMs, empowering them to train longer sequences with the same GPU resources.

Model TP PP DP Micro Mixed GPU Throughput TFLOPS MFU Weight-related Comm. BS Precision Mem. (GB) (#samples/s) (%) Rate (%) Volume (GB)

GPT-7B 1 1 32 2 BF16 69.6 159.2 445 45.0 10.1 37.2 2 FP8 (TE) 77.3 224.5 627 31.7 9.7 37.2 2 FP8 (Ours) 49.4 (-29%) 219.8 (+38%) 615 31.1 7.9 13.9 (-63%) 4 FP8 (Ours) 69.3 230.5 (+45%) 645 32.6 10.4 13.9 (-63%)

GPT-13B 2 1 16 2 BF16 68.2 79.3 420 42.5 11.1 34.3 2 FP8 (TE) 76.4 111.7 592 29.9 7.1 34.3 2 FP8 (Ours) 48.9 (-28%) 109.5 (+38%) 575 29.1 3.9 12.4 (-64%) 4 FP8 (Ours) 67.8 121.5 (+53%) 644 32.5 9.3 12.4 (-64%)

GPT-175B 8 4 4 1 BF16 66.1 22.4 386 39.0 8.8 23.4 1 FP8 (TE) 69.6 28.7 493 24.9 3.9 23.4 1 FP8 (Ours) 40.3 (-39%) 27.1 (+21%) 473 23.9 2.5 8.2 (-65%) 4 FP8 (Ours) 57.7 39.3 (+75%) 677 34.2 10.9 8.2 (-65%)

Table 5: System-level performance on Nvidia H100 GPUs 80GB. Here, TP, PP, and DP represent tensor, pipeline, and data parallelism respectively. BS indicates batch size, while MFU denotes model FLOPs utilization. Weight-related communication contains the all-gather operator on weights and the reduce-scatter operator on weight gradients.

0 5 10 15 20 25 30 Block ID

## SN R pre scale

post scale auto scale

(a) SNR (Signal to Noise Ratio)

0 5 10 15 20 25 30 Block ID

pre scale post scale auto scale

(b) Underflow rate

0 5 10 15 20 25 30 Block ID

pre scale post scale auto scale

(c) Overflow rate

Figure 7: Comparing different strategies, i.e., pre-scaling, post-scaling, and auto-scaling, for FP8 gradient all-reduce. We investigate SNR, underflow rate, and overflow rate across different Transformer blocks. The experiment is conducted using a GPT-7B model with a data parallelism factor of 128.

Moreover, our FP8 mixed-precision scheme shows a superior training throughput compared to the prevalent BF16 scheme, achieving a notable speed-up of 75% when applied to GPT-175B model. The model FLOPS utilization (MFU) of FP8 mixed-precision training is 34.2% on H100 GPUs, being 37.3% superior to TE. These findings provide substantial evidence that our FP8 scheme effectively conserves memory, reduces communication costs during the training of largemodels, and ultimately enhances systemutilization efficiency on the latest H100 GPU platform.

3.3 Ablation Study

We ablate various design choices of FP8 mixed-precision training strategy for LLMs and report the per-formance in Tab. 6 – 8 and Fig. 7 – 8. The ablation experiments are conducted on GPT models, whose architectures and training settings are elaborated in Tab. 1. Importantly, our ablation study yields several guidelines for the effective utilization of 8-bit datatype in LLM training, which can facilitate future research on low-bit model training. Communication. We first analyze the limitations of the conventional pre-scaling and post-scaling methods when aggregating low-bit gradients during the all-reduce communication process. As shown in Fig. 7, we conduct a statistical analysis on SNR, underflow rate, and overflow rate of weight gradients across different Transformer blocks. It is observed that the pre-scaling method has relative larger underflow rate when quantifying gradients from 32-bit to 8-bit, while the post-scaling method has higher overflow rate. In contrast, the proposed auto-scaling technique can diminish both the underflow ratio and the overflow ratio, while getting much better SNR, as shown in Fig. 7 (a). This demonstrates the effectiveness of auto-scaling method in reducing quantization errors when utilizing 8-bit datatype for gradient all-reduce.

Low-bit Settings

Compute (GEMM) Comm. Master

## Weight Optimizer States

FP32 #0 FP32 FP32 FP32 FP32+FP32 BF16 #1 BF16 FP32 FP32 FP32+FP32 FP8 #2a FP8 FP8 FP16 FP8+FP16 FP8 #2b FP8 FP8 BF16 FP8+FP16 FP8 #3 FP8 FP8 FP8 FP8+FP16 FP8 #4 FP8 FP8 FP16 FP8+FP8

Table 6: Precision decoupling for the variables within the optimizer. Here, our focus is on ablating the master weight and optimizer states, as these components are precision sensitive. The optimizer states include both first-order and second-order gra-dient moments. Note that the FP16 master weight uses tensor scaling.

0 20 40 60 80 100 Billions of Tokens

FP32 #0 BF16 #1 FP8 #2a FP8 #2b FP8 #3 FP8 #4

Figure 8: Training losses of GPT-125M models with the settings presented in Tab. 6. The loss curve for FP8 #4 has diverged.

Model TP PP DP Micro Mixed Act-related Comm. BS Precision Rate (%) Volume (GB)

GPT-13B 2 1 16 2 BF16 12.9 4.7 FP8 (Ours) 5.3 3.1

GPT-175B 8 4 4 1 BF16 14.9 5.9 FP8 (Ours) 5.2 3.9

Table 7: Activation-related communication vol-ume reduction in sequence and tensor paral-lelism, including the all-gather operator on ac-tivation and the reduce-scatter on activation gra-dients.

## Model TP PP DP Micro Mixed GPU Memory BS Precision Min Max

GPT-7B 1 1 32 2 BF16 69.07 69.63

FP8 (TE) 76.97 77.28 FP8 (Ours) 49.06 49.36

GPT-13B 2 1 16 2 BF16 67.98 68.18

FP8 (TE) 73.68 76.36 FP8 (Ours) 48.45 48.85

GPT-175B 8 4 4 1 BF16 65.60 66.12

FP8 (TE) 69.04 69.57 FP8 (Ours) 38.64 40.28

Table 8: Comparing ZeRO distribution methods in terms of memory load across GPUs. Here “Min” and “Max” denote the minimum and max-imummemory utilization observed across GPUs. Our FP8 ZeRO method uses less memory while achieving memory-aware load balancing.

Optimizer. We further ablate the impact of re-duced precision for the variables in the AdamW optimizer. We set the BF16 mixed-precision op-timizer as the baseline, since it has been widely used in existing LLM training frameworks (Mi-cikevicius et al., 2017; Shoeybi et al., 2019; Nvidia, 2022b). Tab. 6 presents the settings of reduced precision for the variables, while Fig. 8 plots the corresponding training losses. We observe that: 1) FP8 master weight induces performance degra-dation (see the #2a vs. #3 lines in Fig. 8), while FP16 can maintain accuracy as FP32 (see #2a vs. #0 and #1) but requiring using tensor scaling. It re-veals that the master weight is precision-sensitive. This can be attributed to the master weight’s role in updating weights, which tend to exhibit small magnitudes, necessitating high precision to main-tain accuracy. 2) The training loss of BF16 master weight is slightly higher than that of FP16 with a scaling factor because BF16 has fewer mantissa bits, resulting in lower precision (see #2a vs. #2b). 3) The second-order gradient moment is more precision-sensitive than the first-order one, be-cause the square calculation is easy to cause under-flow and leads to accuracy degradation. Utilizing FP8 for the second-order gradient moment can lead to divergent training loss (see the #4 dot in Fig. 8). Parallelism. In our FP8 LLM training framework, we introduce FP8 low-bit convertors into sequence parallelism and tensor parallelism to reduce activation communication costs across GPUs. Here we conduct an analysis experiment to count the activation-related communication volume during GPT model training, and report the numbers in Tab. 7. It is observed that our FP8 parallel scheme results in a substantial reduction of 34% in activation-related communication costs compared to the original method utilizing BF16. Furthermore, in ZeRO distributed training, our method distributes each FP8 tensor along with its associated scaling factor as a whole, rather than partitioning the tensor into splits across GPUs. This strategy not only results in more GPU memory savings but also maintains a balanced memory load across GPUs, as demonstrated in Tab. 8.

4 Related Work

Mixed-precision Training. Efficient training through reduced mixed-precision has been widely used in modern deep learning to save computing costs. While some works have taken bit-reduction to the extreme, i.e. 1-bit binary networks (Hubara et al., 2016; Rastegari et al., 2016), they have not been successful in maintaining model accuracy (Micikevicius et al., 2022). The most practical scheme now is the FP16 half-precision method (Micikevicius et al., 2017), which can maintain accuracy while improving training efficiency. The computations during forward pass and back propagation use FP16 while the master weights use FP32. Since FP16 has a narrower dynamic range, FP16 mixed-precision entails loss scaling (Micikevicius et al., 2017) to prevent loss of accuracy. Fortunately, the need for loss scaling can be avoided by using BF16 datatype, because BF16 maintains the same dynamic range as the full-precision FP32. This results in that large model training now prefers to use BF16 mixed-precision scheme, which is more stable during training (Smith et al., 2022; Scao et al., 2022; Zeng et al., 2022). FP8 is a natural progression from 16-bit data formats to further reducing computing cost. Early pioneering efforts in FP8 low-bit model training (Wang et al., 2018; Sun et al., 2019; Dettmers et al., 2021) have largely remained at the simulation stage. Consequently, there exists a notable gap between the projected capabilities of these approaches and their actual performance on hardware (Micikevicius et al., 2022). With the advent of Nvidia Hopper GPU architecture (Nvidia, 2022a), FP8 is emerging as a viable and practical data type for the next-generation low-precision training, as discussed in (Micikevicius et al., 2022). At present, the Nvidia Transformer Engine (TE) (Nvidia, 2022b) serves as the primary framework for FP8 mixed-precision training. However, its support for FP8 usage remains somewhat constrained. TE’s current implementation restricts FP8 usage solely to weight computation, retaining the storage of model weights and gradient calculations with 16-bit data types. Consequently, the end-to-end speed-up, memory and communication cost savings are limited. In contrast, our work infiltrates FP8 gradient, optimizer, and distributed training into the whole progress of model training, fully unveiling the capabilities of FP8. Large Language Models. Recent years have witnessed a substantial evolution in the field of LLMs. Autore-gressive language modeling – predicting the future of a text sequence from its past – provides a simple yet powerful objective that admits formulation of numerous tasks. While there exist alternative methodologies, such as masked language modeling (Devlin et al., 2019) and permutation language modeling (Yang et al., 2019), the autoregressive method now is more promising because of its strong performance. Following the scaling laws (Brown et al., 2020) and the refined laws (Hoffmann et al., 2022), various LLMs are have been proposed, including dense models: GPT-3 (Brown et al., 2020), Jurassic-1 (Lieber et al., 2021), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022), Bloom (Scao et al., 2022), OPT (Zhang et al., 2022) Megatron-Turing NLG (Smith et al., 2022), PaLM (Chowdhery et al., 2022), LaMDA (Thoppilan et al., 2022), LLaMA (Touvron et al., 2023), and sparse models: GLaM (Du et al., 2022), and Switch transformers (Fedus et al., 2022). Each of them has demonstrated remarkably competitive few-shot performance across a wide range of tasks at the time of their respective releases. Nonetheless, these models still encounter challenges, such as overwhelming computational requirements and the need for acquiring more high-quality training data. In this work, we delve into the utilization of low-precision techniques to mitigate the training costs, which is a crucial step for the continued expansion of language models. Low-precision training has been widely used in LLM training to reduce compute cost. OPT (Zhang et al., 2022) and GLM (Zeng et al., 2022) utilize FP16 for forwards and backwards and FP32 for optimizer states and master weights, to reduce the GPU memory usage and improve training efficiency. Bloom (Scao et al., 2022) find that FP16 can cause numerical instabilities and irreversible divergences, especially when training models larger than 100B parameters, because FP16’s dynamic range is limited. Consequently, Bloom and other LLMs, such as Gopher (Rae et al., 2021) and Chinchilla (Hoffmann et al., 2022), adopt BF16mixed-precision, because BF16 has a wide dynamic range that is the same as FP32. LLM training and tuning with 8-bit low-precision were not well-explored in previous works, because the hardware support for FP8 is not available before the release of Nvidia Hopper infrastructure. This work presents the first exploration of FP8 pre-training and fine-tuning for LLMs, while proposing an extremely-optimized FP8 mixed-precision scheme. We hope this work could facilitate future research in FP8 and, potentially, extend to exploring even lower precision training, such as 4-bit and 1-bit.

5 Conclusion

In this work, we explore 8-bit training for LLMs. We introduce a new FP8 mixed-precision training frame-work, which incorporates 8-bit collective communication, optimizer, and distributed parallel training in an incremental manner. To our best knowledge, this is the first work infiltrating FP8 compute, storage and communication into thewhole progress of large languagemodel training. Extensive experiments demonstrate the proposed method effectively diminishes communication overhead and curtails memory utilization in the context of GPT model training at various scales. In future work, we plan to scale up the size and training steps of the FP8 GPT models and further train them with our 8-bit mixed-precision scheme. Moreover, we will also use the proposed FP8 scheme to train multi-modal large models, and explore low-bit deployment of LLMs on various edge devices, such as smart phones.

## Contribution and Acknowledgement

This project was initially proposed by Han Hu and Peng Cheng, who are the directional lead. Shuguang Liu served as the product lead throughout the project. The contributions for all the co-authors are detailed as follows: FP8 Framework: Kan Wu, Houwen Peng, Ze Liu, Peng Cheng, Han Hu System: Yifan Xiong, Ziyue Yang, Yuxiang Yang, Guoshuai Zhao, Peng Cheng Hardware Infrastructure: Guoshuai Zhao, Yuxiang Yang, Yifan Xiong, Peng Cheng, Shuguang Liu, Joe Chau Data: Ruihang Li, Miaosen Zhang, Jia Ning, Chen Li, Ruizhe Wang, Houwen Peng, Han Hu Pre-training: Yixuan Wei, Kan Wu, Ze Liu, Miaosen Zhang, Zheng Zhang, Houwen Peng, Han Hu Alignment (SFT, RS, and RLHF): Bolin Ni, Jingcheng Hu, Yixuan Wei, Houwen Peng, Han Hu Evaluation: Yixuan Wei, Bolin Ni, Jingcheng Hu Product Engineering: Yuxiang Yang, Kan Wu, Yifan Xiong, Ziyue Yang, Guoshuai Zhao, Peng Cheng

We thank Eric Chung, Bita Darvish Rouhani, Yu Pei, Hyunseung Harry Yoo, Zhenghong Zhou, Gongrui Zhang, and Zhirong Wu for helpful discussions. We thank Baining Guo and Lidong Zhou for their guidance and support for this project.

## References

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.

Yuntao Bai, Andy Jones, KamalNdousse, AmandaAskell, AnnaChen, NovaDasSarma, DawnDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.

Microsoft Bing. Bing webmaster tools. 2022. URL https://www.bing.com/webmasters/. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in

natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432–7439, 2020.

Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. In Proceedings of BigScience Episode# 5–Workshop on Challenges & Perspectives in Creating Large Language Models, pages 95–136, 2022.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, SamMcCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc., 2020.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.

Christopher Clark, Kenton Lee, Ming-Wei Chang, TomKwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924–2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018.

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022.

Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. In International Conference on Learning Representations, 2021.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),

pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.

Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 5547–5569. PMLR, 2022.

Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.

William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232–5270, 2022.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.

Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv:2203.15556, 2022.

Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. Advances in neural information processing systems, 29, 2016.

HuggingFace. wikipedia - datasets at hugging face. 2022. URL https://huggingface.co/datasets/ wikipedia.

Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711–732, 2021.

Armand Joulin, Édouard Grave, Piotr Bojanowski, and Tomáš Mikolov. Bag of tricks for efficient text classification. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 427–431, 2017.

Diederik P. Kingma and Jimmy Ba. Adam: Amethod for stochastic optimization. In 3rd International Conference on Learning Representations, San Diego, CA, 2015. URL http://arxiv.org/abs/1412.6980.

Denis Kocetkov, Raymond Li, LI Jia, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, et al. The stack: 3 tb of permissively licensed source code. Transactions on Machine Learning Research, 2022.

Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Ab-dullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversations– democratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023.

Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8424–8445, 2022.

Shenggui Li, Hongxin Liu, Zhengda Bian, Jiarui Fang, Haichen Huang, Yuliang Liu, Boxiang Wang, and Yang You. Colossal-ai: A unified deep learning system for large-scale parallel training. In Proceedings of the 52nd International Conference on Parallel Processing, pages 766–775, 2023a.

Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https: //github.com/tatsu-lab/alpaca_eval, 2023b.

Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. White Paper. AI21 Labs, 1, 2021.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12009–12019, 2022.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.

Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.

Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022.

Microsoft. Azure high-performance computing. 2023. URL https://azure.microsoft.com/en-us/ solutions/high-performance-computing.

Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381–2391, 2018.

Nvidia. Apex. 2018. URL https://nvidia.github.io/apex. Nvidia. The nvidia collective communications library. 2020. URL https://developer.nvidia.com/nccl. Nvidia. Nvidia h100 tensor core gpu architecture. 2022a. URL https://resources.nvidia.com/ en-us-tensor-core.

Nvidia. Nvidia transformer engine. 2022b. URL https://docs.nvidia.com/deeplearning/ transformer-engine/index.html.

Nvidia. Using fp8 with transformer engine. 2022c. URL https://docs.nvidia.com/deeplearning/ transformer-engine/user-guide/examples/fp8_primer.html.

OpenAI. Model index for researchers. 2022. URL https://platform.openai.com/docs/ model-index-for-researchers.

OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word predic-tion requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525–1534, 2016.

Shawn Presser. Books3. https://twitter.com/theshawwn/status/1320282149329784833, 2020. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.

Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations, 2019.

Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.

Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–16. IEEE, 2020.

Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821–8831. PMLR, 2021.

Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European conference on computer vision, pages 525–542. Springer, 2016.

Redpajama. Redpajama-data: an open source recipe to reproduce llama training dataset. 2023. URL https://github.com/togethercomputer/RedPajama-Data.

Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI Spring Symposium, 2011.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.

David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations, 2018.

Teven Le Scao, 388Authors, and ThomasWolf. BLOOM:A 176B-parameter open-accessmultilingual language model. ArXiv, abs/2211.05100, 2022.

John Schulman, FilipWolski, Prafulla Dhariwal, Alec Radford, andOleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

ShareGPT. Openchat: Advancing open-source language models with imperfect data. 2023. URL https: //sharegpt.com/.

Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.

Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.

Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.

Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi Viji Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit floating point (hfp8) training and inference for deep neural networks. Advances in neural information processing systems, 32, 2019.

Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. Ultra-low precision 4-bit training of deep neural networks. Advances in Neural Information Processing Systems, 33:1796–1807, 2020.

Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.

Jörg Tiedemann. Finding alternative translations in a large corpus of movie subtitle. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 3518–3522, 2016.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Trieu H Trinh and Quoc V Le. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847, 2018.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Process-ing Systems, pages 5998–6008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 7181-attention-is-all-you-need.pdf.

VicunaTeam. Vicuna: An open-source chatbot impressing gpt-4 with 90quality. 2023. URL https://lmsys. org/blog/2023-03-30-vicuna/.

Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. Advances in neural information processing systems, 31, 2018.

Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. arXiv preprint arXiv:1911.00359, 2019.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XLNet: Gen-eralized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelz-imer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791–4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472.

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations, 2022.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books andmovies: Towards story-like visual explanations by watchingmovies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27, 2015.

## A Appendix

A.1 FP8 Data Formats

In September 2022, NVIDIA, ARM, and Intel published FP8 specification for standardization as an interchange format for AI (Micikevicius et al., 2022). The industry hasmoved from 32-bit precision to 16-bit, and now even 8-bit precision for AI model training. This development reflects a broader industry trend that has transitioned from high-precision to low-precision training. Notably, the proposed FP8 specification introduces two distinct data types, E5M2 and E4M3, which offer a trade-off between a larger range and higher precision of stored values (Nvidia, 2022c).

E4M3 consists of 1 sign bit, 4 exponent bits and 3 bits of mantissa. It can store values up to +/-448 and NaN.

E5M2 consists of 1 sign bit, 5 exponent bits and 2 bits of mantissa. It can store values up to +/-57344, +/- inf and NaN.

The FP8 format (Micikevicius et al., 2022) roughly follows the IEEE 754 standard. Compared to higher precision data formats such as FP16 and FP32, FP8 suffers from two kinds of representation degradation:

Lower representation range. The representation range in a data format specifies the range between the maximum and minimum values that the format can accurately represent. There are two modes, a normal mode, which defines a regular range with relatively constant precision, and a subnormal mode, which extends the range to represent smaller values with lower precision. The normal range primarily depends on the number of exponent (E) bits, with more E bits resulting in a larger normal range. On the other hand, the subnormal range is primarily influenced by the number of mantissa (M) bits, where an increase in M bits leads to a larger subnormal range. As illustrated in Tab. 9, the representation range of FP8 is notably narrower compared to that of FP16 and FP32, especially in the case of the S1E4M3 sub-format (S denotes the sign bit). This discrepancy represents the primary challenge when employing FP8 for training large models.

Lower representation precision. The limited number of mantissa (M bits) leads to quantization rep-resentation errors. Due to the considerably fewer M bits in FP8, the representation precision of FP8 is substantially lower than that of FP16, as depicted in Tab. 9. This challenge stands as another significant hurdle when considering the use of FP8 for training large models.

FP8 consists of two sub-formats: S1E4M3 and S1E5M2. The former offers a narrower representation range but higher precision, while the latter provides a larger range but lower precision. These two sub-formats give users the flexibility to strike a balance between their requirements for range and precision in model training.

Table 9: Representation range and error for different data formats Data format Representation Range Maximum Relative Error

Max normal Min normal Min subnormal Min - Max (normal) Min ∼Max (subnormal) FP32

(S1E8M23) 3.40× 1038 1.18× 10−38 1.40× 10−45 1.19× 10−7 ∼ 5.96× 10−8 5.00× 10−1 ∼ 1.19× 10−7

FP16 (S1E5M10) 65, 504 6.10× 10−5 5.96× 10−8 9.76× 10−4 ∼ 4.89× 10−4 5.00× 10−1 ∼ 9.78× 10−4

BF16 (S1E8M7) 3.39× 1038 1.18× 10−38 9.18× 10−41 7.75× 10−3 ∼ 3.94× 10−3 5.00× 10−1 ∼ 7.94× 10−3

FP8 (S1E4M3) 448 1.56× 10−2 1.95× 10−3 1.11× 10−1 ∼ 7.69× 10−2 5.00× 10−1 ∼ 1.67× 10−1

FP8 (S1E5M2) 57, 344 6.10× 10−5 1.53× 10−5 2.00× 10−1 ∼ 1.67× 10−1 5.00× 10−1 ∼ 5.00× 10−1

https://lh3.googleusercontent.com/notebooklm/AG60hOpeqiOngb83ya0ZSBNbHpWV0il3KcDzNVmooxAPdN6M9DBhW7ziQOmPCT3erC0oPbWu7FdCtgTOxuijy_wT4yTwMzjA0E9RDDU-7EK_oC5ksLfVFoUO3J48mQNA6WTGNebb1GDDQw=w682-h399-v0

a19ad1f2-e5c1-4242-8b77-2ae3feb9f34e

A.2 FP8 Tensor Scaling

We now discuss the underlying mechanisms for how large model training with FP8 overcomes the challenges associated with representation range and precision degradation. The key technique behind is tensor scaling, which scales the tensor values that originally locate out the representation range of a data format to its comfort zone, as visualized in Fig. 9. The pioneer scaling techniques (Micikevicius et al., 2017; Nvidia, 2018) apply a global scaling factor to the loss, such that gradients of all layers are scaled by a single adaptive factor. The utilization of the global loss scaling technique, in conjunction with various other training strategies, has facilitated the widespread adoption of FP16 mixed-precision training on V100 and A100 GPUs. Remarkably, this approach has resulted in minimal to no degradation in accuracy, particularly for small to medium-sized models (Micikevicius et al., 2017). Nonetheless, when dealing with super-large models or complex tasks, such as in the training of models like DALL-E (Ramesh et al., 2021), the global loss scaling technique still encounters significant underflow issues. As a consequence, block-wise (Ramesh et al., 2021) and layer-wise (Sun et al., 2020) gradient scaling are proposed. While the global scaling technique enables almost no accuracy drop for FP16 training (with a range of [5.96E-8, 6.55E+4]), the fine-grained per-tensor scaling will enable stable model training using even shallower range by FP8 (with a range of [1.95E-3, 448] for E4M3 and a range of [1.53E-5, 5.73E+4] for E5M2). Fig. 9 shows that the representation range of FP8 has been large enough to deal with general model training. In the per-tensor scaling technique, various strategies are available for choosing the suitable scaling factor for a given FP8 tensor. Two common approaches are “just-in-time scaling" and “delayed scaling" (Nvidia, 2022c).

Just-in-time scaling. This strategy involves determining the scaling factor based on the maximum absolute value (amax) of the tensor being generated. However, in practical applications, this approach is often infeasible because it necessitates multiple passes through the data. Specifically, the operator first produces and writes out the output in higher precision, then calculates the maximum absolute value of the output, and finally applies this scaling factor to all values to obtain the final FP8 output. This process introduces a significant amount of overhead, which can substantially reduce the benefits of using FP8.

Delayed scaling. This strategy involves selecting the scaling factor based on the maximum absolute values observed in a certain number of preceding iterations. This approach allows for the full performance benefits of FP8 computation but necessitates the storage of a history of maximum values as additional parameters of the FP8 operators.

Figure 9: Scaling gradients to fall within the representation range of the FP8 datatype.

A.3 Pre-training Data

Tab. 10 presents an overview of our collected data sources along with the corresponding sampling weights employed in pre-training. The arXiv and StackExchange subsets are collected from Redpajama (Redpa-jama, 2023), while BookCorpus2 (Zhu et al., 2015), Books3 (Presser, 2020), DM-Math (Saxton et al., 2018), Gutenberg (Rae et al., 2019), HackerNews6, NIH ExPorter7, OpenSubtitles (Tiedemann, 2016), and USPTO8

subsets are extracted from The Pile (Gao et al., 2020). The Wikipedia data is downloaded from HuggingFace (HuggingFace, 2022). We use the 20220301 dump, including 24 languages: bg, ca, cs, da, de, en, es, fr, hi, hr, hu, it, jp, ko, nl, pl, pt, ro, ru, sl, sr, sv, uk, zh. We pre-process 11 CommonCrawl snapshots, ranging from 2018 to 2023, with the CCNet pipeline (Wenzek et al., 2019). This process involves data deduplication at the line level, followed by language identification utilizing a fastText linear classifier (Joulin et al., 2017) to eliminate non-English pages. A filtering mechanism based on an n-gram language model is employed to exclude low-quality content. In addition, we train a linear classifier (Redpajama, 2023) to distinguish documents similar to Wikipedia pages from randomly sampled CommonCrawl documents. Documents not classified as resembling Wikipedia are excluded. Finally, we perform fuzzy deduplication (Lee et al., 2022) across all the processed snapshots from CommonCrawl. We collect Python code data from Github using a repository list provided by Bing indexing (Bing, 2022). The cleaning of the code data includes three steps. First, we remove control characters, except for \t and \n. Next, we remove copyright comments in the code. An alphanumeric rate filter is then applied, removing lines with a rate below 0.5 if they are comments, and discarding the entire file if its overall alphanumeric rate is less than 0.98. Files with less than 5 lines or a maximum line length exceeding 1,000 characters are also discarded. Also, files with an average line length of more than 100 characters are discarded. Lastly, a pattern search is conducted to identify key Python keywords (e.g., import, from, def, class, if, for, try, etc.) within the code. Files containing less than 3 instances of these keywords are eliminated. This comprehensive process ensures that the remaining Python code data is of high quality and suitable for use in academic research. We additionally add Python code from Stack (Kocetkov et al., 2022), and perform fuzzy deduplication within all the collected Python code.

6https://news.ycombinator.com 7https://exporter.nih.gov 8https://bulkdata.uspto.gov

Dataset Sampling prop. Epochs Training Tokens (Billion) Web Crawls

CommonCrawl 51.71% 0.16 51.71 C4 25.56% 0.16 25.56

OpenWebText 2.73% 0.16 2.73 Technical & Science content

arXiv 1.54% 0.05 1.54 StackExchange 1.42% 0.08 1.42

DM-Math 0.39% 0.05 0.39 USPTO 0.52% 0.05 0.52

NIH ExPorter 0.04% 0.05 0.04 Programming Languages

Python 4.50% 0.11 4.50 Other Curated Sources

Wikipedia 4.50% 0.16 4.50 Books 4.50% 0.09 4.50 News 2.00% 0.11 2.00

Dialogue 2.00% 0.27 2.00 Total 100.00

Table 10: Pre-training data. For each subset we list the sampling weight, number of epochs, and training tokens. Books data includes BookCorpus2 (Zhu et al., 2015), Books3 (Presser, 2020), and Gutenberg (Rae et al., 2019). Dialogue data includes HackerNews and OpenSubtitles (Tiedemann, 2016). For experiments with a training token count of less than 100 billion, we employ the same sampling proportion.

