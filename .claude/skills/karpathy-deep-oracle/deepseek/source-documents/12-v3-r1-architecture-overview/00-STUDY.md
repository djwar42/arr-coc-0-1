# V3 & R1 Architecture Overview - Study

**Source**: Fireworks AI (DeepSeek v3 and R1 Model Architecture: Why it's powerful and economical)
**Date Processed**: 2025-10-28
**Category**: DeepSeek Models (Overview)

---

## üìù Summary

Fireworks AI's technical overview of V3 and R1 architectures.

**V3 Coverage**:
- MoE architecture (671B total, 37B active)
- MLA for inference efficiency
- FP8 training economics
- DualPipe for communication

**R1 Coverage**:
- GRPO algorithm for reasoning
- Multi-stage training pipeline
- Distillation to smaller models

**Value**: Accessible technical overview for practitioners

**Complements**: Official technical reports with practical deployment perspective

---

## üîó Cross-References

- [V3 Technical Report](../01-deepseek-v3-technical-report/00-STUDY.md) - Full V3 details
- [R1 Paper](../04-deepseek-r1-paper/00-STUDY.md) - Full R1 details
- `knowledge-categories/model-architectures/` - Architecture deep dive

---

**Last Updated**: 2025-10-28
**Status**: Overview resource
