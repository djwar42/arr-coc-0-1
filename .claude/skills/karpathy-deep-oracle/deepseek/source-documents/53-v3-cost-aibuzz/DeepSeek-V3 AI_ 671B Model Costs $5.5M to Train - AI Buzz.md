---
sourceFile: "DeepSeek-V3 AI: 671B Model Costs $5.5M to Train - AI Buzz"
exportedBy: "Kortex"
exportDate: "2025-10-29T02:36:19.549Z"
---

# DeepSeek-V3 AI: 671B Model Costs $5.5M to Train - AI Buzz

ee03e375-30d1-419a-9baa-9fa8fbfafa66

DeepSeek-V3 AI: 671B Model Costs $5.5M to Train - AI Buzz

3b94badc-f7a4-4f36-8990-3ad4fbcb0888

https://www.ai-buzz.com/deepseek-v3-ai-outperforms-openai-gpt-4o-open-source

## Skip to main content

## Skip to footer

AI Business & Enterprise

AI Legal & Policy

AI Research & Innovation

AI in Software & Tech

AI Tutorials & Guides

## AI Development

## AI Fundamentals

AI Tools & Applications

AI Business & Enterprise

AI Legal & Policy

AI Research & Innovation

AI in Software & Tech

AI Tutorials & Guides

## AI Development

## AI Fundamentals

AI Tools & Applications

DeepSeek-V3 AI: 671B Model Costs $5.5M to Train

Nick Allyn January 16, 2025

https://www.ai-buzz.com

## AI News and Industry Impact

https://www.ai-buzz.com/ai-news

AI Research & Innovation

https://www.ai-buzz.com/ai-news/research

The artificial intelligence (AI) landscape is rapidly shifting. A new contender from China, DeepSeek-V3, is making headlines. This open-source AI model is not only powerful but also incredibly cost-effective, posing a serious challenge to established giants like OpenAI and sparking a debate about the future of AI development.

DeepSeek-V3: The Underdog AI with Overachieving Performance

Developed by the Chinese AI lab DeepSeek, DeepSeek-V3 is turning heads with its impressive performance on various AI benchmarks. What’s remarkable is that it achieved this feat on a shoestring budget compared to its competitors. While OpenAI’s GPT-4 reportedly cost around $100 million to train, DeepSeek-V3 was developed for a mere $5.5 million, according to a

report by Tech Startups

https://techstartups.com/2024/12/30/deepseek-a-free-open-source-ai-model-outperforms-openai-and-metas-latest-models-at-a-fraction-of-the-cost/

. This cost-effectiveness is raising eyebrows and challenging the notion that cutting-edge AI development requires astronomical investments.

DeepSeek-V3 is a Mixture-of-Experts (MoE) language model. In simple terms, it’s like having a team of specialist AI models working together, each an expert in a specific area. This model boasts a whopping 671 billion parameters. However, only 37 billion are actively used for any given task. This strategic activation, as detailed in the

DeepSeek-V3 Technical Report

https://arxiv.org/html/2412.19437v1

, allows the model to deliver top-notch performance without needing excessive computing power.

Andrej Karpathy, a founding member of OpenAI and former director of AI at Tesla, highlighted the significance of this achievement in a

https://twitter.com/karpathy

(formerly Twitter). He noted that while leading AI models often require clusters of 16,000 GPUs and immense computational resources, DeepSeek-V3 was trained using only 2,048 GPUs over two months.

The Nuts and Bolts of DeepSeek-V3’s Efficiency

DeepSeek-V3’s efficiency stems from several innovative techniques:

Mixture-of-Experts (MoE) Architecture:

As mentioned, this “team of experts” approach allows for focused expertise and efficient resource allocation.

Multi-Head Latent Attention (MLA):

This technique optimizes memory usage and enhances the model’s ability to extract crucial information from text, leading to improved accuracy, according to a

post by Dirox

https://dirox.com/post/deepseek-v3-the-open-source-ai-revolution

Auxiliary-Loss-Free Load Balancing:

This feature minimizes performance degradation, a common issue with MoE models, making DeepSeek-V3 a strong contender for computationally demanding tasks.

Multi-Token Prediction (MTP):

This allows the model to predict multiple words simultaneously, dramatically increasing its processing speed. The

## Dirox article

https://dirox.com/post/deepseek-v3-the-open-source-ai-revolution

highlights that DeepSeek-V3 can process information at a rate of 60 tokens per second, three times faster than its predecessor.

These advancements enable DeepSeek-V3 to compete with, and in some cases surpass, some of the most advanced closed-source models available today. Its ability to process up to 128,000 tokens in a single context gives it an edge in tasks requiring a deep understanding of lengthy texts, like legal document review or academic research.

Outperforming the Competition: Benchmarks and Capabilities

DeepSeek-V3 isn’t just cost-effective; it’s a performance powerhouse. In various benchmark tests, it has outperformed models like OpenAI’s GPT-4o and Anthropic’s Claude 3.5 Sonnet. Notably, it excelled in mathematics and coding benchmarks, demonstrating superior problem-solving and programming capabilities. For instance, it surpassed Claude-3.5 Sonnet on the Codeforces benchmark, a popular platform for competitive programming, as highlighted in the

## Dirox article

https://dirox.com/post/deepseek-v3-the-open-source-ai-revolution

. It’s also remarkably strong in Chinese language tasks, showcasing exceptional proficiency.

However, it’s important to acknowledge that DeepSeek-V3 isn’t without its limitations. While it is more efficient than its predecessors, its real-time inference capabilities may require further optimization. Additionally, some have pointed out that its focus on excelling in Chinese language tasks may have come at the expense of its performance in English factual benchmarks.

## According to

## Andrej Karpathy

https://www.linkedin.com/in/andrej-karpathy-9a0b225/

, “While leading models would usually require clusters of 16,000 GPUs and large computational resources, the Chinese lab achieved remarkable results with just 2,048 GPUs, trained for two months at as low as $6 million.”

An anonymous AI researcher from a leading tech company noted, “DeepSeek-V3’s performance is a testament to the power of innovative architectures and efficient training techniques. It challenges the assumption that only massive, resource-intensive models can achieve state-of-the-art results.”

OpenAI’s Dominance Challenged: The Rise of Open-Source AI

OpenAI has long dominated the AI field. Its models like GPT-4 have become household names, integrated into numerous applications. Factors like early market entry, strong brand recognition, substantial funding from partners like Microsoft, and strategic partnerships have fueled OpenAI’s success. In fact, a

report from Replit

https://blog.replit.com/ai-on-replit

states that OpenAI dominates over 80% of distinct AI projects on their platform.

However, the landscape is changing. DeepSeek-V3’s emergence, along with other models, signals a shift towards increased competition. As a

report by The Decoder

https://the-decoder.com/ai-progress-in-2025-will-be-even-more-dramatic-says-anthropic-co-founder/

suggests, AI progress in 2025 is expected to be even more dramatic. The rise of open-source models like DeepSeek-V3 is democratizing access to powerful AI tools, empowering smaller players to compete with industry giants.

The Bigger Picture: Implications for the Future of AI

DeepSeek-V3’s success has significant implications for the future of AI. It demonstrates that state-of-the-art AI development doesn’t necessarily require exorbitant budgets. It also underscores the growing importance of open-source AI in fostering innovation and competition. As countries like the US and China vie for AI supremacy, DeepSeek-V3 shows that the race is far from over and that open-source models may play an increasingly important role in shaping the future of this transformative technology. This budget-friendly powerhouse may pave the way for a more accessible and competitive AI landscape, where innovation thrives beyond the confines of tech giants.

DeepSeek-V3

https://the-decoder.com/ai-progress-in-2025-will-be-even-more-dramatic-says-anthropic-co-founder/

https://www.ai-buzz.com/tag/gpt-4o

Open-Source AI

https://www.ai-buzz.com/tag/open-source-ai

https://www.ai-buzz.com/tag/openai

## Read More From AI Buzz

## Anthropic Overtakes OpenAI in Enterprise AI Market Share

October 28, 2025 . By Nick Allyn

New market data showing Anthropic overtaking OpenAI in enterprise indicates a significant shift in the corporate AI landscape, challenging the narrative of OpenAI’s undisputed dominance.…

AI vs. AI in Real Estate: Dubai’s Fraud Detection Model

October 27, 2025 . By Nick Allyn

The real estate industry is facing a crisis of authenticity as a flood of low-quality, AI-generated content creates what insiders are calling the “AI Slop…

Reddit’s ‘Marked Bill’ Evidence Targets Perplexity AI Scraping

October 24, 2025 . By Nick Allyn

Social media giant Reddit has filed a federal lawsuit against AI startup Perplexity, accusing it of orchestrating an “industrial-scale” scheme to illegally harvest user content…

