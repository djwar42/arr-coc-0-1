---
sourceFile: "DeepSeek - Wikipedia"
exportedBy: "Kortex"
exportDate: "2025-10-29T02:36:11.235Z"
---

# DeepSeek - Wikipedia

2a6bbe19-7cd0-462f-be8b-a2f6ed88d1f2

DeepSeek - Wikipedia

892169e2-7f33-4965-9118-01ce5678f95a

https://en.wikipedia.org/wiki/DeepSeek

## Jump to content

## Main menu    Navigation

## Current events

## Random article

## About Wikipedia

## Contribute

## Learn to edit

## Community portal

## Recent changes

## Upload file

## Special pages

## Create account

## Create account

## Pages for logged out editors

https://en.wikipedia.org/wiki/Help:Introduction

## Contributions

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

1   History

https://en.wikipedia.org/wiki/Help:Introduction

1.1   Founding and early years (2016–2023)

https://en.wikipedia.org/wiki/Help:Introduction

1.2   Model releases (2023–present)

https://en.wikipedia.org/wiki/Help:Introduction

2   Company operation

https://en.wikipedia.org/wiki/Help:Introduction

2.1   Strategy

https://en.wikipedia.org/wiki/Help:Introduction

3   Training framework

https://en.wikipedia.org/wiki/Help:Introduction

4   Development and release history

https://en.wikipedia.org/wiki/Help:Introduction

5   Overview of models

https://en.wikipedia.org/wiki/Help:Introduction

5.1   DeepSeek Coder

https://en.wikipedia.org/wiki/Help:Introduction

5.2   DeepSeek-LLM

https://en.wikipedia.org/wiki/Help:Introduction

5.2.1   MoE

https://en.wikipedia.org/wiki/Help:Introduction

5.2.2   Math

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

6   Significance

https://en.wikipedia.org/wiki/Help:Introduction

7   See also

https://en.wikipedia.org/wiki/Help:Introduction

8   Notes

https://en.wikipedia.org/wiki/Help:Introduction

9   References

https://en.wikipedia.org/wiki/Help:Introduction

10   External links

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

Azərbaycanca

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

Беларуская (тарашкевіца)

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

## Bahasa Indonesia

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

Oʻzbekcha / ўзбекча

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

## Qaraqalpaqsha

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

## Simple English

https://en.wikipedia.org/wiki/Help:Introduction

Slovenščina

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

Српски / srpski

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

ئۇيغۇرچە / Uyghurche

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

## Tools    Actions

https://en.wikipedia.org/wiki/Help:Introduction

https://en.wikipedia.org/wiki/Help:Introduction

## View history

https://en.wikipedia.org/wiki/Help:Introduction

## What links here

https://en.wikipedia.org/wiki/Help:Introduction

## Related changes

https://en.wikipedia.org/wiki/Help:Introduction

## Upload file

https://en.wikipedia.org/wiki/Help:Introduction

## Permanent link

https://en.wikipedia.org/wiki/Help:Introduction

## Page information

https://en.wikipedia.org/wiki/Help:Introduction

## Cite this page

https://en.wikipedia.org/wiki/Help:Introduction

## Get shortened URL

https://en.wikipedia.org/wiki/Help:Introduction

## Download QR code

https://en.wikipedia.org/wiki/Help:Introduction

Print/export

## Download as PDF

https://en.wikipedia.org/wiki/Help:Introduction

## Printable version

https://en.wikipedia.org/wiki/Help:Introduction

## In other projects

## Wikimedia Commons

https://en.wikipedia.org/wiki/Help:Introduction

## Wikidata item

https://en.wikipedia.org/wiki/Help:Introduction

From Wikipedia, the free encyclopedia   Chinese artificial intelligence company   This article is about the company. For the chatbot, see

DeepSeek (chatbot)

https://en.wikipedia.org/wiki/DeepSeek_(chatbot)

Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd.

Native name 杭州深度求索人工智能基础技术研究有限公司 Company type

https://en.wikipedia.org/wiki/Privately_held_company

## Information technology

https://en.wikipedia.org/wiki/Information_technology

## Artificial intelligence

https://en.wikipedia.org/wiki/Information_technology

Founded 17 July 2023 ; 2 years ago  ( 2023-07-17 )

## Liang Wenfeng

## Headquarters

https://en.wikipedia.org/wiki/Hangzhou

https://en.wikipedia.org/wiki/Zhejiang

, China Key people

Liang Wenfeng (CEO)

https://en.wikipedia.org/wiki/High-Flyer

Number of employees 160 (2025)

deepseek.com

https://www.deepseek.com/

Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd.

doing business as

is a Chinese

artificial intelligence

https://en.wikipedia.org/wiki/Artificial_intelligence

(AI) company that develops

large language models

https://en.wikipedia.org/wiki/Large_language_model

(LLMs). Based in

Hangzhou, Zhejiang

https://en.wikipedia.org/wiki/Hangzhou,_Zhejiang

, Deepseek is owned and funded by the Chinese

https://en.wikipedia.org/wiki/Hedge_fund

https://en.wikipedia.org/wiki/Hedge_fund

. DeepSeek was founded in July 2023 by

## Liang Wenfeng

https://en.wikipedia.org/wiki/Liang_Wenfeng

, the co-founder of High-Flyer, who also serves as the

https://en.wikipedia.org/wiki/Chief_executive_officer

for both of the companies.

## The company launched

an eponymous chatbot

https://en.wikipedia.org/wiki/DeepSeek_(chatbot)

alongside its DeepSeek-R1 model in January 2025.

## Released under the

## MIT License

https://en.wikipedia.org/wiki/MIT_License

, DeepSeek-R1 provides responses comparable to other contemporary large language models, such as

https://en.wikipedia.org/wiki/OpenAI

https://en.wikipedia.org/wiki/GPT-4

https://en.wikipedia.org/wiki/OpenAI_o1

Its training cost was reported to be significantly lower than other LLMs. The company claims that it trained its V3 model for $6 US million—far less than the $100+ US million cost for OpenAI's

https://en.wikipedia.org/wiki/GPT-4

—and using approximately one-tenth the computing power consumed by

https://en.wikipedia.org/wiki/Meta_Platforms

's comparable model,

https://en.wikipedia.org/wiki/Llama_3.1

DeepSeek's success against larger and more established rivals has been described as "upending AI".

DeepSeek's models are described as "open weight," meaning the exact parameters are openly shared, although certain usage conditions differ from typical

open-source software

https://en.wikipedia.org/wiki/Open-source_software

## The company reportedly recruits AI researchers from top Chinese universities

and also hires from outside traditional

computer science

https://en.wikipedia.org/wiki/Computer_science

fields to broaden its models' knowledge and capabilities.

DeepSeek significantly reduced training expenses for their R1 model by incorporating techniques such as

mixture of experts

https://en.wikipedia.org/wiki/Mixture_of_experts

(MoE) layers.

The company also trained its models during ongoing trade restrictions on AI chip exports to China, using weaker AI chips intended for export and employing fewer units overall.

Observers say this breakthrough sent "shock waves" through the industry which were described as triggering a "

## Sputnik moment

https://en.wikipedia.org/wiki/Sputnik_moment

" for the US in the field of artificial intelligence, particularly due to its open-source, cost-effective, and high-performing AI models.

## This threatened established AI hardware leaders such as

https://en.wikipedia.org/wiki/Nvidia

; Nvidia's share price dropped sharply, losing US$600 billion in market value, the largest single-company decline in U.S.

stock market

https://en.wikipedia.org/wiki/Stock_market

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=1

Founding and early years (2016–2023)

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=2

In February 2016, High-Flyer was co-founded by AI enthusiast

## Liang Wenfeng

https://en.wikipedia.org/wiki/Liang_Wenfeng

, who had been trading since the

2008 financial crisis

https://en.wikipedia.org/wiki/2008_financial_crisis

while attending

## Zhejiang University

https://en.wikipedia.org/wiki/Zhejiang_University

## The company began stock trading using a

https://en.wikipedia.org/wiki/GPU

-dependent deep learning model on 21 October 2016; before then, it had used

https://en.wikipedia.org/wiki/CPU

-based linear models. By the end of 2017, most of its trading was driven by AI.

Liang established High-Flyer as a hedge fund focused on developing and using AI trading algorithms, and by 2021 the firm was using AI exclusively,

often using

https://en.wikipedia.org/wiki/Nvidia

In 2019, the company began constructing its first

computing cluster

https://en.wikipedia.org/wiki/Computing_cluster

, Fire-Flyer, at a cost of 200 million yuan; it contained 1,100 GPUs interconnected at 200 Gbit/s and was retired after 1.5 years in operation.

By 2021, Liang had started buying large quantities of Nvidia GPUs for an AI project,

reportedly obtaining 10,000

Nvidia A100

https://en.wikipedia.org/wiki/Ampere_(microarchitecture)#A100_accelerator_and_DGX_A100

before the United States restricted chip sales to China.

Computing cluster Fire-Flyer 2 began construction in 2021 with a budget of 1 billion yuan.

It was reported that in 2022, Fire-Flyer 2's capacity had been used at over 96%, totaling 56.74 million GPU hours. 27% was used to support scientific computing outside the company.

During 2022, Fire-Flyer 2 had 5,000

https://en.wikipedia.org/wiki/PCI_Express

A100 GPUs in 625 nodes, each containing 8 GPUs. At the time, it exclusively used PCIe instead of the

https://en.wikipedia.org/wiki/Nvidia_DGX

version of A100, since at the time the models it trained could fit within a single 40 GB GPU

https://en.wikipedia.org/wiki/Video_random-access_memory

and so there was no need for the higher bandwidth of DGX (i.e., it required only data parallelism but not model parallelism).

Later, it incorporated

https://en.wikipedia.org/wiki/NVLink

and NCCL (Nvidia Collective Communications Library) to train larger models that required model parallelism.

On 14 April 2023,

High-Flyer announced the launch of an

artificial general intelligence

https://en.wikipedia.org/wiki/Artificial_general_intelligence

(AGI) research lab, stating that the new lab would focus on developing AI tools unrelated to the firm's financial business.

Two months later, on 17 July 2023,

that lab was spun off into an independent company, DeepSeek, with High-Flyer as its principal investor and backer.

## Venture capital

investors were reluctant to provide funding, as they considered it unlikely that the venture would be able to quickly generate an "

https://en.wiktionary.org/wiki/exit

Model releases (2023–present)

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=3

DeepSeek released its first model, DeepSeek Coder, on 2 November 2023, followed by the DeepSeek-LLM series on 29 November 2023.

: section 5

In January 2024, it released two DeepSeek-MoE models (Base and Chat),

and in April 3rd DeepSeek-Math models (Base, Instruct, and RL).

DeepSeek-V2 was released in May 2024, followed a month later by the DeepSeek-Coder V2 series.

In September 2024, DeepSeek V2.5 was introduced and revised in December.

On 20 November 2024, the preview of DeepSeek-R1-Lite became available via chat.

In December, DeepSeek-V3-Base and DeepSeek-V3 (chat) were released.

## The DeepSeek login page following a

cyberattack

https://en.wikipedia.org/wiki/Cyberattack

around its 21 January 2025 launch

On 20 January 2025, DeepSeek launched the

## DeepSeek chatbot

https://en.wikipedia.org/wiki/DeepSeek_(chatbot)

—based on the DeepSeek-R1 model—free for

https://en.wikipedia.org/wiki/IOS

https://en.wikipedia.org/wiki/Android_(operating_system)

. By 27 January, DeepSeek surpassed

https://en.wikipedia.org/wiki/ChatGPT

as the most downloaded freeware app on the

iOS App Store

https://en.wikipedia.org/wiki/App_Store_(iOS)

in the United States,

triggering an 18% drop in Nvidia's share price.

On 24 March 2025, DeepSeek released DeepSeek-V3-0324 under the MIT License.

On 28 May 2025, DeepSeek released DeepSeek-R1-0528 under the MIT License.

## The model has been noted for more tightly following official

## Chinese Communist Party ideology

https://en.wikipedia.org/wiki/Ideology_of_the_Chinese_Communist_Party

https://en.wikipedia.org/wiki/Censorship_in_China

in its answers to questions than prior models.

On August 21, 2025, DeepSeek released DeepSeek V3.1 under the MIT License.

This model features a hybrid architecture with thinking and non-thinking modes. It also surpasses prior models like V3 and R1, by over 40% on certain benchmarks like SWE-bench and Terminal-bench.

It was updated to V3.1-Terminus on September 22, 2025.

V3.2-Exp was released on September 29 2025. It uses DeepSeek Sparse Attention, a more efficient

attention mechanism

https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#Sub-quadratic_transformers

based on previous research published in February.

## Company operation

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=4

DeepSeek is headquartered in Hangzhou, Zhejiang, and is owned and funded by

https://en.wikipedia.org/wiki/High-Flyer

. Its co-founder,

## Liang Wenfeng

https://en.wikipedia.org/wiki/Liang_Wenfeng

, serves as CEO. As of May 2024, Liang personally held an 84% stake in DeepSeek through two

shell corporations

https://en.wikipedia.org/wiki/Shell_corporation

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=5

DeepSeek has stated that it focuses on research and does not have immediate plans for commercialization.

This posture also means it can skirt certain provisions of China's AI regulations aimed at consumer-facing technologies.

DeepSeek's hiring approach emphasizes skills over lengthy work experience, resulting in many hires fresh out of university.

The company likewise recruits individuals without computer science backgrounds to expand the range of expertise incorporated into the models, for instance in poetry or advanced mathematics.

## According to

## The New York Times

, dozens of DeepSeek researchers have or have previously had affiliations with

People's Liberation Army

https://en.wikipedia.org/wiki/People%27s_Liberation_Army

laboratories and the

## Seven Sons of National Defence

https://en.wikipedia.org/wiki/Seven_Sons_of_National_Defence

DeepSeek also expanded on the African continent as it offers more affordable and less power-hungry AI solutions. The company has bolstered African language models and generated a number of startups, for example in

https://en.wikipedia.org/wiki/Nairobi

. Along with

https://en.wikipedia.org/wiki/Huawei

's storage and cloud computing services, the impact on the tech scene in sub-saharan Africa is considerable. DeepSeek offers local data sovereignty and more flexibility compared to Western AI platforms.

## Training framework

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=6

High-Flyer/DeepSeek had operated at least two primary computing clusters: Fire-Flyer (萤火一号) and Fire-Flyer 2 (萤火二号). Fire-Flyer 1 was constructed in 2019 and was retired after 1.5 years of operation. Fire-Flyer 2 is still in operation as of 2025. Fire-Flyer 2 consists of co-designed software and hardware architecture. On the hardware side, Nvidia GPUs use 200

https://en.wikipedia.org/wiki/Data-rate_units

interconnects. The cluster is divided into two "zones", and the platform supports cross-zone tasks. The network topology was two

https://en.wikipedia.org/wiki/Fat_tree

, chosen for high

bisection bandwidth

https://en.wikipedia.org/wiki/Bisection_bandwidth

. On the software side are:

3FS  (Fire-Flyer File System): A

distributed parallel file system

https://en.wikipedia.org/wiki/Clustered_file_system

, specifically designed for asynchronous random reads. It uses Direct I/O and

https://en.wikipedia.org/wiki/Remote_direct_memory_access

. In contrast to standard Buffered I/O, Direct I/O does not cache data. Caching is useless in this case, since each data read is random and is not reused.

hfreduce : Library for asynchronous communication, originally designed to replace Nvidia Collective Communication Library (NCCL).

## It is mainly used for

https://en.wikipedia.org/wiki/Allreduce

, especially of gradients during

backpropagation

https://en.wikipedia.org/wiki/Backpropagation

. It is asynchronously run on the CPU to avoid blocking

https://en.wikipedia.org/wiki/Compute_kernel

on the GPU.

two-tree broadcast

https://en.wikipedia.org/wiki/Two-tree_broadcast

like NCCL.

hfai.nn : Software library of commonly used operators for neural network training, similar to  torch.nn  in

https://en.wikipedia.org/wiki/PyTorch

HaiScale Distributed Data Parallel  (DDP): Parallel training library that implements various forms of parallelism such as

## Data Parallelism

https://en.wikipedia.org/wiki/Data_parallelism

## Pipeline Parallelism

https://en.wikipedia.org/wiki/Pipeline_(computing)

(PP), Tensor Parallelism (TP), Experts Parallelism (EP), Fully Sharded Data Parallel (FSDP) and Zero Redundancy Optimizer (ZeRO). It is similar to PyTorch DDP, which uses NCCL on the backend.

HAI Platform : Various applications such as task scheduling, fault handling, and disaster recovery.

As of 2022, Fire-Flyer 2 had 5,000

https://en.wikipedia.org/wiki/PCI_Express

A100 GPUs in 625 nodes, each containing 8 GPUs.

It later incorporated NVLinks and NCCL to train larger models that required model parallelism.

## Development and release history

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=7

]   Major versions of DeepSeek models. SFT stands for supervised finetuning.  Major versions  Release date  Status  Major variants  Remarks  DeepSeek Coder  November 2, 2023   Discontinued  Base (pretrained); Instruct (with instruction-finetuned)  The architecture is essentially the same as Llama.  DeepSeek-LLM  November 29, 2023   Discontinued  Base;

Chat (with SFT)

DeepSeek-MoE  January 9, 2024   Discontinued  Base;

## Developed a variant of

mixture of experts

https://en.wikipedia.org/wiki/Mixture_of_experts

(MoE).  DeepSeek-Math  April 2024   Discontinued  Base  Initialized with DS-Coder-Base-v1.5  Instruct (with SFT)  RL (using a process reward model)  Developed

## Group Relative Policy Optimization

https://en.wikipedia.org/wiki/Group_Relative_Policy_Optimization

(GRPO), a variant of

## Proximal Policy Optimization

https://en.wikipedia.org/wiki/Proximal_Policy_Optimization

(PPO).  DeepSeek V2  May 2024   Discontinued  DeepSeek-V2, DeepSeek-V2-Chat

DeepSeek-V2-Lite, DeepSeek-V2-Lite-Chat

DeepSeek-Coder-V2

DeepSeek-V2.5

Developed multi-head latent attention (MLA). Also used mixture of experts (MoE).

Implemented KV caching.

DeepSeek V3  December 2024   Active  DeepSeek-V3-Base 
 DeepSeek-V3 (a chat model)  The architecture is essentially the same as V2. Updated on 2025-03-24.  DeepSeek-Prover-V2  May 1, 2025   Active  DeepSeek-Prover-V2-671B 
 DeepSeek-Prover-V2-7B  DeepSeek VL2  December 13, 2024   Active  DeepSeek R1  November 20, 2024   Active  DeepSeek-R1-Lite-Preview  Only accessed through API and a chat interface.  January 20, 2025   Active  DeepSeek-R1

DeepSeek-R1-Zero

Initialized from DeepSeek-V3-Base and sharing the V3 architecture.  Distilled models  Initialized from other models, such as Llama, Qwen, etc. Distilled from data synthesized by R1 and R1-Zero.

May 28, 2025   Active  DeepSeek-R1-0528  DeepSeek V3.1  August 21, 2025   Active  DeepSeek-V3.1-Base 
 DeepSeek-V3.1 (a chat model)  Hybrid architecture (thinking and non-thinking modes available). Trained on over 800B additional tokens on top of V3.  September 22, 2025   Active  DeepSeek-V3.1-Terminus  Reducing instances of mixed Chinese-English text and occasional abnormal characters on top of V3.1.

The first DeepSeek models were essentially the same as Llama,

which were dense decoder-only

transformers

https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)

. Later models incorporated the multi-head latent attention (MLA), Mixture of Experts (MoE), and KV caching.

decoder-only transformer

https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#decoder-only

consists of multiple identical decoder layers. Each of these layers features two main components: an attention layer and a

feedforward network

https://en.wikipedia.org/wiki/Feedforward_neural_network

(FFN) layer.

V2 replaced the standard

multi-head attention mechanism

https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#MHA

(MHA) with

multi-head latent attention

https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#MLA

(MLA). This introduces compressed latent vectors to reduce

KV (key–value) cache size

https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#KV_caching

, and thus memory usage.

## A standard MoE Transformer generally use the

sparsely-gated MoE

https://en.wikipedia.org/wiki/Mixture_of_experts#Sparsely-gated_MoE_layer

layers in the FFN layers. In such an MoE layer, there are several FFN modules in parallel ("routed experts") and a small classifier ("gate") to compute a score for all these modules upon each token. Only the highest-scoring modules are activated. Starting with DeepSeekMoE, DeepSeek adopted a variant that adds "shared experts", which are always activated.

## Overview of models

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=8

## This section

relies excessively on

https://en.wikipedia.org/wiki/Wikipedia:Verifiability

primary sources

https://en.wikipedia.org/wiki/Wikipedia:No_original_research#Primary,_secondary_and_tertiary_sources

.  Please improve this section by adding

secondary or tertiary sources

https://en.wikipedia.org/wiki/Wikipedia:No_original_research#Primary,_secondary_and_tertiary_sources

Find sources:

https://www.google.com/search?tbm=nws&q=%22DeepSeek%22+-wikipedia&tbs=ar:1

https://www.google.com/search?tbm=nws&q=%22DeepSeek%22+-wikipedia&tbs=ar:1

https://www.google.com/search?tbm=nws&q=%22DeepSeek%22+-wikipedia&tbs=ar:1

https://www.google.com/search?tbm=nws&q=%22DeepSeek%22+-wikipedia&tbs=ar:1

https://www.google.com/search?tbm=nws&q=%22DeepSeek%22+-wikipedia&tbs=ar:1

( February 2025 )

## Learn how and when to remove this message

DeepSeek's models are "open weight", which provides less freedom for modification than true

open source

https://en.wikipedia.org/wiki/Open_source

## DeepSeek Coder

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=9

DeepSeek Coder is a series of eight models, four pretrained ( Base ) and four instruction-finetuned ( Instruct ). All have 16K context lengths. The model was made

source-available

https://en.wikipedia.org/wiki/Source-available

under the DeepSeek License, which includes "open and responsible downstream usage" restrictions.

https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets

program was:

Pretraining: 1.8T tokens (87% source code, 10% code-related English (GitHub markdown and

## Stack Exchange

https://en.wikipedia.org/wiki/Stack_Exchange

), and 3% code-unrelated Chinese).

Long-context pretraining: 200B tokens. This extends the context length from 4K to 16K. This produced the  Base  models.

## Supervised

https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)

(SFT): 2B tokens of instruction data. This produced the  Instruct  models.

They were trained on clusters of A100 and

https://en.wikipedia.org/wiki/Hopper_(microarchitecture)

Nvidia GPUs, connected by

https://en.wikipedia.org/wiki/InfiniBand

https://en.wikipedia.org/wiki/NVLink

https://en.wikipedia.org/wiki/NVSwitch

## DeepSeek Coder properties

Params .   #  Layers   Model  dim.   Intermediate  dim.   # Heads   # Kv-heads  1.3B   24   2048  5504   16   16  5.7B  32  4096  11008  32  1

6.7B   32   4096  11008   32   32  33B   62   7168  19200   56   7

DeepSeek-LLM

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=10

The DeepSeek-LLM series was released in November 2023. It has 7B and 67B parameters in both Base and Chat forms. DeepSeek's accompanying paper claimed benchmark results higher than

https://en.wikipedia.org/wiki/Llama_2

and most open-source LLMs at the time.

: section 5

The model code is under the source-available DeepSeek License.

## The architecture was essentially the same as the

https://en.wikipedia.org/wiki/Llama_(language_model)

series. They used the

https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#pre-LN

decoder-only Transformer

https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#pre-LN

https://en.wikipedia.org/wiki/RMSNorm

as the normalization,

https://en.wikipedia.org/wiki/SwiGLU

in the feedforward layers,

rotary positional embedding

https://en.wikipedia.org/wiki/Rotary_positional_embedding

(RoPE), and

grouped-query attention

https://en.wikipedia.org/wiki/Grouped-query_attention

(GQA). Both had vocabulary size 102,400 (

byte-level BPE

https://en.wikipedia.org/wiki/Byte_pair_encoding#Byte-level_BPE

) and context length of 4096. They trained on 2 trillion tokens of English and Chinese text obtained by deduplicating the

## Common Crawl

https://en.wikipedia.org/wiki/Common_Crawl

## DeepSeek LLM properties

Params .   # Layers   Model  dim.   Intermediate  dim.   # Heads   # Kv-heads  7B   30   4096  11008   32   32  67B   95   8192  22016   64   8

The Chat versions of the two Base models was released concurrently, obtained by training Base by

supervised finetuning (SFT) followed by direct policy optimization (DPO)

https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=11

DeepSeek-MoE models (Base and Chat), each have 16B parameters (2.7B activated per token, 4K context length). The training was essentially the same as DeepSeek-LLM 7B, and was trained on a part of its training dataset. They claimed performance comparable to a 16B MoE as a 7B non-MoE. It is a variant of the standard

sparsely-gated MoE

https://en.wikipedia.org/wiki/Mixture_of_experts#Sparsely-gated_MoE_layer

, with "shared experts" that are always queried, and "routed experts" that might not be. They found this to help with expert balancing. In standard MoE, some experts can become overused, while others are rarely used, wasting space. Attempting to balance expert usage causes experts to replicate the same capacity. They proposed the shared experts to learn core capacities that are often used, and let the routed experts learn peripheral capacities that are rarely used.

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=12

DeepSeek-Math includes 3 models: Base, Instruct, and RL. Math was trained as follows:

Initialize with a previously pretrained DeepSeek-Coder Base v1.5 7B.

Further pretrain with 500B tokens (6% DeepSeekMath Corpus, 4% AlgebraicStack, 10% arXiv, 20% GitHub code, 10% Common Crawl). This produced Base.

Train an instruction-following model by SFT Base with 776K math problems and tool-use-integrated step-by-step solutions. This produced Instruct.

## Reinforcement learning

(RL): The reward model was a

process reward model

https://en.wikipedia.org/wiki/Reasoning_language_model#PRM

(PRM) trained from Base according to the Math-Shepherd method.

## This reward model was then used to train Instruct using

## Group Relative Policy Optimization

https://en.wikipedia.org/wiki/Group_Relative_Policy_Optimization

(GRPO) on a dataset of 144K math questions "related to

GSM8K and MATH

https://en.wikipedia.org/wiki/Language_model_benchmark

". The reward model was continuously updated during training to avoid reward hacking. This resulted in RL.

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=13

The architecture of V2, showing both shared-routed MoE and MLA

: Figure 2

In May 2024, DeepSeek released the DeepSeek-V2 series. The series includes 4 models, 2 base models (DeepSeek-V2, DeepSeek-V2 Lite) and 2 chatbots (Chat). The two larger models were trained as follows:

Pretrain on a dataset of 8.1T tokens, using 12% more Chinese tokens than English ones.

Extend context length from 4K to 128K using YaRN.

This resulted in DeepSeek-V2.

SFT with 1.2M instances for helpfulness and 0.3M for safety. This resulted in Chat SFT, which was not released.

RL using GRPO in two stages. The first stage was trained to solve math and coding problems. This stage used 1 reward model, trained on compiler feedback (for coding) and ground-truth labels (for math). The second stage was trained to be helpful, safe, and follow rules. This stage used 3 reward models. The helpfulness and safety reward models were trained on human preference data. The rule-based reward model was manually programmed. All trained reward models were initialized from Chat (SFT). This resulted in the released version of Chat.

They opted for 2-staged RL, because they found that RL on reasoning data had "unique characteristics" different from RL on general data. For example, RL on reasoning could improve over more training steps.

The two V2-Lite models were smaller, and trained similarly. DeepSeek-V2 Lite-Chat underwent only SFT, not RL. They trained the Lite version to help "further research and development on MLA and DeepSeekMoE".

Architecturally, the V2 models were significantly different from the DeepSeek LLM series. They changed the standard attention mechanism by a

low-rank approximation

https://en.wikipedia.org/wiki/Low-rank_approximation

multi-head latent attention

https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#MLA

(MLA), and used the previously published

mixture of experts

https://en.wikipedia.org/wiki/Mixture_of_experts

(MoE) variant.

DeepSeek V2 properties

: Section 3.1.2, Appendix B

Name  Params .  Active  params   # Layers   Context length  # Shared experts   # Routed experts  V2-Lite  15.7B  2.4B   27   32K  2   64  V2  236B  21B   60   128K  2   160

## Financial Times

reported that it was cheaper than its peers with a price of 2

https://en.wikipedia.org/wiki/Renminbi

for every million output tokens. The

## University of Waterloo

https://en.wikipedia.org/wiki/University_of_Waterloo

Tiger Lab's leaderboard ranked DeepSeek-V2 seventh on its LLM ranking.

The DeepSeek-Coder V2 series included V2-Base, V2-Lite-Base, V2-Instruct, and V20-Lite-Instruct.. Training:

Base models were initialized from corresponding intermediate checkpoints after pretraining on 4.2T tokens (not the version at the end of pretraining), then pretrained further for 6T tokens, then context-extended to 128K context length.

DeepSeek-Coder and DeepSeek-Math were used to generate 20K code-related and 30K math-related instruction data, then combined with an instruction dataset of 300M tokens. This was used for SFT.

RL with GRPO. The reward for math problems was computed by comparing with the ground-truth label. The reward for code problems was generated by a reward model trained to predict whether a program would pass the unit tests.

DeepSeek-V2.5 was made by combining DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct.

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=14

Multi-token prediction

DeepSeek-V3-Base and DeepSeek-V3 (a chat model) use essentially the same architecture as V2 with the addition of

multi-token prediction

https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#Multi-Token_Prediction

, which (optionally) decodes extra tokens faster but less accurately. Training process:

Pretraining on 14.8T tokens of a multilingual corpus, mostly English and Chinese. It contained a higher ratio of math and programming than the pretraining dataset of V2.

Extend context length twice, from 4K to 32K and then to 128K, using YaRN.

This produced DeepSeek-V3-Base.

SFT for 2 epochs on 1.5M samples of reasoning (math, programming, logic) and non-reasoning (creative writing, roleplay, simple question answering) data. Reasoning data was generated by "expert models". Non-reasoning data was generated by DeepSeek-V2.5 and checked by humans.

The "expert models" were trained by starting with an unspecified base model, then SFT on both <problem, original response> data, and synthetic <system prompt, prompt, problem, R1 response> data generated by an internal DeepSeek-R1-Lite model. The system prompt asked R1 to reflect and verify during thinking. Then the expert models were RL using an undisclosed reward function.

Each expert model was trained to generate just synthetic reasoning data in one specific domain (math, programming, logic).

Expert models were used instead of R1 itself, since the output from R1 itself suffered "overthinking, poor formatting, and excessive length".

Model-based reward models were made by starting with a SFT checkpoint of V3, then finetuning on human preference data containing both final reward and chain-of-thought leading to the final reward. The reward model produced reward signals for both questions with objective but free-form answers, and questions without objective answers (such as creative writing).

An SFT checkpoint of V3 was trained by GRPO using both reward models and rule-based reward. The rule-based reward was computed for math problems with a final answer (put in a box), and for programming problems by unit tests. This produced DeepSeek-V3.

DeepSeek released its DeepSeek-V3-0324 model, which used the same architecture as V3, on 24 March 2025 under the MIT License.

DeepSeek V3 properties

: Section 4.2

Name  Params .  Active  params   # Layers   Context length  # Shared experts   # Routed experts  V3  671B  37B   61   128K  1   256

Mixed-precision framework for  V3

: Figure 6

The DeepSeek team performed extensive low-level engineering to improve efficiency. They used

mixed-precision arithmetic

https://en.wikipedia.org/wiki/Mixed-precision_arithmetic

. Much of the forward pass was performed in

8-bit floating point numbers

https://en.wikipedia.org/wiki/Floating-point_arithmetic

(5E2M: 5-bit exponent and 2-bit

https://en.wikipedia.org/wiki/Mantissa_(floating_point_number)

) rather than the standard

https://en.wikipedia.org/wiki/Single-precision_floating-point_format

, requiring special

https://en.wikipedia.org/wiki/General_matrix_multiply

routines to accumulate accurately. They used a custom 12-bit float (E5M6) only for the inputs to the linear layers after the attention modules. Optimizer states were in 16-bit (

https://en.wikipedia.org/wiki/Bfloat16_floating-point_format

). They minimized communication latency by extensively overlapping computation and communication, such as dedicating 20 streaming multiprocessors out of 132 per H800 for only inter-GPU communication. They lowered communication by rearranging (every 10 minutes) the exact machine each expert was on so as to avoid querying certain machines more often than others, adding auxiliary load-balancing losses to the training loss function, and other load-balancing techniques.

After training, it was deployed on clusters of H800 GPUs. The 8 H800 GPUs within a cluster were connected by NVLink, and the clusters were connected by InfiniBand.

Total cost of training the DeepSeek-V3 model

Stage  Cost (in one thousand GPU hours)  Cost (in one million USD$)  Pre-training  2,664  5.328  Context extension  119  0.24  Fine-tuning  5  0.01  Total  2,788  5.576

## The cost has been discussed

and called misleading, because it covers only parts of the true cost.

Benchmark tests show that V3 outperformed

https://en.wikipedia.org/wiki/Llama_(language_model)

https://en.wikipedia.org/wiki/Qwen

2.5 while matching

https://en.wikipedia.org/wiki/GPT-4o

https://en.wikipedia.org/wiki/Claude_(language_model)

3.5 Sonnet.

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=15

]   See also:

## Reasoning language model

https://en.wikipedia.org/wiki/Reasoning_language_model

The multistage training pipeline of DeepSeek-R1

In January 2025, DeepSeek released the DeepSeek-R1 model under the

## MIT License

https://en.wikipedia.org/wiki/MIT_License

DeepSeek-R1-Lite-Preview

was trained for logical inference, mathematical reasoning, and real-time problem-solving. DeepSeek claimed that it exceeded performance of

https://en.wikipedia.org/wiki/OpenAI_o1

on benchmarks such as

## American Invitational Mathematics Examination

https://en.wikipedia.org/wiki/American_Invitational_Mathematics_Examination

(AIME) and MATH.

## The Wall Street Journal

reported that on 15 problems from the 2024 edition of AIME, the o1 model reached a solution faster.

DeepSeek-R1 and DeepSeek-R1-Zero

were initialized from DeepSeek-V3-Base and share its architecture. DeepSeek-R1-Distill models were instead initialized from other pretrained open-weight models, including

https://en.wikipedia.org/wiki/Llama_(language_model)

https://en.wikipedia.org/wiki/Qwen

, then fine-tuned on

synthetic data

https://en.wikipedia.org/wiki/Synthetic_data

generated by R1.

Template for  DeepSeek-R1-Zero

A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: <prompt>. Assistant:

– <prompt> is replaced with the specific reasoning question during training.

DeepSeek-R1-Zero was trained exclusively using GRPO RL without SFT. Unlike previous versions, it used no model-based reward. All reward functions were rule-based, "mainly" of two types (other types were not specified): accuracy rewards and format rewards. Accuracy reward was checking whether a boxed answer is correct (for math) or whether a code passes tests (for programming). Format reward was checking whether the model puts its thinking trace within a <think>...</think> tag.

R1-Zero has issues with readability and mixing languages. R1 was trained to address these issues and further improve reasoning:

SFT DeepSeek-V3-Base on "thousands" of "cold-start" data all with the standard format of  |special_token|<reasoning_process>|special_token|<summary> , designed to improve model output readability.

Apply the same GRPO RL process as R1-Zero, adding a "language consistency reward" to encourage it to respond monolingually. This produced an un released internal model.

Synthesize 600K reasoning data from the internal model, with rejection sampling (i.e. if the generated reasoning had a wrong final answer, then it is removed). Synthesize 200K non-reasoning data (writing, factual QA, self-cognition, translation) using DeepSeek-V3.

SFT DeepSeek-V3-Base on the 800K synthetic data for 2 epochs.

Apply the same GRPO RL process as R1-Zero with rule-based reward (for reasoning tasks), but also model-based reward (for non-reasoning tasks, helpfulness, and harmlessness). This produced DeepSeek-R1.

Distilled models were trained by SFT on 800K data synthesized from DeepSeek-R1, in a similar way as step 3. They were not trained with RL.

There were reports that R2, the intended successor to R1, was originally planned for release in early May 2025.

However, on 28 May 2025, R1 was instead updated to version R1-0528.

As of early July, R2 was not yet released, as Liang Wenfeng was not yet satisfied with its performance. Most Chinese cloud providers of R1 used

https://en.wikipedia.org/wiki/Nvidia_H20_GPU

As of August, R2 was not yet released. Sources cite slow data labelling and chip problems. Specifically, DeepSeek was encouraged by authorities to adopt Huawei’s Ascend chips for training, but it had stability issues, slower inter-chip connectivity and inferior software. Consequently it has opted to use Nvidia chips for training and Huawei chips for inference.

## It is also reported that the

## Cyberspace Administration of China

https://en.wikipedia.org/wiki/Cyberspace_Administration_of_China

requested several large corporations to stop buying Nvidia H20 and buy from domestic suppliers instead.

With the release of R1 in January, the DeepSeek team published a preprint on arXiv.

Later, an updated version was published in

in September.

## Significance

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=16

DeepSeek's success against larger and more established rivals was a surprise to both the industry and to markets,

and has been compared by investors and pundits to the "

## Sputnik moment

https://en.wikipedia.org/wiki/Sputnik_crisis

The DeepSeek-R1 model provides responses comparable to other contemporary large language models, such as

https://en.wikipedia.org/wiki/OpenAI

https://en.wikipedia.org/wiki/GPT-4o

https://en.wikipedia.org/wiki/OpenAI_o1

https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets

cost is reported to be significantly lower than other LLMs.

The company claims that it trained V3, a predecessor of R1, for US$6 million compared to $100 million for OpenAI's

https://en.wikipedia.org/wiki/GPT-4

and approximately one tenth of the computing power used for

https://en.wikipedia.org/wiki/Meta_Platforms

's comparable model,

https://en.wikipedia.org/wiki/Llama_(language_model)

After the January 2025 release of the R1 model, which offered significantly lower costs than competing models, some investors anticipated a

https://en.wikipedia.org/wiki/Price_war

in the American AI industry.

It was dubbed the "

https://en.wikipedia.org/wiki/Pinduoduo

of AI", and other Chinese tech giants such as

https://en.wikipedia.org/wiki/ByteDance

https://en.wikipedia.org/wiki/Tencent

https://en.wikipedia.org/wiki/Baidu

https://en.wikipedia.org/wiki/Alibaba_Group

cut the price of their AI models. Despite its low price, it was profitable compared to its money-losing rivals.

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=17

Free and open-source software portal

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=17

## Business portal

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=17

## China portal

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=17

## Artificial intelligence industry in China

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=17

## List of large language models

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=17

Lists of open-source artificial intelligence software

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=17

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=18

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=18

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=18

:  杭州深度求索人工智能基础技术研究有限公司 .

## Sometimes simply referred to in English as

## Hangzhou DeepSeek Artificial Intelligence

https://en.wikipedia.org/wiki/Pinyin

Shēndù Qiúsuǒ

宁波程信柔兆企业管理咨询合伙企业（有限合伙） and 宁波程恩企业管理咨询合伙企业（有限合伙）

The number of heads does not equal the number of KV heads, due to GQA.

https://en.wikipedia.org#cite_ref-fn1_71-0

Inexplicably, the model named  DeepSeek-Coder-V2 Chat  in the paper was released as  DeepSeek-Coder-V2-Instruct  in HuggingFace.

https://en.wikipedia.org#cite_ref-fn1_71-0

At that time, the  R1-Lite-Preview  required selecting "Deep Think enabled", and every user could use it only 50 times a day.

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=19

"DeepSeek突传消息"

https://en.wikipedia.org#cite_ref-DeepSeek突传消息!_1-0

. 1 February 2025 . Retrieved  1 February  2025 .

Wu, Zijing (14 March 2025).

"DeepSeek focuses on research over revenue in contrast to Silicon Valley"

https://www.ft.com/content/fb5c11bb-1d4b-465f-8283-451a19a3d425

## Financial Times

. Retrieved  14 March  2025 .

"Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd"

Bloomberg L.P.

"DeepSeek Coder Model Service Agreement"

, 19 October 2023,

https://web.archive.org/web/20250221091648/https://chat.deepseek.com/downloads/DeepSeek%20Coder%20Model%20Service%20Agreement_1019.pdf

(PDF)  from the original on 21 February 2025 , retrieved  11 February  2025

https://web.archive.org/web/20250221091648/https://chat.deepseek.com/downloads/DeepSeek%20Coder%20Model%20Service%20Agreement_1019.pdf

"DeepSeek Coder Privacy Policy"

https://web.archive.org/web/20250221091648/https://chat.deepseek.com/downloads/DeepSeek%20Coder%20Model%20Service%20Agreement_1019.pdf

. Retrieved  19 February  2025 .

"全国互联网安全管理平台"

beian.mps.gov.cn

https://web.archive.org/web/20250209181351/https://beian.mps.gov.cn/#/query/webSearch?code=33010502011812

from the original on 9 February 2025 . Retrieved  9 February  2025 .

https://web.archive.org/web/20250209181351/https://beian.mps.gov.cn/#/query/webSearch?code=33010502011812

Jiang, Ben (21 January 2025).

"Beijing puts spotlight on China's new face of AI, DeepSeek's Liang Wenfeng"

https://www.scmp.com/tech/policy/article/3295662/beijing-meeting-puts-spotlight-chinas-new-face-ai-deepseek-founder-liang-wenfeng

## South China Morning Post

https://web.archive.org/web/20250121221843/https://www.scmp.com/tech/policy/article/3295662/beijing-meeting-puts-spotlight-chinas-new-face-ai-deepseek-founder-liang-wenfeng

from the original on 21 January 2025 . Retrieved  4 March  2025 .

https://web.archive.org/web/20250121221843/https://www.scmp.com/tech/policy/article/3295662/beijing-meeting-puts-spotlight-chinas-new-face-ai-deepseek-founder-liang-wenfeng

Baptista, Eduardo (28 January 2025).

"Who is Liang Wenfeng, the founder of DeepSeek?"

https://www.reuters.com/technology/deepseek-founder-liang-wenfeng-puts-focus-chinese-innovation-2025-01-28/

https://web.archive.org/web/20250219122827/https://www.reuters.com/technology/deepseek-founder-liang-wenfeng-puts-focus-chinese-innovation-2025-01-28/?

from the original on 19 February 2025 . Retrieved  4 March  2025 .

https://web.archive.org/web/20250219122827/https://www.reuters.com/technology/deepseek-founder-liang-wenfeng-puts-focus-chinese-innovation-2025-01-28/?

"Behind DeepSeek lies a dazzling Chinese university"

https://web.archive.org/web/20250219122827/https://www.reuters.com/technology/deepseek-founder-liang-wenfeng-puts-focus-chinese-innovation-2025-01-28/?

## The Economist

https://en.wikipedia.org/wiki/ISSN_(identifier)

https://en.wikipedia.org/wiki/ISSN_(identifier)

https://archive.today/20250224111435/https://www.economist.com/china/2025/02/19/behind-deepseek-lies-a-dazzling-chinese-university

from the original on 24 February 2025 . Retrieved  5 March  2025 .

Gibney, Elizabeth (23 January 2025).

"China's cheap, open AI model DeepSeek thrills scientists"

https://www.nature.com/articles/d41586-025-00229-6

(8049):  13– 14.

https://en.wikipedia.org/wiki/Bibcode_(identifier)

2025Natur.638...13G

https://ui.adsabs.harvard.edu/abs/2025Natur.638...13G

https://en.wikipedia.org/wiki/Doi_(identifier)

10.1038/d41586-025-00229-6

https://doi.org/10.1038%2Fd41586-025-00229-6

https://en.wikipedia.org/wiki/PMID_(identifier)

https://en.wikipedia.org/wiki/PMID_(identifier)

https://web.archive.org/web/20250129122940/https://www.nature.com/articles/d41586-025-00229-6

from the original on 29 January 2025 . Retrieved  12 February  2025 .

Vincent, James (28 January 2025).

"The DeepSeek panic reveals an AI world ready to blow"

https://www.theguardian.com/commentisfree/2025/jan/28/deepseek-r1-ai-world-chinese-chatbot-tech-world-western

## The Guardian

Metz, Cade; Tobin, Meaghan (23 January 2025).

"How Chinese A.I. Start-Up DeepSeek Is Competing With Silicon Valley Giants"

https://www.nytimes.com/2025/01/23/technology/deepseek-china-ai-chips.html

## The New York Times

https://en.wikipedia.org/wiki/ISSN_(identifier)

https://en.wikipedia.org/wiki/ISSN_(identifier)

https://web.archive.org/web/20250123102900/https://www.nytimes.com/2025/01/23/technology/deepseek-china-ai-chips.html

from the original on 23 January 2025 . Retrieved  27 January  2025 .

Cosgrove, Emma (27 January 2025).

"DeepSeek's cheaper models and weaker chips call into question trillions in AI infrastructure spending"

https://www.businessinsider.com/explaining-deepseek-chinese-models-efficiency-scaring-markets-2025-1

## Business Insider

https://web.archive.org/web/20250129043218/https://www.businessinsider.com/explaining-deepseek-chinese-models-efficiency-scaring-markets-2025-1

from the original on 29 January 2025 . Retrieved  27 January  2025 .

Erdil, Ege (17 January 2025).

"How has DeepSeek improved the Transformer architecture?"

https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture

https://web.archive.org/web/20250203005101/https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture

from the original on 3 February 2025 . Retrieved  3 February  2025 .

Metz, Cade (27 January 2025).

"What is DeepSeek? And How Is It Upending A.I.?"

https://www.nytimes.com/2025/01/27/technology/what-is-deepseek-china-ai.html

## The New York Times

https://en.wikipedia.org/wiki/ISSN_(identifier)

https://en.wikipedia.org/wiki/ISSN_(identifier)

https://web.archive.org/web/20250127211403/https://www.nytimes.com/2025/01/27/technology/what-is-deepseek-china-ai.html

from the original on 27 January 2025 . Retrieved  27 January  2025 .

https://web.archive.org/web/20250127211403/https://www.nytimes.com/2025/01/27/technology/what-is-deepseek-china-ai.html

Roose, Kevin (28 January 2025).

"Why DeepSeek Could Change What Silicon Valley Believes About A.I."

https://www.nytimes.com/2025/01/28/technology/why-deepseek-could-change-what-silicon-valley-believes-about-ai.html

## The New York Times

https://www.nytimes.com/2025/01/28/technology/why-deepseek-could-change-what-silicon-valley-believes-about-ai.html

https://en.wikipedia.org/wiki/ISSN_(identifier)

https://en.wikipedia.org/wiki/ISSN_(identifier)

https://web.archive.org/web/20250128131926/https://www.nytimes.com/2025/01/28/technology/why-deepseek-could-change-what-silicon-valley-believes-about-ai.html

from the original on 28 January 2025 . Retrieved  28 January  2025 .

Delbert, Caroline (31 January 2025).

"DeepSeek Is Cracking the 'Black Box' of Corporate AI Wide Open"

https://www.popularmechanics.com/science/a63633889/deepseek-open-weight/

## Popular Mechanics

https://web.archive.org/web/20250213051908/https://www.popularmechanics.com/science/a63633889/deepseek-open-weight/

from the original on 13 February 2025 . Retrieved  12 February  2025 .

https://web.archive.org/web/20250213051908/https://www.popularmechanics.com/science/a63633889/deepseek-open-weight/

Metz, Cade (12 February 2025).

"How Did DeepSeek Build Its A.I. With Less Money?"

https://www.nytimes.com/2025/02/12/technology/deepseek-ai-chip-costs.html

## The New York Times

https://web.archive.org/web/20250319114813/https://www.nytimes.com/2025/02/12/technology/deepseek-ai-chip-costs.html

from the original on 19 March 2025 . Retrieved  21 March  2025 .

https://web.archive.org/web/20250319114813/https://www.nytimes.com/2025/02/12/technology/deepseek-ai-chip-costs.html

Allen, Gregory C. (7 March 2025).

"DeepSeek, Huawei, Export Controls, and the Future of the U.S.-China AI Race"

https://www.csis.org/analysis/deepseek-huawei-export-controls-and-future-us-china-ai-race

## Center for Strategic and International Studies

Hawkins, Amy (28 January 2025).

"Who is behind DeepSeek and how did it achieve its AI 'Sputnik moment'?"

https://www.theguardian.com/technology/2025/jan/28/who-is-behind-deepseek-and-how-did-it-achieve-its-ai-sputnik-moment

## The Guardian

Cassidy, John (3 February 2025).

"Is DeepSeek China's Sputnik Moment?"

https://www.newyorker.com/news/the-financial-page/is-deepseek-chinas-sputnik-moment

## The New Yorker

– via www.newyorker.com.

Ruwitch, John (28 January 2025).

"DeepSeek: Did a little-known Chinese startup cause a 'Sputnik moment' for AI?"

https://www.npr.org/2025/01/28/g-s1-45061/deepseek-did-a-little-known-chinese-startup-cause-a-sputnik-moment-for-ai

. Retrieved  2 August  2025 .

Saah, Jasper (13 February 2025).

"DeepSeek sends shock waves across Silicon Valley"

https://liberationnews.org/deepseek-sends-shock-waves-across-silicon-valley/

Liberation News – The Newspaper of the Party for Socialism and Liberation

https://web.archive.org/web/20250217044644/https://liberationnews.org/deepseek-sends-shock-waves-across-silicon-valley/

from the original on 17 February 2025 . Retrieved  13 February  2025 .

https://web.archive.org/web/20250217044644/https://liberationnews.org/deepseek-sends-shock-waves-across-silicon-valley/

Sillars, James (28 January 2025).

"DeepSeek: Tech firm suffers biggest drop in US stock market history as low-cost Chinese AI company bites Silicon Valley"

https://news.sky.com/story/deepseek-us-tech-stocks-tumble-on-fears-of-cheaper-chinese-ai-13297788

. Retrieved  13 February  2025 .

Chen, Caiwei (24 January 2025).

"How a top Chinese AI model overcame US sanctions"

https://www.technologyreview.com/2025/01/24/1110526/china-deepseek-top-ai-despite-sanctions/

## MIT Technology Review

https://web.archive.org/web/20250125180427/https://www.technologyreview.com/2025/01/24/1110526/china-deepseek-top-ai-despite-sanctions/

from the original on 25 January 2025 . Retrieved  25 January  2025 .

"幻方 | 幻方历程"

https://en.wikipedia.org#cite_ref-HI_28-0

(in Chinese (China)).

https://web.archive.org/web/20250203125004/https://www.high-flyer.cn/history/

from the original on 3 February 2025 . Retrieved  2 February  2025 .

Ottinger, Lily (9 December 2024).

"Deepseek: From Hedge Fund to Frontier Model Maker"

https://www.chinatalk.media/p/deepseek-from-hedge-fund-to-frontier

https://web.archive.org/web/20241228030725/https://www.chinatalk.media/p/deepseek-from-hedge-fund-to-frontier

from the original on 28 December 2024 . Retrieved  28 December  2024 .

Olcott, Eleanor; Wu, Zijing (24 January 2025).

"How small Chinese AI start-up DeepSeek shocked Silicon Valley"

https://www.ft.com/content/747a7b11-dcba-4aa5-8d25-403f56216d7e

## Financial Times

https://web.archive.org/web/20250125094520/https://www.ft.com/content/747a7b11-dcba-4aa5-8d25-403f56216d7e

from the original on 25 January 2025 . Retrieved  31 January  2025 .

https://web.archive.org/web/20250125094520/https://www.ft.com/content/747a7b11-dcba-4aa5-8d25-403f56216d7e

Leswing, Kif (23 February 2023).

"Meet the $10,000 Nvidia chip powering the race for A.I."

https://www.cnbc.com/2023/02/23/nvidias-a100-is-the-10000-chip-powering-the-race-for-ai-.html

https://www.cnbc.com/2023/02/23/nvidias-a100-is-the-10000-chip-powering-the-race-for-ai-.html

https://web.archive.org/web/20250129054857/https://www.cnbc.com/2023/02/23/nvidias-a100-is-the-10000-chip-powering-the-race-for-ai-.html

from the original on 29 January 2025 . Retrieved  30 January  2025 .

"hfreduce | 高性能的多卡并行通信工具"

https://en.wikipedia.org#cite_ref-RD_32-0

. 4 March 2020.

https://web.archive.org/web/20250128032837/https://www.high-flyer.cn/blog/hf-reduce/

from the original on 28 January 2025 . Retrieved  3 February  2025 .

DeepSeek-AI; Liu, Aixin; Feng, Bei; Xue, Bing; Wang, Bingxuan; Wu, Bochao; Lu, Chengda; Zhao, Chenggang; Deng, Chengqi (27 December 2024),

DeepSeek-V3 Technical Report

https://en.wikipedia.org/wiki/ArXiv_(identifier)

https://arxiv.org/abs/2412.19437

An, Wei; Bi, Xiao; Chen, Guanting; Chen, Shanhuang; Deng, Chengqi; Ding, Honghui; Dong, Kai; Du, Qiushi; Gao, Wenjun; Guan, Kang; Guo, Jianzhong; Guo, Yongqiang; Fu, Zhe; He, Ying; Huang, Panpan (17 November 2024). "Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning".

SC24: International Conference for High Performance Computing, Networking, Storage and Analysis

. IEEE. pp.  1– 23.

https://en.wikipedia.org/wiki/ArXiv_(identifier)

https://arxiv.org/abs/2408.14158

https://en.wikipedia.org/wiki/Doi_(identifier)

10.1109/SC41406.2024.00089

https://doi.org/10.1109%2FSC41406.2024.00089

https://en.wikipedia.org/wiki/ISBN_(identifier)

979-8-3503-5291-7

https://en.wikipedia.org/wiki/ISBN_(identifier)

https://en.wikipedia.org/wiki/ISBN_(identifier)

"独家|幻方量化回应市场关注：AGI不是用来炒股的，"和金融没关系" "

https://en.wikipedia.org/wiki/ISBN_(identifier)

. Retrieved  3 February  2025 .

Yu, Xu (17 April 2023).

"[Exclusive] Chinese Quant Hedge Fund High-Flyer Won't Use AGI to Trade Stocks, MD Says"

https://www.yicaiglobal.com/news/exclusive-chinese-quant-fund-high-flyer-will-not-use-agi-to-trade-stocks-managing-director-says

## Yicai Global

https://web.archive.org/web/20231231030712/https://www.yicaiglobal.com/news/exclusive-chinese-quant-fund-high-flyer-will-not-use-agi-to-trade-stocks-managing-director-says

from the original on 31 December 2023 . Retrieved  28 December  2024 .

Jiang, Ben; Perezi, Bien (1 January 2025).

"Meet DeepSeek: the Chinese start-up that is changing how AI models are trained"

https://www.scmp.com/tech/tech-trends/article/3293050/meet-deepseek-chinese-start-changing-how-ai-models-are-trained

## South China Morning Post

https://web.archive.org/web/20250122160046/https://www.scmp.com/tech/tech-trends/article/3293050/meet-deepseek-chinese-start-changing-how-ai-models-are-trained

from the original on 22 January 2025 . Retrieved  1 January  2025 .

McMorrow, Ryan; Olcott, Eleanor (9 June 2024).

"The Chinese quant fund-turned-AI pioneer"

https://www.ft.com/content/357f3c68-b866-4c2e-b678-0d075051a260

## Financial Times

https://web.archive.org/web/20240717030903/https://www.ft.com/content/357f3c68-b866-4c2e-b678-0d075051a260

from the original on 17 July 2024 . Retrieved  28 December  2024 .

DeepSeek-AI; Bi, Xiao; Chen, Deli; Chen, Guanting; Chen, Shanhuang; Dai, Damai; Deng, Chengqi; Ding, Honghui; Dong, Kai (5 January 2024),

DeepSeek LLM: Scaling Open-Source Language Models with Longtermism

https://en.wikipedia.org/wiki/ArXiv_(identifier)

https://arxiv.org/abs/2401.02954

Dai, Damai; Deng, Chengqi; Zhao, Chenggang; Xu, R. X.; Gao, Huazuo; Chen, Deli; Li, Jiashi; Zeng, Wangding; Yu, Xingkai (11 January 2024),

DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models

https://en.wikipedia.org/wiki/ArXiv_(identifier)

https://arxiv.org/abs/2401.06066

Shao, Zhihong; Wang, Peiyi; Zhu, Qihao; Xu, Runxin; Song, Junxiao; Bi, Xiao; Zhang, Haowei; Zhang, Mingchuan; Li, Y. K. (27 April 2024),

DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

https://en.wikipedia.org/wiki/ArXiv_(identifier)

https://arxiv.org/abs/2402.03300

DeepSeek-AI; Zhu, Qihao; Guo, Daya; Shao, Zhihong; Yang, Dejian; Wang, Peiyi; Xu, Runxin; Wu, Y.; Li, Yukun (17 June 2024),

DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence

https://en.wikipedia.org/wiki/ArXiv_(identifier)

https://arxiv.org/abs/2406.11931

"deepseek-ai/DeepSeek-V2.5 · Hugging Face"

https://en.wikipedia.org#cite_ref-HF_43-0

## Hugging Face

. 3 January 2025.

https://web.archive.org/web/20250130091452/https://huggingface.co/deepseek-ai/DeepSeek-V2.5

from the original on 30 January 2025 . Retrieved  28 January  2025 .

"Deepseek Log in page"

https://en.wikipedia.org#cite_ref-DSLI_1_44-0

. Retrieved  30 January  2025 .

"News | DeepSeek-R1-Lite Release 2024/11/20: 🚀 DeepSeek-R1-Lite-Preview is now live: unleashing supercharged reasoning power!"

https://en.wikipedia.org#cite_ref-RP_45-0

## DeepSeek API Docs

. Archived from

the original

https://api-docs.deepseek.com/news/news1120

on 20 November 2024 . Retrieved  28 January  2025 .

https://api-docs.deepseek.com/news/news1120

Field, Hayden (27 January 2025).

"China's DeepSeek AI dethrones ChatGPT on App Store: Here's what you should know"

https://www.cnbc.com/2025/01/27/chinas-deepseek-ai-tops-chatgpt-app-store-what-you-should-know.html

https://web.archive.org/web/20250128201052/https://www.cnbc.com/2025/01/27/chinas-deepseek-ai-tops-chatgpt-app-store-what-you-should-know.html

from the original on 28 January 2025 . Retrieved  27 January  2025 .

https://web.archive.org/web/20250128201052/https://www.cnbc.com/2025/01/27/chinas-deepseek-ai-tops-chatgpt-app-store-what-you-should-know.html

Picchi, Aimee (27 January 2025).

"What is DeepSeek, and why is it causing Nvidia and other stocks to slump?"

https://www.cbsnews.com/news/what-is-deepseek-ai-china-stock-nvidia-nvda-asml/

https://web.archive.org/web/20250129002522/https://www.cbsnews.com/news/what-is-deepseek-ai-china-stock-nvidia-nvda-asml/

from the original on 29 January 2025 . Retrieved  27 January  2025 .

https://web.archive.org/web/20250129002522/https://www.cbsnews.com/news/what-is-deepseek-ai-china-stock-nvidia-nvda-asml/

Nuñez, Michael (24 March 2025).

"DeepSeek-V3 now runs at 20 tokens per second on Mac Studio, and that's a nightmare for OpenAI"

https://venturebeat.com/ai/deepseek-v3-now-runs-at-20-tokens-per-second-on-mac-studio-and-thats-a-nightmare-for-openai/

## VentureBeat

. Retrieved  24 March  2025 .

"deepseek-ai/DeepSeek-V3-0324 · Hugging Face"

## Hugging Face

https://web.archive.org/web/20250324193733/https://huggingface.co/deepseek-ai/DeepSeek-V3-0324

from the original on 24 March 2025 . Retrieved  24 March  2025 .

https://web.archive.org/web/20250324193733/https://huggingface.co/deepseek-ai/DeepSeek-V3-0324

"deepseek-ai/DeepSeek-R1-0528 · Hugging Face"

https://web.archive.org/web/20250324193733/https://huggingface.co/deepseek-ai/DeepSeek-V3-0324

huggingface.co

. 28 May 2025.

https://web.archive.org/web/20250528192921/https://huggingface.co/deepseek-ai/DeepSeek-R1-0528

from the original on 28 May 2025 . Retrieved  28 May  2025 .

https://web.archive.org/web/20250528192921/https://huggingface.co/deepseek-ai/DeepSeek-R1-0528

Colville, Alex (12 June 2025).

"China's Global AI Firewall"

https://chinamediaproject.org/2025/06/12/chinas-global-ai-firewall/

## China Media Project

. Retrieved  30 June  2025 .

"deepseek-ai/DeepSeek-V3.1 · Hugging Face"

huggingface.co

. 21 August 2025 . Retrieved  25 August  2025 .

"DeepSeek-V3.1 Release | DeepSeek API Docs"

api-docs.deepseek.com

. Retrieved  25 August  2025 .

"deepseek-ai/DeepSeek-V3.1-Terminus · Hugging Face"

huggingface.co

. 22 September 2025 . Retrieved  24 September  2025 .

Yuan, Jingyang; Gao, Huazuo; Dai, Damai; Luo, Junyu; Zhao, Liang; Zhang, Zhengyan; Xie, Zhenda; Wei, Y. X.; Wang, Lean (27 February 2025),

Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention

https://en.wikipedia.org/wiki/ArXiv_(identifier)

https://arxiv.org/abs/2502.11089

https://arxiv.org/abs/2502.11089

"deepseek-ai/DeepSeek-V3.2-Exp · Hugging Face"

https://arxiv.org/abs/2502.11089

huggingface.co

. 29 September 2025 . Retrieved  2 October  2025 .

"大模型价格又砍一刀 这次"屠夫"竟是量化私募？"

. 10 May 2024.

https://web.archive.org/web/20241227042059/https://www.cls.cn/detail/1672635

from the original on 27 December 2024 . Retrieved  3 February  2025 .

Schneider, Jordan (27 November 2024).

"Deepseek: The Quiet Giant Leading China's AI Race"

https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas

https://web.archive.org/web/20241129213821/https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas

from the original on 29 November 2024 . Retrieved  28 December  2024 .

https://web.archive.org/web/20241129213821/https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas

Mickle, Tripp; Swanson, Ana; Tobin, Meaghan; Metz, Cade (16 April 2025).

"US Officials Target Nvidia and DeepSeek Amid Fears of China's A.I. Progress"

https://www.nytimes.com/2025/04/16/technology/nvidia-deepseek-china-ai-trump.html

## The New York Times

https://en.wikipedia.org/wiki/ISSN_(identifier)

https://en.wikipedia.org/wiki/ISSN_(identifier)

https://archive.today/20250416175114/https://www.nytimes.com/2025/04/16/technology/nvidia-deepseek-china-ai-trump.html

from the original on 16 April 2025 . Retrieved  17 April  2025 .

https://archive.today/20250416175114/https://www.nytimes.com/2025/04/16/technology/nvidia-deepseek-china-ai-trump.html

Rai, Saritha, Loni Prinsloo, and Helen Nyambura

"China's DeepSeek Is Beating Out OpenAI and Google in Africa"

https://www.bloomberg.com/news/features/2025-10-22/china-s-deepseek-pushes-into-africa-making-ai-accessible-to-millions?embedded-checkout=true

## Bloomberg Technology

. Accessed 27 Oct 2025.

https://www.bloomberg.com/news/features/2025-10-22/china-s-deepseek-pushes-into-africa-making-ai-accessible-to-millions?embedded-checkout=true

"幻方力量 | 高速文件系统 3FS"

https://www.bloomberg.com/news/features/2025-10-22/china-s-deepseek-pushes-into-africa-making-ai-accessible-to-millions?embedded-checkout=true

. 13 June 2019.

https://web.archive.org/web/20250203011728/https://www.high-flyer.cn/blog/3fs/

from the original on 3 February 2025 . Retrieved  3 February  2025 .

https://web.archive.org/web/20250203011728/https://www.high-flyer.cn/blog/3fs/

deepseek-ai/3FS

, DeepSeek, 28 February 2025,

https://web.archive.org/web/20250228054402/https://github.com/deepseek-ai/3FS

from the original on 28 February 2025 , retrieved  28 February  2025

https://web.archive.org/web/20250228054402/https://github.com/deepseek-ai/3FS

"HFAiLab/hai-platform"

https://web.archive.org/web/20250228054402/https://github.com/deepseek-ai/3FS

, 2 February 2025 , retrieved  3 February  2025

DeepSeek-AI; Guo, Daya; Yang, Dejian; Zhang, Haowei; Song, Junxiao; Zhang, Ruoyu; Xu, Runxin; Zhu, Qihao; Ma, Shirong (22 January 2025),

DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning

https://en.wikipedia.org/wiki/ArXiv_(identifier)

https://arxiv.org/abs/2501.12948

https://arxiv.org/abs/2501.12948

"DeepSeek-Coder/LICENSE-MODEL at main · deepseek-ai/DeepSeek-Coder"

https://arxiv.org/abs/2501.12948

https://web.archive.org/web/20250122195853/https://github.com/deepseek-ai/deepseek-coder/blob/main/LICENSE-MODEL

from the original on 22 January 2025 . Retrieved  24 January  2025 .

Guo, Daya; Zhu, Qihao; Yang, Dejian; Xie, Zhenda; Dong, Kai; Zhang, Wentao; Chen, Guanting; Bi, Xiao; Wu, Y. (26 January 2024),

DeepSeek-Coder: When the Large Language Model Meets Programming – The Rise of Code Intelligence

https://en.wikipedia.org/wiki/ArXiv_(identifier)

https://arxiv.org/abs/2401.14196

https://arxiv.org/abs/2401.14196

"DeepSeek Coder"

https://arxiv.org/abs/2401.14196

deepseekcoder.github.io

https://web.archive.org/web/20250127000632/https://deepseekcoder.github.io/

from the original on 27 January 2025 . Retrieved  27 January  2025 .

https://web.archive.org/web/20250127000632/https://deepseekcoder.github.io/

deepseek-ai/DeepSeek-Coder

, DeepSeek, 27 January 2025,

https://web.archive.org/web/20250127054244/https://github.com/deepseek-ai/DeepSeek-Coder

from the original on 27 January 2025 , retrieved  27 January  2025

https://web.archive.org/web/20250127054244/https://github.com/deepseek-ai/DeepSeek-Coder

"deepseek-ai/deepseek-coder-5.7bmqa-base · Hugging Face"

https://web.archive.org/web/20250127054244/https://github.com/deepseek-ai/DeepSeek-Coder

## Hugging Face

. Retrieved  27 January  2025 .

deepseek-ai/DeepSeek-LLM

, DeepSeek, 27 January 2025 , retrieved  27 January  2025

Wang, Peiyi; Li, Lei; Shao, Zhihong; Xu, R. X.; Dai, Damai; Li, Yifei; Chen, Deli; Wu, Y.; Sui, Zhifang (19 February 2024),

Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations

https://en.wikipedia.org/wiki/ArXiv_(identifier)

https://arxiv.org/abs/2312.08935

DeepSeek-AI; Liu, Aixin; Feng, Bei; Wang, Bin; Wang, Bingxuan; Liu, Bo; Zhao, Chenggang; Dengr, Chengqi; Ruan, Chong (19 June 2024),

DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model

https://en.wikipedia.org/wiki/ArXiv_(identifier)

https://arxiv.org/abs/2405.04434

Peng, Bowen; Quesnelle, Jeffrey; Fan, Honglu; Shippole, Enrico (1 November 2023),

YaRN: Efficient Context Window Extension of Large Language Models

https://en.wikipedia.org/wiki/ArXiv_(identifier)

https://arxiv.org/abs/2309.00071

https://arxiv.org/abs/2309.00071

"config.json · deepseek-ai/DeepSeek-V2-Lite at main"

https://arxiv.org/abs/2309.00071

## Hugging Face

. 15 May 2024 . Retrieved  28 January  2025 .

"config.json · deepseek-ai/DeepSeek-V2 at main"

## Hugging Face

. 6 May 2024 . Retrieved  28 January  2025 .

Feng, Coco (25 March 2025).

"DeepSeek wows coders with more powerful open-source V3 model"

https://www.scmp.com/tech/big-tech/article/3303798/deepseeks-upgraded-foundational-model-excels-coding-and-maths

## South China Morning Post

. Retrieved  6 April  2025 .

"config.json · deepseek-ai/DeepSeek-V3 at main"

## Hugging Face

. 26 December 2024.

https://web.archive.org/web/20250126101005/https://huggingface.co/deepseek-ai/DeepSeek-V3/blob/main/config.json

from the original on 26 January 2025 . Retrieved  28 January  2025 .

https://web.archive.org/web/20250126101005/https://huggingface.co/deepseek-ai/DeepSeek-V3/blob/main/config.json

Patel, Dylan; Kourabi, AJ; O'Laughlin, Dylan; Knuhtsen, Doug (31 January 2025).

"DeepSeek Debates: Chinese Leadership On Cost, True Training Cost, Closed Model Margin Impacts"

https://semianalysis.com/2025/01/31/deepseek-debates/

## SemiAnalysis

https://web.archive.org/web/20250213182726/https://semianalysis.com/2025/01/31/deepseek-debates/

from the original on 13 February 2025 . Retrieved  13 February  2025 .

https://web.archive.org/web/20250213182726/https://semianalysis.com/2025/01/31/deepseek-debates/

Thubron, Rob (3 February 2025).

"DeepSeek's AI costs far exceed $5.5 million claim, may have reached $1.6 billion with 50,000 Nvidia GPUs"

https://www.techspot.com/news/106612-deepseek-ai-costs-far-exceed-55-million-claim.html

. Retrieved  13 February  2025 .

Kajal, Kapil (31 January 2025).

"Research exposes DeepSeek's AI training cost is not $6M, it's a staggering $1.3B"

https://www.yahoo.com/news/research-exposes-deepseek-ai-training-165025904.html

https://web.archive.org/web/20250213015354/https://www.yahoo.com/news/research-exposes-deepseek-ai-training-165025904.html

from the original on 13 February 2025 . Retrieved  13 February  2025 .

https://web.archive.org/web/20250213015354/https://www.yahoo.com/news/research-exposes-deepseek-ai-training-165025904.html

"Martin Vechev of INSAIT: "DeepSeek $6M Cost Of Training Is Misleading" "

https://web.archive.org/web/20250213015354/https://www.yahoo.com/news/research-exposes-deepseek-ai-training-165025904.html

TheRecursive.com

. 28 January 2025.

https://web.archive.org/web/20250213130710/https://therecursive.com/martin-vechev-of-insait-deepseek-6m-cost-of-training-is-misleading/

from the original on 13 February 2025 . Retrieved  13 February  2025 .

https://web.archive.org/web/20250213130710/https://therecursive.com/martin-vechev-of-insait-deepseek-6m-cost-of-training-is-misleading/

Jiang, Ben (27 December 2024).

"Chinese start-up DeepSeek's new AI model outperforms Meta, OpenAI products"

https://www.scmp.com/tech/tech-trends/article/3292507/chinese-start-deepseek-launches-ai-model-outperforms-meta-openai-products

## South China Morning Post

https://web.archive.org/web/20241227191529/https://www.scmp.com/tech/tech-trends/article/3292507/chinese-start-deepseek-launches-ai-model-outperforms-meta-openai-products

from the original on 27 December 2024 . Retrieved  28 December  2024 .

https://web.archive.org/web/20241227191529/https://www.scmp.com/tech/tech-trends/article/3292507/chinese-start-deepseek-launches-ai-model-outperforms-meta-openai-products

Sharma, Shubham (26 December 2024).

"DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch"

https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/

## VentureBeat

https://web.archive.org/web/20241227195503/https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/

from the original on 27 December 2024 . Retrieved  28 December  2024 .

https://web.archive.org/web/20241227195503/https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/

Wiggers, Kyle (26 December 2024).

"DeepSeek's new AI model appears to be one of the best 'open' challengers yet"

https://techcrunch.com/2024/12/26/deepseeks-new-ai-model-appears-to-be-one-of-the-best-open-challengers-yet/

https://web.archive.org/web/20250102103526/https://techcrunch.com/2024/12/26/deepseeks-new-ai-model-appears-to-be-one-of-the-best-open-challengers-yet/

from the original on 2 January 2025 . Retrieved  31 December  2024 .

https://web.archive.org/web/20250102103526/https://techcrunch.com/2024/12/26/deepseeks-new-ai-model-appears-to-be-one-of-the-best-open-challengers-yet/

Edwards, Benj (21 January 2025).

"Cutting-edge Chinese "reasoning" model rivals OpenAI o1—and it's free to download"

https://arstechnica.com/ai/2025/01/china-is-catching-up-with-americas-best-reasoning-ai-models/

## Ars Technica

. Retrieved  16 February  2025 .

Franzen, Carl (20 November 2024).

"DeepSeek's first reasoning model R1-Lite-Preview turns heads, beating OpenAI o1 performance"

https://venturebeat.com/ai/deepseeks-first-reasoning-model-r1-lite-preview-turns-heads-beating-openai-o1-performance/

## VentureBeat

https://web.archive.org/web/20241122010413/https://venturebeat.com/ai/deepseeks-first-reasoning-model-r1-lite-preview-turns-heads-beating-openai-o1-performance/

from the original on 22 November 2024 . Retrieved  28 December  2024 .

https://web.archive.org/web/20241122010413/https://venturebeat.com/ai/deepseeks-first-reasoning-model-r1-lite-preview-turns-heads-beating-openai-o1-performance/

Huang, Raffaele (24 December 2024).

"Don't Look Now, but China's AI Is Catching Up Fast"

https://www.wsj.com/tech/ai/china-ai-advances-us-chips-7838fd20

## The Wall Street Journal

https://web.archive.org/web/20241227183842/https://www.wsj.com/tech/ai/china-ai-advances-us-chips-7838fd20

from the original on 27 December 2024 . Retrieved  28 December  2024 .

https://web.archive.org/web/20241227183842/https://www.wsj.com/tech/ai/china-ai-advances-us-chips-7838fd20

"Release DeepSeek-R1 · deepseek-ai/DeepSeek-R1@23807ce"

https://web.archive.org/web/20241227183842/https://www.wsj.com/tech/ai/china-ai-advances-us-chips-7838fd20

https://web.archive.org/web/20250121104009/https://github.com/deepseek-ai/DeepSeek-R1/commit/23807ced51627276434655dd9f27725354818974

from the original on 21 January 2025 . Retrieved  21 January  2025 .

https://web.archive.org/web/20250121104009/https://github.com/deepseek-ai/DeepSeek-R1/commit/23807ced51627276434655dd9f27725354818974

Eduardo Baptista; Julie Zhu; Fanny Potkin (25 February 2025).

"DeepSeek rushes to launch new AI model as China goes all in"

https://www.reuters.com/technology/artificial-intelligence/deepseek-rushes-launch-new-ai-model-china-goes-all-2025-02-25/

https://web.archive.org/web/20250321225322/https://www.reuters.com/technology/artificial-intelligence/deepseek-rushes-launch-new-ai-model-china-goes-all-2025-02-25/

from the original on 21 March 2025 . Retrieved  25 February  2025 .

https://web.archive.org/web/20250321225322/https://www.reuters.com/technology/artificial-intelligence/deepseek-rushes-launch-new-ai-model-china-goes-all-2025-02-25/

Ding, Luz (29 May 2025).

"DeepSeek Says Upgraded Model Reasons Better, Hallucinates Less"

https://www.bloomberg.com/news/articles/2025-05-29/deepseek-says-upgraded-model-reasons-better-hallucinates-less

. Retrieved  9 June  2025 .

"DeepSeek R2 launch stalled as CEO balks at progress, The Information reports"

. 26 June 2025 . Retrieved  5 July  2025 .

Olcott, Eleanor; Wu, Zijing (14 August 2025).

"DeepSeek's next AI model delayed by attempt to use Chinese chips"

https://www.ft.com/content/eb984646-6320-4bfe-a78d-a1da2274b092

## Financial Times

"China cautions tech firms over Nvidia H20 AI chip purchases, sources say"

. 12 August 2025.

Guo, Daya; Yang, Dejian; Zhang, Haowei; Song, Junxiao; Wang, Peiyi; Zhu, Qihao; Xu, Runxin; Zhang, Ruoyu; Ma, Shirong; Bi, Xiao; Zhang, Xiaokang; Yu, Xingkai; Wu, Yu; Wu, Z. F.; Gou, Zhibin (September 2025).

"DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12443585

(8081):  633– 638.

https://en.wikipedia.org/wiki/Bibcode_(identifier)

2025Natur.645..633G

https://ui.adsabs.harvard.edu/abs/2025Natur.645..633G

https://en.wikipedia.org/wiki/Doi_(identifier)

10.1038/s41586-025-09422-z

https://doi.org/10.1038%2Fs41586-025-09422-z

https://en.wikipedia.org/wiki/ISSN_(identifier)

https://en.wikipedia.org/wiki/ISSN_(identifier)

https://en.wikipedia.org/wiki/PMC_(identifier)

https://en.wikipedia.org/wiki/PMC_(identifier)

https://en.wikipedia.org/wiki/PMID_(identifier)

https://en.wikipedia.org/wiki/PMID_(identifier)

https://en.wikipedia.org/wiki/PMID_(identifier)

Roose, Kevin (28 January 2025).

"Why DeepSeek Could Change What Silicon Valley Believe About A.I."

https://www.nytimes.com/2025/01/28/technology/why-deepseek-could-change-what-silicon-valley-believes-about-ai.html

## The New York Times

https://www.nytimes.com/2025/01/28/technology/why-deepseek-could-change-what-silicon-valley-believes-about-ai.html

https://en.wikipedia.org/wiki/ISSN_(identifier)

https://en.wikipedia.org/wiki/ISSN_(identifier)

https://web.archive.org/web/20250128131926/https://www.nytimes.com/2025/01/28/technology/why-deepseek-could-change-what-silicon-valley-believes-about-ai.html

from the original on 28 January 2025 . Retrieved  28 January  2025 .

https://web.archive.org/web/20250128131926/https://www.nytimes.com/2025/01/28/technology/why-deepseek-could-change-what-silicon-valley-believes-about-ai.html

"Beyond the Headlines on DeepSeek's Sputnik Moment: A Conversation with Jimmy Goodrich - IGCC"

https://web.archive.org/web/20250128131926/https://www.nytimes.com/2025/01/28/technology/why-deepseek-could-change-what-silicon-valley-believes-about-ai.html

UC Institute on Global Conflict and Cooperation (IGCC)

. 12 February 2025.

https://archive.today/20250802161410/https://ucigcc.org/interview/beyond-the-headlines-on-deepseeks-sputnik-moment-a-conversation-with-jimmy-goodrich/

from the original on 2 August 2025.

https://archive.today/20250802161410/https://ucigcc.org/interview/beyond-the-headlines-on-deepseeks-sputnik-moment-a-conversation-with-jimmy-goodrich/

"Is 'Sputnik Moment' an appropriate analogy for the launch of DeepSeek? - LCFI"

https://archive.today/20250802161410/https://ucigcc.org/interview/beyond-the-headlines-on-deepseeks-sputnik-moment-a-conversation-with-jimmy-goodrich/

LCFI - Leverhulme Centre for the Future of Intelligence

. 2 February 2025.

Roeloffs, Mary Whitfill.

"What Is DeepSeek? New Chinese Artificial Intelligence Rivals ChatGPT, OpenAI"

https://www.forbes.com/sites/maryroeloffs/2025/01/27/what-is-deepseek-new-chinese-ai-startup-rivals-openai-and-claims-its-far-cheaper/

. Retrieved  5 August  2025 .

DeepSeek-AI; et al. (2024). "DeepSeek-V3 Technical Report".

https://en.wikipedia.org/wiki/ArXiv_(identifier)

https://arxiv.org/abs/2412.19437

https://arxiv.org/archive/cs.CL

https://arxiv.org/archive/cs.CL

Chow, Andrew R.; Perrigo, Billy (30 January 2025).

"Is the DeepSeek Panic Overblown?"

https://time.com/7211646/is-deepseek-panic-overblown/

https://web.archive.org/web/20250317061531/https://time.com/7211646/is-deepseek-panic-overblown/

from the original on 17 March 2025 . Retrieved  17 March  2025 .

## External links

https://en.wikipedia.org/w/index.php?title=DeepSeek&action=edit&section=20

## Wikimedia Commons has media related to

https://commons.wikimedia.org/wiki/Category:DeepSeek

## Official website

https://commons.wikimedia.org/wiki/Category:DeepSeek

https://commons.wikimedia.org/wiki/Category:DeepSeek

https://en.wikipedia.org/wiki/GitHub

https://en.wikipedia.org/wiki/GitHub

## Hugging Face

https://en.wikipedia.org/wiki/Hugging_Face

## Official API documentation

https://en.wikipedia.org/wiki/Hugging_Face

## Anthology of DeepSeek papers

https://en.wikipedia.org/wiki/Hugging_Face

Research blog of High-Flyer

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Generative AI

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## List of chatbots

https://en.wikipedia.org/wiki/Hugging_Face

## List of LLMs

https://en.wikipedia.org/wiki/Hugging_Face

Character.ai

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Generative AI

https://en.wikipedia.org/wiki/Hugging_Face

## Autoencoder

https://en.wikipedia.org/wiki/Hugging_Face

## Deep learning

https://en.wikipedia.org/wiki/Hugging_Face

Fine-tuning

https://en.wikipedia.org/wiki/Hugging_Face

## Foundation model

https://en.wikipedia.org/wiki/Hugging_Face

## Generative adversarial network

https://en.wikipedia.org/wiki/Hugging_Face

Generative pre-trained transformer

https://en.wikipedia.org/wiki/Hugging_Face

## Large language model

https://en.wikipedia.org/wiki/Hugging_Face

## Model Context Protocol

https://en.wikipedia.org/wiki/Hugging_Face

## Neural network

https://en.wikipedia.org/wiki/Hugging_Face

## Prompt engineering

https://en.wikipedia.org/wiki/Hugging_Face

## Reinforcement learning from human feedback

https://en.wikipedia.org/wiki/Hugging_Face

Retrieval-augmented generation

https://en.wikipedia.org/wiki/Hugging_Face

Self-supervised learning

https://en.wikipedia.org/wiki/Hugging_Face

## Stochastic parrot

https://en.wikipedia.org/wiki/Hugging_Face

## Synthetic data

https://en.wikipedia.org/wiki/Hugging_Face

Top-p sampling

https://en.wikipedia.org/wiki/Hugging_Face

## Transformer

https://en.wikipedia.org/wiki/Hugging_Face

## Variational autoencoder

https://en.wikipedia.org/wiki/Hugging_Face

## Vibe coding

https://en.wikipedia.org/wiki/Hugging_Face

## Vision transformer

https://en.wikipedia.org/wiki/Hugging_Face

## Waluigi effect

https://en.wikipedia.org/wiki/Hugging_Face

## Word embedding

https://en.wikipedia.org/wiki/Hugging_Face

## Models Text

Character.ai

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Microsoft Copilot

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Mistral Medium

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Claude Code

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## GitHub Copilot

https://en.wikipedia.org/wiki/Hugging_Face

Grok Code Fast 1

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

Qwen3-Coder

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

GPT Image 1

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Stable Diffusion

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Dream Machine

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Gemini Speech

https://en.wikipedia.org/wiki/Hugging_Face

GPT-4o mini TTS

https://en.wikipedia.org/wiki/Hugging_Face

## MiniMax Speech

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Eleven Music

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Stable Audio

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## ChatGPT Agent

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## OpenAI Codex

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Replit Agent

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Aleph Alpha

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Cognition AI

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Contextual AI

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Google DeepMind

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Hugging Face

https://en.wikipedia.org/wiki/Hugging_Face

## Inflection AI

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Moonshot AI

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Perplexity AI

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Safe Superintelligence

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Stability AI

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Thinking Machines Lab

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Controversies

https://en.wikipedia.org/wiki/Hugging_Face

## Deepfake pornography

https://en.wikipedia.org/wiki/Hugging_Face

## Generative AI pornography

https://en.wikipedia.org/wiki/Hugging_Face

## Taylor Swift deepfake pornography controversy

https://en.wikipedia.org/wiki/Hugging_Face

## Google Gemini image generation controversy

https://en.wikipedia.org/wiki/Hugging_Face

## Pause Giant AI Experiments

https://en.wikipedia.org/wiki/Hugging_Face

## Removal of Sam Altman from OpenAI

https://en.wikipedia.org/wiki/Hugging_Face

## Statement on AI Risk

https://en.wikipedia.org/wiki/Hugging_Face

Tay (chatbot)

https://en.wikipedia.org/wiki/Hugging_Face

Théâtre D'opéra Spatial

https://en.wikipedia.org/wiki/Hugging_Face

## Voiceverse NFT plagiarism scandal

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

## Authority control databases

https://en.wikipedia.org/wiki/Hugging_Face

https://en.wikipedia.org/wiki/Hugging_Face

Retrieved from "

https://en.wikipedia.org/w/index.php?title=DeepSeek&oldid=1319022322

https://en.wikipedia.org/w/index.php?title=DeepSeek&oldid=1319022322

https://en.wikipedia.org/wiki/Help:Category

Chinese companies established in 2023

https://en.wikipedia.org/wiki/Help:Category

## Artificial intelligence companies

https://en.wikipedia.org/wiki/Help:Category

## Artificial intelligence laboratories

https://en.wikipedia.org/wiki/Help:Category

## Companies based in Hangzhou

https://en.wikipedia.org/wiki/Help:Category

Technology companies established in 2023

https://en.wikipedia.org/wiki/Help:Category

## Chinese brands

https://en.wikipedia.org/wiki/Help:Category

Open-source artificial intelligence

https://en.wikipedia.org/wiki/Help:Category

2023 in artificial intelligence

https://en.wikipedia.org/wiki/Help:Category

Hidden categories:

Articles containing Chinese-language text

https://en.wikipedia.org/wiki/Help:Category

Articles containing simplified Chinese-language text

https://en.wikipedia.org/wiki/Help:Category

CS1 Chinese (China)-language sources (zh-cn)

https://en.wikipedia.org/wiki/Help:Category

## Articles with short description

https://en.wikipedia.org/wiki/Help:Category

## Short description matches Wikidata

https://en.wikipedia.org/wiki/Help:Category

Use dmy dates from February 2025

https://en.wikipedia.org/wiki/Help:Category

Use American English from February 2025

https://en.wikipedia.org/wiki/Help:Category

## All Wikipedia articles written in American English

https://en.wikipedia.org/wiki/Help:Category

Articles lacking reliable references from February 2025

https://en.wikipedia.org/wiki/Help:Category

## All articles lacking reliable references

https://en.wikipedia.org/wiki/Help:Category

## Commons category link from Wikidata

https://en.wikipedia.org/wiki/Help:Category

