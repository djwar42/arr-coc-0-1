---
sourceFile: "[2510.18234] DeepSeek-OCR: Contexts Optical Compression - arXiv"
exportedBy: "Kortex"
exportDate: "2025-10-29T02:36:42.396Z"
---

# [2510.18234] DeepSeek-OCR: Contexts Optical Compression - arXiv

f420773f-0aac-4ac0-8a51-6d5fdd3e79e9

[2510.18234] DeepSeek-OCR: Contexts Optical Compression - arXiv

a7ba2678-163e-4a79-95f7-8f30da4d4ad5

https://www.arxiv.org/abs/2510.18234

Happy Open Access Week from arXiv!

YOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.

## Skip to main content

We gratefully acknowledge support from the Simons Foundation,

member institutions

https://info.arxiv.org/about/ourmembers.html

, and all contributors.

https://info.arxiv.org/about/donate.html

https://www.arxiv.org/list/cs/recent

>  arXiv:2510.18234

https://www.arxiv.org/list/cs/recent

## Advanced Search

https://arxiv.org/search/advanced

Computer Science > Computer Vision and Pattern Recognition

arXiv:2510.18234

(cs)   [Submitted on 21 Oct 2025]

Title: DeepSeek-OCR: Contexts Optical Compression

https://arxiv.org/search/cs?searchtype=author&query=Wei,+H

## Yaofeng Sun

https://arxiv.org/search/cs?searchtype=author&query=Sun,+Y

https://arxiv.org/search/cs?searchtype=author&query=Li,+Y

View a PDF of the paper titled DeepSeek-OCR: Contexts Optical Compression, by Haoran Wei and 2 other authors

https://www.arxiv.org/pdf/2510.18234

HTML (experimental)

https://www.arxiv.org/pdf/2510.18234

Abstract: We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder and DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core engine, designed to maintain low activations under high-resolution input while achieving high compression ratios to ensure an optimal and manageable number of vision tokens. Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a compression ratio < 10x), the model can achieve decoding (OCR) precision of 97%. Even at a compression ratio of 20x, the OCR accuracy still remains at about 60%. This shows considerable promise for research areas such as historical long-context compression and memory forgetting mechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value. On OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens, and outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than 800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs at a scale of 200k+ pages per day (a single A100-40G). Codes and model weights are publicly accessible at

this http URL

http://github.com/deepseek-ai/DeepSeek-OCR

.  Subjects:   Computer Vision and Pattern Recognition (cs.CV)   Cite as:

arXiv:2510.18234

https://arxiv.org/abs/2510.18234

[cs.CV]   (or

arXiv:2510.18234v1

https://arxiv.org/abs/2510.18234v1

[cs.CV]  for this version)

https://doi.org/10.48550/arXiv.2510.18234

https://doi.org/10.48550/arXiv.2510.18234

arXiv-issued DOI via DataCite

## Submission history

From: Haoran Wei [

https://www.arxiv.org/show-email/f6a1695a/2510.18234

Tue, 21 Oct 2025 02:41:44 UTC (7,007 KB)

Full-text links:

Access Paper:

View a PDF of the paper titled DeepSeek-OCR: Contexts Optical Compression, by Haoran Wei and 2 other authors

https://www.arxiv.org/pdf/2510.18234

HTML (experimental)

https://www.arxiv.org/pdf/2510.18234

## TeX Source

https://www.arxiv.org/pdf/2510.18234

view license

https://www.arxiv.org/pdf/2510.18234

Current browse context:  cs.CV

https://www.arxiv.org/prevnext?id=2510.18234&function=prev&context=cs.CV

https://www.arxiv.org/prevnext?id=2510.18234&function=next&context=cs.CV

https://www.arxiv.org/prevnext?id=2510.18234&function=next&context=cs.CV

https://www.arxiv.org/list/cs.CV/recent

https://www.arxiv.org/list/cs.CV/2025-10

Change to browse by:

https://www.arxiv.org/abs/2510.18234?context=cs

References & Citations

https://www.arxiv.org/abs/2510.18234?context=cs

## Google Scholar

https://www.arxiv.org/abs/2510.18234?context=cs

## Semantic Scholar

https://www.arxiv.org/abs/2510.18234?context=cs

export BibTeX citation   Loading...

## BibTeX formatted citation

Ã—   loading...   Data provided by:

## Bibliographic and Citation Tools

## Bibliographic Explorer Toggle   Bibliographic Explorer

What is the Explorer?

https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer

## Connected Papers Toggle   Connected Papers

What is Connected Papers?

https://www.connectedpapers.com/about

## Litmaps Toggle   Litmaps

What is Litmaps?

https://www.litmaps.co/

scite.ai Toggle   scite Smart Citations

What are Smart Citations?

https://www.scite.ai/

Code, Data and Media Associated with this Article

alphaXiv Toggle   alphaXiv

What is alphaXiv?

https://alphaxiv.org/

## Links to Code Toggle   CatalyzeX Code Finder for Papers

What is CatalyzeX?

https://www.catalyzex.com

## DagsHub Toggle   DagsHub

What is DagsHub?

https://dagshub.com/

GotitPub Toggle   Gotit.pub

What is GotitPub?

http://gotit.pub/faq

## Huggingface Toggle   Hugging Face

What is Huggingface?

https://huggingface.co/huggingface

## Links to Code Toggle   Papers with Code

What is Papers with Code?

https://paperswithcode.com/

## ScienceCast Toggle   ScienceCast

What is ScienceCast?

https://sciencecast.org/welcome

## Replicate Toggle   Replicate

What is Replicate?

https://replicate.com/docs/arxiv/about

## Spaces Toggle   Hugging Face Spaces

What is Spaces?

https://huggingface.co/docs/hub/spaces

Spaces Toggle   TXYZ.AI

What is TXYZ.AI?

https://txyz.ai

## Recommenders and Search Tools

## Link to Influence Flower   Influence Flower

What are Influence Flowers?

https://influencemap.cmlab.dev/

## Core recommender toggle   CORE Recommender

What is CORE?

https://core.ac.uk/services/recommender

https://core.ac.uk/services/recommender

https://core.ac.uk/services/recommender

## Institution

https://core.ac.uk/services/recommender

https://core.ac.uk/services/recommender

arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community?

## Learn more about arXivLabs

Which authors of this paper are endorsers?

https://info.arxiv.org/labs/index.html

## Disable MathJax

javascript:setMathjaxCookie()

What is MathJax?

https://info.arxiv.org/help/mathjax.html

https://info.arxiv.org/help/mathjax.html

