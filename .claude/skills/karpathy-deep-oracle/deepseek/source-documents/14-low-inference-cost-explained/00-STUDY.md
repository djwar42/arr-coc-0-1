# Low Inference Cost Explained - Study

**Source**: IntuitionLabs (DeepSeek's Low Inference Cost Explained: MoE & Strategy)
**Date Processed**: 2025-10-28
**Category**: Training Efficiency (Economics)

---

## üìù Summary

Explains why DeepSeek's inference is economical.

**Key factors**:
- **MoE sparse activation**: Only 37B active from 671B total
- **MLA compression**: 93% KV cache reduction
- **Efficient routing**: Load-balanced expert selection
- **Hardware optimization**: FP8, custom kernels

**Combined effect**: Competitive performance at fraction of cost

**Value**: Business/economics perspective on technical choices

---

## üîó Cross-References

- [V3 Technical Report](../01-deepseek-v3-technical-report/00-STUDY.md) - Technical foundation
- [DeepSeekMoE Paper](../03-deepseek-moe-paper/00-STUDY.md) - Sparse activation
- [MLA Explained](../06-mla-explained/00-STUDY.md) - Memory efficiency
- `knowledge-categories/training-efficiency/` - Full context

---

**Last Updated**: 2025-10-28
**Status**: Economics perspective
