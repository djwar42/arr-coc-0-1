# FP8 Training and Inference arXiv - Study

**Source**: arXiv (Training and inference of large language models using 8-bit floating point)
**Date Processed**: 2025-10-29
**Category**: MEDIUM - FP8 Research (likely duplicate of doc 05)

---

## ğŸ“ TL;DR

FP8 training/inference paper. Likely same/similar to doc 05 (fp8-lm-paper). Covers 8-bit floating point for LLMs - training stability, quantization strategies, performance maintenance.

---

## ğŸ”— Connections

- **05-fp8-lm-paper**: Primary FP8 paper (check for overlap)
- **10-fine-grained-fp8**: Detailed FP8 quantization

---

## ğŸ’­ Karpathy Take

Another FP8 paper. The key insight across all FP8 work: 8-bit is enough if you're smart about scaling factors and mixed precision. See doc 05 for main analysis.
