# DeepSeek-MoE: Mixture of Experts Architecture

**Status**: Preliminary structure - awaiting deep dive
**Codebase**: DeepSeek Mixture of Experts (671B total params, 37B active)

---

## ğŸ¯ What This Codebase Does

**DeepSeek-MoE** implements a highly efficient Mixture of Experts architecture.

**Key Innovation**: Fine-grained experts with load balancing for 671B parameter model

**Core Features**:
- Fine-grained expert design (smaller experts, more of them)
- Load balancing to prevent expert collapse
- Sparse activation (only 37B params active per token)
- Expert routing with auxiliary losses

---

## ğŸ“ Expected Codebase Structure

```
03-DeepSeek-MoE/
â”œâ”€â”€ 00-OVERVIEW.md           # This file
â”œâ”€â”€ 01-architecture.md       # MoE system design (TO BE CREATED)
â”œâ”€â”€ 02-expert-routing.md     # Routing algorithm (TO BE CREATED)
â”œâ”€â”€ 03-load-balancing.md     # Balancing strategies (TO BE CREATED)
â”œâ”€â”€ 04-training.md           # Training with auxiliary losses (TO BE CREATED)
â”œâ”€â”€ 05-inference.md          # Efficient inference (TO BE CREATED)
â”œâ”€â”€ code-snippets/           # Key code with line numbers (TO BE CREATED)
â””â”€â”€ examples/                # Usage examples (TO BE CREATED)
```

---

## ğŸ” What Needs to Be Done

### Phase 1: Architecture Analysis
- [ ] Document expert structure
- [ ] Explain routing mechanism
- [ ] Map token flow through experts
- [ ] Show capacity factors

### Phase 2: Code Deep Dive
- [ ] Extract MoE layer implementation with line numbers
- [ ] Document routing function
- [ ] Explain load balancing auxiliary loss
- [ ] Show expert weight initialization

### Phase 3: Usage Documentation
- [ ] Training MoE models
- [ ] Expert capacity tuning
- [ ] Load balancing configuration
- [ ] Inference optimization

---

## ğŸ”— Related Knowledge

**Will connect to**:
- Knowledge category: `model-architectures/01-moe-design.md`
- Cross-reference: DualPipe (pipeline parallelism for MoE)
- Comparison: Standard MoE vs DeepSeek fine-grained approach

---

## ğŸ“ Next Steps

1. Locate MoE layer implementations
2. Read expert routing code
3. Understand load balancing mechanism
4. Extract key code snippets
5. Document training procedure
6. Create usage examples

---

**Last Updated**: 2025-10-28
**Status**: Awaiting Phase 4 deep dive
