# World Models for Affordances

**ML-HEAVY**: PyTorch implementations, model-based RL, planning architectures

## Overview

World models are learned generative models of environment dynamics that enable agents to predict future states and plan actions through mental simulation. In the context of affordances, world models serve as the internal representation through which an agent can imagine "what would happen if I do X?" - directly embodying Gibson's concept of affordance as action possibility.

**The Core Insight**: A world model IS an affordance detector - by simulating future trajectories, the agent discovers which actions afford which outcomes.

From [World Models (Ha & Schmidhuber, 2018)](https://arxiv.org/abs/1803.10122):
- Learn compressed spatial and temporal representations unsupervised
- Train policies entirely in "hallucinated dreams" generated by world model
- Transfer dream-trained policies back to real environment
- Enables rapid learning through mental simulation vs real interaction

From [Dreamer (Hafner et al., 2019-2025)](https://danijar.com/project/dreamerv3/):
- First algorithm to collect diamonds in Minecraft from sparse rewards
- Learns purely from imagined rollouts in latent space
- Single hyperparameter configuration across 150+ diverse tasks
- "Planning = Active Inference = Affordance Detection"

---

## 1. World Model Architectures

### 1.1 Core Components

**Three key modules:**

1. **Representation Model** (Encoder): Observations â†’ Latent states
   - Vision: CNN/ViT for pixel observations
   - Compression: High-D observations â†’ Low-D latent codes
   - Stochastic: Captures uncertainty (VAE-style)

2. **Transition Model** (Dynamics): Predicts next latent state
   - Recurrent: LSTM/GRU for temporal consistency
   - Action-conditioned: s_{t+1} = f(s_t, a_t)
   - Stochastic: Models environment randomness

3. **Observation Model** (Decoder): Latent states â†’ Predicted observations
   - Reconstruction: Decode latent to pixel space
   - Reward prediction: Predict expected rewards
   - Value prediction (optional): For planning

**Mathematical Formulation**:

```
Encoder:    p(z_t | o_t)              # Observation â†’ Latent
Transition: p(z_{t+1} | z_t, a_t)     # Dynamics in latent space
Decoder:    p(o_t | z_t)              # Latent â†’ Observation reconstruction
Reward:     p(r_t | z_t)              # Reward prediction
```

### 1.2 Ha & Schmidhuber World Models (2018)

**Architecture** (VizDoom, CarRacing):

```
VAE (Vision) â†’ MDN-RNN (Memory) â†’ Controller
   â†“              â†“                    â†“
Compress     Predict future      Choose action
o_t â†’ z_t    z_{t+1} | z_t,a_t   a_t | z_t
```

**Key Innovation**: Train agent entirely inside dreams!

1. Collect random trajectories
2. Train VAE (vision model) on observations
3. Train MDN-RNN (memory model) on latent sequences
4. Train controller (policy) via evolution in dreams
5. Transfer policy to real environment (works!)

**Why it works**:
- VAE compresses 64x64 pixels â†’ 32D latent (2000x compression!)
- MDN-RNN learns temporal dynamics (what happens next?)
- Controller evolves in simulation (100x faster than real)
- Compressed representation = simplified planning space

From [World Models paper](https://arxiv.org/abs/1803.10122):
"We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment."

---

## 2. Dreamer: Model-Based RL via Latent Imagination

### 2.1 Dreamer Architecture (2019-2025)

**Hafner et al.** developed the Dreamer family (v1, v2, v3, v4) - the current SOTA for world model-based RL.

**Core Idea**: Learn behaviors by imagination in latent space.

**Architecture** (DreamerV3):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WORLD MODEL                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  Encoder:  o_t â†’ z_t (CNN/ViT)                  â”‚
â”‚  RSSM*:    Recurrent State-Space Model          â”‚
â”‚    - Deterministic: h_t = f(h_{t-1}, z_{t-1}, a_{t-1}) â”‚
â”‚    - Stochastic:    z_t ~ p(z_t | h_t)          â”‚
â”‚  Decoder:  z_t â†’ o_t (deconv/pixel decoder)     â”‚
â”‚  Reward:   z_t â†’ r_t (reward predictor)         â”‚
â”‚  Continue: z_t â†’ c_t (episode termination)      â”‚
â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“ Imagined trajectories
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ACTOR-CRITIC (in dreams)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  Actor:  Ï€(a_t | z_t) - policy in latent space  â”‚
â”‚  Critic: V(z_t) - value in latent space         â”‚
â”‚                                                  â”‚
â”‚  Training: Imagine rollouts from world model    â”‚
â”‚    z_0 â†’ a_0 â†’ z_1 â†’ a_1 â†’ ... â†’ z_H            â”‚
â”‚    â†“                                             â”‚
â”‚  Policy gradient on imagined returns             â”‚
â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

*RSSM = Recurrent State-Space Model
```

**RSSM Details** (key innovation):

Combines deterministic RNN state + stochastic latent:

```
Deterministic state: h_t = f_Î¸(h_{t-1}, z_{t-1}, a_{t-1})  # GRU/LSTM
Stochastic state:    z_t ~ p_Î¸(z_t | h_t)                   # Categorical/Gaussian

Full state: s_t = [h_t, z_t]
```

Why hybrid?
- **h_t (deterministic)**: Captures long-term dependencies, temporal structure
- **z_t (stochastic)**: Captures environment randomness, multi-modal futures
- Together: Expressiveness + stability

### 2.2 Dreamer Training Loop

**Two-phase learning**:

**Phase 1: World Model Learning** (unsupervised on replay buffer)

```python
# Sample batch from replay buffer
observations, actions, rewards = buffer.sample(batch_size)

# Encode observations â†’ latent sequence
z_t = encoder(o_t)

# RSSM dynamics loss
h_t = rnn(h_{t-1}, z_{t-1}, a_{t-1})  # Deterministic
z_hat_t = stochastic_predictor(h_t)   # Predicted stochastic

# Losses
dynamics_loss = KL(z_t || z_hat_t)     # Prediction error
recon_loss = ||decoder(z_t) - o_t||Â²  # Reconstruction
reward_loss = ||reward_pred(z_t) - r_t||Â²

total_loss = dynamics_loss + recon_loss + reward_loss
```

**Phase 2: Behavior Learning** (policy + value via imagination)

```python
# Start from encoded states in buffer
s_0 = [h_0, z_0]

# Imagine rollout using learned world model
imagined_trajectory = []
for t in range(horizon):
    a_t = actor(s_t)                        # Policy samples action
    h_{t+1} = rnn(h_t, z_t, a_t)            # Deterministic step
    z_{t+1} ~ stochastic_model(h_{t+1})     # Stochastic step
    r_t = reward_predictor([h_t, z_t])      # Predicted reward
    c_t = continue_predictor([h_t, z_t])    # Continue probability
    imagined_trajectory.append((s_t, a_t, r_t, c_t))

# Compute returns (Î»-returns with learned value)
returns = compute_lambda_returns(imagined_trajectory, critic)

# Policy gradient on imagined returns
actor_loss = -E[returns]  # Maximize imagined returns
critic_loss = (V(s_t) - returns)Â²  # Value prediction
```

**Key Insight**: Agent learns ENTIRELY from imagination after world model is trained!

### 2.3 DreamerV3 Robustness (Nature 2025)

From [DreamerV3 paper](https://www.nature.com/articles/s41586-025-08744-2):
"Dreamer learns a model of the environment and improves its behaviour by imagining future scenarios."

**Robustness techniques for single hyperparameter config across 150+ tasks:**

1. **Symlog predictions**: Log-scale for large value ranges
   ```python
   def symlog(x):
       return torch.sign(x) * torch.log(1 + torch.abs(x))

   def symexp(x):  # Inverse
       return torch.sign(x) * (torch.exp(torch.abs(x)) - 1)
   ```

2. **Layer normalization**: Stable gradients across domains
3. **Categorical latents**: Better than Gaussian for discrete-ish dynamics
4. **Free bits**: Prevent posterior collapse in VAE-style models

**Performance** (single config, no tuning):
- Atari: Matches tuned Rainbow DQN
- DMC: Outperforms SAC, TD3
- Minecraft: **First to collect diamonds from sparse rewards without human data!**
  - 30M steps (17 days playtime)
  - No demonstrations, no curriculum
  - Pure imagination-based learning

---

## 3. Planning with World Models

### 3.1 Model-Predictive Control (MPC)

**Classic approach**: Roll out world model, optimize action sequence.

```python
def mpc_planning(world_model, current_state, horizon=15, num_samples=1000):
    """
    Model-Predictive Control with learned world model.

    Sample many action sequences, simulate in world model,
    pick sequence with highest predicted return.
    """
    best_return = -float('inf')
    best_actions = None

    for _ in range(num_samples):
        # Sample random action sequence
        actions = sample_action_sequence(horizon)

        # Simulate trajectory in world model
        state = current_state
        total_return = 0
        for t in range(horizon):
            state = world_model.predict_next(state, actions[t])
            reward = world_model.predict_reward(state)
            total_return += gamma**t * reward

            if world_model.is_terminal(state):
                break

        if total_return > best_return:
            best_return = total_return
            best_actions = actions

    # Execute only first action (replanning at next step)
    return best_actions[0]
```

**Limitations**:
- Sampling-based: Inefficient in high-D action spaces
- Myopic: Only optimizes over horizon H
- Expensive: Requires many rollouts per decision

### 3.2 Learned Policy via Imagination (Dreamer approach)

**Better approach**: Learn policy by gradient descent on imagined returns.

```python
def imagination_based_learning(world_model, actor, critic,
                               replay_buffer, num_imagination_steps=15):
    """
    Train policy by imagining trajectories in learned world model.
    Dreamer-style approach.
    """
    # Sample starting states from replay
    states = replay_buffer.sample_states(batch_size)

    # Imagine trajectories
    trajectory = []
    for t in range(num_imagination_steps):
        # Sample action from current policy
        actions = actor(states)

        # Predict next state using world model
        next_states = world_model.predict_next(states, actions)
        rewards = world_model.predict_reward(next_states)
        continues = world_model.predict_continue(next_states)

        trajectory.append({
            'state': states,
            'action': actions,
            'reward': rewards,
            'continue': continues
        })

        states = next_states

    # Compute returns using value function (TD-lambda style)
    returns = compute_lambda_returns(trajectory, critic, lambda_=0.95)

    # Policy gradient
    actor_loss = -returns.mean()  # Maximize returns

    # Value learning
    critic_loss = F.mse_loss(critic(trajectory_states), returns.detach())

    return actor_loss, critic_loss
```

**Advantages**:
- **Amortized**: Policy learns general strategy (no replanning)
- **Gradient-based**: Efficient optimization
- **Scalable**: Imagined rollouts are cheap (GPU parallel)

### 3.3 Hybrid: MPC + Learned Policy

**Combined approach** (e.g., MuZero, EfficientZero):

1. Learn world model + value function
2. Use MCTS for planning (guided by learned value)
3. Improve policy via self-play

```python
def mcts_with_world_model(world_model, value_net, root_state,
                          num_simulations=50):
    """
    Monte Carlo Tree Search with learned world model and value function.
    Similar to AlphaZero/MuZero but with explicit world model.
    """
    tree = MCTSTree()
    tree.add_node(root_state)

    for _ in range(num_simulations):
        # 1. Selection: Traverse tree using UCB
        node = tree.select(root_state)

        # 2. Expansion: Add children if not terminal
        if not node.is_terminal and not node.is_expanded:
            for action in range(num_actions):
                # Predict next state using world model
                next_state = world_model.predict_next(node.state, action)
                reward = world_model.predict_reward(next_state)
                tree.add_child(node, action, next_state, reward)
            node.is_expanded = True

        # 3. Simulation: Use value network (no rollout needed!)
        value = value_net(node.state)

        # 4. Backpropagation: Update statistics
        tree.backup(node, value)

    # Return action with highest visit count
    return tree.best_action(root_state)
```

**Why it works**:
- World model enables look-ahead
- Value network guides search (learned from experience)
- MCTS balances exploration/exploitation
- No need for expensive rollouts (value net replaces)

---

## 4. PyTorch Code: Complete World Model Implementation

### 4.1 RSSM World Model (Dreamer-style)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal, Independent, Categorical

class RSSM(nn.Module):
    """
    Recurrent State-Space Model (Dreamer's world model core).

    Combines:
    - Deterministic state (GRU): h_t = f(h_{t-1}, z_{t-1}, a_{t-1})
    - Stochastic state (categorical): z_t ~ p(z_t | h_t)
    """
    def __init__(self,
                 action_dim=6,
                 stoch_dim=32,      # Number of categorical variables
                 stoch_classes=32,  # Classes per categorical
                 hidden_dim=512,    # Deterministic state size
                 embed_dim=1024):   # Observation embedding size
        super().__init__()

        self.action_dim = action_dim
        self.stoch_dim = stoch_dim
        self.stoch_classes = stoch_classes
        self.hidden_dim = hidden_dim

        # Observation encoder (CNN for images)
        self.encoder = ObservationEncoder(embed_dim)

        # Deterministic state model (GRU)
        self.gru = nn.GRUCell(
            input_size=stoch_dim * stoch_classes + action_dim,
            hidden_size=hidden_dim
        )

        # Stochastic prior: p(z_t | h_t)
        self.prior_net = nn.Sequential(
            nn.Linear(hidden_dim, 512),
            nn.ELU(),
            nn.Linear(512, stoch_dim * stoch_classes)
        )

        # Stochastic posterior: p(z_t | h_t, o_t)
        self.posterior_net = nn.Sequential(
            nn.Linear(hidden_dim + embed_dim, 512),
            nn.ELU(),
            nn.Linear(512, stoch_dim * stoch_classes)
        )

        # Observation decoder
        self.decoder = ObservationDecoder(
            input_dim=hidden_dim + stoch_dim * stoch_classes
        )

        # Reward predictor
        self.reward_net = nn.Sequential(
            nn.Linear(hidden_dim + stoch_dim * stoch_classes, 512),
            nn.ELU(),
            nn.Linear(512, 1)
        )

        # Continue predictor (episode termination)
        self.continue_net = nn.Sequential(
            nn.Linear(hidden_dim + stoch_dim * stoch_classes, 512),
            nn.ELU(),
            nn.Linear(512, 1),
            nn.Sigmoid()
        )

    def initial_state(self, batch_size, device):
        """Initialize recurrent state."""
        h = torch.zeros(batch_size, self.hidden_dim, device=device)
        z = torch.zeros(batch_size, self.stoch_dim, self.stoch_classes, device=device)
        z = z.reshape(batch_size, -1)  # Flatten
        return h, z

    def observe(self, observation, prev_action, prev_h, prev_z):
        """
        Encode observation and update state (posterior).
        Used during data collection and world model training.
        """
        # Encode observation
        embed = self.encoder(observation)

        # Update deterministic state
        h = self.gru(torch.cat([prev_z, prev_action], -1), prev_h)

        # Compute posterior: p(z_t | h_t, o_t)
        posterior_logits = self.posterior_net(torch.cat([h, embed], -1))
        posterior_logits = posterior_logits.reshape(-1, self.stoch_dim, self.stoch_classes)
        posterior = Categorical(logits=posterior_logits)
        z_sample = posterior.sample()  # Sample categorical
        z_onehot = F.one_hot(z_sample, self.stoch_classes).float()
        z = z_onehot.reshape(-1, self.stoch_dim * self.stoch_classes)

        return h, z, posterior

    def imagine(self, prev_action, prev_h, prev_z):
        """
        Predict next state without observation (prior).
        Used during imagination/planning.
        """
        # Update deterministic state
        h = self.gru(torch.cat([prev_z, prev_action], -1), prev_h)

        # Compute prior: p(z_t | h_t)
        prior_logits = self.prior_net(h)
        prior_logits = prior_logits.reshape(-1, self.stoch_dim, self.stoch_classes)
        prior = Categorical(logits=prior_logits)
        z_sample = prior.sample()
        z_onehot = F.one_hot(z_sample, self.stoch_classes).float()
        z = z_onehot.reshape(-1, self.stoch_dim * self.stoch_classes)

        return h, z, prior

    def decode_observation(self, h, z):
        """Reconstruct observation from latent state."""
        state = torch.cat([h, z], -1)
        return self.decoder(state)

    def predict_reward(self, h, z):
        """Predict reward from latent state."""
        state = torch.cat([h, z], -1)
        return self.reward_net(state)

    def predict_continue(self, h, z):
        """Predict episode continuation probability."""
        state = torch.cat([h, z], -1)
        return self.continue_net(state)


class ObservationEncoder(nn.Module):
    """CNN encoder for image observations."""
    def __init__(self, embed_dim=1024):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 32, 4, stride=2), nn.ReLU(),  # 64x64 â†’ 31x31
            nn.Conv2d(32, 64, 4, stride=2), nn.ReLU(), # 31x31 â†’ 14x14
            nn.Conv2d(64, 128, 4, stride=2), nn.ReLU(), # 14x14 â†’ 6x6
            nn.Conv2d(128, 256, 4, stride=2), nn.ReLU(), # 6x6 â†’ 2x2
            nn.Flatten(),
            nn.Linear(256 * 2 * 2, embed_dim)
        )

    def forward(self, obs):
        """obs: (batch, 3, 64, 64) â†’ embed: (batch, 1024)"""
        return self.conv(obs)


class ObservationDecoder(nn.Module):
    """Deconvolution decoder for image reconstruction."""
    def __init__(self, input_dim=512 + 32*32):
        super().__init__()
        self.fc = nn.Linear(input_dim, 256 * 2 * 2)
        self.deconv = nn.Sequential(
            nn.ConvTranspose2d(256, 128, 4, stride=2), nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 4, stride=2), nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, stride=2), nn.ReLU(),
            nn.ConvTranspose2d(32, 3, 4, stride=2)
        )

    def forward(self, state):
        x = self.fc(state).reshape(-1, 256, 2, 2)
        return self.deconv(x)  # (batch, 3, 64, 64)
```

### 4.2 Training the World Model

```python
def train_world_model(rssm, optimizer, replay_buffer, num_steps=100000):
    """
    Train RSSM world model on collected experience.
    """
    for step in range(num_steps):
        # Sample batch: (batch, seq_len, ...)
        obs, actions, rewards = replay_buffer.sample(
            batch_size=50, seq_len=50
        )

        batch_size, seq_len = obs.shape[:2]
        device = obs.device

        # Initialize recurrent state
        h, z = rssm.initial_state(batch_size, device)

        # Losses
        reconstruction_loss = 0
        kl_loss = 0
        reward_loss = 0

        # Sequence processing
        for t in range(seq_len):
            # Encode observation and update posterior
            h, z, posterior = rssm.observe(
                obs[:, t],
                actions[:, t],
                h,
                z
            )

            # Predict prior (for KL)
            if t > 0:
                _, _, prior = rssm.imagine(actions[:, t-1], prev_h, prev_z)

                # KL divergence: KL(posterior || prior)
                kl = torch.distributions.kl_divergence(posterior, prior)
                kl_loss += kl.sum(-1).mean()  # Sum over stoch_dim, mean over batch

            # Reconstruction
            obs_recon = rssm.decode_observation(h, z)
            reconstruction_loss += F.mse_loss(obs_recon, obs[:, t])

            # Reward prediction
            reward_pred = rssm.predict_reward(h, z)
            reward_loss += F.mse_loss(reward_pred, rewards[:, t])

            # Store for next iteration
            prev_h, prev_z = h, z

        # Average over sequence
        reconstruction_loss /= seq_len
        kl_loss /= seq_len
        reward_loss /= seq_len

        # Total loss (free bits for KL to prevent collapse)
        free_bits = 1.0  # Minimum KL per dimension
        kl_loss = torch.maximum(kl_loss, torch.tensor(free_bits * rssm.stoch_dim))

        total_loss = reconstruction_loss + kl_loss + reward_loss

        # Optimize
        optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(rssm.parameters(), 100.0)
        optimizer.step()

        if step % 1000 == 0:
            print(f"Step {step}: Recon={reconstruction_loss:.3f}, "
                  f"KL={kl_loss:.3f}, Reward={reward_loss:.3f}")
```

### 4.3 Imagination-Based Actor-Critic

```python
class ActorCritic(nn.Module):
    """Policy and value networks operating in latent space."""
    def __init__(self, state_dim, action_dim):
        super().__init__()

        # Actor: Ï€(a | s)
        self.actor = nn.Sequential(
            nn.Linear(state_dim, 512),
            nn.ELU(),
            nn.Linear(512, 512),
            nn.ELU(),
            nn.Linear(512, action_dim)
        )

        # Critic: V(s)
        self.critic = nn.Sequential(
            nn.Linear(state_dim, 512),
            nn.ELU(),
            nn.Linear(512, 512),
            nn.ELU(),
            nn.Linear(512, 1)
        )

    def get_action(self, state):
        """Sample action from policy."""
        logits = self.actor(state)
        return Categorical(logits=logits).sample()

    def get_value(self, state):
        """Predict state value."""
        return self.critic(state)


def train_actor_critic(rssm, actor_critic, optimizer_ac, replay_buffer,
                       imagination_horizon=15, num_steps=100000):
    """
    Train policy and value by imagining trajectories in world model.
    Dreamer's core learning algorithm.
    """
    for step in range(num_steps):
        # Sample starting states from replay buffer
        obs, _, _ = replay_buffer.sample(batch_size=50, seq_len=1)
        obs = obs[:, 0]  # (batch, obs_shape)

        # Encode to latent state
        with torch.no_grad():
            h, z = rssm.initial_state(obs.shape[0], obs.device)
            h, z, _ = rssm.observe(obs,
                                   torch.zeros(obs.shape[0], rssm.action_dim, device=obs.device),
                                   h, z)

        # Imagine trajectory
        states_h, states_z, actions, rewards, continues = [], [], [], [], []

        for t in range(imagination_horizon):
            state = torch.cat([h, z], -1)

            # Sample action from actor
            action_logits = actor_critic.actor(state)
            action_dist = Categorical(logits=action_logits)
            action = action_dist.sample()
            action_onehot = F.one_hot(action, rssm.action_dim).float()

            # Predict next state
            h, z, _ = rssm.imagine(action_onehot, h, z)

            # Predict reward and continuation
            reward = rssm.predict_reward(h, z)
            continue_prob = rssm.predict_continue(h, z)

            states_h.append(h)
            states_z.append(z)
            actions.append(action)
            rewards.append(reward)
            continues.append(continue_prob)

        # Stack trajectories
        states_h = torch.stack(states_h, dim=1)  # (batch, horizon, hidden_dim)
        states_z = torch.stack(states_z, dim=1)
        rewards = torch.stack(rewards, dim=1)
        continues = torch.stack(continues, dim=1)

        # Compute returns (TD-lambda style with learned value)
        states = torch.cat([states_h, states_z], -1)
        values = actor_critic.critic(states).squeeze(-1)  # (batch, horizon)

        # Lambda-returns
        lambda_ = 0.95
        gamma = 0.99
        returns = compute_lambda_returns(rewards, values, continues, lambda_, gamma)

        # Actor loss: Maximize returns
        actor_loss = -returns.mean()

        # Critic loss: Predict returns
        critic_loss = F.mse_loss(values, returns.detach())

        # Optimize
        total_loss = actor_loss + critic_loss
        optimizer_ac.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(actor_critic.parameters(), 100.0)
        optimizer_ac.step()

        if step % 1000 == 0:
            print(f"Step {step}: Actor={actor_loss:.3f}, Critic={critic_loss:.3f}")


def compute_lambda_returns(rewards, values, continues, lambda_=0.95, gamma=0.99):
    """
    Compute TD(Î») returns for imagined trajectories.

    Args:
        rewards: (batch, horizon) - predicted rewards
        values: (batch, horizon) - predicted values
        continues: (batch, horizon) - continuation probabilities
        lambda_: TD-lambda mixing parameter
        gamma: discount factor

    Returns:
        returns: (batch, horizon) - target returns
    """
    batch_size, horizon = rewards.shape

    # Bootstrap from final value
    returns = torch.zeros_like(rewards)
    last_value = values[:, -1]

    # Backward pass computing returns
    for t in reversed(range(horizon)):
        # TD error: r_t + Î³*c_t*V(s_{t+1}) - V(s_t)
        if t < horizon - 1:
            next_value = values[:, t+1]
        else:
            next_value = last_value

        # Î»-return: r_t + Î³*c_t*[(1-Î»)*V(s_{t+1}) + Î»*G_{t+1}]
        returns[:, t] = rewards[:, t] + gamma * continues[:, t] * (
            (1 - lambda_) * next_value + lambda_ * returns[:, t+1] if t < horizon - 1
            else next_value
        )

    return returns
```

---

## 5. TRAIN STATION: World Model = Generative Model = Active Inference = Affordance

**THE UNIFICATION**:

```
World Model (RL)
    â†“
Generative Model (ML)
    â†“
Active Inference (Neuroscience/FEP)
    â†“
Affordance Detection (Gibson)
    â†“
ALL THE SAME THING!
```

### 5.1 World Model = Active Inference

**Active Inference** (Friston):
- Agent maintains generative model p(o, s, a)
- Minimizes free energy = prediction error
- Actions selected to minimize expected free energy

**World Model** (Dreamer):
- Agent learns generative model p(o_t | s_t), p(s_{t+1} | s_t, a_t)
- Minimizes reconstruction error + dynamics error
- Actions selected to maximize imagined returns

**SAME UNDERLYING STRUCTURE**:

```
Active Inference:
  Generative model: p(o_t | s_t) p(s_{t+1} | s_t, a_t)
  Minimize: F = -log p(o) = E_q[log q(s) - log p(o,s)]
  Policy: Ï€(a) minimizes expected free energy G

Dreamer:
  Generative model: p(o_t | z_t) p(z_{t+1} | z_t, a_t)
  Minimize: L_recon + L_dynamics
  Policy: Ï€(a | z_t) maximizes E[Î£ Î³^t r_t]

MAPPING:
  s_t (active inference) â†” z_t (Dreamer latent state)
  Free energy â†” Prediction error (ELBO)
  Expected free energy â†” Value function (expected returns)
```

**From Friston's perspective**: Dreamer IS active inference!
- World model = generative model of observations
- Imagination = prospective inference (what if I do X?)
- Policy learning = minimize expected free energy (via value)

### 5.2 World Model = Affordance Detector

**Gibson's Affordances**: Action possibilities perceived directly from environment.

**World Model Perspective**:

```python
def detect_affordances(world_model, current_state, possible_actions):
    """
    World model directly computes affordances!

    Affordance = "What happens if I do this action?"
    """
    affordances = {}

    for action in possible_actions:
        # Simulate action in world model
        next_state = world_model.predict_next(current_state, action)
        reward = world_model.predict_reward(next_state)
        value = world_model.predict_value(next_state)

        # Affordance = predicted outcome
        affordances[action] = {
            'next_state': next_state,
            'immediate_reward': reward,
            'future_value': value,
            'total_value': reward + gamma * value  # What this action affords
        }

    return affordances
```

**THE INSIGHT**:
- Affordance is NOT a separate concept to learn
- Affordance EMERGES from world model predictions
- "Graspable" = world model predicts high value after "grasp" action
- "Climbable" = world model predicts upward state transition after "climb"

**Example - Minecraft Diamond Collection**:

```
State: Agent in cave, sees blue ore
Actions: [mine, ignore, flee]

World model affordances:
  mine â†’ high reward (diamond collected!) + high value (can craft better tools)
  ignore â†’ zero reward + medium value (nothing changes)
  flee â†’ zero reward + low value (miss opportunity)

Agent chooses: MINE (highest affordance!)
```

**This is exactly how Dreamer collected diamonds**:
1. World model learned: "blue ore + mine action = diamond reward"
2. Policy learned: "when I see blue ore, imagine mining â†’ high value â†’ do it!"
3. Affordance = world model's prediction of valuable outcome

### 5.3 The Generative Model Connection

**All three use same generative model structure**:

```
p(observations, states, actions) = p(o_0) Î _t p(o_t|s_t) p(s_t|s_{t-1},a_{t-1}) Ï€(a_t|s_t)
                                     â†‘         â†‘              â†‘                    â†‘
                                   Prior   Decoder      Transition            Policy
```

**Training objective** (VAE-style):

```
ELBO = E_q[log p(o_t | s_t)] - KL(q(s_t | o_t, a_{t-1}) || p(s_t | s_{t-1}, a_{t-1}))
        â†‘                       â†‘
  Reconstruction loss    Dynamics loss (prediction error)
```

**Applications**:
- **ML**: Generative modeling, VAE, world models
- **RL**: Model-based RL, Dreamer, MuZero
- **Neuroscience**: Predictive coding, active inference
- **Cognitive**: Affordance perception, planning, mental simulation

**ALL USE THE SAME MATH! Coffee cup = donut!** â˜•ï¸ðŸ©

---

## 6. Performance & Implementation Notes

### 6.1 Computational Efficiency

**World model advantages**:
- **Sample efficiency**: Learn from imagined data (100x cheaper than real)
- **Parallel simulation**: GPU can imagine 1000s of trajectories simultaneously
- **Amortized planning**: Policy learns general strategy (no replanning)

**Costs**:
- **Model learning**: Need to train generative model (upfront cost)
- **Model error**: Compounding errors in long rollouts
- **Memory**: Storing replay buffer (though shared with model-free methods)

**Benchmark - Atari (DreamerV3)**:
- Data efficiency: Match Rainbow DQN at 400M frames
- Wall-clock: 8 GPU hours (vs 60 for Rainbow on 128 CPUs)
- Model size: 50M parameters (vs 10M for DQN)

**Benchmark - DMC (DreamerV3)**:
- Data efficiency: Match SAC at 1M steps (500k for Dreamer)
- Imagination: 15-step horizon (100 imagined steps per real step!)
- Speed: 10,000 imagined steps/sec on single GPU

### 6.2 Hyperparameters (DreamerV3 defaults)

```python
# World model architecture
config = {
    # RSSM
    'stoch_dim': 32,           # Categorical variables
    'stoch_classes': 32,       # Classes per categorical
    'hidden_dim': 512,         # Deterministic state size
    'embed_dim': 1024,         # Observation embedding

    # Training
    'batch_size': 16,          # Sequence batch size
    'seq_len': 64,             # Sequence length
    'imagination_horizon': 15, # Imagination rollout length
    'lambda': 0.95,            # TD-lambda
    'gamma': 0.99,             # Discount factor

    # Optimization
    'lr_model': 1e-4,          # World model learning rate
    'lr_actor': 3e-5,          # Actor learning rate
    'lr_critic': 3e-5,         # Critic learning rate
    'grad_clip': 100.0,        # Gradient clipping

    # Regularization
    'free_bits': 1.0,          # Free bits for KL
    'weight_decay': 0.0,       # No weight decay
}
```

**Key insight**: Same hyperparameters work across Atari, DMC, Minecraft!

### 6.3 Common Issues & Solutions

**Problem**: Model collapse (posterior = prior)
- **Solution**: Free bits regularization (minimum KL divergence)
- Force model to use at least 1 bit of information per stoch dimension

**Problem**: Value explosion in long imagination
- **Solution**: Symlog predictions (log-scale for large values)
- Prevents gradient explosion from huge imagined returns

**Problem**: Poor long-horizon planning
- **Solution**: Increase imagination horizon slowly during training
- Start: 5 steps, increase to 15 over time
- Use learned value to bootstrap (don't rely purely on model)

**Problem**: Model overfitting to recent data
- **Solution**: Large replay buffer (1M transitions)
- Mix old and new data (uniform sampling)
- Continue predictor prevents bootstrap from "dead" states

---

## 7. ARR-COC Connection: Relevance Through World Model Simulation (10%)

**How ARR-COC could use world models for relevance**:

### 7.1 Simulate Future Tokens

**Current**: Relevance computed from current context only.

**With World Model**: Simulate future dialogue turns!

```python
class DialogueWorldModel(nn.Module):
    """
    World model for dialogue: predict future user utterances.
    Enables relevance via simulation: "Will this token help future turns?"
    """
    def __init__(self):
        super().__init__()

        # Encoder: Current dialogue â†’ latent state
        self.encoder = TransformerEncoder(...)

        # Transition: Predict next user utterance latent
        self.transition_rnn = nn.GRU(
            input_size=latent_dim + response_dim,
            hidden_size=hidden_dim
        )

        # Decoder: Latent â†’ user utterance
        self.decoder = TransformerDecoder(...)

        # Relevance predictor: How relevant is response to future?
        self.relevance_net = nn.Sequential(
            nn.Linear(latent_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 1),
            nn.Sigmoid()
        )

    def simulate_future_turns(self, current_state, proposed_response, num_turns=3):
        """
        Imagine future dialogue if we give this response.
        Compute relevance based on imagined future.
        """
        state = current_state
        future_relevance = []

        for t in range(num_turns):
            # Predict next state given our response
            state = self.transition_rnn(
                torch.cat([state, proposed_response], -1)
            )

            # Predict future user utterance
            future_user_utterance = self.decoder(state)

            # Score relevance of our response to future needs
            relevance = self.relevance_net(state)
            future_relevance.append(relevance)

            # Simulate our response to future (recursive)
            proposed_response = self.generate_response(state)

        # Total relevance = immediate + discounted future
        total_relevance = sum(gamma**t * r for t, r in enumerate(future_relevance))
        return total_relevance


def relevance_via_world_model(context, tokens, world_model):
    """
    Compute token relevance by simulating future dialogue.

    Tokens are relevant if they lead to better future interactions!
    """
    current_state = world_model.encoder(context)

    relevances = []
    for token in tokens:
        # Imagine dialogue with this token in response
        response_with_token = torch.cat([context, token.unsqueeze(0)])

        # Simulate future turns
        future_relevance = world_model.simulate_future_turns(
            current_state,
            response_with_token,
            num_turns=3
        )

        relevances.append(future_relevance)

    return torch.stack(relevances)
```

**Why this helps ARR-COC**:
- **Farsighted relevance**: Tokens relevant for future turns, not just current
- **Context-aware**: Model learns dialogue dynamics
- **Adaptive**: Relevance changes based on predicted user reactions

### 7.2 Action-Affordance Relevance

**Tokens as actions**: Each token affords different future contexts.

```python
def token_affordances(world_model, context, candidate_tokens):
    """
    Compute what each token 'affords' in future dialogue.

    Affordance = predicted outcome quality in world model.
    """
    affordances = {}

    for token in candidate_tokens:
        # Simulate adding this token
        next_context = torch.cat([context, token])

        # Predict future dialogue quality
        future_engagement = world_model.predict_engagement(next_context)
        future_coherence = world_model.predict_coherence(next_context)
        future_task_success = world_model.predict_task_success(next_context)

        # Affordance = weighted combo of predicted outcomes
        affordances[token] = {
            'engagement': future_engagement,
            'coherence': future_coherence,
            'task_success': future_task_success,
            'total': 0.3 * future_engagement +
                     0.3 * future_coherence +
                     0.4 * future_task_success
        }

    return affordances


# Use in relevance scoring
def relevance_is_affordance(context, tokens, world_model):
    """
    Relevance = What this token affords in future dialogue.
    """
    affordances = token_affordances(world_model, context, tokens)

    # Tokens with high affordance (good predicted outcomes) are relevant!
    relevance_scores = torch.tensor([
        affordances[token]['total'] for token in tokens
    ])

    return relevance_scores
```

**Concrete example**:

```
User: "I'm frustrated with the slow download."

Candidate tokens:
1. "sorry" â†’ Affordances: {engagement: 0.3, task_success: 0.1} â†’ Relevance: 0.2
2. "bandwidth" â†’ Affordances: {engagement: 0.7, task_success: 0.9} â†’ Relevance: 0.8
3. "check" â†’ Affordances: {engagement: 0.6, task_success: 0.8} â†’ Relevance: 0.7

World model predicts:
- "sorry" â†’ user stays frustrated, no resolution
- "bandwidth" â†’ user understands issue, provides more info, gets help
- "check" â†’ user follows diagnostic steps, likely resolves

Relevance via world model correctly identifies "bandwidth" and "check" as most relevant!
```

**Implementation strategy**:
1. Train world model on dialogue datasets (predict next user utterance)
2. Train affordance scorers (predict dialogue quality metrics)
3. Integrate into ARR-COC relevance computation
4. Tokens allocated based on predicted future value

**Benefit**: Relevance becomes **predictive** (future-oriented) not just **reactive** (current context).

---

## Sources

**Papers**:
- [World Models (Ha & Schmidhuber, 2018)](https://arxiv.org/abs/1803.10122) - arXiv:1803.10122
- [Dream to Control: Learning Behaviors by Latent Imagination (Hafner et al., 2019)](https://arxiv.org/abs/1912.01603) - Original Dreamer
- [Mastering Atari with Discrete World Models (Hafner et al., 2020)](https://arxiv.org/abs/2010.02193) - DreamerV2
- [Mastering Diverse Domains through World Models (Hafner et al., 2023)](https://arxiv.org/abs/2301.04104) - DreamerV3 preprint
- [Mastering Diverse Control Tasks through World Models (Hafner et al., 2025)](https://www.nature.com/articles/s41586-025-08744-2) - DreamerV3 Nature paper

**Code & Projects**:
- [World Models Interactive Paper](https://worldmodels.github.io/) - Ha & Schmidhuber official site
- [DreamerV3 Project Page](https://danijar.com/project/dreamerv3/) - Hafner's site with demos
- [DreamerV3 GitHub](https://github.com/danijar/dreamerv3) - Official implementation

**Web Research** (accessed 2025-11-23):
- Google Search: "world models deep learning Ha Schmidhuber"
- Google Search: "model-based RL world models Dreamer Hafner"
- Google Search: "world models affordances active inference planning"

**Additional References**:
- Inference of Affordances and Active Motor Control (Scholz et al., 2022) - Frontiers in Neurorobotics
- Planning and Navigation as Active Inference (Kaplan & Friston, 2018) - Biological Cybernetics
- World Model Learning and Inference (Friston et al., 2021) - Neural Networks

---

**Lines**: ~760
**ML-HEAVY**: âœ“ PyTorch implementations, architectural details, performance benchmarks
**TRAIN STATION**: âœ“ World model = active inference = affordance detection
**ARR-COC**: âœ“ Dialogue world models for future-oriented relevance (10%)
