---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=cAkMcPfY_Ns&list=PLDbgk_jPPZaDvtbw9USUw91AV85y0xX2S&index=8&pp=iAQB"
exportedBy: "Kortex"
exportDate: "2025-10-28T19:01:51.648Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=cAkMcPfY_Ns&list=PLDbgk_jPPZaDvtbw9USUw91AV85y0xX2S&index=8&pp=iAQB

84db6d95-9f6e-4722-8bce-d2606e71f5ee

2025-10-28 https://www.youtube.com/watch?v=cAkMcPfY_Ns&list=PLDbgk_jPPZaDvtbw9USUw91AV85y0xX2S&index=8&pp=iAQB

48aca99d-346f-4e48-b7cf-24e5cfb2d071

https://www.youtube.com/watch?v=cAkMcPfY_Ns&list=PLDbgk_jPPZaDvtbw9USUw91AV85y0xX2S&index=8&pp=iAQB

cAkMcPfY_Ns

this is a neural network and today I'm going to build it from scratch that means no machine learning libraries no pre-built Frameworks only numpy Python and some mats we're going to go from Individual neurons to more complex networks and finally I'll test if my creation can learn the numbers and if it has a fashion sense let's start by coding a single neuron imagine a neuron that has three inputs and one output and this output is the weighted sum of the inputs wait wait wait what do you mean by weighted sum well each of these connections have as a weight and to get the output we multiply them by their inputs and add a bias we sum it all up and that's the output now how do neural networks learn well by tweaking the weights and the bias to get the output we want but we'll get to that okay that's all good and fun but a single neuron is pretty boring how does a network with more neurons look like well it looks like this yeah I also shove my pants the first time I saw this but if you actually visualize this it looks like this all the neuron neur are interconnected with each other all these connections mean that the output value of this neuron is calculated as the weighted sum of all the previous ones and the same thing happens on the next layer and so on and so on until you get to the final output and you might be asking well if we have to calculate all these little connections we're going to be here all day and you're right that's why we have linear algebra basically many many years ago some of these guys invented the dot product which converts all this mumbo jumbo into to this one python line pretty convenient huh okay let's code this real quick and boom we have some output values let's go and this is a very pretty good you can definitely train it and use it for some cool stuff but if you excuse me I'll have a 20 second math explanation of why this isn't good enough ready all right start the clock so now we have all these neurons that multiply each other to produce a value but if you really really really think about it this is just a linear function with a bunch of parameters we don't want that cuz it's no better than linear regression we want to introduce some nonlinear and we do that with reu which stands for rectified linear unit it looks like this and if we add to the network it will be better at understanding nonlinear data just trust me on this one but are you sure you know what you're doing just trust me finally we slap a softmax activation function this is one of those functions that looks really scary but it just converts a bunch of weird numbers that the network outputs into a probability distribution which is just a scary way of saying that it tells you what the network thinks is the likelihood of each class being the correct one and as you see we're getting a pretty random probability distribution which makes sense cuz the weights and biases are all random and here's when I had a 500 AQ moment you you'll see I'm a genius what I did is I trained this exact same network on pytorch got the correct weights pop them into our Network and boom we got 97.53% accuracy all right that's it video's over nah just kidding now we have implemented all the forward passs from scratch and We Know It Works cuz we tested it with drain weights now comes the difficult part teaching the network to actually learn but before we actually do that we need a way to calculate how wrong the network is for that we use cross-categorical entropy law which is more maths that I can't really be bothered to explain right now all you got to know is that it calculates how wrong our prediction was that's it plain and simple okay little recap we input an image or whatever the network does its networking with relu and linear layers we get an output we calculate how wrong the NW work was and now we need a way to update the weights and biases so that the model actually learns something this is what's called back propagation spoilers it's more math but we're going to pretend it's not and explain it in simpler terms now we know how wrong our model is but we can also calculate how much each weight contributes to the output and this is done through partial derivatives hey I said no maths dude get this get this out of here come on go away go away okay let's break it down imagine our new Network as a team of chefs working to create a dish each Chef is responsible for a specific ingredient and the taste of the dish the output depends on how well each ingredient is balanced if the dish tast is off we need to figure out which chef added too much or too little of their ingredient and here's where back propagation comes in first we need to taste the actual dish in our case we calculate the loss or the error using the cross categorical entropy loss function this tells us how off is the Taste from what we want now we need to adjust the amounts each Chef is adding to get closer to the perfect taste to do this we go backwards through the network and figure out how much each weight that Chef's contributions need to change to reduce the loss it's like tasting the dish realizing it's too salty and telling the chef responsible for the salt to add less next time and finally once we know how much each Chef contributed to the final dish we tweak their ingredient amounts their weights based on the feedback they get until the dish taste this just right so forward pass calculate the loss backwards pass update the weights forward pass calculate the loss backwards pass update weights and if we do this enough times eventually the networks just gets better okay so implementing this took me way longer than the forward pass but I'm going to pretend it was really easy and it only took me half an hour instead of 5 hours and here's something I didn't tell you the amount the network changes its parameters is called The Learning rate so the higher this number is the more rapidly the network is going to change its parameters well why don't we put it super high so it learns super fast well if we do that then the network is going to stumble all over the place we don't want that we want to take big steps at first and then smaller and smaller steps once we get closer to the solution and this is where optimizers come in they bury the learning rate so that the networks learn fast at first and then slower and slower you can also do a bunch of fancy with them like implementing momentum adaptative gradients root mean Square propagation we don't really need that we're happy with our little SGD Optimizer so after I implemented back propagation my network was not learning like at all it was misclassifying a bunch of data and it was basically completely useless so I spent 3 hours debugging it only to realized that I had made a mistake in this one light of code instead of my neural network. backwards why it should be my neural network backwards output I love it when that happens and after fixing that I also started getting some weird looking plots yeah I know is this abstract art or what it looks kind of cool so I decided to turn it into a t-shirt go buy it at ww. Weir plot t-shirt.com we only have 10,000 in stock so go buy before somebody else us just kidding just kidding not doing merch just yet anyway I got this weird plot when training on some sample data and just for reference it's supposed to look like this this sent me into a 4-Hour Rabbit Hole of trying different learning rates initializing the weights differently and I even started second guessing my beautiful forward pass code and after fixing a bunch of stuff bug after bug after bug I finally got this it's not perfect but it's definitely less weird and with my new fun confidence on my code I decided to tackle the amness data set I have already done a video about this but the amness data set is a collection of 60,000 28x 28 pixels of hand rated digits and with it you can teach a neural net to recognize a 100r number like if I draw this a Dr quick and pass it through the network boom boom it notes it's an eight this might look kind of simple but honestly I think it's pretty cool at first after training my neon net it had horrible accuracy I'm talking 40 50% but after tweaking some parameters and implementing mini batches I got up to 9742 accuracy on the test set pretty sick right so let's test it out okay let's see what he thinks of this okay he predicted a nine nice okay let's try a different nine wow he's 99.99% sure that this is a nine low key it's getting kind of cocky so what about this five again 99.99% confidence that this is a five okay let's find a more challenging one wow still 98.88% sure even though it looks like a 2-year-old dud this so I wrote some code to see what the model got wrong and I got to say it's pretty understandable I mean come on who who did this now some people say that training on normal amness is too easy that's too easy that's why they created fashion amnest it's the same concept as amnest there are 10 classes and 60,000 images but instead of numbers boom we have pants we have sneakers backs Uncle boots let's stain it on this data set and see if our neural network has some fashion sense okay training training training and boom 87% accuracy not bad huh not going to lie I feel like it could do better but I literally copy pasted the same neural net code from amnest and got 87% so I'm happy with that let's see what this puppy can do okay let's see what he thinks of this bag nice what about dpan what about this jumper now this guy's good and again the stuff it got wrong was pretty understandable and there you have it from Individual neurons layers of neurons to actually doing some pretty cool stuff hope you enjoyed this video And subscribe I got some cool stuff coming in the next couple of weeks see you

