---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=0PGRqDy9C04"
exportedBy: "Kortex"
exportDate: "2025-10-28T19:01:48.858Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=0PGRqDy9C04

30070f05-fec7-4081-a6ea-5023ab84c43b

2025-10-28 https://www.youtube.com/watch?v=0PGRqDy9C04

77feec94-0aa3-4fe1-93c1-bf58acc377df

https://www.youtube.com/watch?v=0PGRqDy9C04

0PGRqDy9C04

## LearnOpenCV

welcome to our podcast artificial intelligence papers and concepts I am Jack Malik your AI generated host and with me is my AI generated co-host Sophia Lane This podcast is based on material curated by Dr Satya Malik Today we're diving into something that's really stirred things up in the AI community Andre Carpathy's nanohat project It's open source and well the promise is pretty bold right it's basically a road map for building your own chat GPT like model but like minimally full stack start to finish Exactly So our mission here is to figure out how Carpathy pulled this off making it so efficient and why everyone's calling it this great learning tool for you know people wanting to build AI Yeah And what's cool is it's not just code It's the whole process He calls it a minimal from scratch full stack training inference pipeline of a simple chat GPT clone The keywords there are minimal and clean Dependency light hackable sounds ideal if you actually want to understand what's going on under the hood Precisely If you want to see how everything connects from handling the raw text all the way to like the chat window you type into this is kind of the blueprint Okay let's hit the big headline first Accessibility We always hear LLMs cost millions need huge server farms but Carpathy talks about a speedrun How cheap how fast are we talking here it's honestly pretty remarkable He literally made a script called speedrun.sa You run it and assuming you've got access to uh a decent GPU setup The example uses an 8 by100 node you can go from zero to a working conversational AI in about 4 hours Four hours Seriously And the cost That's the kicker Around $100 in cloud compute costs for that whole run Maybe a bit more depending on pricing but yeah 100 bucks Wow Okay Compared to the millions we usually hear about that's well it's a different league entirely A game changer for learning Definitely It just lowers the barrier so much But let's be real for a second $100 4 hours Yeah What kind of model do you actually get the source material itself mentions things like naivity and hallucinations right yeah And Carpathy is very upfront about that This isn't going to beat GBT4 Not even close It can do basic chat answer simple questions maybe write a little poem if you ask nicely but it acts like a kindergarten child as they put it Sort of Yeah it can get things wrong make stuff up It doesn't have that deep world knowledge or reasoning power yet Its value isn't peak performance It's the educational transparency So the point isn't the final model's genius but understanding the whole journey to a model Exactly It demystifies the process You see the entire stack tokenizer training scripts the inference engine even a basic web UI all working together And it's all done in roughly 8,000 lines of pretty clean Python and Rust code It shows you the mechanics right understanding the plumbing even if the water pressure isn't super high yet Yeah Okay that makes sense Let's unpack that pipeline then Five stages going from just raw text to a chatbot Where does stage one kick off it starts uh efficiently with tokenization To make that 4-hour speedrun feasible they need something fast So Carpathy built a custom tokenizer in Rust Rust not Python right it's a bite pair encoding tokenizer BPE with a decent vocabulary size 65,536 tokens Using REST makes it GPU fast way faster than standard Python libraries for this step which really helps speed up getting the data ready for training Ah so optimization starts right at the beginning with how you chop up the text You got it Speed matters from step one Interestingly though while they use this custom Rust tokenizer for the training part for the inference part when you're actually chatting with the model they often switch back to OpenAI's standard tick token library So it's like bespoke speed for training proven reliability for chatting Clever Okay so text is tokenized Stage two that's pre-training or base training There's a script script space train py This is where the model just reads a ton of text Think massive web scrapes like the fine web edu data set they mentioned All shuffled up And the goal here is just raw language learning Yep Learn grammar facts how sentences work general world knowledge Basically learn what language looks like Okay So it knows language but how does it learn to converse to do the back and forth thing that's stage three mid-training Another script scripts midtrain.p Here they shift gears They start feeding it conversational data They mention small talk as an example And crucially they introduce special chat tokens like user and assistant Exactly those And also stop tokens like assistant end These teach the model the structure of a conversation Who's talking when to respond and importantly when to stop talking so it doesn't just ramble on forever I remember reading about a little hiccup here Something about the learning rate schedule in this mid-training script A bug they found Oh yeah that's a good point There was an early gotcha with the learning rate decay It highlights that even in these minimal systems tiny details in the training setup can really matter Getting that fixed was key for making the conversational fine-tuning actually work well Shows how sensitive these things are Okay so pre-training mid-training What's next stage four is supervised fine-tuning or SFT You take the conversationally aware model and polish it further using highquality curated examples Think question and answer pairs This really helps align his behavior to be more helpful and follow instructions better And there's an optional stage after that RL right optional reinforcement learning They use a method called GRPO which stands for generalized policy optimization This isn't always needed but they found it was really effective for boosting specific skills particularly math problem solving like getting better scores on the GSM 8K benchmark Why RL specifically for math couldn't SF handle that sft is good for style and factual recall from examples but complex reasoning like multi-step math problems benefits from the model getting feedback on the final answer RL methods like GRPO let the model try things out and get rewarded for correct solutions which helps it learn those step-by-step reasoning paths more effectively than just seeing examples Okay that makes sense So SFT for general alignment RL for targeted skill boosts And the final stage stage five is just inference running the model you've trained either using a command line tool or the simple web interface they provide which is script chat webpy And that's the whole sequence start to finish Got it Tokenize pre-train mid-train SFT maybe RL then infer Okay Now for the really juicy part for tinkerers Let's look inside the machine What are the core code files defining the actual model architecture right if you want to mess with the code there are three main files in the nanohat directory you absolutely need to know Okay first is nanohatgpd.py This is the transformer model itself but it's surprisingly concise and it uses modern high performance components even though it's minimal like what things like RMS norm instead of layer norm for stability rotary positional embeddings which helps the model handle sequence length better and efficient attention mechanisms like multi-query or grouped query attention MQA or GQA Hold on GQA isn't that usually for massive models to save memory during inference Why put it in a nano chat that's actually a really smart choice GQA drastically cuts down the size of the key value tach needed during inference This means faster generation and lower memory use even for a smaller model It helps make the $100 speedrun actually produce something reasonably quick and usable not just theoretically possible So the advanced features actually enable the minimalism to be practical Exactly Oh and one other slightly unusual thing in GPT.I is the activation function ROU literally just F reloox.square simple effective nonlinearity Interesting Okay so GPT.py is the model brain What handles making it actually talk that's nano chat engine.pi This implements how the model generates text and it does it like a production system would a two-phase process There's prefill where it processes your initial prompt really fast Then there's decode where it generates the response token by token streaming it out And that decoding part needs to be efficient right yeah absolutely The key here is KV caching engine stores the calculated keys and values from previous tokens so it doesn't have to recmp compute them every single time it generates the next token That's essential for low latency streaming Makes sense Anything else special in the engine yeah one really cool thing tool use The engine is designed to watch for special tokens If the model outputs say Python start the engine pauses generation calls an actual Python interpreter in a safe sandbox to run the code the model generated gets the result and then lets the model continue It lets the model use calculation tools Wow Okay So it can actually do things not just talk about them That's pretty sophisticated for a minimal setup And the third key file that would be config.py This is basically your control panel for experimentation Want a deeper model change end layer more attention heads adjust head bigger embeddings tweak nemed It lets you scale the architecture up or down easily without digging deep into the GPT Py code Nice Okay Let's circle back to that simplicity idea Yeah You mentioned they deliberately avoided using a padding token Why is that such a big deal for simplifying things ah padding It's a common headache in LLM training See you usually train on batches of sequences but sequences have different lengths to make them fit neatly into a batch You often add padding tokens to the shorter ones right just fill up the space Yeah But then you need complex attention masks to tell the model "Hey ignore these fake padding tokens." This adds code complexity and can actually waste computation on the GPU because you're processing tokens that don't mean anything So Nano doesn't do that exactly It avoids padding entirely It relies on careful batching attention masks that only cover the real data and those special tokens like assistant tend to signal clearly where generation should stop It strips out that whole layer of complexity That's a neat trick It sounds like the whole thing is really designed to be easy to modify and scale Then if I wanted to take this and say train it on my company's internal documents should be relatively straightforward Scaling the model size is mostly about changing that depth parameter in the config and maybe adjusting the device batch size if you hit memory limits The scripts handle the gradient accumulation automatically to compensate And the data pipeline itself is it locked into specific formats nope That's another area they kept simple The nanohat dataet.py PI file intentionally avoids heavy complex data set libraries It's designed to be simple to understand and modify If you have your custom data maybe as parquet files like they use swapping it in should be much easier than with some bigger frameworks Less abstraction more clarity I like it Uh what about reproducibility that's always a challenge in AI How do you know if your changes actually improved things they thought about that too There's a nano chat report Script After a training run finishes it automatically spits out a report.md file Report card Basically pretty much it logs everything Your environment details the exact hyperparameters you used key performance metrics like core score MMLU accuracy that GSMAK math score It makes comparing different runs much more reliable because all the context is saved automatically That's really valuable Okay so wrapping this up Nanohat sounds less like a competitor to giant models and more like an incredibly clear functional and affordable teaching tool I think that's exactly it It success isn't about raw power but about providing that complete understandable end-to-end stack And maybe just maybe it poses a bit of a challenge to people building much bigger systems How so well it makes you ask all those incredibly complex optimizations and exotic techniques people add to giant models are they always worth the trade-off do you lose too much clarity too much of that hackability that Nanohat really champions sometimes being able to easily see and change every gear in the machine might be the most valuable feature you can have It's definitely something to think about Thank you so much for listening This podcast was sponsored by Bigvision.ai AI a consulting and product development company that helps companies of all sizes build computer vision and AI solutions You can reach them at contact at bigvision.ai See you our next episode

