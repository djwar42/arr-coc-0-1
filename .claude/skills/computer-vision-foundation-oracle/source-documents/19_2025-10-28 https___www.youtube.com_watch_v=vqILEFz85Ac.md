---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=vqILEFz85Ac"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:04.577Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=vqILEFz85Ac

dbb37444-4b77-457c-ba91-1a47d3aa146f

2025-10-28 https://www.youtube.com/watch?v=vqILEFz85Ac

1d28a48b-17ed-431a-83e4-b0fbd654ef22

https://www.youtube.com/watch?v=vqILEFz85Ac

vqILEFz85Ac

## ComputerVisionFoundation Videos

good morning everyone thanks for being the the brave ones to be here at 9:00 a.m. doesn't happen very frequently or you're so jetti the other way around that you haven't gone this sleep yet like me um okay so what we are going to do is I'll start with an introduction can we increase the volume sensitivity on the mic somebody said volume um volume of what the microphone yeah maybe just change it something like that is it better to see using actually yes epon this speaker I don't know mess if the volume's lot right now with it okay so I will um so people in Zoom if there there was a problem is it am I audible now or do I still should position myself better someone Zoom it's okay no worries okay no worries okay in the meantime maybe we can figure out how this thing works too okay um so what we're going to do is um I'll first give a little bit of a a little bit of a more technical um introduction and then we'll go to a schedule so you have an idea what other talks are coming up and then we'll take that and so uh welcome to the tutorial so I'm going to start with this little motivational video so here is a fish you might have seen this video before but it's always fun there's a fish that is appears to be swimming and what's special about it is that the fish is actually dead so there's a string that is connected from the tip of the fish to the front and the water is just streaming and what you're seeing is actually the interaction between a body of a dead fish and the water around it which appears to be actually gracefully swimming so it's dead but it's still clearly doing something right so it brings up the question that actually um um is the fish intelligent yes no and I mean it's dead right I mean the brain is not working how could it be smart but again it's like clearly doing something right so in that sense it's exhibiting some intelligence but the question is that where is the intelligence like where is it housed um so uh again like I said the brain is dead so if I get the fish and you know cut into two pieces and one piece would be the brain and the other would be the body now generally when we in AI in machine learning when we are thinking of like developing intelligence we primarily working on what looks like the brain right the cognitive function the neural network that does control perception and so on so forth but in this case the brain is dead so if everything is happening is coming from the other side so um so exhibiting intelligence can come from the non-brain part of an agent to so I can think of it as this agent has some specification this is specifications by some parameters some of these parameters are in the brain like a lot of it but also some of the other parameters that makes this is agent Special is under right so there is basically a bunch of parameters that Define this complete intelligent agent and both of them are parameters but they are characteristically a bit different the one that is on the left is flexible and fast in learning like our brain it sort of keeps receiving feedback every second from the environment and updates and so on the thiger on the right are more rigid and more slow like the body doesn't adapt too much you know after birth there some things Happ but it doesn't move that fast so it's the rigid part is a slow part but still there are some parameters still to some extent do adaptation happen so both parts like I said contribute to exhibiting intelligence in the in the end of the day so this event generally talks about this part of the graph the right part like but what we call basically the morphology of the agents okay so now morphology is not similar to the example that I provided before which is is more like the mechanical the physical morphology of the agent but also perceptual morphology we can see it in the sensory system for example anatomically the shape of the eye the you know positioning of the eye a bunch of parameters that I'll go through later so this is also also part of the morphology if you embedded in the previous chart that I showed there's a slow changing parameters and there's fast changing parameters and this is very basically diverse if you look at is that there's a lot of I mean in the physical body of course there's a large diversity but also perceptually as we go forward you'll see it is that these diversities like why are they coming from they're a result of basically specialization so they're different you can imagine these animals live in kind of different conditions different ecological conditions underwater above water flowing and so on so forth and they have different needs so the ecology is basically tailored to to each of it and that's where the diversity comes from and this is basically people like Gibson what they said they they argued for the role of ecology in vision for example that uh the properties of a perceptual system is a reflection of the environment that is around us so the environment is specific it has certain properties and patterns and biases and that kind of gets reflected in our perceptual system too and we are saying it more broadly also that gets reflected in the morphology of the agent all these parameters in general and why is it so diverse like I said is that um you can think of it as you know there's a there's like little islands of ecology you know on water above water like a cockroach that lives in the pipes versus a bird that flies in the sky um there are these little islands of ecologies that they niches they don't have to you don't have to Traverse them and therefore that provides an opportunity to specialize towards each of these and that's why we have the diversity now at least agent at least nature suggests that we don't really have to have one agent that works for everything and indeed we might not even want to be able to go there if we can um we can use anecdotally like a no Freel launch theorem from learning the is that if you try to have be so General that you win everywhere that means that nowhere you're going to win nowhere you're going to be very efficient so if you want to have like one agent that satisfy all different parts of like tasks and environment and and so on so forth we are not going to eventually be able to have the agent in an efficient way so we'll basically see that how designing morphology how specialization in various forms actually brings efficiency in the end of the day just like the fish was very efficient in the sense that the brain do had to stop working but it still is doing its job right but obviously if you bring it you know turn it into a terrestrial animal drag it on the on the ground it's not going to work the same way so it's being in the right Niche environment that leads to that um efficiency so that leads to us wanting to design agents that fit and get tailored to the environment and tasks and needs and so on around them how do we do it this to the so-call like automated design method so there's this example classic example from Carl Sims evolving virtual creatures from 94 so 30 years ago now that it basically was an automated system that through an evolutionary algorithm it learns it basically assembles a number of like primitive Parts in order to satisfy Downstream uh utility in this case it was primarily Mobility oriented so you can see that you know it obviously satisfies the property of uh the objective of mobility of course sometimes it leads to weird answers because it's only satisfying Mobility it doesn't for example minimize energy consumption or contact and M muscle and tissue damage and so on so forth but it is an example of computational design in the sense that these shapes that are solving the problem are entirely emergent as a result of an algorithm interacting with the environment nobody has engineered them more than engineering what you see on the left in terms of primitive parts and so on so um so that was 30 years ago uh things have gone forward in the meantime now we have things that are a lot more effective um um mostly due to advances and Computing a lot more computes or a lot more parameters to optimize um new optimization a lot of new mathematical models and simulators that we have that we will see later that how it is used actually in the design and also machine learning has come to rescue where we don't actually have these uh models so uh I think it's useful to have a little bit of a structure of how the automatic design methods work and I would actually approach it this way is that let's say there two sets of parameters um two spaces one set is the design parameter space and another one is down a stream goal that the design parameter you can think of it is let's say we want to design the mechanical body of a robot so we parameterize it with where the boxels are and some physical properties of them for example elasticity and so on and the other space is the utility that we looking for for example running faster so we want to design this left part in a way that the agent on the right after a controller is applied to it it actually runs faster so the way it works is that we typically have a model that relates this space the left space to the right space and um we use this model as a result of this linkage is that we can now use classic optimization ideas in order to set the goal here maximize it and get the get the arc Max of the left so now useful categorization is one way of it to look at is that where these models come from so one category of methods that we'll see today is um they have physics based for example knowledge they have physics based knowledge about the certain domain for example Newtonian mechanics that turns into a compact mathematical form so we will use this as a model to relate this thing to that thing and we'll use more standard optimization techniques for it so that's a physics base for example view but sometimes we don't have these models we don't really know how to link these two to each other and that's as usual whenever we don't have a model we approach it with datadriven learning and we learn a model so for example Vision mostly is in in the learn learning base the perceptual morphology is in the learning base because let's say we have some you know property of the eye or you know parameter revision system and on the right we want to have perceptual perceptual behavior that are competent and so on this thing goes through basically cognition eyes some see something goes through the brain and then Behavior comes out we don't really have a compact mathematical model of it so this thing doing perceptual design usually ends up learning ano model through dat learning and then use it as um a way to do the optimization so we'll see it in UNR do for example that everything that happens perceptually is through this process and also it's useful to um remember that this General methodology is similar to other fields not just computational design in the context of Robotics and vision for example X Discovery like material Discovery drug Discovery and so on so forth so they basically do the same thing they find a model that relate the input space to Output space this is the output spaces where they care about for example the input could be you know various chemical compositions compounds of different uh compositions different compounds and on the right is that what kind of impact that can have on the body of a patient who takes it and um so they learn a model for example is that they have a data set of known compounds and known effects and they learn a model and then use that model to sort of go back and search in the original space in order to find this time pose some new compound in order to maximize some certain effect like maximally reduced fever and so on so forth and they go through clinical trials and so on to see whether that is actually correct or not but that's generally one of the main ways that the XC Discovery pipeline works and you know game play to like Alpha go you learn a model of actions in the space of the game so actions towards utility and then you use that basically to search and plan and so on but it involves learning a model and doing some searching and planning on top of it so in this this of the you know related problems one of the talks will be from neural concept which is interesting because a uh it it solves for example aerodynamic problem on designing airplanes and cars and so on but it's a it's an industry solution they're actually selling it to various companies last time I heard they actually had a uh contract with a Ros uh if I remember correctly we can ask here so it's interesting to see one that actually works and is being sold and it's going to be the last Talk of the day right before lunch so in terms of U Rel fields and one last thing that I want to say here is that is it is it actually Worth or when is it worth to to do the computational design rather than man design so um it is worth under the following conditions like when we are not good at understanding like what matters basically then we anthrop anthropomorph too much so an example here is the you know zebra and Aon so zebras have these you know distinctive patterns right why do they have them any guess yeah try to avoid they stand in the yeah so yeah you're very sophisticated for exactly um so you have both answers I want to cover you know so usually people go say oh it's a camouflage mechanism right right so the lion you know when they it you know they blend in the background like zebras and so on because of course like deer and so on it's a very common uh the the patterns of a camouflage or when they stick together as a hert they look bigger than what they are so there just you know it's not actually the case because we are basically anthropomorphizing too much we're looking at it too much like a human the Predators lions and so on they don't have that high resolution of a vision compared to humans um so from a distance especially when they look at it they don't resolve the patterns actually so the a donkey and a zebra are indistinguishable from each other to line um so that is not the reason the reason is exactly like the mosquitoes so mosquitoes are as big of a problem like a death threat to zebras and so on compared to Lions so blood sucking mosquitoes um and actually if you there are farmers then paint their cows and so on with zebra patterns as a like mosquito repellent mechanism and it works so well yeah we figured it out but if you wanted to design it with that world model that we had is that all this should be camouflage and so on we would be going wrong and generally domains that get increasingly unintuitive they become increasingly harder to design them um intuitively and I mean this is not just a scientific thing also from an engineering perspective they're plenty of perceptual systems that we have they don't look like a robot that live in our household but instead they're basically you know something called pill cam is a pill that you take but it's actually a camera and as it goes down it records um a video and it's used for diagnosis and so on and ultimately they want to make them active instead of being just passive that is being swollen so if they get get a stuck they can you know do a little bit of vibration to unstuck themselves or eventually they want to make them much smaller to be able to inject it in the blood stream so now imagine these perceptual domains like how does it look like it's very unintuitive to us to be able to even design them in the first place so competitional design has the potential to be at least less biased in these cases okay so now in schedule so there are two parts the morning is primarily about vision and afternoon is is primarily about um robotics the mechanical the physical morphology um so first I'll finish off with uh a few more like case studies of different parameters that contribute to the perceptual morphology then Andre will talk about an actual computer Vision case um not you know just motivations but a project in which the several of the parameters the morphology of an agent is designed computation and it does work and it has somewhat like interesting and surprising answers such as you know you can use very low resolution cameras very efficient when solve problems that would be hard to initially think they can be solved the same way and um so there's generally little work in the space of Designing perceptual morphology there's a lot more in the space of Robotics so Andy is the expert of the robotics so he'll give you a crash course um on that but in the morning we're going to go through a few different phases and but just you know just the state of the field that there hasn't as much work on on perceptual morphologies so then we'll have sonki uh Soni is uh we are very happy to have him because he's a real biologist not me that I'm just you know shadowing being a biologist here while being a computer vision you know going through biological cases so um the diverse cases studies in in the biology there always basically motivations for what we are discussing today there's always diversity there always adaptation to neighboring environment and therefore it makes sense for us to ultimately design our robotic agents and perceptual agents that day like bring the body into it and design the body rather than the the sort of disembodied llm thing that only lives on the on the internet that plays a role too but ultimately when it comes to cyber physical system this Body Matters and biology is examples of that plenty of it so he will give us a um a set of cases from an environment that is somewhat alien to us like deep dark in the ocean deep down and you'll see that actually how this again this space really accommodates new forms of uh morphologies and uh like I said Andy will talk about the mechanical part and then comes the neural concept one the the industrial one so is there any question up until disc started introduction this would be good point before I move forward any and please ping me if there's something on Zoom too in case okay so um it's useful to look at kind of more cases of um specialization when it comes to Vision only I'm going to abandon the uh the mechanical morphology for now is that how much diversity actually exists really because again if you look at you know if we look at it as humans it might look like the cat's eyes and dog eyes and human eyes and on they kind of look kind of the same and this is not just about the eye the entire you know perceptual system but we use eyes because there are anatomical examples that we can directly inspect so is there is there really a lot of diversity the answer is yes at least compared to our computer vision system so in computer vision pipelines we typically go with like an ordinary camera like a lens based camera with CCD or seos many megapixels and so on so forth and at least compared to this pipeline that we have nature is a lot a lot more more diverse and specialized and think of that no free lunch theorem kind of the story that I said in the beginning is that it's it's really this is not this is not bringing us that efficiency is not benefiting from those things that we discussed so let's go through a few cases a few of these parameters of design that are common at least known so far and it's more than that because really when we do end end learning we can design all of the parameters many parameters that might not be exposed and obvious and discreet in this sense but um at least these things exist like a field of view the first one like the most common you know intuitive one is that placement of the eye for example Predator animals versus prey Predators usually have the uh prise they usually have uh eyes on the side because that gives them a wider field of view because they can detect the Predator coming from any direction and that's very useful for survival on the other hand Predators they usually have they're more likely to have eyes that are frontal so they have overlapping field of view and that gives them a sense of depth and that's better for targeting and so on and give up that high field The View they have plenty of blind spot but because they're not at risk of being prayed then they don't care about it and um so this pattern is relatively common you can look at a birds for example Hogs versus dogs and and so so one pattern um another pattern that is similar is pupil shape so um you have seen cats the cats have these vertical pupes but um it's an ambush predator and then there's the goats and sheeps and you know those things that have these horizontal PS and the reason is very similar so these horizontal P this sort of opening it gives slightly slightly higher field of view which makes sense because compared to the Predators these things have to watch for you know things coming from different directions more so this pattern is common all of these patterns have violations too because none of them is the singular rule that applies but they are one of the many rules so generally you know you can you can pretty clearly see two moles that form to them uh sometimes the adaptation is degradation and in this case I would actually call it like simplification like as a good thing so for example there's this um fish and what's distinct about it is that it doesn't doesn't have eyes um so what happened is that there's a type of fish that was on the surface but migrated to the cave and what's in the cave there's not much light it's kind of too dark so vision is not really that useful so it kind of evolved to get rid of this uneconomical uh toy that it has Vision generally uses a lot of metabolic energy so if you don't use it you better get rid of it um so they it's not like they didn't have eyes in the beginning they had eyes they migrated to the to the uh to the cave and they lost the eyes evolutionary um oh yeah this is their surface cousin that is very similar except that this one migrated this one didn't and then they branched off in terms of I um another example common thing is Darkness adaptation um you know animals live in the dark there are plenty of nocturnal ones and there are form of adaptations and specializations that happen that comes with its own tax but it's useful so ters like one obvious trick is that if you want to adapt to the dark what do you do like just make your eyes bigger so they have a bigger aperture bigger opening to collect more light so you see better in the dark so that's a common trick like ters are like that they have these huge eyes their eyes are actually as big as their brain this is their eye and this is their brain and compared to humans this is the brain for orally and this is our eye so you can see that how much how drastic this adaptation in terms of size is there are other forms of adaptation too for example reflective coding coding so you have seen deer and cats and so on so they have these um you know when you shine light into their eyes like with the torch or when you're driving their eyes are reflected right that is a Darkness adaptation mechanism because uh when light lands on your retina your retina doesn't absorb all of it it misses some of it the efficiency is not 100% so it kind of some of it passes through but they when when when it's so dark you need to recycle it you cannot afford losing it so there's this reflective coding that acts like a mirror then reflects some of it back so it can recycle some of the photons that have gone through and so that's another uh Darkness adaptation or sweat bees basically integrate the uh photo receptors to go integrate an area when there's not enough light to distribute among them to basically this swap resolution for uh sensitivity to light so they integrate five of them they use it as one photo receptor as if they were bigger in the first place another case is number of phobias um so phobia is the high resolution area in the vision humans have one is roughly like um as wide as your thumb and it's extended the arm length so that's where you see high resolution everything else around it is blurry and we have one of it so phobia is basically a pit on your retina which is has the high density of photo receptors and then there are other animals that have multiple of these for example and Falcons they have two of them so they can basically they see two points in high resolution simultaneously supposedly because again they do something that they benefit from it that leads to sort of spiral patterns of flying and a whole bunch of behavior too and our thing that is obvious is the placement of the eye so we saw you know how the placement can lead to a higher field of view but then there's you know in terms of Predator prey but even for animals that are not you know even are not terrestrial uh still the same thing play of the eye seems to be an important parameter to optimize so the clams um these are clams the clam C clams that they have uh a lot of eyes somewhere between two to 300 eyes these little blue ones are are eyes so they're distributed around the shells upper and lower so they have a particular placement and so I basically decided that instead of packing them and create the dense great and create a high resolution image just like for example the way our eyes work they decided that it needs to go with a distributed solution and resolution also is one of the interesting ones because um humans have one of the highest resolution I we don't have the highest resolution uh Falcons they have higher resolution eyes that high resolution R than us it makes sense because they fly very high they need to detect like small like animals to to um to cat them but humans generally we have very high resolution eyes and we are generally um proportionally we are visual agents compared to other animals so we are skewed towards using Vision a lot our Nos and so on are not as useful that for example dogs and cats and many animals that are to a large part like chemical sensing like smell a lot so what's interesting is that there are a lot of animals that have poor vision but in terms of resolution but they're very perceptually cap behaviorally capable so like if you look at you know the human fine but once you get to the rat and fly in mosquito is just really hard to believe that's all they see but if you look at their behavior um it's hard to believe that you know the the agile motions and everything that they have it's coming from such low resolution ice and this is the same thing for butterfly case but this time shown over distance too so this is a this is a map butterfly this is a map butterfly seees another map butterfly only from 10 cm away and this is 1 m and this 2 m almost like unresolvable and um yeah so it's a lot you know this is just one meter away like this much and this is even the bird sees it that way so it's very poor in terms of resolution and it's actually funny because one of the hypothesis again speaking of um you know um it was that why for example butterflies have these patterns earlier hypothesis was that this could be for example a courtship uh behavior for mating but then it turned out they don't even see it like the resolution of the eye is so small that you don't see it so how could it be court so it turns out it's more like for Predators like birds that have a slightly better resolution so they see it but the butterflies themselves don't see it so this is more like to look like a flower that is poisonous to avoid it for for the bird but not for other fles so resolution is one of the important ones too and um we'll see in Andre's talk that if you design it well like some things that seem surprising come out of it is that basically how far can you push it how far can you can you go with a very simple ey in terms of resolution like a one pixel camera or 2 by two camera and it turns out you can do a lot of embodied uh AI task with that which is going to be the next and so I'm going to end it here uh but there's one point worth mentioning is that now that we event through these cases studies and so on it's worth remembering that animals are almost never unimodal they always solve the problems in a multimodal way and so when we say Vision Works this is poor vision is still perceptual Behavior we have to remember that they're never almost entirely relying on their Vision so there's the caveat there so in the end of the day it's always like multimodal but the multimodel aspect also has as much you know perceptual morphology design to do that that Vision does um common example is that you have seen is the snout of the dogs so dogs are very reliant on their old factory system to smell and this thing is a very well-designed um equipment so when as humans when we when we want to smell as we exhale we kind of let molecules out so we stop the smelling it's like inconvenient so the design of their snout is in a way that there these slits on the S side so regardless of whether they're for example inhaling or exhaling or any activity they're always intaking molecules because it creates a low pressure and then it always takes morees so it's a morphology clearly is a morphology design and um makes their tongue is basically a smelling device so they touch this tongue on different um U they sample molecules they bring it in the their sensing organ is on the top of their mouth so they leave it there and why there's two does anyone yes gradient gradient exactly so this is a stereo Vision basically so it samples from two so it can compute Direction so they smell in a spatial way again beautiful designed like and this doesn't also happen only to perceptually weak animals perceptually visually weak animals visually strong animals the same thing happens too so turkey volters usually have uh very good eyes because they fly high and they have to again detect things uh but turns out they actually have very good uh sense of a smell too they can detect a dead body from roughly eight miles away and the funniest story is that there was actually a company that figured out and used uh hercu vultures as like a free surveillance team for or a gas line so you know the gas lines are usually in the middle of nowhere you know there's they extend for thousands of miles it's very hard to detect like a small leakage so what they did they would put like some chemical in the gas that is attractive to vultures and whenever there was a leakage it would you know go in the air and they would basically whenever they see vultures hovering around uh gas pipes there would be an alarm that potentially there's a Le there so the real the story that they actually use these animals that it was debated for a long time whether they have a sense of a smell or not strong sense of smell for that purpose so multimodality is the thing it's not only for the perceptual visually weak animals okay with that I'm going to hand it over to Andre for doing a design with two of the parameters that we discussed like I said there's not much work in designing Vision systems yet so hopefully it's going to come in the future but the two parameters resolution and placement we have one example that we can talk was there any question or so yeah oh the function of it I'm not sure if I have read a compelling story about it um it is just known that there is two because anatomic we can and there is consequences of it for example when they want to land you know when they are hunting when they want to land they go in this a spiral fashion because they can keep one of the phobias focused on the on the Target and they kind of go around it and that because at 45 degrees it kind of leads to these particular flying patterns I don't know if it is exactly known or maybe I don't know exactly why they have it but anatomically they do and they're not rare actually multiple you know flying uh hunting bears they have I think there one more question oh yeah yes is the idea obviously we're trying to learn nature how to optimiz is it really so your question goes into basically how we should parameterize the design space in a way that is manufacturable and has um so for example in three printing uh that thing is incorporated you cannot just imagining a solid 3D shape doesn't mean that you can necessarily print it so in the 3D printing planning algorithms that is included that you have to constrains satisfy additional constraints so the idea here is that like this is all at the research level so really fabrication and Manufacturing hasn't really been part of the problem yet in andies the stuff will be so in in a mechanical morphology with perceptual we're not really there yet but in the end of the day it comes down to kind of augmented the designer space in a way that it comes practical but it goes through the same pipeline it's more like a design space that as additional constraint versus less constraint so the example that I had for you know Vox Souls it's it can 3D print so the problem is not that we shouldn't design because our manufacturing process enables that level of flexibility already and similarly for perception so the examples that Andre will talk about is like placement of the photo receptors there's a sensor called photo diet which is basically a photo receptor so there's no problem actually implementing it as of today the problem is like how to actually find the design that actually Le the work thanks uh so yeah hello everyone uh my name is Andre and I'm going to talk about our recent project on solving Vision task with simple photo receptors instead of cameras and how to design them and I'll try to go deeper into details and specifics uh doing this talk so yeah let's start um in computer vision uh usually when we solve a vision tasks uh for example like a visual navigation we use a pretty standard recipe so we use a high resolution camera and we place it intuitively on an agent and then train a deep Learning System to solve different types of tasks from this high resolution um signal uh and as we saw uh as am discussed in previous talk right in nature we see a wide variety of different designs they vary in like pup shape the type of the lens system resolution light and density and many more and so in this work uh we focus on two specific aspects of this visual sensor design so first we draw the inspiration from various examples of extremely simple eyes so many ANS have U rather impoverished tyes in terms of their resolution yet they able to uh exhibit rather complex behavior and survive in their environment so what we do if we basically as a question uh how far such simple eyes can go in terms of solving some of the uh Vision tasks so and we basically replace the standard camera sensor uh with a simple photo receptor sensor with resolution as low as one pixel which is U less than 1% of the camera resolution um even for such a low resolution camera 128 by 128 and yes here are examples of U uh visual signals coming from a camera and two photo receptors um that are placed on a agent and B in environment so we then consider some standard Activision based task for example a visual navigation task whereas the goal is to navigate through a Target ball here in an unknown environment and we ask whether agents equipped with such simple photo receptors can solve this task reasonably well so second we draw inspiration from warious eyes designs in terms of the field of view like in between predators and prey uh placement and rotation of the body like in the scalop example um and we therefore explore the role of the design uh in the effectiveness of these uh simple sensors and we also develop a computational design optimization method to find well performing designs so let's start with the first question and our first Tas that we consider is a visual navigation so the goal here is to navigate to a Target denoted by the star uh starting from a random position in known environment uh the not with a pin and we use a data set of real uh scans of real buildings uh the M Port R set and we split them into train test and will it only on Noel scenes and we consider two standard inations of this visual navigation task the first soal point goal navigation where the target position um is convey to a robot to an agent while its position in the scene and the main task here the the the agent needs to do is to efficiently like avoid collisions and navigate as fast as possible to the Target and the second sitation is the target navigation where the agent doesn't know the target's position but it's uh specified visually by having a green ball uh in that position and during the episode the agent has accept to a visual signal that comes from either a camera or photo receptor sensors and also as per standard practice for this task we provide the GPS plus Compass sensor which uh provides a d u position and Direction uh of the agent during the episode with respect to the start position so and now how do we actually implement the photo receptor sensor and in our experimentation we use like a simple model that allows us to implement it in any simulator that has access to a rendering engine so for photo receptor placed on agent in simulator uh we first run a corresponding camera view with the same intrinsic and extrem parameters and simply average it specially to get the corresponding photo receptor signal and when we increase the number of sensors uh we can either spawn a different sensor in a different location and repeat the same process or we can arrange them in a some form of a grid sharing the same location uh for like here for example the four Pho receptors arranged in by two grids and then we split the image into patches and average each of them to get the corresponding photo receptor observations and so one can simply think of it as a very low resolution camera of 2 x two in this case yes so here we Show an example of a photo receptor based agent uh executing the target navigation task in a known environment so it has access to 32 photo receptors as we show here on the left and on the right we show the corresponding top down map during the episode so and we also show the camera view just for visualization the agent doesn't see it so we can see that the agent can successfully detect the Target and navigate towards it while having access only to this uh low resolution visual signal so and now we show the quantitative uh evaluation so we show the performance of the photo receptor agent like in Blue uh for diff for varying number of photo receptors the Y AIS shows a performance on the task and the xais shows the number of receptors used and we arrange them um in this case in a g of 4x4 or 8 by8 having either two or four uh sensors and to to contextualize this results we employ two control Bas lines so the first one is a blind agent and this agent doesn't have have access to any visual information and it simply learns about uh the biases and the structure of those scenes from the training data and as visionless system it provides a lower bound on what we would expect a vision based system to achieve so second we employ a camera agent uh which is basically a standard approach to solving these tasks using a high resolution camera sensor and we show the same for the point goal navigation task and basically what we find in both cases is that even a handful number of photo receptors uh with the resolution as low SED to or 64 um uh is enough to solve those task significantly better than the blind agent and having a comparable performance performance of the camera agent and sometimes even outperforming that so not that like this camera here Baseline uh it's actually using a relatively shell Transformer architecture uh that we use for the photo receptor as well so here we also report yes so here we also report the more stronger Baseline with the reset 50 architecture which is more commonly used uh in this U for these tasks and we find even that even compared to this Baseline right the photo receptors uh perform reasonably well they close most of the Gap right between the blind agent that has access to no visual information and the stronger resonance 50 um camera Baseline so here we show some quotative examples this are trajectories of different agents in the point go navigation task uh we show them for blind photo receptor and the camera agents and um yes as before the start position is a pin and agent needs to navigate towards here and we also display the collisions uh with walls and other obstacles uh as the red points so what we find is that pH receptor agent is able to uh utilize its uh low dimensional visual signal rather effectively to avoid collisions and find much uh more um efficient trajectories uh compared to the blind agent and it achieves overall performance similar to that of a camera agent uh which has access to a very high resolution visual input so and similarly we show trajectories for the Target navigation task and since here the goal uh the go the target position is unknown uh here the uh the scene exploration plays an important role to be able to successfully uh detect the Target and navigate towards it in a limited number of steps so here we show multiple trajectories for each uh agent and the darker dark responds to the points early in the episode and lighter ones later during the episode and so what we can quickly see here is that the blind agent for example it doesn't able it isn't able to expose the scene much so all the dark points are just around the start position where the pho receptor can efficiently explore most of the scene and overall achieve success rate uh similar to the camera um agent um and next to see whether the in this target navigation task the photo receptors uh agent can indeed detect the Target and it doesn't like simply BMP into it uh we designed this very simple control experiment so for the same episode with the same start and the end position uh we create two scenes in one scene we have the the same like V ible tar the target is visible and in the second control scene the target is invisible basically there is no target but it needs to go to the same place so and we run the same for the receptor based agent uh in both those scenes and before uh they have the same trajectory before the target is first comes into the field of view of the the agent sensor and after that uh the agent with a visible Target in the is visible Target can successfully navigate towards it uh whereas when the target is invisible it fails to do so uh which basically suggests that uh the the photo receptor agent can indeed use this visual information about the target detected and navigate over it so and finally like even though we use cons of real world in all our of of real world Apartments in our experimentation we also try to see whether those observations are somewhat specific to the simulator or those apartments uh and whether this can be transferred to a real world so we have time to to uh study this with this simple real world experiment so we deploy a pho receptor policy on an Turtle B agent shown here and it needs to navigate to this uh Pink Target ball uh over the over the box so and we equip it with a simple 8 by8 uh sensor uh with 64 photo receptors and here we show the corresponding camera view just for the visual visualization and we train the policy in a simulator and directly deployed on the robot without any real voline tuning so yeah so here we find that it it can uh expose the scene and it can detect detect the target here I guess yeah here it goes it can successfully detect the Target and navigate towards it and this is of course like a successful example and of course it's not 100% successful all the time uh but we definitely see an onal behavior of it exporing the scene uh detecting the Target and navigating towards uh towards it yes so um so yeah uh finally uh we also evaluate the effectiveness of this spot receptor sensor in continuous control Tas from Deep Mind Control suit so we use the reacher Walker and figer agents and we solve this Tas Solly from the vision uh signal so as before we compare the pH receptor agent shown in blue uh with a blind uh shown with u gray and the camera Baseline and as before we see uh that in these cases like just having a few or two or four photo receptors yeah two of phors uh can be enough to solve those tasks uh meaningfully uh compared to the camera agent and this example shows uh like um a walking task solved by photo receptor agent so here we show the design uh each color like this uh arrows correspond to each photo receptor there are four of them and this design was found by computational Design method which we'll describe uh in a few moments so this shows a corresponding observations for each photo receptor so here each row shows the RGB values uh recorded by the photo receptor and this plot shows the the luminant signal is just a reduction of those sgb values into a single number that shows a lightness uh just for realization purpose and this is a reward function it's not seen by the agent and this is a side view so yeah so as we can see uh it is able to successfully control its body based on just this for the ditional uh visual signal and what's interesting we also can know that like even though at every point in time we have only like a four dimensional input uh which might not tell much about the world State and Joints position of the agent body uh but over time over some period of time uh we can see that there is a nonrival like patterns like for example illation that correlates to the movement of the legs which provide which might provide actually useful information uh to solve this task yes so these previous results suggest that even simple qu receptors can in fact uh provide useful information to solve those uh Vision tasks and now let's uh address a second question on the role of the design so and first of all uh let's clarify what the design specifically here and so we consider the following parameters first is the position of the photo receptor the orientation and it's field of view and we denote the all these parameters by Thea uh and yeah there are other parameters we don't consider but could be interesting C to explore is for example non uniform resolution mimicking acute zones founding different animals uh light sensitivity and like frame rate uh yes and now given these designs how we can actually uh choose uh design from this design space uh and we consider the poing approaches first it's just sampling them randomly like a random design uh uninformed uh second is a computational design coming out of a design optimization that tailor design to particular agent morphology uh the environment and the task it needs to solve and finally we explore how in how easily those photo receptors can be designed intuitively so first of all given all this design uh let's see how how much it matters actually to have a good design uh for the final performance so on these plots we show um we show with each dot like a performance of a given design and we scale them to the range of a blind agent corresponding to zero level and the B PR for that t and we actually see that in all of the cases uh the performance of the agent varies widely uh for different designs that and the poorly and the poorly designed photo receptors can lead to the like blind agent performance that has access to no visual signal at all and which suggest basically that uh we need a computational way uh to design those photo receptors uh to be effic be effective yes so yeah how can we find those well performing designs automatically for a given agent morphology the scene and the target so let's start with a symbol like uh reinforcement cling observation election Loop so the simulator outputs an observation and based on that the policy uh predicts in the next action and so on uh so after the after the episode ends after multiple actions uh we get the reward function and the corresponding objective function which we use to uh update the policy ways to improve it and in this uh framework the task for example Target navigation would Define our objective and the the scenes like the environment and the Agents morphology would be used by simulator to uh model physics and produce a corresponding action uh observations so now uh in in the case when we change design uh the simulator and the renderer also depends on the visual design that we employ to produce a corresponding observations which make them dependent on the design and the whole process now depends on the design and the reward that we achieve at the end uh also depends on the design and the goal of design optimization is to uh maximize this like utility function right and this essential results in a B level optimization where in the Inner Loop we optimize uh the policy to work the best for a given design and in the outer loop we uh update the design uh to improve the performance of the policy and a standard approach could be for example use a Bion optimization or an evolution research uh but those would be prohibitively expensive for example training a single navigation posy can take up to one or two days so instead of that uh we model the design as an additional design action uh which is in Ed throughout the single roll out uh while uh while the renderer to render the views uh during the control actions and then uh and then we also condition the policy on the design so it can Implement different uh behaviors based on the current design uh and now our objective depends on both the design parameters and the control parameters and we use it to jointly update both the design and the control at the same time yes so now let's look uh into the effectiveness of this design optimization method um quantitatively so we run optimization for each of the tasks we described before for and measure the design performance before and after optimization and therefore One Design optimization R correspond to a to a point on this scatter plot where the x-axis shows a performance of the initial random random designs okay uh do you hear me the yeah yeah yeah that's I don't know um okay I'll be loud I guess oh oh it's back all right it's probably the speakers uh yeah so the xaxis shows the performance of the initial random design and so for example this is the visualization of it in this case and it achieves the reward of 200 and the Y AIS shows the performance of the design of the computational design after the design optimization and here is corresponding design uh which achieves a much higher reward and while uh this design might look kind of unintuitive to us uh there is a certain structure to it uh which allows us to achieve actually these higher reward so here the same example of before and after for a Target navigation task and for the point goal navigation task so this is an example of a optimization trajectory during the design optimization so one interesting observation that we found that for the point go navigation task usually the one of the photo receptors tend to go uh on the bottom of the agent and one potential explanation could be that since the agent knows the target so what it needs to do is actually AO collisions which might be easier to do if you have sensor down there so you see what's happen happening on the ground directly so and these are basically results for all the tasks for all different number of Auto receptors and we find that in most of the cases uh the design optimization improves the performance which in fact allows us to achieve U performance similar to the camera agent hey so yeah we also apply this uh design optimization method to optimiz the camera design and we find that uh compara to intuitive like a default design employed by the the simulator uh to solve these tasks we find that the computational camera design can actually uh significant significantly improve the performance of these agents yes and here are the examples of these camera designs so this is an intuitive standard design and here's a point for design for Point go navigation and for the Target navigation so as we discussed like similarly to the P receptors where one of the receptors usually um end up being on the bottom uh we find similar trend for the camera and for the Target navigation it stays up and actually in the Target navigation the the target the ball is above the ground and it's actually around the middle of this height of the agent uh where we see the camera conversion so and finally we explore whether it is easy to design photo receptors intuitively and we collect uh intuitive designs U from a by running a human survey we evaluate uh those designs and compare them with a computational design for the corresponding configurations and tasks and so what we find is that in most cases computational design is R among the best design and what's also interesting is that performance of the intuitive designs varies widely in all the cases uh we suggest it might not be an intuitive problem to solve manually and it again calls for a computational approach to the design optimization yes so to conclude uh like in this project uh we first show that even simple uh one pixel photo receptors can be effective uh to solve different Vision tasks then we also show that their design actually matters for uh their effectiveness and to find good well performing designs we develop a design optimization method uh they can find them automatically uh yes thanks um thank for listening and I guess we have time for one one two question yeah time ESS Yeah so basically if I understand your question it's about like having a nonuniform resolution right uh yeah okay yes um so we didn't directly uh like optimize for that but one of the design parameters right so I guess I can see it here yeah so was a field of view right of the photo receptor sensors and basically examples right what about what you're asking like for example if you have a low field of view it means in the limit right you have a pixel camera basically right that shows that like a very narrow uh point and basically for example this is a design that we found for the Target navigation task and it looked like it looked something like like a narrow uh it has a narrow field of view so it's like a very precise information uh where is it for but for the other TOS we found that the field of view it basically integrates uh a larger portion of a scene right so it's more like a blurry so it was a degree of freedom in terms of likei uh field of view um yeah but I think maybe exploring some uh more uh uh more free form structures here could be interesting different forms as you mentioned right Sim yeah so um we didn't try that we we Solly Sol them from the visual signal I think in standard like in this de mind control right you have those I'm not sure about the precious anwers I think they do have them uh but I'm so I think the standard approach to solve them from the world state from the proper reception they might have it there and yeah usually you can solve them pretty well uh using the access to those like pressure or joints that joints exactly yeah and here yeah the idea was to see how much we can put just from the visual information which can actually provide right um those types of signals is the question quick because the next speaker we are running a few minutes not so okay so let's take it offline if you don't mind um great so if can um so so I stop share yes please and uh son do you hear us yes I hear you great and we have to now move you to the room um uh First share the screen yes I share do I share my screen yeah okay exit so remove this there okay so do you see a slideshow um I believe so we are moving it to the bigger screen so bear with yeah you'll see three transparent octopi on the top if you have it that's correct what I show how to Lo it guess we on this right yes is it good let's do the audio how do you how do you change audio these symbols are different from mine no I think we should go to them do you need me to talk for a moment so you can check audio um actually right now we are the moment is switch it they'll ask you to speak uh so can you say something okay one two three four five oh this is uh can I say something yes 1 2 3 3 4 5 6 7 8 9 10 okay reason so I'll uh hi Sun thanks for being here and um so unfortunately there's no camera facing the room this year so you don't see people here but uh they're eager to hear you so in interest of time I'll shorten to introduction because well I describe s SP in the it works in the beginning of the in the intro part of the the talk that I gave and and we're happy to have him he's a professor of biology at Duke and um you know forly I would say that you know his his background he actually has very interesting material not just in popular scientific press but you know in Finding Nemo for example he has contributed to it um but what's I'll just tell you my personal you know introduction to him is that I have actually used a lot of his material in terms of getting inspiration in terms of diversity of vision and adaptation to a college and uh and not only I have read him directly from his talks and so on but what was interesting is that many of the books that I read that there were bestsellers such as edong and man world that actually use his material so I indirectly also landed in his material very well so they're very very happy to have him here today and all the um and earlier we saw in the introduction talk that the diversity of biolog biological Vision um is a great motivation for everything that we discussing today so here this stage is yours okay thank you um and thank you so much to the organizers for inviting me I'm really happy to be here I'm hoping you can see the screen well and hear me well um because I can't get any feedback from you so I'll just have to you know know that you're out there listening um I'm a little bit unusual among a biologists and that actually come from a math and physics background um in addition to the visual arts and so very early in my biological training I became enamored of all the visual properties of animals and their visual systems um so I've looked at you know a number of different eye designs ranging from eyes the size of soccer balls um you know down to microscopic eyes eyes that are made out of different inorganic materials and so on looked at iridescence transparency Ultra black coloration ultra white coloration U mirrored objects camouflage signaling and in a number of different habitats though I'm particularly interested in the open ocean um the technique we use are all over the map um literally in that you know we travel in oceans all over the globe um we do open ocean scuba diving over depths of about 3 to 4,000 meters we lower a lot of different kinds of optical measurement equipment into the ocean we go down and submersibles we develop a lot of cameras that you know can let's say see polarization or some other interesting sort of thing and we do lab work on molecular biophysics um different sorts of materials characterization and a fair bit of mathematical model in and different kinds of imagery um I was asked to give this talk um quite recently and it had a particular sort of length and so this is one of the talks that happens to fit that well and I think it will actually work pretty well for this conference which I did attend a number of years ago in Pittsburgh and so at least I have some idea of you know what you might be interested in so this particular talk um is about Eye Design um as you can see from this nice little splash screen um eyes relative to body size um vary all over the place ey designs you know there are animals with multiple retinas animals with mirror Optics instead of lens Optics animals with extraordinarily low F number lenses of about F number about like 04 um animals with tubular eyes and animals that have eyes of different types depending on which direction they face so they can optimize viewing for different viewing conditions um and it's typical among people in my world that study comparative animal Vision to become very enamored of color um you know we're all very interested in color it's very pretty and you know there's always the temptation to feel that you can like see something you know one animal can see something another animal cannot and that there can be secret channels of information and so on but you're immediately pinned in by the fact that the visual range in the electromagnetic spectrum is actually quite narrow um really only about a factor of two of um wavelengths you know are really there for you to deal with you know the maximum sensitivities of visual pigments only get up to about 630 nimet the minimums get down to about 350 Netter um and within different groups of animals you find that there's a lot of um what we call you know evolutionary conservation in terms of variation you know if you look in the upper leftand c corner of the birds you can see that you know for their different classes of cones there's really very little variation and so there's not as much to play with you know to think about how different animals see the world as you might hope um in addition as I'm sure you're well aware being being a computer Vision Group um not that much information is coded in the color Channel um here's you know just a simple image from duke Marine Lab at top um and at the bottom there's a black and white image and you can see that you can pretty well make out everything this is why black and white TV of course works so well for so many decades um the middle image gets rid of the luminance Channel and presents only the color Channel and you can see immediately that you know any sort of three-dimensional R rendering is very difficult to pull out and the color is really only good for identifying details um you know if somebody's wearing an extremely red coat or if a boat is blue you're able to pick that out but you can't really use color alone to parse out a scene and so we became interested in spatial Vision um you know and the differences of Acuity and here among animals the differences of Acuity even among animals that you would normally consider as having eyes um is enormous um we're near the top with with a you know cuity of about 100 cycles per degree you know a few Eagles beat us out but not by much and then it goes down by about four orders of magnitude until you get to insects that we normally think of as actually having fairly good Vision but their acuities are only about let's say 0.1 cycles per degree um in fact most of the visual world of animals is in what we would call you know legal blindness um by human standards um and so when you look at you know the world from their point of view um it's actually dramatically quite different um if you look on the left here there are some beautiful little um cleaner shrimp noted for their gorgeous color patterns on their body um interestingly these animals do not have color vision at all they only have um one visual pigment and so the middle column is showing um what these animals look like to each other and you know of course you can see there's almost no um information left of course about the color being monochromatic but then you step to the right you know column and you see what they would look like from only about 3 cmers away it turns out their visual Acuity even though they are quite visual animals is only about 0.1 cycles per degree and so even at very close range a static image provides almost no information a moving image would of course provide more but you can see that at the very least all the cute little pattern that they have on their body are gone um and so we mostly we work not in these you know sort of shallow water habitats but we're very interested in the open ocean and we're particularly interested in how Vision has adapted to work at different depths um and as soon as you get into that puzzle even in very clear water you're dealing with the tradeoff between sensitivity and resolution just as you would with a camera um basically comes down to pixel size though the different animals control their pixel size um in different ways you know quite differently than we would control with the camera and quite differently from each other and so you can go from you know a very dim and sharp image um to a brighter and fairly blurry image um you know sort of balancing out these two you can increase both of course by making you know the eye larger just as we can make something larger by making the sensor larger um but even though you know the neuronal system has a wonderful ability to amplify signals the quantal nature of light as you well know gets in the way um as you get dimmer and dimmer images the signal of noise in increases um due to Photon noise and so you can't take a dim image and just amplify it and get a good image you end up with just a lot of sort of noise and this was the problem with the early iPhone cameras they were wonderful during the daytime and truly terrible um the rest of the time so there the final thing to think about before we get into sort of the calculations about this is that when we think in the ocean we mostly think about two different kinds of light Fields um we think about animals that are have what we would call Extended Vision um seeing under ambient light seeing you know basically a spatial description of the world um this is a nice picture here of a poor collection of bait fish being eaten by both birds and sharks at the same time um which is not uncommon in the open ocean um and particular eye designs work particularly well for resolving the world under ambient light and developing you know a nice spatial you know pattern of it many animals in the ocean though especially once they get deeper um are really living in a world of Point sources created by bioluminescence um and then pretty much all the identification they do is by the spatial temporal patterning of these Point sources what you're seeing here are bioluminescent ostracods where the different species will move to the water column in different patterns such as a spiral straight line um changing directions and also emit light at certain periods in that creating a sort of spatial temporal point source dance that allows them to be identified to species and allows other animals to then show up and mate with them and this requires a different sort of eye design that you know is successful for that and so to give you again you know why the world in the ocean changes especially as you get deeper than about 500 meters or so um this is based on a Model I won't go into the details of it of um how far away an animal can be seen um that's about the same size as a viewer um divided by body size so one would mean that a 2 meter person could see a 2 meter object from about 2 meters away on the x-axis there and what you're looking here is a number of different um squid species each sorted by the average depth that they live in and you can see that once you get to about 500 meters um the ability to see things really starts to catastrophically drop um you get to you know sighting distances are only about a body length or less than a body length and eventually down to many many orders of magnitude less than a body length just for fun we worked out how big a pupil would need to be for an animal with 1 th000 meters to see as well as we do at the surface and it would have to be about a kilometer across however once you introduce bioluminescence even if it's quite um dim you know to dim to be seen even in dim room light such as you're probably sitting in right now um the ability to see over decent distances is recovered and they're able to see things at about 10 to 100 times their body size um even working with a small amount of light so even very clear water this is water from the Bahamas in the middle of the day um clear Sun directly overhead um even with that amount of light um a few hundred meters of ocean can just drastically reduce the amount of light available which makes it a nice natural laboratory for looking how animals balance out sensitivity and resolution to maximize you know their ability to see at those depths um what's going to follow here is a relatively simple model um to sort of look at how what would be essentially the best Acuity as a function of death how would an animal best balance Acuity um and sensitivity to be able to see things the furthest away and and so you have here a squid looking at a striped fish it not only wants to be able to see the fish it wants to actually be able to resolve the stripes um which have you know a length of s for one black and one white stripe together distance D um and the first thing that you have to deal with I'm going to sort of increase the complexity of this model as we go is you have to deal with the modulation transfer function of the eye um and as you know an animal this target moves away from the squid um it's going to become more and more difficult for the strip to be made out at the moment we're not even considering the water um nicely enough um the modulation transfer function of the eye has been measured a number of times in different eyes um and is a relatively common feature um that can be determined almost entirely by what we call the resolution of the eye which we can work out using both behavioral and morphological techniques um it is this Delta row and it typically follows something not so different from a gaussian which you see here um you know with a particular gamma um and so using this you can then in theory work out you know how far away you know an object these Stripes can be seen as the contrast drops to the modulation transfer function of the eye down to the contrast minimum however it gets more complicated than that because the minimum contrast threshold itself depends on how much light is entering each pixel um because you're dealing with a fair bit of noise as the light levels drop it turns out the light levels drop enough for each individual pixel of this eye this you know collection of photo receptors such that the signal to noise ratio just from the photon noise itself um ends up being quite low and So based on the fact that you know photons enter and interact with matter in a poison process where the standard deviation is you know closely related to the average um you can then work out what the minimum contrast threshold would be um and it's essentially you know a function of the resolution of the eye and the amount of light that enters the eye which is just a function of how much available light there is and the pupil size and so on um you put that all together you can actually very simply analytically work out you know what the sighting distance would be as a function of light level the resolution of the eye the contrast of the original Target and so on and as you might expect in a world where you know an eye with one pixel would more or less be useful useless and an i with an infinite number of pixels would also be useless um and sighting distance always has to be a positive number somewhere in the middle there is a maximum and you see that here just for a number of different light levels at different depths you can see that as um we change the um resolution of the eye the sighting distance goes up and then it begins to go back down as you know and where that m is a peak is where we have sort of the optimal balance of resolution and sensitivity for that particular set of parameters the problem though is that you also have to consider the water itself um all these calculations were essentially done in a vacuum so you have to consider the modulation transfer function of the water um which has two components it has a frequency independent component um you can think of sort of scattering you know the veiling light um and that can relatively simply be calculated using um sort of adaptations of old oce Optics um papers back from um the visibility Lab at scripts um back even as far back as the 50s from priz andorf for and Dunley and so on and then you also have to deal with multiple scattering um which is a frequency dependent effect um that has thank God been actually worked out um by Ron zaneveld and Scott pagal um a number of years ago for the types of waters were're interested in and so you can put all that together this cannot be solved analytically like the earlier equation could um but it can be solved numerically you know relatively easily to work out what is the optimal Acuity for an animal as a function of the contrast of what it's looking at the size of the stripes that it's looking at um how much light is actually entering the eye which depends again on the pupil and the optical qualities of the water and the depth and the time of day and the integration of the time of the eye and so on and um The Clarity of the water which actually has a a fairly large effect as you might guess um here given as the beam attenuation coefficient um of the water um we then had to pull together because we wanted to be able to compare this model with reality we pulled together what were the acuities of quite a number of different fish um for those of you unfamiliar with this um the tree shape structure on the bottom is showing the evolutionary relationship between all these fish um you can see actually that there is some connection between The evolutionary history of a fish and how um sharp its eyes are but it's not as simple as that um we also looked at sharks sharks on average have about half the Acuity of the fish we are looking at it's actually kind of interesting the Sharks mostly go around even though they're quite large um with quite blurry vision um normally large eyes are actually quite a bit sharper than um small eyes um and then we just compare this you the model the model are the you know the different colors of smooth lines models are always you know nice and smooth here we're just looking at you know what are the predictions for different levels of Stripes um the um optimal Acuity depends of course on the size of the stripes you're dealing with and what we found is that for animals at depths less than about 500 meters in that what we call the ambient light range um the predictions of the model match what's actually seen in the animals within about a factor of three um which for biology which mostly lives in the land of one significant figure um isn't bad um we never get you know accuracies of 5% or 10% typically we're trying to get within you know 50 you know factor of two or three or so but what you see is that as you get below 500 meters um the actual measured Acuity of these animals is dramatically higher than what is predicted these animals have much much sharper Vision than you would predict from the models at those depths um the prediction of the model is that the animal should sacrifice nearly all of their Acuity um to be able to see as well as they can but instead um they have much much higher Acuity than you would think um this is just looking at the model on a different sort of slice through the parameter space here we're actually changing um The Clarity of the water um the sea and open ocean water for reference is about 0.005 um we again see you know about the same effect where to depths of about 500 600 meters or so um the model and the actual measurements of the animals are within about a factor of three but then when you get deeper um you end up with much much sharper eyes than you expect um this actually follows um predictions that were made um probably about 10 years ago by a colleague of mine that as you move from the ambient light realm you know the top 500 meters of the world where you're seeing under you know diffused sunlight even though it gets quite dim um you get a very different world than when you're in the point sour realm you know below about 500 meters which when you're in the point source realm all you need to be able to do is know where the point source is happening um leads to like quite different principles and you end up having eyes that um in reality are actually quite a bit sharper than you would expect um because all the light just sort of falls on one photo receptor or one small group of photo receptors anyway and you're not worried about the F number of the system and so on um so in conclusion um even a small amount of light attenuation in Clear Water so in these depths the water has a visibility Believe It or Not of about 100 m it's extraordinarily clear water it's enough to dramatically change um the predicted Acuity um this has a very very large effect um for species at um daytime depths um less than about 600 meters um you are able to work out the balance between Acuity and resolution to fairly accurately predict what the animals are actually doing and so it does look like you know they're working to optimize their ability to see things as far away as possible but once you get deeper the measured Acuity the actual animals is much much higher than the predicted Acuity um and this does match previous predictions that very deep SE animals that are looking entirely at bioluminescent flashes um work better having very high Acuity to localize those flashes rather than trying to optimize their ability to see under ambient light which at this depth is more or less gone for them um again just would like to thank the organizers um thank um the Air Force and the Navy for funding this work and be happy to take any questions thank you so much s um are there any questions and if there is you have to walk up to the microphone here um and it's okay if you and I have one by the way like one um question in the sense of so most of the discussions that we had earlier today were about like terrestrial vision um the biological Vision that we discussed so I wonder like since you are quite an expert um in this area so other than you know the intuitive thing that comes to mind that the biggest difference between terrestrial and underwater would be the Distortion of light like and darkness that would be caused by water but is there like another characteristic difference between the two that U is missing in your opinion yeah well there are a number actually um so you know one of course is you know the tremendous change in light level with depth another is the tremendous change in both light level and Spectrum depending on viewing angle so that when we look in different directions it doesn't necessarily change you know the quality of the light entering our eyes of course it does if you're looking at like something on the ground but imagine let's say a bird in the air whether it's looking horizontally or at a 45 degree angle or you know further up towards the Zenith things don't change that much um but because of you know the water you know interacting so strongly with the light um there can be an enormous difference um depending on the viewing angle um another difference of course is that the Spectra of light are dramatically affected even you know on top of you know the light level changing and that is also affected by viewing angle so if you're in the open ocean and you're looking down the upwelling light is purple the downwelling light is close to White and the horizontal light is blue which has a big effect on um on Eye Design I mean there are a number of animals like I mentioned that will have a one eye looking up that's completely different than the eye looking down because they're dealing with different Spectra potentially different um sort of things are looking at like one animal only looks down for bioluminescence and only looks up for Silhouettes um and just has a completely different eye for that um the other thing of course you have to deal with underwater if you're within let's say the top 10 meters um with YF schner has talked about a number of times artics you have to deal with the fact that the surface of the water unless it's very still which is rare um is act acting as a number of um positive and negative lenses and so you end up with the CICS moving around and the level of brightness going up and down by several orders of magnitude over time SC you know time periods as short as let's say a 30th of a second um Darius strumsky did a fair bit of that work at scripts showing that particularly when you're looking up it's extremely complicated um both the cosics and the sort of basically the spatial temporal aspects of the costic lensing at the surface um make a dramatically difficult um light field to work with um so those are sort of the main ones the final one of course is that the ocean varies a great deal depending on where you are depending on what is dissolved in it um we you know have to deal a little bit with pollution of course we deal with clouds and we deal with Haze in the terrestrial world but in the ocean you know just a small amount of dissolved sediment um a higher amount of chlorophyll can dramatically change you know the light levels as a function of depth and also the Spectra as a function of depth and the angular um dependence of light level and spectrum and so you know you're dealing with a world where very little bit of addition of materials can change almost everything that's needed to be able to see well wow I I I hope somebody makes a VR sort of experience for this to for us to wear goggles and you know experience different that would be good it wouldn't be too hard because there is um unless you're dealing with very water there's some very good radiative transfer um systems out there that would allow you to actually work out just based on the inherent Optical properties of the medium what the apparent Optical properties are so it would be doable is there is there a question um go ahead the quick one yeah uh yeah thanks for the talk and I want to talk maybe for an opinion so it's related to am's question so before you start even work out the formulas right and the optimal like uh you need to Define all this uh variables that will be influential actually right uh to uh and to identify uh the optimal accurity and assuming those designs that we find in animals actually optimal how do you think far we can go by finding those Ables and explaining them qualitatively on the formula level other than employing some sort of a blackbox you know model that takes uh those condition environments and tries to predict as close as poss uh like as accurate as possible the actual eye configuration in the animal um I'm not quite sure what you're trying to add is there what are you trying to ask so I'm trying to ask like how do you think like from your experience right you're trying to identify those variables like the depths right the the the the water like the the fraction and and so on um from your experience how easy you think it will be for us to figure out all this variables and work out the formulas instead of doing some kind of a blackbox approach where we try to how easy is it to measure those it's well a lot of it's known um so I mean I know this is you know somewhat alien to your conference um but there are worldwide databases let's say of the you know the big things I mean in the end it actually only depended on about four things um it depends on what are you know what you might call the inherent Optical properties of the water which are its absorption coefficient it's scattering coefficient and so on and those are known globally um to a fair bit of accuracy um you know they can vary on time of year and so on the other parts are just you know how much light enters the eye which is a simple ma or the camera which is a simple matter of the aperture and the light level available and then some features of the Target and that's about it so these are not um difficult things to measure um the hardest thing if you know you want to think about particularly Nearshore Waters that can be quite variable over time um that can be a trick because you know you can have let's say I don't know a whole bunch of rain in the mountains behind Seattle and it dumps a lot of fresh water into Puget Sound and then Puget Sound is going to have different Optical properties than it normally does because it can lead to a bacterial bloom or a death of you know phytoplankton or things of that sort um but the rest are relatively simple geometric parameters um so you can you can do something like this like let's say you want to work out you know a camera that will work best at different depths for looking at different targets um it wouldn't be too difficult to do this um there's a nice demo by a colleague of mine he actually built a small train set and then he you know drives through it you know using you know the sort of the parameters you might expect let's say for human vision and then adjust the um Acuity so that you lose some of the resolution but you gain sensitivity um to the optimal point and you end up you know with actually not a bad image um you know I could actually probably get my colleague to send that along um it's so these things actually have been um adapted for um technological use no sounds interesting is there one more question if you don't mind yeah because we are running like over at the coffee break by 15 minutes yeah I assume I'm running into your coffee break by now yeah well thank you so much it was great I appreciate it very much I'm glad that it actually worked out and you enjoyed it well good and um if anybody in the audience does have a question I'm easy to find on email thank you so much I'm the only person in the country that has my name so perfect thank you so much so uh let's thank S one more time and we'll see you at 11 for the next session enjoy the coffee break thank you so much I appreciate it of course sure thing take care enjoy the conference thank you bye-bye I gu we'll leave it leave the zoom on no uh yeah I guess you can just turn off the MBE I'm going to try to pound two cups of coffee e e e e e e e e e e e e e e e e e e test test e e e e e e e e e e e e e e e e e e e okay I'm just going to get started just going to get started um this is all working fine I think um if can people hear me okay on the zoom we changed computer so oh my gosh I'm too loud okay a test test this seems okay okay sorry I'm loud okay um if anybody can't hear me on the zoom I guess um just uh oh okay there's a thumbs up on the zoom that's great um all right I guess I'll kicking off the the second half not the afternoon but the second half of the morning uh here um so I'm going to be talking about the other side of things the everything that's not visual today so uh for you brave souls who are here uh I'm sorry if you wanted Vision but there'll be very little vision it'll be a little bit sensing um but I'm going to talk about the we'll say the stack of all the keep cap abilities that go into a computational design pipeline for Designing robots um and I guess with that uh let's begin um so this why is this important I want to start off by motivating why it's important to have computational design workflows it's scientifically interesting of course uh as you saw in the morning there was a lot of connections to ecology but I also want to point out that the world is already full of really complicated machines that require a lot of time and energy and labor investment in order to make these things work and they're all dynamical and they're interacting with the world around them so they are all environmentally driven from construction to things that we wear on us to things that might live inside of us um and increasingly we have this demand and this dream of more and more complex machines and so you'll have people pitching these days to Startup uh VC companies and whatnot all these crazy ideas about lab Automation and manipulators and and maybe even you know biologically organiz um you know engineered organisms and this is really really exciting and the question though is how if it's already hard to design the things on the top how much harder is it going to be to design the things on the bottom and what tools will we use how will we Design This future dream of ecosystems of machines robots and artificial life um this is especially important as AI is starting to take over because not only are the machines themselves complicated but also the intelligence living upon them is going to be um you know very complicated as well and I think that it's really important that we come up with a better way to do this because I don't think we should be playing um you know uh risking things on critical infrastructure here and right now the current industrial design workflow looks something like this uh you have some sort of task specification that you come up with some sort of mission that you want to accomplish so maybe I need a robot that can quickly walk upstairs for some reason you've decided this is something that you need um and then from that you go through the following process you spec out what are the different components that you're going to use um and and come up with some sort of conceptual Design This is maybe a little explorative or maybe at a certain point you might fine tune it and come down to the to to the the final working version of this thing um and then when you build it you then have to program it and then if you make any mistake at any point here and you find out that the thing that you program doesn't work well then you have to go back to square one or Square two it's not always clear where it's not clear when you should go uh move backwards in this uh in this cycle and it's also not clear if you should just give up with this idea alt together if it's not going to work we don't have certificates for any of these types of things there's not a lot of advice in the manual process it all comes from intuition and that intuition has been great but it's also led to some products that are more complex having lengthy spiraling development pipelines be slow it could be expensive and laborious and basically this is trial and error-based design with a lot of intuition to guide this of course what we're going to talk about today is specification driven codesign where you simultaneously take a a uh a sort of uh Mission specification and you simultaneously come up with how do you reason about all the different aspects about this simult mously in order to come up with a design that is generated by a computer um that can be fabricated and would ideally immediately work for you that's the dream that's what we want to sort of achieve this is an example from a project we worked on called Robo grammar everything here the topology of the robots and the controllers they're all automatically synthesized without a human in the loop so this is very similar to the Carl Sim stuff you saw from earlier this is just taking more of a new age approach to that right and I I want to also stress here that when I talk about robot um or cyber physical machines this is really a very broad range of different types of devices so you can use the same Paradigm to design robots of course um but really anything with a body and a brain and that intelligence that we have can be put completely into the brain or completely into the body or somewhere in the middle so just to take the as an example if you wanted to make something that's going to balance yes you can have a complicated robot that learns to stand with a lot of onboard control but you can also put more of that intelligence into the more ology of the system make it softer give it four-legged so it's able to balance more easily statically or in the extreme example it's not really a robot or maybe it is I don't know but you know you don't need Advanced controllers to enjoy the porch so these are all examples of systems that can balance with varying amounts of onboard intelligence and these are all valid solutions to this problem of making something that balances and this idea of a cyberphysical system really can range over a whole wide of different idea different forms so you can have things that live completely virtual and agents or you can have something that's completely physical and you can think of things at the material Level or you can think of things at more of the larger systems level so this these ideas will really generalize to a lot of different areas um and you know I'm hopeful that these algorithms can be applied to more domains in the future So speaking of those different types of domains there's roughly four different types of uh of of problems that you can consider you can think of Designing for the virtual world or the physical world and then you can also think of things where you care about just the morphology or things where you care about morphology plus some amount of control on board um so this will determine whether or not you're working with static objects or things that are going to move and whether or not you have to care about whether or not you can fabricate it and make it work in the real world and these are types of considerations that are going to drive some of your decisions for how to do your computational design work I think there's four main goals that you should have when you're coming up with algorithms that are going to help you to design different types of robots or different types of structures in general so design technology ideally you don't want it to be very slow you want it to be fast evolution is very slow evolution is beautiful but we don't have a billion years to design new robots or even a million years so we need something that's going to work faster than that um second you want the things that your algorithm produces to be performant so it should be something that gives you good designs not things that are fast to generate but crap um third ideally you want your algorithms to be very explorative so that they give you options because maybe you didn't Define your your mission specification exactly perfectly um so there's you know you want to think about what other ways you can go about solving your problem and finally you want it to be actually realizable in the physical world so this is sort of the problem setting that we're dealing with here today and for the rest of today's talk I'm going to go about introducing a little bit of theory this isn't really a formal area yet so to speak um and then talk about some preliminary simulation and control considerations and then again there some computational design um algorithms um in the second half of today which starts on the on the fifth point there um that's where we'll start more of the learning based approaches and think about how do we fabricate and uh and uh and handle bringing things to the real world um if there are any questions throughout this process of course please feel free to interrupt me and ask questions um otherwise I'll jump right into this and you know I'm going to wind up using a bunch of things that I've worked on to motivate a bunch of this uh work um and I'll try try to give you guys a bunch of other examples of other work that's relevant um just just for the larger uh chunk of this it's easier for me to talk about things I'm more familiar with but there's a lot of great work out there and I'll try to highlight some cool things as um in the community so um this is my one requisite math slide um so I'm going to briefly describe from a control theory point of view uh how we can design these sorts of computational problems because I think it really helps to digest uh some of the unique uh computational challenges here so in control theory you have some sort of dynamical system you have some time evolution of a state that is contingent on what is your current state and also the control inputs that are going to be sent into the system right so you have a system State a control signal and then you also have some sort of dynamic Cent this could be simulation or this could be the real world and then ultimately you're trying to measure how good is my state Evol um my state Evolution and that's governed not only by the uh the physics of the system but also the controllers right and so this L this loss function determines how bad did I do in solving a task so over time you know you can imagine I want a robot to follow a certain trajectory okay so this quadcopter oh you know does okay doesn't solve it exactly but at least you can quantify how bad it performed right and then the game of the game is can we minimize the Badness of that ideally bring it down to Zero have it track that thing perfectly because if we can do that then we have perfect control the catch here is that you might not be able to do that and I would argue that the reason why you might get suboptimal results here um or maybe not be able to solve the problem that you're trying to solve is that it's not only a matter of the controller that you have but also the physical design parameters of the system so we want to simultaneously reason over the controllers uh that the robot is going to have the brain as well as the physical body now to break this down into four rough systems that you might care about here first of all you have a design space this is a parameterization of your system and this is the space over which you're going to search so some variables that represent how you're going to get control signals or some variables that translate you know floating Point numbers into some physical instantiation of a system the second thing you have here is you have some sort of objective function or some sort of cost this is a mathematical function that is going to essentially um uh codify your problem into mathematical terms something that an algorithm can reason about the third thing here is that you have an Optimizer this is your computational design algorithm and this is something that a lot of people are you know usually really really excited to talk about this is usually the the meat of of a computational design problem but I will also note that the simulator is going to have a lot of effects as well on how you solve your problem and the right simulators can help you to solve problems more efficiently or more accurately now this is a is a world where I didn't mean to start this video already okay well anyway this is a world where really you have three types of inverse problems that are stacked at top of each other so starting from some sort of design parameters you're going to come up with some sort of gamut of virtual designs this is an inverse design problem from some sort of objectives that you care about what is the design parameters that optimize you in your Paro front for your objectives then you have a second inverse problem of I have a virtual design but I only have certain physical realization ability maybe you're using 3D printing maybe you're using Milling it doesn't matter what it is but you're limited to certain set that you can actually fabricate so you need to come up with what are some virtual designs that actually can achieve be achieved in the physical world and lastly just because you can actually fabricate a design doesn't mean that it's going to work so you need to also come up with what are the types of systems that when you try to translate them through the physical world are going to be accurate um and they're going to translate Faithfully so these are this is a really complex like triple nested inverse design problem um and we're going to try to talk about all as much of this as we can today the last thing I think I want to talk about before um we we go on to uh the simulation uh part of this is the different types of robots that you see in the world today there's a lot of different classes but I would say broadly these days the most popular types are either rigid soft or some sort of hybrid and I think that's true for nature as well so you can think of the rigid world as things that have skeletons right uh these have a compact number of degrees of freedom from just the joint angles I can tell you exactly where I am also if you have you know my six degree of Freedom uh body frame and from that you can reconstruct exactly my pose but there's not a lot of elasticity in a rigid system so that can be a positive or A negative usually it's a negative um and it's also skeletal um it's not deformable um that has both pros and cons in terms of the fact that if I am skeletal I have the ability to sort of lock joints and have a also easier to control system and an understandable system but I don't have the nice um deformable manifolds maybe that are useful for manipulation and these are you know the the the the benefits of Soft Robotics is that you can have elasticity you can have softness um that allows you to grasp things more more robustly but of course some of the drawbacks of this is that they're really really hard to model there's infinite degrees of freedom um and they're they can move in all sorts of crazy ways dynamically so they they do require a lot more uh computation in order to reason about them so that's sort of our problem setting and now I'm going to talk a little bit about different types of simulation algorithms and this is really important because if you want to know if a robot works you need to test it and it's really expensive to build every single prototype so you want to try to do as much as you can in simulation and that's where that comes in and is useful so of course as you have different types of robot types different types of organism types yes you also have different types of simulation types there is rigid body simulation which over the years has become very highly performant um and you can do it extremely efficiently on the GPU um this is some work from Ron fos group uh back in I think 2004 so you know 20 years old now and we can do this even better these days with the crazy gpus we have um some work from uh Karen L group back in 2012 these are rigid soft hybrid systems you know they can you can simulate rigid walking uh characters um but also on their feet uh they add some soft compliance and this can naturally help these uh characters to also um to balance without having to put in extra work and finally of course fluid simulation is very very popular and very important especially in organisms that uh you or agents that are going to be flying or going underwater uh these types of systems you want to be able to understand the coupling between the body itself and the environment really this notion of coupling has come up at least twice here and there's a whole area of called multiphysics and multiphysics simulation engines um packages like console and Etc do this especially in the Statics regime um but this is you know there's there's the coupling problem is extremely difficult but in theory you want to be able to simulate as many different phenomena whether it be fluid solid uh soft materials but also things like thermal uh uh you know heat transfer and and and electromagnetism these might be relevant to your problem they might not um and you have to determine that when you pick a simulator in simulation we have uh it's basically broken down broadly into five steps no matter if you're a soft robot or a rigid robot you have some sort of full state of the system and then some observation of the system that's something that the that an algorithm is going to be able to reason about some compact representation and that might be important because you might have a partially observable system maybe because you're an environment and you can't see all of your environment at once um or maybe because you're a very high dimensional system and there's you know an infinite dimensional system and you need a compact representation that's fed into a controller which you're going to want to train or optimize which then provides actuation and then this feeds into simulation and there really should be a subing thing here for the people who are in the know but then the steps and simulation so for every like you know maybe maybe it depends on your system but maybe every like tenth of a 100th of a second or tenth of a second uh you're going to want to do a simulation step and you're going to have to do all these calculations so it can get expensive um but there's uh there's a number of uh really trade-offs ultimately that can help you to choose whether or not you have a fast simulator or an accurate simulator so you have to decide what type of media do I have to simulate is it rigid is it soft is it high dimensional is it low dimensional um do I need a high highly accurate simulation or can something course get me most of the way there so I need perfect models before I build something before I'm confident that it's going to work in the real world what is the dimensionality of my system if I'm working with soft materials that's a infinite dimensional system and I'll try to represent that in a high dimensional finite way so based on that you might want to make some decisions about whether or not you want to use for instance GPU acceleration or something else in order to make this go faster and of course you care about the speed because you don't want it to take forever um certain systems are extremely stiff and so they take a very very long time naturally to simulate or they will break and they will actually numerically explode which causes everything to go to not a number so you also want to sometimes pick certain aspects of your simulators particularly your time integrators based on conditioning and we'll talk about a few other things that you might care about uh very soon which is differentiability of a system and incorporating control and I'll talk about these uh very coming up ultimately there's a whole bunch of different components you have to choose um the nice thing is that they're roughly modular so even though off the shelf you're going to pick one simulator that exists that's open source or you're going to develop your own a lot of these different building blocks time integrators State representations even observation models controllers these are all agnostic to the actual physics themselves sometimes certain ones work better with different physics systems but they're all plug andplay so you can swap things out usually according to what you need okay now I mentioned that you know not Al um that there's something called differentiable simulation um so all models are wrong some are useful some are particularly useful some simulators you can get more information than just a certificate or an evaluation of how well a system is performing so um I'm going to talk about a project we worked on called chain Queen this is a fast accurate by which I mean physically based and differentiable simulator and by differentiable I mean that any output of the system can be efficiently computed with respect to any input of the system um the gradients of of any output can be sorry the gradients of any output of the system can be computed with respect to any input of the system and that's usually smooth and fast those are two things that you want to care about for this to be useful so you can think of this as a a simulation as a neural network really long complicated nonlinear function and what we want to be able to do is like a neural network also be able to compute gradients of that and be able to optimize different parameters of that simulation those parameters can correspond to the physics of the system or they can correspond to parameters of your design so to go through this process really briefly at a very high level here if you have a task such as I want a robot to run forward as far far as possible in a certain amount of time given well from my physical design parameters and my control design parameters I can use the physical design parameters to instantiate some sort of robot design and then I can use my controller that I have here in order to EP the system forward and it hops forward very happily and eventually we can evaluate how far forward did it walk over a certain amount of time now maybe it didn't do so good that's okay because we can then do back propagation through the physical simulation itself and compute the gradients both with respect to the controller and the design parameter simultaneously updating it and allowing it to optimize and this is agnostic to the simulators um that you use and I'll I'll show highlight a few differentiable simulators later um but I'm going to use soft robotic simulation here um it seems to be uh a little bit more amendable to having smooth system uh outputs even though the drawback of course is that they're very very slow to compute so I'll show you how you can use this for control optimization so for a pure control optimization problem um for design variables you have I guess we're sort of leaking into the controllers section of this talk here but let's let's just go with it um neural network weights so we have a neural network that takes as input some sort of representation of the state of the robot and as output sends actuator signals to these red and green Chambers and I'm sorry it's not super clor friendly um I wish I could go back and render these videos um so these actuators uh signals will actually cause these Chambers to contract or expand and that's going to allow you to um move the system forward so we just have four outputs but we have and we have maybe you know if we use a a low dimensional representation of the observation like 20 inputs or so so this can lead to a neural network if it has two layers 64 neurons we're talking about maybe a ,000 network parameters so it's not a small problem it's not a huge problem but it's it's pretty big and just using atom as an Optimizer we're going to show you how you can use this to do control optimization of of the system so one thing that I want to note here is that you can get these simulations to be really really fast by using a GPU so every gradient I show you um on all the systems I think that I show you today um the simulation plus the gradient calculation no more than 15 seconds and for some of the simpler systems it can be a half a second long so for something like this this is going to show you the time evolution of this soft robot as it's learning to walk forward really just after um you know after after about 12 iterations it really starts walking forward pretty well um and that's if that's five um seconds per simulation and gradient computation and all that we're doing other than that is doing a gradient update of the system then what you have is a robot that learns to walk forward in just six seconds which is really hard to do if you try to use something like reinforcement learning um because reinforcement learning uh is very very noisy unfortunately and Sample inefficient so some so this will converge faster than using reinforcement learning and it will also be able to solve some problems that it can't caveat it will also be unable to solve some problems that reinforcement learning can um and that becomes that's because a gradient based approach just like this is really int you know intuitively a greedy approach so if it needs some sort of exploration as well then you're going to want to go to something that's a reinforcement pring type approach okay um there are a whole bunch of different Auto differentiation types um and whether or not you have a large output of your system or a small output of your system or a large input of your system or a small input of your system you're going to want to choose different types of differentiation choices to get your gredients um and I also want to note though that you know if you you're going to do back propagation that's going to be very very memory intensive uh for large systems because you need to save the information you have to checkpoint it at every single layer of the system for when you do your back propagation pass to compute your gradient so if you have a very long system if it's a very high dimensional system which can happen in Soft Robotics um then this is going to be something that's going to use a ton of memory and you're going to run into some serious problems so one way to ameliorate this problem um is to change the integration time so the Integra ation time is a function of the stiffness of the system and unfortunately you have to pick a time step of using an explicit integrator um so that the system doesn't numerically explode and that time step for a software robot looks something like seven micros seconds which is extremely slow you're doing a million simulation steps in order to do one second in real time you're going to run out of memory with a million steps each holding maybe a million data points like really fast so um you can use uh implicit integration which will allow your system to be a little bit less accurate but it'll be much more stable and you can take larger time steps and you won't run out of memory so that's two benefits for the price of one and the drawback is that your simulator is going to be a little less accurate usually I would argue that a less accurate simulator doesn't really matter too much unless the accuracy that you're decreasing it by is very very large so small hits to accuracy are usually not a big deal because your simulator was wrong to begin with like the model is wrong and when you actually you bring it to the real world there's going to be a whole bunch of uh changes in the way that you actually fabricate it based on the virtual design so this should only be used as some sort of um at least at this stage where we don't have robust syrial pipelines for design um right now you should think of this as you want the simulation to guide you but not to be something you take as as Faith um there's been a lot of other work that's been doing this um type of thing with implicit integration more um especially in the clothing regime um so you can check out a paper called diff cloth as well and the area of differentiable simulation now has grown to where you have rigid body simulations that are that are um uh differentiable soft body simulations fluid dynamics you can even find molecular Dynamics differentiable simulation and if you want to play with this at home I really recommend checking out two domain specific languages there's uh taichi and there's also Nvidia warp these are relatively easy to get started with and they give you a bunch of uh uh starter code so you can build some cool demos pretty quickly um we also wrote a um a review article if you want to check it out that sort uh brings together and marries the ideas in one differentiable pipeline um from differentiable geometry uh physics simulation and v um and rendering and vision so that might be especially relevant to people who are interested in the connections between physics and Optics um if you want to check out some demos and play around with this yourself I I whipped up two demos we don't have time to really do Hands-On uh demo here um but these were originally for previous tutorial but I think they're still just as relevant here and you can learn about the how to use differentiable simulation and also use it for Co optimization and additional resources uh these are some very popular physics simulators um and control tool boxes that you can use highly recommend checking them out with Joko I used to not like so much but it got really good recently so it's now in Jacks so very fun okay going to quickly talk about just a few other ideas in control because they'll be relevant as I go more into the code design uh aspect of this talk so um three types of controllers uh you have open loop controllers um where you prescribe what the motion is going to be um you have close loop controllers which repeatedly online will recompute obviously these are more expensive but they will recompute based on the state uh and sensing how what type of actions you should take and then there's some methods that can be thought of as an online method or an offline method things like model predictive control or reinforcement learning um these do some planning offline or online and then they do some execution offline or online that's similar um so one very common way to do motion planning and is relevant to some things we're going to talk about later is something called trajectory optimization I'll go over this pretty quickly but the idea here is that from sort some sort of starting configuration and some sort of goal configuration um what are the Motions that you need to get to to to take from the start to the goal so this becomes an optimization over what are the actuation values and also in order to make this a little more tractable we add additional auxiliary variables which say that what we also want to solve for what are going to be all the intermediate States so the real thing that this becomes is a constrainted optimization problem which says that all these intermediate states that we're going to solve for we also need to make sure that they are physically coherent that if you have also the control input that they time integrate to get you to the goal and this can be very very powerful so um come on play the thing oh yeah is this this going to happen this is uh all right check out this paper has a video um if we can't get it to work uh if any other videos have any problems sometimes that's the case I'll I'll try to show them offline or I'll restart PowerPoint all right let's see if this one works this one's working okay um so I'll I'll try to find that other video for you later but there's also model predictive control um is another approach to and and this is a really great control algorithm if you just want to get something that works immediately maybe not optimal but is going to be pretty robust and reliable um this looks a little bit into the future at at any time uh in in during its simulation it forecasts a whole bunch of different outcomes if you were take different actuation strategies and then it picks the the best one or it Blends them together and then it recomputes a New Horizon um so it's constantly recomputing Horizons it's very very expensive but the benefit there is that it's sort of doing a Monte Carlo search of all sorts of different possible ways in order to move throughout space and so you can use this in order to have some exploration built in and lastly the most popular way today of doing control is these neural networks I've been talking about I'm sure most people in the room are familiar with this but reinforcement learning is able to really efficiently is um design controllers for robotss um the harder the task of course the the harder the uh the the you know the more becomes not efficient but for a lot of things these days you know like walking or manipulation even um some of these things you can solve on Commercial Hardware like a laptop for in in an hour or less so in simulation and increasingly these things are translating to the real world so I want to bring a couple of uh resources um if you want to read more about this uh type of material especially trajectory optimization um or reinforcement learning you can check out some dedicated tutorials to that and um underactuated robotics at that MIT is a really great online textbook uh that gives sort of an overview of all these different methods it's a little dense but it's pretty good okay last thing for the for the first half today I'm running a little bit behind I guess because we're at the 30 minute Point um I want to talk a little bit about some traditional methods in in customization which is a weird thing to say because this field is really only like 30 years old at best probably um for for robotics at least but I mentioned that the goal here was that we want to design over the um the body of the system and the brain of the system and all the different components simultaneously to get something that's going to be really highly performant and I would say that broadly speaking there's three approaches to doing this one is these large types of heuristic searches um evolutionary searches um things like the Carl Sims work that I'll show in just a second um and then there's going to be gradient based approaches is that they're going to use these differentiable simulations in order to efficiently search for optimal configurations and then to provide a little more exploration similar to reinforcement learning you're going to have some generative based approaches that are going to use learning to propose learned models to propose uh ideas and then grade them and then recompute so maybe I don't have to show this exactly again um this was uh I'm going to skip over this I'm worried the videos aren't playing um okay we'll figure it out this is just going to be the Carl Sims video which which Amir showed earlier today oh that place I don't understand technology okay the way that this algorithm works though um where in order to do an evolutionary algorithm is assume you have a population of different robots you evaluate how well those agents are going to perform and part of that of course is going to be to have to generate some sort of controller so you generate a controller for your your morphologies and you see how well they form According to some Metric that you care about and some environment that you care about then you have to then decide how do I look at that population and based on that feedback modify the population and decide what I'm going to sample next what I'm going to try next so here you have some way to do modification and then you resample based on what are the best performing robots you do these two things sort of simultaneously and that gives you a new population of robots and so it's this procedure where you're constantly filtering on on the best robotss and maybe tweaking them a little bit so you get some exploration built in so um these are sometimes thought of as chromosomes some sort of binary string or some sort of floating Point string this is your design space will be translated into some sort of robots um you need this some sort of generation uh function I would say this approach is really also not that disimilar from particle filtering which sort of does an intelligent resampling of the particles to track objects so if you're familiar with this method um classical Vision approach um then this should also make sense most design algorithms will look like a b l optimization procedure where you are as as I just showed you choose some sort of morphology of your system you do some sort of control optimization um and that also has an in Loop that's simulation itself so that can get very very expensive um here's some work uh if you want to get started with playing around with something that I just showed on the previous slide um there's a paper called evoy and you can you can um basically try these these types of algorithms in real time um and it's it's actually pretty efficient some problems it solved some are I think unsolved now people are using this um as a uh as a as a toolbox to try to prototype different types of evary algorithms um it's a little bit Limited in resolution of course um but it's also a pretty fun sandbox and a and yeah so evolutionary algorithms can be pretty slow um but they are also pretty um quick to get started with so ultimately you know you see all these cool emergent behaviors come out of this and you really just want what I just showed you on the previous slides to be done faster and you'd probably be pretty happy um right if you got the cwl some stuff pretty quickly so of course using gradients is one way that we're going to use to speed this up and this is the last thing I'll talk about I think in the first half of today um so in uh chain Queen I showed you this slide earlier but now what if we also wanted to optimize over the material properties of the system so the material properties may be spatially varying of each particle so sort of think about how I'm going to design the skeleton of the system by programming the stiffness or the compressibility or the density of of every particle in the system and this is going to add way more variables than you just saw before but I want to highlight that this is can be done really really efficiently with gradients even though this is maybe a 100,000 design uh variable problem um you can see that over time these um the the system is going to be able to find more and more more highly performance uh design so these are distributions of different materials um and they also look a little bit biomimetic so um over time you'll see like for this quadruped example it want run forward and you'll sort of see areas that are redder that these are more incompressible areas um and also feet patterns that sort of look like they're stiffer at the push off point and softer where it's going to land so it it all also makes intuitive sense and I want to drive home that for every single design design variable that you add into the system performance tends to improve so it can cut through these really really high dimensional problems if you're adding stiffness as a design variable the performance will improve if you're getting pan ratio as a design variable performance will improve if you're adding density as a design variable performance will improve in when you search over control and design simultaneously um co-optimization always RNs out so the blue curves here you see that's pure control optimization and the gr curves at the top those are when everything is considered at once so even though this is high dimensional this is not a problem for the cursive dimensionality lastly um showing you how you can use trajectory optimization to also optimize over um rigid body uh parameters so I showed you earlier you have these problems where you want to start from some sort of starting location and you want to get to some sort of goal location and you need to search for what are going to be the controllers and the states that are going to the path that's going to get you there um so what are the control inputs that you need in order to solve this um I'm just going to go a little bit quickly here there's of course these complicated constraints that I say have to be um you know pilled out and and satisfied but you can actually reason about these really quickly with today's uh second order optimizers specifically sequential quadratic programming optimizers so that's fine and then the question becomes what if you also want to not only optimize over control but also over design variables and again one of the most beautiful things here that I can say is that it kind of just works so if you build build a differentiable system that can differentiate over your gradients of your of your um of your design variables and also you can compute the gradients of your uh control variables and as long as you make sure that you have contact constraints so that as you extend your system it doesn't penetrate the ground or do something that's crazy with its environment right if the legs get longer they should they should not go underground they should the body should move upward then you can get um optim co-optimization of these systems so give an example of this if you have this parameterized geometric system and your goal is you want it to move forward and you want the middle of its body to hit that um that red point it's too short first of all to do that so the parameters are going to have to change and you give it just some sort of initial guess as to what that trajectory is going to look like and this really matters very little honestly um then the system can automatically co-optimize over what are going to be the optimal physical parameters of the system as well as what is going to be the controller of the system as well as if you're really intelligent and you choose I want to um make my objective function minimize the max maximal torque you can sort of use that to choose what type of actuators you want so you can use your objective function that you care about to choose you know minimize the amount of energy that you actually needs to solve this so I'll just show one more quick little video of this um I think I showed this one already actually so maybe this isn't as exciting um you can bring this to the real world of course there's a gap some real gap of course mostly on friction parameters which is always the sticking point no pun intended if you want to build at-ats that have ridiculous Gates you can also do that so I can also solve if you need it to go uphill front leg shorter back legs longer so I can stand and there's been some work by colleagues on how do you um co-optimize these systems over um or actually do interactive search after you've actually optimize these systems um where as you change the the parameters interactively it will ALS the physical design param interactively it'll also search for what are going to be the how do you change the control parameters in order to compensate so I running a little bit behind I think I'm running about 10 minutes behind um I'm going to let this video play I know this is a whirlwind introduction um and I want to stop and see if there are any questions no yes the goal is is an image like I want to reach this image in the world in that case you would have to essentially have some sort of function that when it sees that image it provides some sort of signal mathematically that it has achieved its goal so if it sees the the image at the end of the like a maze or something like that it's doing a navigation task then you if you want to do like a reinforcement learning type approach you can give it a really large reward or if you're doing some other approach you know if you're doing more a penalty function type of way uh you can you know not penalize it for actually reaching the goal but in general you have to give it some sort of signal some sort of score about how good it's seeing that um that that image and then through optimization it will promote strategies that will see that image more often because that leads to a higher score and that's where the gradients come in whether you're doing a a reinforcement learning based approach or if you're doing a modelbased approach you'll use gradients either from data or from the model in order to push the parameters of the system to based on feedback of the simulation to see that image more often this is the high level um since I'm trying to give like a general overview today um I'm you know going obviously through this a little fast but for the actual methods um I can send you more information in terms of those tutorials for instance other questions part two okay um so this is where the new age stuff sort of comes in um and so I'll I might need to go a little fast at the or skip a section at the end we'll see how we're doing on time um but I want to at least uh bring to you guys uh the generative design side of things which is now growing in in popularity um and and also uh talk a little bit at least about fabrication and the s real Gap here so um of course if you want to use generative AI in order to design something then the obvious question is just ask chat GPT to do it for you as long as it's not having an outage here there was an outage yesterday okay um and so I did this of course I had to try this because what if I've been spending the last 10 years coming up with algorithms when chat PT can just solve it for me so I asked it to generate for me a a strawberry picking robot uh something that can just a soft robot that can grasp strawberries and it tried of course um so it generated me this thing that looks like an arm but actually is like a completely locked linkage so not very good um and then it also tried of course uh arms that just float in the air with two hands I guess this is not a good idea either and these are the most San examples because after here it's just some nightmare fuel this is not a strawberry picking robot this is this is just a spider strawberry and that's nightmare fuel as well okay that's a ton okay anyway so these are these are bad Solutions why does cha GPT come up with bad solutions to this this is not a hypothetical problem we did a big study where we tried to do this more with furniture and robots um we asked CH GPT to generate different designs that's a car don't get in that car the wheels are sideways uh please don't get in that car you put enough elbow grease you work with these LS yes you can get them to make cool things but in that process you also get a lot of generation that look like floing tabletops which are are just really frustrating the problem here is that there's no visual intelligence and there's no physical intelligence so there's no certificate of performance it doesn't know whether or not this is a good tabletop or not because it can't simulate and it doesn't know its function so simulation is so important um throughout this whole story whether it's for computing gradients or whether it's for just you know checking something's going to work in the real world if you're going to propose a lot of things you want to be able to check you know if they're going to work so um this's a recent project that we worked on in the soft robotic space called the fuse bot we go from specification of here's an objective function or you can do it from language um generate it use this to generate some diffused um 3D geometry and actuators and then simulate this and then use that as feedback in order to optimize the system and eventually we we brought some of these to the real world okay so this is an example of something that's that you know has pretty fast simulation um it's it's optimizing and now with with the ability to do some generative sampling now you can start to explore and this is really nice so I'm sure a lot of people in the room now are familiar with diffusion processes but the idea is you have some sort of data set you add a bunch of noise to it and then you get some sort of noisy version of your data sets and you want to come up with learn some sort of inverting function what if I want a cat how do I generate a cat from this noise what is this Den noising process that's a machine learning problem and it's also one that requires conditioning it's one that requires conditioning on saying I want a cat because from that noise you could really Denise it into anything get a dog or or or a baseball bat or anything right so in this project uh we use an off-the-shelf uh diffusion process called Point e um this is something that was uh released by open AI about a year and a half ago I think um for creating Point clouds I think it was one of the first 3D geometry um diffusion methods and now there's a ton of them um and we just use clustering based on the geometry to choose where to put the actuators those your actuators okay um and then for a control in this example to keep it a little simple um we're going to use open loop control so just your time varying signals normally in point e the way that this works is you have some sort of text input and then that's going to be diffused over time and that's going to involve some sort of Transformer in the loop ultimately this diffusion process has a lot of computations that go translate language to some sort of embedding we can work directly with that embedding we don't need to worry about the text here because we mostly want to see how will something that we search over is going to perform in simulation so this algorithm that I'm going to show you has really two main steps to it first you propose a robot and you evaluate how well it performs and you put that into a data set and you're going to try to move your data your your your embeddings that you're using in your diffusion process in order to come up with a high performance robot and second as you're doing this diffusion process you need some sort of feedback because you can't use language to say whether or not something is going to perform well you need simulation to say whether or not something's going to perform well and some sort of objective function so we'll use some simulation in the loop here in order to steer this highly performing robots um there's some there's some dirty work you have to do up front here um with which is to say that these representations in the middle if you want to simulate them not so good so we need to make them a little bit denser um so we have a process called robotization um which can be made differentiable which is just the main thing I want to note here because differentiability is going to be very key to this um and so we can converse convert the sparse Point Cloud into a denser one um and I said for actuators we're going to use K means clustering that's a discrete process but it turns that you can approximate the gradients very very accurately by assuming that the um importance of uh the the the actual simulation gradients with respect to the actuator labels is basically zero the geometry matters most of all and that also affects the Clusters themselves so okay so those two main steps that I talked about from some sort of embedding that we want to search over this is our search design space they're just floating Point numbers we want them to be diffused into a highly performant robot and also on the side we're going to have control inputs that I don't draw here but those are also going to have to be optimized so the way this works is we are going to propose some robotss and we're going to basically do something that's similar to reinforcement learning where we're going to generate some candidates we're going to see how well they're going to perform based on simulation and then we're going to throw them into a data set and we're going to keep the best ones and we're going to throw away the old ones so this is also sound a little similar to evolutionary algorithms and then in a learning phase we're going to take every once in a while take this data set and then we're going to do some sort of stochastic Radiance a s simple supervised learning problem to just you know for for objective function to move the embedding towards something that's going to perform well as we do the diffusion process itself like I said we need to condition it on something usually with images use clip here we're going to not be able to use that we're going to use physics instead and so our Sim our our diffus our our our biasing method our conditioning method here is actually going to be exactly the gradients from simulation with respect to control and with respect to uh uh design variables and so this is the a nice way to link what I just talked about earlier about differentiable simulation optimization and generative processes Al together and shows you how you can use this to really efficiently design robots and soft robots in simulation that's the whole algorithm and it can just by changing the objective function and a little bit about the environment you can get a whole bunch of different uh cool different types of diverse outputs so here's sort of the zoo uh if you care about I have an objective function which says please balance on the on the top of this of this pole or land on that pylon so it just looks at you know I want the distance at the end point to be basically on top of that or at the bottom there it'll generate things that can land the robot in those locations um if you say I want the robot to move to as far to the left as possible in a certain amount of time you'll get different types of locom modity robots with different types of behaviors depending on how you set up your environment and if you care about you know if you set your objective to be something about something about the environment like certain objects and where they end up at the end of the simulation then you can get some sorts of different types of manipulators and this diffusion based process which includes some exploration here um will also out beat out a lot of the Baseline methods um almost all of these examples were the diffuse bot method was better and also intuitively it makes a lot more sense so you'll see you know on the right column here these things look like these that can solve the problems that you care about that thing on the bottom left there that's a gripper that doesn't look like it's going to very very good at gripping um you know that block of cheese is not going to land very well on the pylon and so on so this makes a lot of sense and if you want to condition this with Aesthetics that is also possible because you can add back in your clip um um objectives as well so if you want it to look more like a ball then you can ask it that and you can trade off how much you care about the Aesthetics of what something looks like uh versus how it well it performs or you can say I want it to be wider or thicker and of course um I showed you this earlier but for at least simple Dynamics you can bring this to real world for more complicated dynamical systems there's a big simulator mismatch okay I want to get through one more example of uh ways you can use generative AI to do design um because this is really important for the perceptual side of things so this is some uh one the one thing I didn't show you here is what if you want to design your sensors of your system I showed you open loop control but if we're doing close loop control you also need some way in order to have sensors placed down on the system and used as feedback so this is especially important in the real world right because you want proprioception you don't want you want something to understand where it where where it is uh how its body is is configured and you also ideally wanted to know be able to do navigation as well right um and you can't motion capture the whole world you can't you know set up Vicon systems in the world or or or or external third person camera viewpoints so we need things that can sense within a robot how it's going to move what its joints joint angles are going to be for rigid robots that's often encoders for soft robots for squishier things that's a little bit requires a little bit more type of um uh strain type sensors so where do we put these strain sensors one of the things that we found out here is that humans are really bad at this and um and uh computers are very good at this and I'll get into that a little bit later so given some sort of task that you care about such as I have a gripper and I want to identify this shape and also you have the opportunity to put down Sensors how do you simultaneously solve this classification task and the the sensor placement task at the same time so this is another different type of code design problem that I showed you before with all these Locomotion examples this is more of a of a of of a reasoning um one and this ultimately is going to come down to a clever representation that is going to generalize over lots of different morphologies and the way we do this is that we have an architecture where based where we basically have a neural network architecture um that works on point clouds it's not super fancy but for the input layer of the system we allow it to change whether or not it's going to have a strain sensor at any given location or not so each input to the system represents a single particles strain in its um in its vertical Direction and its horizontal Direction so how much it's stretching and then what we're going to compute here is we're going to solve for here is weights of in the network that say how well the um that that basically do the classification task and input weights that are going to be forced most of them sparsely to zero and by forcing those weights to zero that means that we're going to ignore a lot of the inputs to the system and by choosing which inputs we ignore of the system that chooses tells you where where you should place your sensors in your system so this is one clever model architecture that you can now train with a supervisor uh learning setting and it's going to automatically for you simultaneously optimize and solve for where you should put your sensors at the same time that it learns how to solve the test um this is uh based on something that is going to have a penalty function um and a representation in the neurons that is going to ultimately Force the sensor distribution to either exist or not exist and this process is going to be done by throughout the learning process you're going to stochastically generatively sample a whole bunch of different sensor Arrangements um so at the end of the day if you want to learn something that is also going to be generative and propose different types of sensor uh you know placements you can also soften the necessity that it goes to one or zero and sometimes we found some solutions where some of the the sensors are 5050 um so either solution would probably work pretty well so we have this one penalty function that you have to add on to your typical learning process to force most of these to zero um and this you know our our training algorithm is just build a data set compute the loss that I just showed you and then optimize using back propagation through the model okay so just to show you guys a few different tasks um I showed you this if you want to determine are we Gras what shape are we grasping is it a circle is it a triangle is it a diamond or is it nothing then you you can get really good performance with just five sensors so this is sort of similar to the propr uh the the the the few um um oh my gosh I'm losing the name of this the the the the photo photo receptor case um here this is sort of the equivalent with just a few tactile sensors you can get really good classification as well but you have to do this co-learning process um if you and you need to the the positioning really matters of these of these sensors if you try to do at random you'll find that a lot of times that the the random configuration is actually a very bad performance um the converge sensors placements really make sense they go where the system moves most dynamically and where it's going to interact most with the environment and this is what humans get wrong because they tend to in our experiments choose just places that are interesting geometrically even if those places don't actually move very much um so we've done a lot of reconstruction of these systems and you can see sorry for these different types of trajectories then um oops cooperating see I can do this yeah so these are the different types of reconstructions ground Truth at the top and the reconstruction at the bottom and the big circles you see that's where the sensor placement is the color Maps represent the velocities as well so we are able to reconstruct not only the positions of these systems pretty well but also the velocities pretty well um and we can also do this in three dimensions so sort of you're getting the orthographic views of this thing and the left and the right represent you know what matches and what doesn't um we can outperform human baselines of course I mentioned this earlier um quite significantly across the board and um yeah I guess I'll I'll skip over this for now um one other uh method which I don't have time to talk about today uh we have one for rigid robotics as well if this is more your jam and this is sort of very similar to diffuse bot but you're going to be sampling from a a grammar building up these rid robots evaluating their performance and then training a model based on that so if you're interested in playing around with this as well um we also set up a collab for this as well um so feel free to try uh playing around with that at home it's a little slow on collab this is CPU core heavy and collab doesn't give you a lot of CPU course so the code is also open source and you can try this out yourself um takes about a thousand simulations um but each one of the or Generations I should say each one of those Generations only has one robot that it simulates but the simulation itself is expensive because it's doing model predictive control so that's the the tie also look model predictive control all right I only have about two minutes left um and so uh to wrap this up um I'm going to go really quickly I think through the fabrication and um and the considerations on the simal side of things I'm going to skip some of the method slides um if people want I I think we'll disseminate these later or you can reach out to me and I'm happy to send over some slides but I really want to talk at least about what the considerations are as you want to try to bring these things to the physical world so I'll I'll focus on those slides here um one of the challenges that you have is that when you actually want to physically fabricate these systems um you have this problem of well this might require crazy material gradients every single droplet has a different material how am I going to fabricate something that's made out of thousands of different materials right that's really really difficult to do and so what you need is some other translation method where you can in order to realize your body you have some sort of method that maybe abstracts us away it simplifies it it comes up with some sort of representation that is actually fabricable um fabricable is the key word there because uh one way we we've been exploring this has been in machine knitting and making textile based robots um so just to show this really quickly um by having different swatches in these different um areas you can actually program what is going to be the stiffness with different micr structures so even though you can only have a single um uh maybe discrete materials that you're working with in this case different stitches have different stiffnesses then when you combine them into um uh together you can make micro structures that have different types of emergent behaviors so it doesn't allow you to necessarily program it at the very very small level what every single material is but at the larger level you're able to realize a large swath of of materials themselves um and these are examples of what the different swatches we played around with looked like some are made with elastic materials some are not and some are made with conductive materials so we could also realize the types of sensing strain sensing that I talked about before and I'll show um for the sake of um demonstration at least cannot play media and that's St never fails show this video nope all right first the light demo I'll show this first video which was going to play anyway so so this is um some 's SK to the end time um fabricator robot um it's done with Inflatables but you'll see here that there's different type of stitching here and there um and based on what type of material you put down this allows it to actually bend in a certain direction when it's inflated um okay I think videos are going to be a little bit tricky uh oh hell yeah can not play media I don't know when you get through 30 videos it just stops playing media for some reason power figure okay um the other problem that you have is not only a problem of physically realizing these robots but also when you build them are they actually going to match the real behavior um there's a whole bunch of different ways just to give a quick overview of how to try to make your systems more accurate so they more Faithfully model the world um aside from just better modeling which is always a trade-off as I mentioned earlier you can do things like system identification to come with better parameters of your system that are more accurate when brought to the real world based on real world video data that you have this will get you only so far um so there are other approaches that people have also come up with more recently things like domain randomization make sure that you're robust to problems um in the real world um adapt your system at least the control side you can adapt uh it's harder to adapt the physical designs but you can adapt them after you've built them you can modify the controls with some online reinforcement learning and you can also add learning to your physics itself so you can use real world data to learn new physics engines that are able to um or hybrid simulations which are able to um uh have the best of both worlds of having an analytical simulator and also some learned fine-tuning um and again this is an inverse problem so uh I'm going to skip to the end here now um and I really just want to end with a few notes here um about what are the important problems that are that are left here uh open and what we should be focusing on as a community going forward um you know we want to build these ecosystems where you design something you're able to fabricate it you know that it's going to work when you bring you to the real world and the question now is going to be how are we going to use the data from The Real World in order to really inform the design process itself I think this is extremely important um and that's going to require fabrication Cycles where you can propose a design through some sort of optimization fabricate it really quickly um and then test it and use that as feedback in the real in the in the simulator itself to improve the models um I'll skip over um some other things but I really want to um uh say that the the opportunity that we have here is that uh this will lead to faster design processes um what ability to do more exploration if we get this right and come up with new ideas that we haven't considered before um we'll save Resources by reducing the number of failed prototypes that we have um it will democratize uh creation because people without an experience in mechanical engineering will have some ability to do some work there and mechanical engineers should be able to go faster with more computational tools and finally I'm really hopeful that this will also allow uh computers to run their generative processes and sort of learn from their own experiences and maybe be able to to design on their own um open hard problems of course how do you do some to real not just for control but also something where s u designs are going to transfer for any robot that you consider um how do you make sure that when you transfer something to the real world um that if your problem specifications change that it's actually going to also um you know uh be able to adapt to the real world and how are we going to reason about all these different systems simultaneously I showed you examples that maybe H three of these like actuation planning topology materials sensing but not all of these at once and so that algorithm that can reason about all these different aspects of a physical system much like biology that still sort of eludes us right now um and maybe foundational models will be the way to do that I don't know but I I would not be surprised if by combining all these different types of generative Technologies together um that will build build systems that are able to reason about the vision the physics the text and everything and be able to think about how things are going to work when they're built all right I'm going to skip over this and I think that that should bring me to the end of today um so sorry that was a little bit extended I think one or two more minutes over um but happy to take questions uh maybe one or two quick ones um and also happy to talk more offline I think we're we have only one more session left today and then and then um and then you know people can feel free to mingle so thank you that set for the neural concept um there any questions

