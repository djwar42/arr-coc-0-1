---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=W3fiMBUF3ng"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:02.280Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=W3fiMBUF3ng

ab41de2e-8285-4d04-9585-60cd26d6c7b4

2025-10-28 https://www.youtube.com/watch?v=W3fiMBUF3ng

1724ef4f-5958-4bb4-af3a-dbf77317d884

https://www.youtube.com/watch?v=W3fiMBUF3ng

W3fiMBUF3ng

## ComputerVisionFoundation Videos

all right great hello everyone uh good morning and welcome to the first workshop on AI for 3D generation I'll keep this pretty short I think the speakers will be more interesting than me just wanted to give some context for the day and uh go through a little bit of logistics um so first of all uh though this is really the first iteration of AI for 3D generation Workshop um it's actually the second iteration in spirit I guess we had this AI for 3D content generation workshop last year at iccv 2023 uh so you can go look at that for for more similar content and first I wanted to thank all the organizers for putting in the hard work to um to get this ready today so uh a special shout out to despa who is joining us remotely today but did a lot of the heavy lifting to really make this Workshop possible um and in person today we have George me Davis and amlan so uh if you need anything during the workshop or have any issues feel free to grab one of and we can figure it out um also wanted to thank the program committee that was able to review and go through all of the papers submitted and make sure we have a really high quality program um especially for our poster session later this afternoon so just to give a little bit of context of what we wanted to get out of this Workshop uh this is really motivated by this ubiquitous need for 3D content uh and this comes across many modern applications we have simulation especially in robotics like autonomous vehicles in games films virtual and augmented reality uh there's this requirement for really highquality 3D scenes to build this virtual world we need 3D objects to place in the world and then humans of course to populate uh and dynamically move and interact with the objects even uh and we also have many applications that Focus specifically on digital humans uh where we need these realistic and interactive characters that may may interact with the user um now the big problem here is this is very timec consuming and costly to to build all of this content manually so what we'd like to explore in this workshop and discuss is recent approaches and possible future directions for generating really realistic high quality 3D data and content at scale um because we anticipate that having these generative models using these AI models um that can reliably synthesize meaningful 3 content and be controlled as well robustly uh uh controlled by artists and easily work into their their workflow this will really revolutionize uh how artists and these pipelines these content pipelines will work so in a little more detail the kind of topics we'd like you know to discuss today through our speakers are what are the best representation to get 3D objects that are high quality in terms of texture and geometry as well but also have this intuitive and robust control that they can be easily integrated into artist workflows uh we also need realistic humans to to interact with those objects and this all needs to go in some 3D environment um where we can hopefully manipulate the appearance scene elements as well as their spatial composition so how these how all of these things kind of come together um and then humans moving around in this scene need to move dynamically plausibly and interact with the objects in scene and then also at a higher level uh there's a lot of ethical implications and ethical challenges from how an artist kind of works into these workflows with the a AI models um and how uh you know privacy uh likeness these kind of things work into it so hopefully we'll discuss some of that today as well so to talk about these topics we have a really Stellar lineup of keynote speakers that you'll see throughout the day both from industry and Academia as well to give us their perspective and detail the latest work uh in this area we also have several Spotlight Spotlight talks which will be more of early career researchers that have already contributed impactful work to 3D generation and content creation and finally in the afternoon uh in the same building I think we'll have this poster session with 28 different posters that explore a really wide range of Topics in this area uh and that'll be from 3:30 to 4:30 this afternoon and we're at a specific poster slot I think all of the posters for the for the workshop are in the same area so we're uh poster 178 through 205 and you should have um if you're an author you should have received an assignment already if not feel free to let us know we can tell you where to be so yeah in conclusion um here's the whole schedule for the day this morning we have a series of Keynotes and spotlights and then after lunch have more talks followed by the poster session but I think it's a very exciting lineup and we're looking forward to the day thank you so our first speaker will be Andrea videli and George will come up to introduce him all right uh I don't think there would be a better way to start our Workshop than with Andrea vdal so Andrea is a professor at Oxford University and a research scientist at meta uh his work has been recognized with the best paper award at uh cvpr 2020 and with uh two more best paper awards at from bmvc and uh for all of us in computer vision we are grateful for Andrea because he's the uh the OG open source software contributor in our field like all the way back to uh the V fit library and uh Andrea has made important contributions in unsupervised 3D reconstruction and in generative uh reconstructions of 3D objects and uh um and for the dynamic scenes so we're very excited to start uh I schedule with Andrea and Andrea you can take it away all right great thank you okay let's see what this works maybe thinking about it there you go okay excellent okay is that pleasure uh to be here and thank you very much for introduction also let me see whether my mouse works at all hard to say hope you don't see it okay so I will not be able to indicate unfortunately but I'll try to do my best anyway all right so what I want to talk to you today is uh some progress we have made in three gen AI in the past year or so both at Oxford University as well as in meta all right now before uh so the topic we're going to talk about today is mostly text 3D all right so what I mean by that is our task is given a Texel prompt like Viking ax is to come up with a corresponding 3D model of it all right so the input is text and the output is a 3D output there is a related problem that we're going to use a lot as well which is image to 3D so instead of starting from text we may also start from an image and then from that we can produce CD object all right and one thing you might want to use this to solve the first problem is to just to take the text I'm not in a zoom call okay we have our first technical problem just a second it seems it looks like I need to connect with zoom call here um to which address did you send it in the calendar I don't have it sorry guys um can you send it to vialat mata.com apologies so for the next speaker make sure you have zoom installed and running apparently that's a thing you need to do which I didn't realize not yet is B death matter right the work yeah there you go okay let's try this one so should just share screen right and join us with audio okay all right here we are can you see up okay apologies for that is the audio fine everything's fine on Zoom okay so I suppose maybe I should uh bring it up a few slides and actually close this and that right yeah but now you do okay all right apologies about that let me start again um so I'm going to talk to you about some progress we have made in 3D generative Ai and what I mean by that is mostly text to 3D so starting from a texal prompt like a viking ax we want to generate a 3D object or uh there is related problem to do which is image 3D so in this case we start instead from an image and we want we want also to produce a 3D alting response and of course you can see how you may be able to use the second problem to solve the first as long as you have a text based image generator okay if you think about it for a second so obviously this is a challenging problem because it's very ambiguous so you if only have an image or even worse just a bit of text then there are many different ways you can which you can associate an object to that so this task this reconstruction task if you like is really a sampling task it's stochastic it's not deterministic so what we want to do is to be able to sample a posterior distribution so let let's call our 3D model M the image I not so what we want to do is to draw a sample from a conditional distribution P of M given I not all right now this is a problem that we know how to solve quite well now for a certain class of problems in particular image generation and for that typically what we use today are diff Fusion based models so just as a quick refresher the way this works is you start from an image that you want to generate and I want to train a model which is able to reproduce that started from text so the way you do that you do that you take the image you apply some Gan noise to that so get a image that has some of the information destroyed and uh the training task is set is to learn a network a generator that takes a nice version of the image and produces back the clean version of that all right and not only that but you also condition this generator with the additional information you have for example the Tex prompt now why would do we do this well one reason is that this is really really very good at learning on a large scale so you can learn this from billions of images which are weekly structured all you need is are some captions and these captions you can also get automatically using some captioning system now this is great because you can essentially go on the internet and learn from say a billion images and this is really also crucial because want to be able to interpret any image or any prompt which is given as input and still produce a reasonable result so we want our methods to be open-ended and this is difficult very difficult to to achieve if your training set is not large enough now this is great and in principle you could take that and extend it to 3D all you need all you would know all you would need is a equivalently large collection of 3D objects with the texal Thum to maybe image prompt and this is where things don't go so well for us the problem is that we do not actually have such a large collection of 3D objects so what are we going to do so we have a technique that can scale up to billions of images but we do not have billions of 3D objects to train this from so what we usually do is to copt a pre-train image generator right so we start from an image generator say stable diffusion for example or emo if you're in meta and you f tune it in such a way that instead of starting from a prompt maybe that start from a Tex prompt maybe that starts from an image uh Point change a camera so I have I not and see the camera and condition on that you generate a new image or new view of your object from a different Viewpoint so the reason why this is useful is that you can start doing that from a model which has been primed or or trained from a billion images and now we relatively modest amount of 3D data you can condition it or tune it to be able to solve this new view synthesis task once you have done once once you have done so you can start from an imj not you can apply your model to generate a bunch of other images or other views of your object and now once you have the new views and so far there is no 3D as such yet well you can run some reconstruction algorithm that given the the views can give you the corresponding to the object now this is a very general blue blueprint and if you look at the literature in the past year or so many many different methods have used this so just to repeat so this happens in two stages if you like first we start from our prompt single image or text and the first multiv viw generator produces a set of new views of the object and then there is a second Network or maybe just some handcrafted mechanism that takes that and reconstructs it through the object and these two things can be run in sequence or they can interact somehow in a loop that depends exactly how you do this okay so that's a general way we're going to attach approach this problem now you may wonder what are the variables here that make this harder or easier so a crucial one is how consistent is this three this 2D model that generates new views of your object so ideally what you would like to do is to be able to sample n different views i1 I2 I in condition on The View you have I not all right but because of limitations on how well you can train this joint distribution this joint sampler what you end up doing is sampling something which is in between that the joint and the product of marginals pi1 pin that would also be not condition even all right so the thing that you can actually sample depending on how well you train your multivision rator is going to sit in between this joint distribution condition on prompt to this product of marginals now the least consistent you are uh the more you approach the product of marginals the harder it is to extract a 3D object from the imagees your sample so in order to reconcile this inconsistency you have options so one thing is to use something like score distillation sampling in which you try to iteratively achieve the sort of multiview consistency that the 2D model doesn't quite give you or you can try to work with images which are not perfect but you're training a network that can reconstructed through the object despite the Noise Okay so these two options but it's clear that the better your image generator the more consistent it is the easier is your life right so the the simpler it is to actually sample a consistent views and therefore also to reconstruct the object so now here going to give you some examples of models that appear in the past year or so that that strike a different tradeoff into two these two axes multiv consist consistency and reconstruction effort okay so the most simple is just you have a distribution that doesn't know anything about uh multi multiple views so and that would be the original Dream Fusion now this means that you need really this works but you really need to pay a lot of effort in order to be able to reconstruct a sensible 3 the object from that distribution then you can have something like real fusion which is an extension we did soon after which is at least condition on one real image and that is a little bit better but still is not mul consistent then there are there have been other works like 3dm and 0123 that have taken this idea of generating multiple views condition on a camera or at least a new view condition on a camera that already give you better view consistency and the dream that gives you several views at the same time and in the process of building these networks we have learn some best practices and here I'm going to give you a few so usually you start from a diffusion model which is a unet and what they want to is to Denise jointly more than one image more more than one view so you can set up this as a several denoising processes that happen in parallel but the first thing we're going to do is we're going to extend this to take as input also out a conditioning image and conditioning Viewpoint and the image we're going to put it out as input to the network a additional channel to your noise if you like but we also condition that we also using going to use that as a conditioning for all the layers using some cross subtension all right then also the other thing we're going to do is we're going to set up some cross attention layers between the different parallel streams in such a way that there is some communication between the views and this is NE necessary to encourage the model to make it possible to actually learn this consistency this is well known well for a value well known because this has been happening for about 24 months all right maybe even 12 months okay one or two years uh more recently we have a paper in the CPR where we have found a few more best practices one is to use a different way of conditioning the camera which is taken from light field networks which is used is to use PL array modulation and that gives you better control of the Viewpoint and that actually really does help in achieving better and more consistent results and the other thing you can do is if you want to get VI which are more consistent you can use a very simple trick which which is to use the same noise the same gash noise applied to all the different views because this no in networ are continuous functions at the end of the day then this tends to produce results which are already more consistent between different views when it comes to hallucinating this different aspects of the object and if you do that you can get results like those where this this is called free 3D because there is no 3D model underneath this is just a 2d model which is been fine tuned to during different views and you can see that yes it seems like it looks like it's 3D but it really is not okay so this is 33d and is a little bit better on this axis multiv consistency reconstruction effort tradeoff but then if you want to get even better what you can do is start not from a image generator but start from a video genor generator so let's say this is what we have done in im3d so this is a version of IMU video or a modeling the Emu class that was trained to generate multiple viewer of an object as if this was a little video turn turntable like video so the camera goes around it's almost like get a little animation as you can see here see here on the slides it makes sense that if you were to start from a video generation network instead of image generator Network then this network would actually have a already some comp some experience or some understanding what it means to change Viewpoint and of course there are you know more recent model like soda that makes that quite apparent so the idea here is that if you start by fine-tuning video generator for this solving this task of generating your views then you can get better results more consistent results to points where you can be quite lazy in the way you actually do the do the 3D reconstruction in the end so here we use gash and splatting so everybody knows about that now so you represents your 3D scene as a collection of color 3D gions all right so that's your scene of course this gashi ples are much smaller than that so you can think of those as threedimensional pixels and then we have a differentiable rendering function that takes this collection lotions G takes the camera view point CI and then can generate a corresponding views I I hat all right all you do is you take that compare that to the U you have as input could be gr Tru that could be generated in our case and then you just optimize your 3D gash collection to fit that those views and off you get some 3D model it's a bit more complicated in practice but that's at the high level is the idea all right and these are the kind of results you can get in this case so here we start from videos that are generated using this fine tuned version of Me video and because these are actually quite consistent then we can just apply 3D G and splatting to get corresponding 3D models which looks quite look quite reasonable now if you look carefully you will see that the 3D models are not completely perfect and the reason that the multiv consistency of this video model is still not completely perfect but is already quite good all right and the advantage is that you don't need to do any score decision sampling this is really really very efficient so in about one or two one couple of minutes basically you can get your problem solved now there is another way of doing this which is to use maybe less views a few views or maybe less consistent views and train a better gener 3D Constructor so an example here would be largely construction models by the folks in lby these are great uh but you know they can they're really powerful they can does do a very good job of reconstructing your object from inconsistent views or maybe few views but the issue with that is that it takes forever to train these models it takes 385 GPU days to train one of those okay this is fine if you're Adobe if you're in meta but if you're in Oxford this is actually quite annoying because you don't have that kind of GPU power so what we did in in Oxford is uh was to try to develop methods for 3D construction for few views that were actually much more efficient and to do that we introduce the splatter image so this is also presented here at CPR and this a very basic or simple meod that actually is surprisingly efficient and effective so what we do here is we use G slatting but we train an Network to this construction quite quite quite fast so we start from RGB image then we have some something like a unit that takes that and produces another image as output but this is not an RGB image now each pixel has attached to that a bunch of channels which represent corresponding 3D gions okay this is a pixel oriented version of 3D G splatting now to just to clarify what this actually means so here's your camera so you have a camera Center you have the image pixels then what we do is with this unit we map that to each pixel to a set of parameters Sigma mu capital Sigma RGB plus and Delta so what these are are the opacity of the Gan the mean the location of the gash into three dimensional space the shape of the Gan that would be capital Sigma RGB plus would be the RGB color of the Gan or maybe spherical harmonics if you want to have that so you can get view dependent colors and then you also get an offr displacement which is actually quite crucial so what do we mean by that so we get depth so the primarily the gash we get is some distance along the ray all right some depth along the ray to be more precise but we allow the network to displace the Gan of the ray that we get okay so it means that the Gan does not actually have to sit on the ray this actually crucial now of course this happens for all the pixels in parallel so you get this collection of GS that represent or approximate your object so you see here applied to the quot data set for the class steady Bears so to the left of each pair here you see the input image and to the right you see the 3D reconstruction we get this was obtained using this unet estimating one gash per pixel and you see something quite striking that you can reconstruct the back as well of the object not just the front so not only the pixels which are visible or the points which are visible are reconstructed but also the ones which are not visible and this doesn't work just for one class at a time eventually we train it also on on and then here you can see it applied to Google scan objects so you transfer between data sets and you don't need to make too many assumptions what kind of objects you're looking at now you may wonder how how can this work because at the end of the day this is very much pixel oriented so here you see some bit of analysis of what the spatter image actually does so you see the input image to the left sorry for the low resolution that's was a low resolution experience with did here um then you get the first layer you see there is the opacity so you see that where is Black then that gion is s off so it doesn't matter that you have more gas as you need the networ can decide just throw some some of those off if you don't need them but then you also see something striking so you see almost like a doubling effect where certain features of the object are replicated actually that's your model using sounded gashes that would otherwise belong to the background try to approximate parts of the object that are not visible in the image so you see there at the top for the wheel of the car so you we're modeling the wheel which is not visible that's indicated by the arrow and you see that the displacement vectors there actually make it possible to take that gash on the Ray and displace it such that it falls in the right place to approximate that part of the car all right and the same happens there for the chair where we do not see one of the legs but these pixels that W around the leg are enough to be able to reconstruct that part which would would be otherwise missing and the point of this is that this gives you a construction quality which is very competitive with state-ofthe-art but it is much faster so you can render very fast okay this is Garian splatting so you can render this as basically 6 600 FPS you don't really care about that too much though what you care about is that the Reconstruction is also fast so this reconstructs at about 40 FPS but most importantly especially if you're not in a big company that has thousands of gpus this trains fast so this was trained on seven GPU days it even so it was competitive with state-ofthe-art reconstruction reconstruction quality more recently we have extended that to scenes so of course you don't need to just do 3D objects you can also do 3D scenes so here we go from a single image to a gash spot of the scene we call that flash 3D this is just an archive as of few days ago and I don't really have much time to discuss that in detail but this is suffice to say that one striking thing we were able to get here is by starting from a stateoftheart monoc depth estimator Network we could fine-tune that if you like or extend that and fune it to get state-ofthe-art newv synthesis for scenes okay again very fast we can train that with University level resources just a couple of gpus for a couple of days and we get something that can get the art results say on real estate 10K for this is this newv synthesis tasks but then you can take that model apply to tonit which is a very different data set and you still get State ofthe art results or at least comparable to State ofthe art results okay so this is really powerful and part of the reasons that you can really start from a very good monocular depth estimator and extended to be able to do that okay so that was for the first part of the talk in the second part of the talk we would like to dive into connected but slight different topic and that's meta 3D gen so this is something we didn't show before so we're introducing this at CPR here today and this is about a year of work of us trying to tackle the text3d problem in meta all right so this is not University resources uh this is what you get if you in meta all right so uh the task we set out to solve here was to try to ask yourself what does it take to get Pro level 3D Generation all right and going to say that we get we got there but our goal was to try to get as CL as possible to that if you think about regeneration is actually really hard because it's not just about generating a bunch of pixels as like an image generation or video generation you need to think about a bunch of factors which are actually really hard to Chi together so you need to have high quality shape high quality textures obviously high quality control again that's in common with the image generation or video generation then you also need to get highquality materials maybe you want to get part oriented generation so you get a model which is De composable if want to get very good UV topology and quality you want to get very high quality mesh topology you want to get digging and so on and so forth and all that all of these are things that professional 3 artist require you to do before they go and like yes we can use this in a game all right now here we're going to focus on these four characteristic but of course the rest is something else you need to work on all right so we're going to focus on quality as well as materials so let me talk about a little bit about mat first so what we do here is to use physically based rendering or PBR so what this means is that we don't estimate just the RGB color or shapes but we estimate the albo which more correctly should be called base colors for the family of brbf B by directional reflex and subtitution functions we use here for modeling anyway so that's albo the inic color of your object base color more correct we also estimate roughness and the metalness okay so this would be a pug made of metal so this a dog which is supposed to be made of metal so you see that tness is really high there okay so we want to have that kind of control in our textile prompt and we want to be able to generate those properties so once you have that what you can do is you can relight your object so you can see here the same 3D object that has been subjected subjected to different envir of illuminations you can see that the response is actually realistic the reason is that of course we're using physically based rendering to get there and this is possible because the model is estimating the necessary parameters uh while generating the object so here you can see an examp some examples of control so we have a cat made of shiny plastic so you can see here that um the r roughness is really low that's because the object is shiny the metalness is also really low because this is not metal it's plastic right this color in this case was decided to be black except for the eyes which are yellow and see here you see here on the left how this is being shaded eventually uh here we have a cat which is made of shiny silver so now you can see that still Rel is still low but the metalness is now High because well it's silver and this is a cat made of Rock all right so this is as high roughness and low metalness obviously and finally a cut made of rusted iron so this has both High roughness and high metalness and you can see that this makes sense so once you have this control then the the Shaded object actually looks the way it's supposed to look and the question is how are we going to get there so here is our usual blueprint so we have a prompt a k a cat made of rock we have a multiv generator and you have a reconstructor network now our reconstructor network is supposed to be able to reconstruct also albo roughness and metalness this these are things we didn't have before all right so how would we get there so we have two options here one would be to to try to take our multi generator and extend it to generate bito roughness and metalness which makes it quite easy for the reconstructor to extract those channels so here I say that the the arrow that goes from the 2D views to the 3D reconstruction is deterministic it's a bit of a lie it's almost deterministic but there is very little ambiguity because you are providing enough information from with your 2D model all right but the 2D model on the other hand has a very high St statistical gap between natural images remember that this is trained from say emo images so this didn't see metalness or roughness during training from a billion images so now if you add them when you want to learn to to generate those kind that kind of data when you want to get this to work for 3D then that's a big statistical Gap and that's actually quite hard for the model to learn that without destroying its ability to generate nice looking images so the other option is you don't care so you just generate standard shaded image with your 2D model and then you upload everything else to your reconstructor network basically you task it with constructing metalness and roughness automatically as well as inic image but now that's hard because this task is high ambigous so it's very hard to go from a shaded image to the individual components so this is a nondeterministic mapping so we thought about this for a while and then eventually come up with this which was the best solution for us so what we do is we task the multi generator producing two kind of channels the Shaded Channel as usual and the albo channel which is just that where the shading removed okay but we do not try right away to generate metalness or roughness and instead we task the reconstructor network to do that reconstruction for us starting from metalness sorry starting from albo and the Shaded version of your object now this is fine or better for us because the statistical Gap is smaller in term of multi generation compared to your training data which is billions of Internet images but also the Reconstruction task is deterministic enough okay it's not deterministic again but is close enough to be deterministic that you can get result good results so here you see the difference so this is a shaded reconstruction for views all right that's what we do and this is albo so again shaded albo shaded albo they look very similar they are but the Beed version well doesn't have to have the shading all right so here's an example what you get if you try to do the the composition using a reconstructor which only uses shaded multiv inputs as input okay and you can see that yeah you can do you can do a good reconstruction of the Shaded version maybe also the albo but the metalness and roughness are off so here you should see that the bear has certain part of the AR armor that are are supposed to to be metallic but now all the object is actually turn turns out to be metallic in this case so that's wrong but if you put as input these two versions of the the bear the Shaded as well as the albo then the three Constructor is able to do a much better job and figure out what is supposed to be metallic and what is not what is supposed not to be metallic okay so that's one thing we did another one thing we did was to work on the Reconstruction architecture so you may want so you go from for views and then you have a choice of constructing a threedimensional representation on that that could be a neural field could be maybe using some volumetric representation triplane or it could be G splatting what are you going to do so if you if you want to follow the trend maybe you would use gash splatting but I would say that there are pros and cons in both choices so gash splatting is really fast as we have seen before but when it comes to extracting nice meshes nice 3D surfaces it's actually not that great I know that there are works and we're improving that aspect of splatting as well but for us it wasn't wasn't that great okay and also is a bit more prone to local Optima when you train this so the alternative is instead of to train um a gan SPL sorry to train to generate a Nerf but that tends to have a very large memory footprint which makes it very very hard to use pixel level losses ring all the pixels every time can take gigabytes of memory and that really doesn't B well when you want to have large batch large batch size when you train your The Constructor on the other hand it gives you much nicer three 3 surfaces when you do the rec construction from that and also better convergence we found so what we have what so again this is Nerf so Nerf needs to take this samples along each way to do the rendering and if you implement that intive way when you do back propagation then all those samples get materialized in the mem your GPU with corresponding bu up calculations attached to them if you like or intermediate calculations that can really blow up the amount of memory you need to do to be able to do that so last year we introduced this additional module that is called light plan kernels that Implement effectively rendering using Nerf in a way which is has a memory efficiency which is comparable to gash and splatting all right and in a certain sense uses some of the same tricks although there was concurrent work but at the end of the day there are some tricks you can do to make back propagation memory efficient while rendering high resolution images using these models and now this is also possible with Nerf you use this kernels which should do the calculations in a sort of Fus manner okay and here you see the difference on a log plot so the blue line is light lightly kernels see the image size on the x-axis and the amount L GPU memory which is being used up on the y axis so this is really great because now we can use really Nerf in place of gash and splatting and still be mem efficient so we can learn with high resolution images and pixel level losses which we couldn't do otherwise another thing we did was to switch Nerf to use SDF functions again implementing these kernels uh the Vol SDF Vol the SF version of SDF that allows you to render as if that was radi field with an opacity function and you can see here the difference that you get in term of quality of the 3D Shape which is reconstructed so here put everything together you get your three Construction um you fit your uh mesh very simply using marching cubes marching data leer I should say you get a u map using X Atlas that's also something that should be improved in the future but for now we just use that and you get a texture another thing we did was to work on a texture quality so here you can see four images in this case of a ground toot object popcorn box and you can see that our network is able to reconstruct UV texture map once you have the UV mapping then you can sample the 3D Volume to get that and you can see that you get a text which is okay but not perfect so you can see it's a bit blurry so this was obtained by taking the mesh and sampling the 3D Volume to determine the color of each UV uh element in texture space an alternative way of doing that is to go back to your views which are actually sharp and back project them onto view space but this will give you something which is incomplete and also not completely consistent between views because it views yes will give you sharp information but not every pixel is visible in every View and also you might have inconsistency which are due to the fact that those images in practice will be generated so they will not be perfect so what we did was to introduce another Network that takes the first kind of texture and the for for the projections and using a Transformer reasons about which pixels are essentially which places should be used to determine each point or the U texter to do best the best job possible and also fixes some of the mistakes that you might be able May doing might be doing that case and here you can see the difference between before and after which is quite striking and so here you can see again differences without with and without the text an answer you can see that now we can read what's actually written on the box once you reconstructed details here on the I or the B which becomes much sharper much closer to the ground tooth again this is construction task not generation task and here you can see also this working with complex geometry I put everything together and those are the kind of results you get so you get nice fairly high resolution reconstruction with good quality ALB metallicity and roughness good quality geometry etc etc here's few more examples and because you know meta you need to have some Lamas around so here you can see some physically based Lamas where we can change illumination and because this is physically based rendering then this stuff reacts correctly to your changing light in the last two minutes I'm going to improve these results a little bit further so we're happy with this but not quite so we really want to get some St the art and our text were good but not quite good enough so then we went back to the drawing board and what we did was to develop actually devop this in parallel to tell you the truth a model which can generate a much better higher resolution texture for an existing 3D gometry and this is really crucial so if you try to generate a texture but you already know the sh the object it turns out that you can do a much better job at it this is kind of intuitive but it's actually very useful because now you can take that Network that does this 3D condition generation of the texture to get much better high quality outputs so how does this work so you take the input 3D object you take your prompt in this case brown hair but what you what you do is you also render Tod features on the 2D views you put everything here in a another Transformer essentially which is a fine tune version of another emu model um or model in emu class and off you get the new text views or views of the texture object or views of the object as if it was texture which are actually better than one you get without conditioning on the 3D shape and for condition 3D shape it's actually very important which kind of information you put in you could put it put in depth actually turns out if you do if you put in position maps and normal Maps it works out better because you get information which is because the position Maps more absolute it doesn't it's not relative but absolute to the reference frame of the object and it Cas normal Maps because it really shows your details of the 3D Shape you might be missing if you were to look at dep only so here a simple example of what you can get if you take a 3D artist generated model of butterfly and you apply these these reconstruction Network so here you see the text prompt a majestic monarch butterfly and here go your texture and then you can change that into say um well a magical butterfly Arcane sees on its wings sparkling glitter on its body all right and that's what you get so you can get quite poetic but the point is that you can given the center the object you can replace the text stylizing it the way you want and of course you can use this to to enounce the texture you get originally this brings me to the last few slides where I want to show you the final results that we got so this is the complete package so this is a meta 3D gen and this is meta 3D gen rotating so you can see a collection of a few sample object we got those look quite nice in term of shape materials and texture again the texture are quite high resolution 4K you don't get genous effect you're going to get any of those things because well for a bunch of reasons for example because the texture are condition on this position Maps which really help with that and here you can see that you can also stylize this so this is in sty of Chrismas and now while we rotate it you can get the horror style instead and uh the next next strip is going to be forgot what see staring night this is a little bit cheesy maybe but it really shows how the model is doing a good job of capturing the concept and still trying to mix match it to match it shove it sh it on top of these objects with very sharp boundaries and very nice looking textures and here we get steampunk for instance okay so this kind of things you can get um so we compare this to competitors um commercial competitors like Lumen Meshi and Roden in using user studies here we found crucial to use professional users to or 3D artists to do the evaluation because they're much better able to judge whether your Generations are sensible or not from the Viewpoint of the pro quality and uh yes so in most cases here we win competitors are strong though so it was not easy to to beat them especially we beat them when it comes to a more complex kind of prompts and you can see that here so we divided the kind of prompt based on simple object characters and then combination of objects and characters and and as you get PRS which are more and more complex then the better that we do compar to competitors okay some cases where competitors are still win out although not in majority of cases so here is the results you get with Luma for example here the results you get with Meshi and we can put everything together here and I know that it's very difficult to to see that at this level of Zoom so I'm going to zoom in some details here and you can see the difference when generating for example the dinosaur phas or the pig phas or the pig or the raccoon and you know Mesi does a very good job at high resolution texture but they're a little bit messy sometimes Luma does a very good job all all the way through but maybe this this shapes are a little more cartoonish and we get a different compromise I guess bit more realistic shapes very good texture very high realistic textures if you want to know more the papers will be on archive hopefully later this week there are three papers here one for integration one for the 3D a Generations one for the texture generation and finally just a few thanks for the Oxford team so you see here everybody that has contributed to this and other works that couldn't present today Al our sponsors uh The Meta gen i3d team most of us are in London Tel Aviv and parties here they are then conclusions finally so what I wanted to show you today was bit of progress we have made in the past year or so in threei so I show you some best practices uh we wanted to achieve we control which we have done by adding for example the ability of controlling materials we focus really on quality try to have high quality nice looking textures which also have PBR material de compositions and also efficiency so I mentioned that we have splatter image and now flash 3D that can allow you to do 3D construction really really quickly and you can think that you can combine those techniques with image based construction or generator networks actually get do 3D generation as well and so I think this going to be very useful because again allows you to do experiments this class of problems even if you are not not have company by company level resources just University level once okay that's all I hope that was more or less on time says 35 minutes here apologies for a slight delay due to the technical problems at the beginning thank you thank you very much Andrea for the very impressive talk uh if you have any questions you can come at the center there is a mic uh if you want to ask any questions we have time for one or two questions um oh okay it works could you give a quick intuition to why this multiv view generator works like the idea of spatially tiling the views feels like magic I I assume one reason is the receptive field of the Transformer covers the entire spatial domain yeah correct so I mean you you get this um self potential layers so they can look within the grids that you are generating and find out correlation between different views you can be you can implement this differently you can stock the views along the channel Dimension and do cross attention different ways they're all similar really this is just one simple way of doing it but it's not particularly magic I would say it's just the trans Transformers can take a bit about anything as input and give you about anything as output maybe that's another way of looking at it okay thank you yeah I was I was hoping there is it feels yeah it just feels like magic that just worked I don't think it's magic it's just uh just being able to look between views using cross attention really thank you yeah all uh firstly great work uh I had a question more around uh the temporal consistency of uh maybe an approach like this for uh Dynamic objects in the scene is there any work being done on that front uh we have some Works um looking at that a little bit I wouldn't say it's uh something we doing in a major fashion yet but certainly some people in our team will be looking at Dynamics as well uh there are some Works already out about that too um yeah you can you can I mean it's kind of obious the future is 4D it's not 3D that if that's what you wanted me to say for sure yes and with this new class of high quality video generator models like soda and then you know everybody else is going to have a version of that possibly even better in the future then you can really see 3D conver and video generation converging into 4D I think good sounds good thanks sure I thanks for a great talk I'm actually curious about the meta 3D g&i uh yeah I remember you you just showed this R result on for t to 3D um and compare with mashi and other uh ni and how about image CH really and can you share more about this part like have you uh what's what's the status of meta 3 gen for image to 3D to to 3D and how does compare to like recent recent like yeah it's a good question so I don't think we have direct evaluation of Dos okay so in the paper you'll see that we have evaluations of how well that Constructor Works starting usually from ground shoot multiple views of an object though so we have comparisons against that and that for sure Works quite well if the goal is to start from um well maybe that answers your question suppose but we using sythetic data for those oblations though so if you if you are have a data set of multiple views and you want to know how what we're doing compared to competitors then we would need to run few more oblations yes thanks great let's think Andrea again all right and uh our next speaker is rosi L come set up so rosi is a BD student at Columbia University working with Carl VRI and he's one of the most uh prolific uh pits this this of this era and uh also he's uh particularly famous for his uh uh 0123 work that is uh that people loved both the method there and the name like we have have seen so many papers following like similar uh names there and Ros is interested in uh uh generative models trained from large amounts of unsupervised data with the goal of generalizing and being useful for Downstream tasks and applications and I think that uh The Talk today will be about some this Downstream tasks so uh Rosie you can take it away thank you very much for the generous uh introduction uh so today I want to talk about uh a topic I'm very excited about uh which is 3D generation for physical intelligence so there's no doubt that 3D generation has been one of the biggest Topic in compis this this oh this one got it got it okay all right thank you very much uh yeah so if you so three generation has been probably one of the biggest Topic in compeition we see exciting papers coming out pretty much on a daily basis um but if you look closer at these papers most of them are motivated by applications for the virtual world um such as game master creation or content creation uh while we live in a physical world um and as of now three generation doesn't have much impact on the physical world yet so this is the question I want to ask today how can three generation help us in the physical world specifically I want to break down into uh three components I want to talk about 3D generation that could help us for physical reconstruction uh design and interaction so two questions I want to answer today is how do we build three better 3D generative models and how can 3D gener models help us in the physical world so it's uh let's start with the first question um so if we look at the progress of 2D gener models uh we can easily see that as we have better gen models behind the scen we have better and better higher quality uh large quantity uh high quality data set um which is the Unseen hero behind the scene um at the beginning we only have small data sets like mes and now we we have Allon uh so we have better and better very impressive 2D generator models so that's exactly what we did for the 3D side we try to bring out and scale up data sets uh so we introduced ope Excel last year uh which is one of the biggest uh 3D data set in history of 3D um which is composed of 10 million plus high quality 3D assets fully open source so all the PHD can download the data set and run it in their servers and to put things in perspective um the small dot is everything uh combined before obverse and on the right is Obe Excel and you can see that it's much bigger uh than everything combined uh before obers and ever since the release of obers we've seen a lot of work uh based their work on object verse XL and train their models on object verse XL and we we are very excited by the progress and we expect to see more and more Works exciting Works coming out of it um so three data sets are cool but constant question I've been having is is data set scaling enough for 3D generation right if you put things in perspective uh ope XL is the largest 3D data set which is composed of 10 million plus 3D assets but in front of a uh the largest 2D image data set such as Li on 5 billion it's still all magnitude smaller that is because 3D assets are intrinsically much more expensive to collect uh than 2D images um so that brings me to the uh the next work I want to talk about which is z 1 123 uh which to me basically is a tool to channel knowledge in the vast amount of 2D images into 3D so 0123 as many of you know um is a view N View synthesis model um condition on a input view of an object as well as a specified camera Viewpoint transformation matrix 0123 could basically generate the novel view for you similar to stable diffusion 0123 is a conditional diffusion model and when on a lot of 3D uh data such as renderings from observers and observers XL the Run to3 display very impressive zero shot generalization abilities for example it generalize very well on impressionist paintings works well on uh oil paintings cartoons uh line drawings uh and even sketches which are traditionally very hard uh for classical 3D reconstruction or novel view synthesis methods um so we we just briefly talk about you know 0123 being a bridge to channel Knowledge from QD image data sets to 3D uh what about robotics right robotics is the field where data is even more limited and it's even more expensive to collect data for robotics uh than in 3D um so that brings me to the second part of my talk which is 3D generation for physical interaction so we have a dilemma in robotic data today uh which is the Dilemma between Rob robot data and visual data so on one side we have robot data which is limited but it it's very useful for robot uh any skills learned from robot data can be directly deployed on the robot on the other side we have the ocean of Internet visual data uh which is composed of a lot of examples like a dog running a person folding a cloth contains a lot of useful skills for physical interaction so it's very diverse but there's a crucial problem which is there's a huge EMB embodiment gap between the internet visual data and robot um because you cannot extract explicit robot actions from these visual data and large companies like Google and meta uh train visual Foundation models on these visual data so the important question to to ask here is how can we use the ocean of visual uh data which is kind of unstructured to help robot learning uh I think we basically need an interface between visual data and robot control so we introduce doctor robot or differentiable rendering of robot given a post differentiable robot rendering could render a robot body which is a post condition 3D gion siding of the robot surface uh under that pose and sampling a camera perspective we could differentiably render a robot image and a gradient on the pixels of the rendered image can be brought propagated into the control parameter space for optimiz ation basically Dr robot allows visual appearance of a robot to be differentiable with respect to the control parameters given a pause we render an image and this image can be input into visual Foundation models such as clip or text to video models and a control signal can be backpropagated into the robot action parameters for control um I want to draw a connection here between uh Dr robot and 3D generation a classical work in three generation which is dream Fusion dream Fusion basically optimizes a Nerf field with a text video uh text image diffusion model and this is exactly what Dr robot is Dr robot is a Nerf but for robot which is deformable and if you render image with do robot you could interface it with a visual Foundation model uh for optimization purposes same same thing as dream Fusion so it's basically 3D generation buffer robot so uh before I go into a few applications with Dr robot I want to briefly talk about how we achieve the differentiability of the rendering function so we de compose uh Dr robot into three main components uh with with a post we input it to a for kinematics model uh sorry uh just for kinematics uh which calculate the uh skeleton of the robot in a differentiable way the Second Step condition on the skeleton uh we could use an implicit linear Bland scanning function which is uh used in graphics to project the canonical 3D gions to a the surface of a robot uh under a c a certain post and at the end we use a appearance deformation function to adjust the appearance of the ganss to make them look better and improve the visual quality and this can be trained end to end by optimizing for the L2 loss between the rendering and the gr truth image so after what we learned this differentiable robot rendering model we could basically use it to do a lot of different tasks for example here uh we perform robot post Reconstruction from a single image or video through analysis by synthesis and here shows the example on the left is the original video and on the right is our reconstructed robot uh and their postes and as well as camera parameters um we could also perform visual model predictive control with a clip so here shows an example where we optimize for the join angles the 24 dimensional joint angles of Shadow hands towards uh to optimize for the clip similarity between the rendering as well and and the text prom of Victory sign so you can see that as the clip loss goes down uh we could control the robot uh which is the shadow hand to do a victory sign uh this will be extremely slow and inefficient if you use traditional gradient free optimization algorith such as uh reinforcement learning or evolutionary algorithms and we also uh could uh use Dr robot for robot control with a text video model uh when you when when we fine-tune the stable video model on the ASU tabletop data set we could prompt the model with a prompt with a text prompt like pick up the blue can and the video model can generate a video of the robot executing that prompt uh but because the video doesn't have actions we could use drct robot to extract the robot actions and Replay that trajectory in a simulation or in the real world and we could also use it for motion R targeting uh with a low-level uh point tracker from meta so uh with in this task we could basically transfer the motion uh from a human hand which is my hand here uh and we optimize for the transfer distance between the point tracks of the human hand and the robot hand in a differentiable way because of Dr robot and we could basically perform uh the same action as a human hand uh basically doing motion retargeting so uh we talked about basically a differentiable rendering model for robot what about policy learning so uh before going into detail that I want to add a little bit of context here so a visual motor policy is when you see an observation of an image of tabletop scene a visual motor policy needs to generate a sequence of actions uh basically motor actions that the robot can then execute for example the task here is to scoop uh the pole of mm beans to to the other empty bowl and behavior cloning has no doubt been one of the largest Trend in robotics in the past year uh what it is is basically supervised learning of human behavior uh you have lots of observations and their corresponding expert demonstrations for a certain tasks you basically with this paired data you can train a visual motor policy uh model uh in sort of a regression way right um to imitate human behavior so we introduced an alternative way to parameterize visual motor policy instead we use video generation as an intermediate action representation it's given the same observation that we saw before our model will uh condition on a tool we prompt a video generative model to generate the plan U of a person holding the tool to solve that task and then we just 3D track from the generated video of The Tool location and we could basically use a robot to follow that track to perform the physical execution in the real world uh so in in a nutshell we basically decompose visual motor policy into two components dream and imitate in dream we basically generate a video of a human doing the task and imitate we have the robot follow the trajectory of the tools in the generated video to perform the same task or as we call this framework gate and here shows the uh dream controls the robot to perform a rotation task of objects on the table and these objects are all unseen uh by the policy we we could also perform scooping task uh scooping one uh unseen granular media to an unseen bow with a bunch of unseen distractors on the table and we could perform sweeping tasks sweeping particles towards the Target and we could also push shape into their target which is a very challenging task because it requires multi-step planning it cannot be done in just one planning step uh and compared with the the our Baseline diffusion policy uh trained on the same demonstration data you can see that our uh policy is able to generalize much better to novel shapes so that brings me to the second uh third part of the talk which is 3D generation for physical design um I want to use uh uh specifically I want to talk about designing paper tools which means using a robot take paper as a material to design tools uh why paper you may Wonder well paper is an attractive recyclable and useful medium for design it makes a lot of useful tools in our society such as paper bag paper straw paper filters and is environmentally friendly for our society to make more tools out of paper uh and we pick a very fun and uh uh interesting uh design but still challenging design task of the design problem of a paper airplane so if a human wants to design learn to fold a paper airplane we will think of a design we'll physically fold it we will throw it out and we'll see how far it flies and we repeat this process iteratively to basically learn what kind of design flies further right so the question here now is can we replicate this process uh with robots so to do that we could parameterize the paper airplane design we could have robots physically folded we could have a robot arm to throw it out and we can have camera to measure how far it flies and this is what we built in our lab so a robot will load a paper into the folding Dock and it will fold uh the right and left wing sequentially and it will grasp a holder to stick on the paper to create a grasping point and then uh a Rob arm will throw it out and at the end of this uh uh two top mounted rgbd camera would track how far the plane flies so now we basically automate the the paper airplane manufacturing process but there's still a problem remain which is how do we learn to design the paper airplane to do that we uh adopted a optimization framework based on surgey based models which is parameterized by a neuron Network so the goal of the surgey model is to learn how far each plane can fly uh given various paper airplane design and after seeing a lot number of trial and errors it actually fold paper airplanes in the physical space and measure how far it flies it learns a pretty good function and we could basically perform optimization in the design space of the paper airplane to optimize for the travel distance and here V we visualize the learning process of of the learning to design paper airplane and you can see that um as as you gain as a robot gain more and more experience uh it sees more different designs and how far it flies it learns to find better and better designs and at iteration 91 uh it basically find find the best design Which flies over 20 fet or 6 meter uh we compare our method against several baselines including the classic uh uh gradient free non-convex ization algorithm which is evolutionary algorithms uh we also compare against a human participant uh subject to the same number of Trials and you can see that uh our method actually outperforms human participants by a little bit uh we also apply our uh design framework to a practical task of Designing kuami gripper or basically gripper that's made of uh cutting paper prior work on kirami gripper relies heavily on physical analysis which is timec consuming uh and uh and expensive we basically apply the same automated design framework that we apply to paper airplane design to kirami gripper and we build a physical system uh to manufacture the kirami gripper by using two robot arms and a quicker maker 3 machine which basically cuts paper into certain shapes and we also use two load cells to measure the horizontal gripping force of the gripper uh as a reward signal and as you can see um uh through uh with only 100 trials our method is able to learn kirami gripper design that outperforms or the baselines um at the end of the iteration uh the optimization Cycles we obtain a gripper design that's able to exert 93 Newton of fours uh which isal equal to the weight of four and a half strawberries with only a piece of paper and here we show our design gripper grasping uh a strawberry and here we show uh our design gripper grasping various objects we can find in our lab if you want to know more detail please go to our website and uh as a disclaimer we recycle all the paper used in the experiments so this is a basically automated design framework based on robots um uh to apply this framework we can actually use it to design useful tools uh in our physical world by incorporating priors from 3D journ to models and to do that we basically require a 3D printer which prints out the geometry of the Tool uh in the real world and have a robot to use it we could basically apply the same surgey based optimization framework however there's a problem which is 3D printing is kind of slow so to do this framework to accelerate it we probably need a form of 3D printers which are kind of expensive so it's also kind of the reason I haven't been able to convince my adviser to pay for it yet so if there are industry funders in the audience uh please contact me during the conference um I personally believe that design framework is able to lead to marvelous tool design that's uh Beyond human imagination so that brings me to the last part of the talk which is 3D generation for physical reconstruction so we want to solve the problem of uh 3D reconstruction of completely uded human for examp example in this image there is a boy that's running out of the car that's completely occluded by the car in the front uh that's very physically impossible task right but it's very important for autonomous driving to detect this kind of incident to prevent accidents there would be a way to make this possible if the car in the front turned into a mirror so through the reflection of the boy in the mirror we could basically see uh a reason the 3D location and body information of the boy it turns out this is actually exactly the case in the infrared Spectrum so here's the side by-side comparison of a normal camera and a thermal camera and you can see that there's no reflection of me walking in front of a car in the normal camera but there's a clear reflection uh in the infrared Spectrum so why is this where does this reflection come from and can we use it to reconstruct 3D human so it actually relates to black body radiation which says that every objects Beyond absolute 0 degrees Kelvin uh emits thermal radiations H objects emit variations with shorter wavelength and as they cool down the radiation become longer wavelength and with a constant body temperature of 37 degrees C uh human actually emits long wavelength infrared light uh with a wavelength of about 9.5 microns uh that's why it's far beyond visible like Spectrum which is why we don't see human glowing in everyday environment but with a small thermal camera like this you're able to see human as a light source so that's basically the title of the paper humans are infrared light bulbs so as you can see here as a side by side comparison when you turn off the light in the normal camera you don't see human anymore because human reflects visible light in our environment but you can still see human in the thros Spectrum and cats are also infrared light bulbs and this is my cat uh and many objects because the wavelength of the light is longer they tend to uh reflect in a specular way instead of diffuse such as here uh glass is transparent in the normal camera Spectrum uh but in a thermal camera they're very uh reflective same goes to the Whiteboard you don't see any reflection in the normal camera but you can see a clear reflection of my body in the Whiteboard so uh based on this observation uh this interesting physical phenomena we propos a method based on analysis by synthesis to reconstruct human body and uh 3D objects in the environment so to do this we basically first sample from from a 3D generative models as latent space uh we use 3D generative models of objects as well as 3D generative models of human which is SNL um we basically sample initialization of the scene and we write down the differentiable rendering of reflection function um to basically differentiably render uh the reflection image as if it's captured by the thermal camera and we basically optimize this render image towards the actual observation of the reflection and at the end of the optimization cycle we basically can reconstruct both the human body and the object their geometry as well as their location and orientation in 3D and here shows the results with a single RGB image as an input um as well as the thermal image and the same view we can reconstruct both the geometry of the car as well as the human body their body pose their location and orientation which is consistent with the ground shoe scene captur from another camera's perspective so that concludes my talk uh I talked about two works for 3D generation and uh applications of these work in physical interaction uh physical design and physical reconstruction um I I personally find this to be a very exciting space to use 3D generation in the real world um I hope this inspires more future work in this domain uh and I want at the end I want to thank all the collaborators and my advisers as is the collaborating institutions um and here are the references to the papers I talk about thank you very much right thank you very much rosi for the great talk if you have any question you can come at the center uh there is a mic so we have time for one or two questions yeah I have a question actually so for so great talk it was very you know a lot of fun actually so when it comes to just designing this generating this mechanisms how do you actually do that so you have this parts that I mean especially something mechanical like I know spanner for example you need to have this you know gears and stuff these different parts need to be generated and combined together is that something you can already do properly um I feel like that's related to the design problem that we were trying to solve like the paper airplane design I feel like it's uh it's kind of hard so I I I think the 3D printing is actually the key in this problem do we have if if we have efficient 3D printing that we could have automated design framework uh for this and I also think a related area is uh physics simulation so there's a lot of work on differentiable simulation and if we have tools like that we could basically perform inverse design yeah uh and interface that with the 3D General models which is pretty exciting so don't you think that there is a difference between building a paper plane which is one piece and building a mechanism which has maybe two or three gears to me there's a gap there yeah I think the Gap is a very mature uh 3D printing technology so I the reason we picked paper as material is because uh first of all it's super hard to simulate it's a thin shell object uh it's deformable and uh pretty much there's no simulator that could accurately simulate paper it's let alone like you fold them multiple times and fly them in the air right there's interaction between the formable objects and and fluid so it's extremely hard to simulate so we kind of picked this very challenging task and to motivate uh 3D but I think I think uh it's a very interesting I hope it could become like the the amness of uh of design uh it's one piece it could be folded easily with robots and a perform design so I think whatever method we could use this as an example to work on the design method and this could be applied to if you have multiple components when you actually hook up to a industrial 3D printing or manufacturing pipeline it could still work there yeah okay cool thank you all right thank you Andre we're a bit of our time so we we will conclude here thank you let's thank Ross again and uh can we have our next speaker e uh D can you maybe start sharing your screen and also unmute to see if we can hear you as well okay can you hear me can you hear me now yes yes we can hear you and you see the screen yes we also see it on Zoom okay shall I go ahead then um should I start just let me know yes yes just a moment we're figuring out some technical details right there okay okay so uh after uh figuring out some technical issues our next speaker we can go to the next speaker our next speaker is duu taan so uh duu is a senior research scientist with working with Adobe research she got her PhD from ebfl working with Mark POI and she has been with Adobe uh since then she has received the eurographics uh BHD award in 2014 or 2015 and then the eurographics young investigator award in 20 20 and uh uh du is working on all things 3D with uh always in the intersection between uh computer vision and graphics and uh she has uh special interest in when it comes to humans which makes her work very close to my heart so uh without further Ado I think uh D you can take it away yeah thank you so much for the invitation and also for the kind introduction um so today I'd like to share some of our recent work and some thoughts towards multimodel generation with you um so we are all aware of the fact that um generative a AI is revolutionizing content creation and especially text to image models have led the um LED this Revolution and there has been many many examples of very high quality text to image generators so Adobes Firefly being one of them so basically what this gives us is now we are able to just type short complex text prompts and then immediately get very high quality content such as these images so this is all great and of course once we are able to do such high quality Generations the next question to ask is how do we control this process in addition basically to text prompts so before uh diving into that so how do these uh models work not going into too much detail basically the uh underlying Machinery is this uh diffusion model which is a generator model that would try to model the distribution of uh natural images so starting with um noisy images like sampled from gaussian uh distribution the model would try to basically learn uh to den noise the um signal and then eventually have this high quality image and the two main important blocks of the diffusion model whether it's a unet architecture or more recently the diffusion Transformers are these attention layers and in particular we have these self attention blocks where the image attends to itself and this is very important for um basically for the final structure and the appearance of the image and then there is the cross attention where you can start attending to different signal modalities for example text so now I have this basically very powerful 2D generator that I can prompt with these text descriptions and as I said the next question to ask is what type of other controls can we enable and uh one of the immediate controls one can think about is of course special control for example controlling uh the structure of the image using depth Edge pose or other modalities and there has been a lot of work in this domain and control net was one of the most popular that came up last year basically that enables to uh control this process with various signals of course special signal is one of them but there are many other uh interesting signals people have been starting to look at for example audio because usually Sight and Sound are very connected to each other so how would you do conditioning with a signal that may not be directly that may not be represented as a special signal and the idea is actually um kind of very basic uh similar to how we do text conditioning one can also start introducing these additional cross attention layers where uh we can do uh basically attention with respect to these other signals such as audio and um basically the power here is one can also do that without really changing the base text to image model uh significantly one can introduce only these additional blocks and train only them while keeping the rest of the network uh Frozen and that means basically you can try to add these additional signals with um a moderate uh data set size so once you do this basically uh this is from a recent work that we've done hopefully you can hear the sound but in this case the input is basically this audio clip and the model is able to generate the images that you see so this is a sound clip of basically hitting some dry leaves and this is the image that you generate and in this case it's the sound of basically playing with water and the method is able to generate these images and now that we have basically kept most of the functionality of our base model and only added this additional uh control signal we can also basically apply all the editing uh operations these models are able to do but using an audio signal for example here given an input image we apply some diffusion based editing method conditioned on the audio signal which basically basically tries to uh preserve the original structure or the content of the image as much as possible but only change or edit it based on the condition conditioning signal okay so now basically we have the generator we are able to control it with different modalities of course the next question to ask is how about output so we are now able to generate images can we move towards other modalities like video and of course eventually towards 3D um this was a work that we have worked on last year basically the main question was given this powerful image generators can we use this to edit not only single frames but edit an existing video for example here I have this um input video of a swan and then I provide some text prompt uh using the image model I can obviously edit one of the frames let's say the first frame how do I propagate that edit to the rest of the video while uh basically ensuring temporal coherency as much as possible and again uh it turns out that by just looking at these attention blocks one can do this task using only an image model to a um with surprising success so the idea here is now we repurpose these self attention layers and not only attend the image not only attends to itself but basically it starts seeing and attending to the features of the previous frames and with this very simple basically change we are able to repurpose this image model to do video editing and here are some results and obviously this simple trick can be applied to any underlying base model in this case this is using adobe's first image model and you see the input video and the Ed Ed result and this is another example again given the input video and the text prompt we are able to do this per frame editing in as consistently as possible manner so this kind of brings us to the intersection of 2D and video but it's not 3D yet so um that is the next question to ask how can we basically leverage this technology for 3D workflows or how can we bring 3d to help with these workflows um so we've looked at this problem basically in the context of uh extending the PS to video work so basically um previously I showed that given an input video we can try to stylize it using the image model but in this case I still need the input video but we also know that in 3D we have all these um tools to create the animation or the motion we want for example I can use physically based simulation I can try to set up a scene and then uh do some camera pth planning I can um capture humans or other rigged characters so I can do Mo cap so there are many many ways of creating the exact motion we want and the question to ask then is can we utilize these tools to create the scene to create the motion but then uh turn back to the image generation models to help us basically uh map this scene and animation sequence to the final um pixels that we want to render um and this I think this is basically a first small step towards this direction but overall I feel this concept of generative rendering is very powerful as we bring 3d tools and generation models more tightly together so how do we do this um basically there are some common observations for this task and also for the video stylization task or video to video workflows um I had already mentioned one of them so one key observation is that self attention layers of the generation models are very effective in controlling the final appearance of the frames and by just converting these to cross frame attention basically letting multiple key frames attend to each other one can already generate very consistent looking results the second observation here is of course we do have an input um 3D scene and we do want to preserve the structure of the scene in our final Generations so then we return back to special guidance for example depth or edges to preserve this structure and in this case this special guidance is of course ground tro rendered from the input 3D that we have another important observation with diffusion models is actually the initial noise that we start the diffusion process is um very related to the final images we will get and this is why for many image editing or video your editing tasks you'll see that people use to um use some methods basically to invert the images or the frame or the video frames uh in our case however we do not have um RGB frames to start with so we have a 3D scene which has no texture so we cannot directly do inversion so in instead of that I'll describe next we actually try to initialize the noise in a more 3D AER way and finally um because we also have the 3D information and ground through 3D correspondences we can um start playing with the output of the attention maps to further improve consistency okay so first let's look at uh noise initialization so we have the 3D scene which means for each object we can also assume we have a UV mapping basically that Maps that 3D object to the 2D UV or Texture space so instead of basically initializing noise in the frames the rendered frames we can initialize the noise in the UV space so now you can think of this as a multi-channel basically texture map which is just random noise and using this and the 3D scene we can render a noisy image for each frame so here is like if I stop this any animation at every frame basically it just looks like random noise but actually if you play it you will see a motion hopefully that comes through Zoom but basically the this kind of gives you the idea that this noise pattern is in fact very important and if you just run the diffusion process with such correlated noise you'll see that you will already get um frames that are very similar to each other and that already has quite a bit of temporal coherency so then the next step is basically how to improve this even further and get more coherent results um so let's see how we do that that's basically really playing with these self attention layers um so we have our animation sequence um basically we do this cross frame attention for a set of key frames and basically uh at the end of this self attention blocks we have the key and value features of each frames basically what the self attention layers process um so again because we have the 3D and we have this mapping to the UV space what we can do is take the output of these self attention blocks for each frame and basically map them again to the UV space basically again treat them as text textures we can project them back to our common texture space blend them in the texture space and we can now render it back for each frame and again I can do this because I have this 2D to 3D mapping so now as a final step when processing each frame what we basically do is we get its original post attention features and blend them with these um basically Blended features in the UV space and that are rendered back and U basically update them so what this basically does is um try to correlate the features attention features of each frame in the u space through this 2D 3D mapping and then make them blend them in a consistent way and then render back so now that we have this setup basically what we can do is again as I said we can use any 3D tool to create a scene and a motion and then use the generator just as by um just for the final rendering task so here for example you see this fox um this quadruped motion and then given different text prompts we can easily map that to um different um stylized renderings here is a motion capture sequence which you can basically get from your available moap data or use any um 3D pause estimation methods and then map it again to these different stylized renderings and of course we can also just do it for static objects but the motion could be created by moving the camera so in this case basically even though the object is static we can create this almost 360 type of um sequence and then um try to render it so basically these are examples of how different modalities whether be 2D video 3D or even other modalities like audio coming more and more closely together um of course so far in the works that I use we've been using image models but the video generation world has um moving very fast I think even today Runway has announced their new uh text to video model so that means like all the Machinery we have developed we could now potentially also bring it to these text to video models which are uh more powerful already have a sense of um temporal coherency and also some 3D knowledge and um people have already started extending these works for three generation for example one use case for stable diffusion video was to be able to generate these full 360 turn table videos which you can then reconstruct as 3D um so yeah with that like I really think we are moving towards this multimodal Universal generator which will be able to take multiple modalities as input text other structure controls audio and so on 3D but we also be able to spit out RGB images maybe intrinsic properties of images like depth segmentation mask albo shading will be able to render videos but also be able to um generate 3D and I guess we'll see more and more uh examples of this type of work uh in the next few months and I'm very excited about it and yeah with that I think that's all from me and if you have any questions happy to answer right if you have any question for duu you can come uh the center there is a mic so um I have a question so for the for the last word work presented you had this hard hardcoded effectively 3D representation to generate this consistency for the video uh the question is maybe more of an opinion like do you expect in the future that we will need to have this uh hard code in the model or will be able to have this uh uh type of we'll be able to achieve this type of uh 3D consistency just uh from from data driven data driven approach and I'm referring to the consistency we see it for models like Sora for example where as far as we know there is nothing hardcoded when it comes 3D but in many occasions uh it it seems to be 3D consistent yes yes no I think large models are already learning a lot of the 3D consistency I think what excites me still is two things one is like these powerful generators for 3D workflows I think in uh still like in many applications I may want to design the exact 3D scene and the exact motion that I want and then maybe just use like the video model as a rendering tool I think that is still powerful and will be useful and yes these models are learning a lot about 3D uh but I guess still there would be a place for it may not be a explicit mesh representation but somehow a notion of implicit 3D maybe um to still I guess like capture very long range interactions or if I want to generate maybe multishots of the same scene very zoomed in versus like very zoomed out there I kind of tend to think we would still need some form of 3D representation but it may not be the explicit 3D mesh that I guess traditional workflows are using to today m i see makes sense uh and if there's no other question I think that we can uh thank du again yeah thank you so much and now it's time for our first coffee break and we will uh reconvene at 10:45 I think so yeah see you soon oh for e e e a little resets and does it a few times this is actually my first in I'm a b e e e for e e e e e e e e e e e spe e e e e e e e e e e e e e e e e e e e e e e e e e e e e e all right hope everybody had a good coffee break and uh are excited for the next set of talks next up we have dongu Zang dongu is a researcher at Soul National University uh he works in 3D generative models specifically on a class of models called generative cellular automa that he's going to talk to us about today D obtained his masters from snu as well with Professor Youngman Kim and he's also intern at Nvidia with Professor Sonia fer uh super excited to hear from dongu uh I feel like this is really exciting work it's a new Direction in 3D generation and without further Ado take it away yeah thank you for the introduction he hey um hello I'm D from s National University and I'll present our work generative failure a which is a 3D generative model for seeing completion at scale um and part of this work was done when I was an internet Nvidia whoops the goal of our work is to generate a complete theme geometry from partial Point Cloud obtained from sensors so given an input partial Point cloud of indoor or outdoor scenes obtained from sensors such as lar we aim to reconstruct the whole scene geometry oops this takes a step towards our vision of using 3D sensors as content capture systems to generate high resolution content which might find eventual use in simulation or even Triple A games um previous Works have employed discriminative model for SC completion while discriminative models generate reasonable completions and then point dense point Cloud sometimes Point Cloud obtained from sensors are sparse noisy and incomplete as seen on the left there may be multiple plausible completions for an ambiguous input so discriminative models tend to average out the multiple Solutions and might not generate high quality completions therefore a conditional generative model capable of generating multimodal distribution might be more suable for SC completion thus we present GCA a specially scalable 3D generative model suited for scene completion building a scene level generative net model is challenging because complexity of the dense 3D grid increases in cubic to the size of the comption thus High Fidelity competions are hard to scale using a naive dense grid representation the first key idea to building a spatially scalable generative model is to leverage sparcity in 3D as we use higher resolution the rate of the occupancy cells decrease so if we only focus in occupied parts we will be able to efficiently handle 3D shapes thus we use we utilize sparse convolution layers to handle sparsity contrast to vanilla convolutions that perform convolution on all dense grid sparse convolution operates on only occupied regions therefore it is more efficient compared to vanilla convolutions in 3D filiz shapes the second key idea is to exploit connectivity of 3D shapes most man-made 3D shapes tend to be connected to leverage connectivity we employ the streat of cellular auton cellular auton is defined in Grid of cells and models of transition over time the transition rules follow the local update rules that depend on neighborhood of each self with this simple predefined rules cell auton can create complex structures such as snowflakes in our work we learn a neural network to predict local update rules that generate 3D shapes and sequentially grow them now I'll introduce generative F auton GCA that utilizes sparcity and connectivity GCA represents a 3D shape estate s which is a sparse occupied Vil of the surface using sparsity given an initial State S zero GCA computes the transition chel PST plus one given St modeled with a neuron Network and samples the next state we use the final shape s t by recursively applying the transition T times similar to diffusion models now I'll explain in detail how a single transition works for each transition given an intermediate State St GCA computes the occupancy probability of neighborhood occupancy cells with far CNN then we average to it to obtain neighborhood occupancy probability of the whole state St and we just sample for it because GCA models a transition kernel with only Spar convolution the transition is local and spacially scalable for single shap completion GCA generated sharper shapes compared to the previous state of yard baselines the transition kernel of GCA acts locally due to spars convolution therefore even if GCA is trained to compete on a single instance shape it can complete a whole scene with distant incomplete objects while GCA is a 3D generative model suitable for scene completion due to locality and scalability and has a limitation of generating voxos with a fixed resolution unable to generate continuous geometry for Content creation therefore reacts extended GCA to generate continuous geometry we utilize deep local implicit fields to drain continuous surface local implicit Fields divide a continuous shape into a grid and then stores the latent code for each cell the latent the latent encodes a nearby local continuous surface using a neuron Network encoder we apply the local implicit field to GCA um we introduced Spar VOA embedding which augments The Spar fil representation with the lat latent Cod Z that encapsulates the local shape of the grid we maintain the representation of sparse by setting Z to zero if not occupied sparse F embedding can be created from the implicit function with an encoder and decode it back to produce the implicit function using a decoder now I'll explain continuous trans f cgca that generates continuous geometry with using Spar Fox embedding cgca like vanilla GCA starts from an incomplete initial state is zero and recursively samples from the market CH unlike vanilla GCA it samples the latent code Z for each cell which the easiest way to think of it is sampling color for each um occupi fossils for the last several steps we use maximum like estimation instead of samping to remove the cells with low likely occupancy the final State can be converted to a continuous geometry using the previously mentioned pre-train decoder the transition Coronel of cgca can be factorized into neighborhood cells and further factorized into occupancy probability and latent code for each cell the occupancy properity follows the brui distribution with with the parameter estimated by the neuronet the latent code follows a normal distribution with a mean estimated by the neuron network if occupied if not we just set it to zero this way we remain sparcity during training we only have the initial State S zero and the complete State X so we emulate the internal media states by sampling from an infusion kernel q that is also close to P while it St converges to X the infusion kernel is factorized into occupancy and latent code similar to the transition kernel the occupancy follows a brui distribution but we inject ground truth occupancy x to the probability the same goes for the thing goes for the latent code by injecting the ground truth the transition que is similar to in that the parameters are mixtures of the estimated distribution p and the ground tooth next I'll introduce the full training procedure that uses this in infusion kernel during training we sample from the previously mentioned infusion kernel and minimize the KO Divergence of Q and P sampling stage from Q is expensive since we need a neuronet evaluation thus in practice we use a replay buffer to cash infused intermediate States we compared our method to state of thee art baselines on shamess scene data sets um a synthetic scene comprised of multiple objects dgc generated diverse High Fidelity shakes from the input Point cloud we also trained and tested our model and synthetic 3D data sets comprised of room scale indoor scenes dgc can indeed generate multiple High Fidelity competions from a single observed Point Cloud um in our next project with Nvidia we moved on to using PCA for outdoor scene completion from Real World lighter scans which is accept to this cvpr as a highlight paper the difficulty of competing from Real World input signal is that we often do not have ground truth data for supervision PR prior methods use accumulate liar scans as supervision signals leading to noisy and incomplete results instead we propose a new task named outdoor SC culation to distinguish with a previous work the task aims to generate completed geometry in regions that are completely unseen by liar through height limits or cusions from several lighter scans we achieve this through training on synthetic scenes while showing strong Cal generalization to real world lighter scans with our MTH it we also not that we do not use semantic labels like previous works our model hierarchical generative cellular autom hgca leverages a spatially scalable GCA in two-stage course defined manner in the first core stage given a point cloud from the lighter scans we voxelize the input and use GCA to generate completion and low resolution V then in the second stage we up sample the course completion from stage one we concatenate the course completion with the input Point Cloud then we use cgc in a final resolution conditioned on a horse completion and input to produce high resolution Spar Bo embedding which generates a fin final continuous Mash using a pre- pre-train latent decoder GCA is a local model and often complete themes without Global consistency thus we append Global features to Cor GCA unit architecture by proposing a lightweight planner model that extracts bir eye view features we SP the initial incomplete scan at zero into 2D using a local Point net and pass it onto 2D units to obtain the global B view feature then we promote the B view feature into sparse 3D unit in a spalik manner while predicting the rough dense occupancy during training we apply auxiliary B view laws against the ground Tru occupancy to enforce a planner to learn Global features notes that we name the global consistency model planner since it plans ahead the persisting Brau feature solely with initial State at zero which remains through the mar process of GCA we train our method on a mixed synthetic data set of puros squid scene and Cara We compare our method against the state of the art scene completion method here we show that hgca generates more realistic scenes compared to the recent space sign qualitatively in the main paper we also show that our mthod generates accurate completions quantitatively we validate our method on weo data set to show that our method generalizes well on real world lighter scans hgca General shows Superior generalization cap capability which we suspects them from local transitions of GCA and using a hierarchical formulations which tend to be more robust to noise our method can generate beyond the lighter field of view more than the accumulated scans hgca scales well to large Real World scenes using a single GPU making it scalable and even extrapolate heels on the bottom also hgca generalizes to unseen object from training data such as complex trunks of trees H GCA can generalize to compete three-wheeler unseen from training data to conclude GCA is a skill is a 3D generative model that basically grows it a specially scalable model suitable for scene completion and it generalizes well from s to real so I would like to acknowledge my all my wonderful co-workers from snu and Nvidia and lastly um a bit of advertisement please check out our paper um outdoor scening station with haral generative atanta in Arch 4 AE Friday 10: a.m. also I'm applying to PhD program in 2025 so if one if anyone's interested in having a chat let's chat um and lastly thank you for listening thanks a lot for the talk tongu there's a mic over here if anybody has any questions please come up you can come up right here hi uh I have a quick question for your uh s first nice talk U so for your last word uh do you think it is possible like to take also like RGB into front camera and into of this virualization process so that you can fully recontra the texture the color scene yeah um are you suggesting that we take RGB camera as input and so that combination of uh lar and RGB so like you will have a lar to to be calibrated so that RGB camera can see the same thing with the RAR is possible to bring the color to those uh lar Point cloud and it is possible to do this to draw the this thing competion together uh we actually haven't tried it but one we actually once discussed using the RGB features to extract Dino features and do something with that um but in our current method no but I think that will be kind of a our future work for now okay okay thanks yeah um hi uh thank you so much I learned a lot today from you um I'm Shri and I work for Volkswagen Innovation and I'm working in this space right now uh so I have a couple of questions but I'll stick to one for today um at the end when you go from point Cloud to mesh what type of decoder do you use oh um it's actually the same decoder that we used in the cgca and I think you're referring to this part right exactly it's just a local ifnet type of um hierarchical decoder that takes in the features and the um the XYZ coordinates and it outputs the SDF values not SDF but unsign distance Fields so and we use marching cubes to um trate mesh okay uh what about when it's a volumetric mesh and you have things like castings or extrusions um that maybe join two parts together in a car for example do you think that it's a good method or is applicable here uh I'm honestly not sure what makes the difference if like more details so like in this case you can't make out the difference between the bumper of the car and the Crash box right so these are kind of practical implications of using Point clouds versus graphs and I'm just trying to figure out if this is a good solution for that uh we use kind of like the basic um like um the most vanilla decoders that we could think of um and IET papers like three or four years ago so I think if you use the techniques from like recent um like like Mosaic SCF type of representations you will we could get better meshes gotcha yeah there's something called point to mesh um that's the current State ofthe art but still doesn't perform well enough to have a well defined generated mesh um so super interesting work uh excited to see where where it goes thank you thank you thank you so much thanks for the great talk thank you all right so next up we have uh Barun jumani Von is a lead researcher at stability AI where he leads the generative Vision team uh he's worked at Google and Nvidia research and throughout his career he's made key contributions across computer vision he's worked in inverse graphics 3D 2D perception 3D reconstruction and now 2D 3D generative models that I hope he he's going to talk to us about really thrilled to have Veron here today for for his keynote talk and just gonna let Von take this stage thank you yeah thanks Amin for the nice introduction uh sh hi everyone uh I am warin Jani a lead researcher at stability AI uh today I'll will be going to talk uh I'll be talking about adapting image and video generative models for 3D Generations uh before going into the main topic of the discussion for this presentation I would like to share some of my thoughts on what it means to have a generative 3D model uh so 3D 3D generative models just like image and language generative models they learn a conditional distribution of 3D content given some practical input such as text image or video of thing which are which are quite easy to input and so these 3D generative models can make the 3D content uh uh can make the 3D content creation quite practical and feasible but unlike language and image there are several differences to 3D 3D is different uh there are several key differences which I'll will briefly mention here so one is that there is no standard Universal 3D representation so different 3D applications require different 3D representations like workel meshes graphs projections descriptors Etc and also 3D is for Niche right and and it's and it's for power user so most of us on a daily basis interact with text and also we interact with images and videos on a daily basis but most of us uh we don't usually interact directly with 3D so it's mostly for power users like uh graphic designers uh movie art uh movie uh VFX designers or game artist Etc another key difference is that like the 3D data the available 3D data is orders of magnitude smaller uh for instance we train some uh basic image generative models with using five to 10 billion samples of images but on the other hand the 3D data sets at most are kind of like a 250k to 10 million samples kind of thing and also much of the data is not very high quality to be honest uh so this makes it quite difficult to tr try a 3D generative model from scratch and so because of all these differences there is no single unified generative model that exist for different 3D use cases kind of thing for for images if you have a generated model like stable diffusion that is widely applicable for lot of image tasks and most of the image tasks and for video we we have let's say several models like Sora or stable video diffusion that can be useful for a lot of video tasks but because of all these differences there is no single unified 3D model also it's not clear whether we will have have it uh whether it makes sense to have a single unified 3D model because of the diversity in 3D applications and use use cases despite that I still believe that like generative 3D Technologies can form foundation for a wide range of 3D applications so I'm quite excited to be in this field and to be working on this new 3D generative models stuff uh what this means is that like for instance we have a lot of 3D applications like gaming virtual reality movies uh 3D shopping uh Etc and what used to happen is that like lot of these applications are uh are still like there they're actually supported by a lot of Technologies 3D Technologies like human avatars virtual Tron or like scene understanding and synthesis Etc so what is going to happen or what is happening recently is that like there are particular some core models on 3D that actually these these core generative models that 3D object generative models or 4D generative models or phace human generative models that are going to support these lot of these Technologies which in turn which in turn can support lot of 3D applications so the so the goal of research in my 3D team is that like let see if I can Mo this so the goal of research in my 3D team is actually work on uh some of these core 3D generative models that can have wide range of applications in 3D uh 3D Technologies and applications so despite not having a single unified 3D generative models uh there are actually two emerging technologies that are coming up in 3D generative AI uh in the last one two years I would say one is a direct 3D prediction and second one is multi viw generation and for direct 3D generation these generator models takes a single input image or text as input and they directly output a 3D model as output like a 3D mesh or a triplane Nerf or some other 3D representation uh one of the the positive side is that they're they're usually quite fast due to direct predictions and on the negative side they they need a quite a good amount of 3D data sets to train and generalize thing so lrm work from Adobe log construction model uh forms one of the pioneering Works in this space uh they they showed that like from this single image by using a Transformer Network we can actually get a very high very good quality 3D generation outputs like this here uh the triplanes in this case nothing thing so at the time this model is not open source there is some open version of this called open ERM so so we also trained some in-house model which we call trios this is in collaboration with vast a research and where with this with this tripos Sr model this can generate 3D mesh from a single image in less than 05 seconds on this bottom left I saw some 3D generation results that is compared with open lrm which is an open source version of lrm and from this results we can see this this Trio Sr has much better 3D reconstruction uh results compared to open Alm on on the bottom right I'm showing a plot on the x-axis it's an inference inference time and on the Y axxis is a 3D reconstruction accuracy measured in terms of f score so in this plot we want to be on the top left side we want to have a high 3D reconstruction accuracy while being quite fast while being low inference time so as you can see from this plot this this tripo Sr has one of the best U uh has it's actually one of the best and fastest 3D generative models among open source and since we since we open source this model model and because it's quite fast and versatile there's a quick adoption of trios Sr in the community there actually several interesting use case and workflows that already people have been using for instance people are using tripos Sr to actually create 3D models within Apple Vision Pro environments and they're also chaining this triar with some sketch to image models so that we can generate from sketch to image to 3D within just a matter of few seconds related to work that we are presenting at this cvpr is called zero shape uh regression based zero short shape reconstruction here we take slightly different approach where we use uh where we make use of of the Shelf depth networks called DP like like this DPT networks to we adapt it to predi both depth as well as camera intrinsics and then we unpro and this depth and do some 3D normalization to get a partial Point cloud and then we actually complete this partial Point Cloud into a complete shape uh using a using a Transformer Network so this in this work we mostly focus on shapes we get some quite good quality shape uh shape estimations from a single image in this work uh please see the paper for more details of this work uh so so so so this so This direct direct 3D prediction is one of the emerging techniques and the second techniques that is widely Ed recently is this multi viw generation right like so where we take existing image and video generative model mod like stable diffusion or stable video diffusion and then we do view prompting to it to generate an image from different views uh novel views of that given object uh given input image and then from this from this novel multiview predictions we can optimize a 3D object using uh some loss functions like SDS loss function or reconstruction losses so the nice thing about this uh technology is that like it leverages and image and video models that are trained on large data and this have very good generalization and on the down side they are usually slow because these image and video models are usually quite slow uh they're usually diffusion based and US slow and they also require further processing to get 3D objects out we do some kind of like Nerf optimization and stuff so there are two main aspects to these uh multi viw generation pipelines uh one is this this view prompting uh earlier works like dream fusion and stuff they use text based view prompting and more recent works like 01 to3 XL and other more recent Works use camera based more precise precise view prompting with camera POS based control and second important aspect is that like how can we go from multi viw predictions to 3D objects uh so there we we we typically use core distillation sampling losses and reconstruction losses kind of thing so in the rest of the presentation I will explain some of the works in this space uh some use text based prompting and Camera post based prompting and some use SDS losses and other reconstruction losses so so much of the focus of the presentation is on this multiv generation so for so so one of the pioneering Works in this space is this dream Fusion that introduced this code distillation sampling laws followed by other recent works like followed by other works at the time like latent Nerf magic 3D Etc so here the here the the key thing is that like uh we optimize a Nerf representation uh and we and project this nerve uh we render this nerve onto different views like side view back view front view kind of thing and then we pass this rendering along with this text based view promting like the peacock from a side view peacock from the back view peacock from the top front view kind of thing and then we pass this through this here the imag based Network and also or or we could use other networks like stable diffusion as well and then we can get the loss function which is called the score distillation sampling loss function that we can back prop through this Nerf model that we are optimizing of thing so here this here this view prompting uh is kind of like a hidden thing but it's kind actually is very important to get these 3D consistent results and one of the nice surprising things with this dream Fusion is that like with this with this simple text based view promting with SDS LW we can get quite good 3D reconstruction results and in the in the next few slides I will introduce a bit more detail in one of our works called dream boo 3D where we use this text based view prompting and SDS losses for subject driven text to 3D generation so in this work the problem setting is that like in addition to the text prompt like a photo of a dog or or some of text prompt we also have some input examplars as input like this let's say this 3 to six casual cap of this dog and the internet output is this like the the the 3D Nerf model of this particular subject that is following this text prompt so we based this work based on the dream Booth which is one of our earlier Works where we showed that like we can easily adapt existing text to image models with this some with some with some input images with some with some simple fine tuning we can actually generate very photorealistic image of this particular subject uh with this dream Booth model so once we have this dream Booth model one could simply say that like we can actually first train a dream Booth model on this few Exemplar images and then we can actually do text based view prompting with SDS LW and then to get these 3D models out kind of but unfortunately this this did not work out one of the key issues is that like this dream boo model overfits to the given input views of the this object of this subject thing and the and when we when we when we actually query this such a dream Booth model with different views like side view front view back view we usually don't get very very consistent outputs so that's why that's why the resulting 3D model is not very coherent or consistent so as a as a remedy we actually propose a three-stage optimization pipeline for this so in in the first step instead of training a full dream Booth model we actually train a partial dream Booth model uh that means we train a we we we do early stopping for dream Booth so this way we can still get a a 3D consistent Nerf model with SDS loss on this dream Booth model on this partial dream Booth model but this but this initial Nerf is not do not have high subject Fidelity but it's but it's but it's more 3D consistent so from this initial Nerf model we do this multiview Nerf renderings and from these Nerf renderings we actually translate into this pseudo multiview subject images using image to image translation like SD edit uh this is again using the fully trained dream Booth model for this subject and then from these multiv view generated data from this multiv view subject images we turn another dream boo model which we call multiv viw Dream Boo and since it saw more views of this object and this SDS law with this view based prompting works better and we can get a final Nerf model from this multiview dream Booth which is more which is more 3D coherent so these are some of the results one of the nice things is that like since it takes also the text prompt as input so we can actually generate uh we can actually easily recontextualize given subject in 3D in different context like for example these uh in this bottom row examples we can put this dog with a green number in 3D also we can do some interesting stuff like we can also do some fancy post changes just with a simple text prompt changes like a a v dog sleeping or a v dog uh jumping and even though even though none of the input images have a sleeping or Jumping dogs we can actually we can actually create quite realistic jumping and sleeping dogs from this so that is about dream boo 3D uh next I will briefly discuss explain this for Artic nois so here the here the goal is that like given a given a noisy we images in the wild for animals like a zra Gira or elepant or any other animal um so we want to reconstruct articulated 3D models from these noisy web image collections in a self-supervised fashion so we do not assume any ground truth supervision like like any keyo annotations or any other annotation so it is it actually completely self-supervised from these images so it's kind of like a quite challenging task so we again leverage uh SDS LW on stable diffusion model for this tasks so when we experimented with standard stable diffusion model with SDS laws we find some issues um this this SDS can we need to back propop it actually back props the gradients through encoder in stable diffusion model because it is a latent diffusion model and we find that it actually provides oops yeah with this SDS L we find that that that it it actually provides uh it actually provides some noisy gradients and it also has a high high computational cost due to this back propping through the encoder and here I I'm showing an example image and after after one training step of SDS loss the how his image is updated you can see you can see a lot of noise around the B boundaries stuff so as a remedy we actually propose a decoder based accumulative score sampling loss uh so here the key idea is that like instead of back propping the gradients to encoder we actually do the score distillation in the latent space and then uh we actually accumulate gradients in the latent space and then we decode the gradients to get the image Target and then we compute the loss based on this image Target so it has a low memory consumption and we also observe that it has a cleaner gradients and provides more stable training and here I'm showing an example with after after one step of training with this Dash loss we can get much cleaner image so we we use this Dash LW in an overall reconstruction pipeline for Animals I won't go into too many details here where we actually try a optimize a part-based ml piece to optimize each part of the animals along with the texture maps for each of the parts and we have this previous work called hassi where we actually introduced several reconstruction based losses and in addition to that we use this introduce this Dash loss on top of stable diffusion and these are some of the results top row shows some input images and the middle row shows highy results uh one of the previous one of our works which is a previous state of-the-art on this task and in the bottom row shows Arctic 3D results and as you can see from with arctic 3D we can get much better 3D reconstructions uh compared to previous works and yeah stepping backs yeah so far these works these dream boo 3D Dream Fusion Arctic 3D and all these works are mainly based on text based view prompting and on the stable diffusion or imag or some other image Genera model but it but but it only provides rough Viewpoint control right that's why the results are not still not precise and stuff and one could do more precise camera control with this camera the camera post based view prompting so one of the first Works in this space is 0123 uh Yoshi presented this earlier uh in this Workshop uh where we can get the precise view control with camera conditioning kind of thing so one of the nice things with this work is that like uh uh it showed how easy it is to adapt an image generative model like stable diffusion to have a view control on top kind of thing and it works surprisingly well and um one could also use the SDS losses on top of it or reconstruction losses to actually do some reconstruction of nerve on top of it so we try to improve uh internally at stability we try we have an improved improved version of 0123 which we call stable 0123 and with some improved training uh things and we show that like with this table 0123 we we we get considerably better normal view synthesis results compared to 0123 or 0123 Xcel uh so here I'm showing some results of stable 0123 in the bottom row and compared with 0123 Excel as you can see stable 0123 results are like much more multiview consistent and coherent and this and this model is open source so you can also give it a try and along the similar one of the one of the one of the key issues with this Viewpoint control with this even with camera poses is that like there is still lot of multiview inconsistencies in these models 0 one to three and other things so there are several Works in this space uh that try to improve multiv view consistency I'm just uh mentioning two representative works here uh which which which which takes slightly different directions one is the syn dreamer type of Works where they actually also maintain they propos to maintain some explicit 3D represent representation during this diffusion process here here some Vel representation so that we can actually get some multi view consistent novel view synthesis outputs another another set of directions take a example fled by MV dream is that like we always try to predict if novel views fixed camera angles so that the network has can be more multiv viw consistent along the similar way we have a work at the cvpr which we call MVD Fusion uh where which is single view 3D via depth consistent multi view generation so we actually propos some depth aware 3D attention layers uh to actually generate these uh to actually have these consistent novel view synthesis using uh using stable diffusion model of thing uh I'm not going to do the details of this uh work here so please uh see the paper for details uh next uh I will go uh next I will go into some details of this table video 3D model which is along these lines of generating novel multi viw synthesis and 3D generation from a single image using latent video diffusion model so this work is based on stable video diffusion which is also released from our lab at stability AI uh which is uh scaling where we scale latent video diffusions to large data sets and it is still one of the probably one of the best open source models out there for video generation so we make use of this table video diffusion which has a very good temporal priors on it and then we we actually conditioned it with the camera post trajectories uh on in our stable video 3D model and then we fine tuned it using some uh 3D data set yeah specifically this stable video 3D model takes single image as input and also takes some static orbits this camera orbits and it outputs quite consistent novel views of that single image kind of thing so here I'm not showing the 3D results here here here these are actually the predictions of this video model which we can see are quite temporally quite consistent in addition this stable video 3D model can also take this Dynamic camera orbits they need not be like like a circular orbit bits and it canot it can also generate novel views along these camera paths so these are some of the sample results of this table video 3D model as you can see that uh results are quite multi view consistent and this is the comparison with several state of the art Works in this space for noval be synthesis and most of these Works actually try to leverage stable diffusion or adapt stable diffusion for this tasks and as you can see with the stable video 3D we can get much more multi viw consist results compared to most Works in this space uh so in addition once we have these multiv viw novel multiv view synthesis using stable video 3D models we also propose some novel techniques to get 3D objects from generated views uh as a result we get St of the multiv view and 3D generation results specifically you given these novel multiview video thing and we first optimize a Nerf uh using a course Nerf using a using purely image and geometry losses and then we use uh we also optimize the illumination modeling so that we can have an illumination disentangling from this optimized Nerf model otherwise shadows will be baked onto these Nerf models and then we use marching cubes to convert these 3D Nerf model into this uh uh this tetrahedral mesh which is a higher resolution tetr hydral mesh and then we further optimize with image and geometry B Bas reconstruction losses and since these stable video 3D provides quite good multiv viw synthesis results just with image and geometry losses even without SDS loss we get quite good 3D uh 3D reconstruction results in addition optionally we can actually uh since these novel views do not cover all the all all the corners of the object we also propos to use uh a soft mask SDS loss using this s sp3d model for further improving the 3D reconstructions so these are some of the sample 3D Generations from this s stable video 3D model as you can see they're quite uh 3D coherent and quite decent quality so in this work in this previous work we showed that like we can adapt these video and image models for 3D reconstruction for for 3D generation but we can also go beyond simple 3D shape and textures with these image and video models uh the stable diffusion can also be adapted to predict other related 3D attributes such as environment illumination or material properties Etc so one work we're going to present in Ed cvpr is this is diffusion light where we where we show that like uh we can get light prob for free by painting a chromeball u where we adapt stable diffusion uh to in paint chrome balls to estimate HDR lighting from a single image as as we can see from these results these environment uh Maps predicted are quite realistic un feasible yeah briefly about the technique we use this depth based control net to actually in paint chrome balls onto a given input image but since this is a diffusion process we usually get different Chrome balls uh if we if I should run it if I should run it 10 times we get a different different Chrome ball each each time kind of thing so we have some simple techniques where we use a medium Chrome ball and again repeat the process until we get some a robust estimate of this environment illumination so we also estimate HDR map HDR environment illumination by using by training some luras and with a different uh that actually that actually covers different Dynamic ranges so please see the paper for more details or also come to the presentation and another work along this direction is that like where we this is Alchemist where we adapt stable diffusion for precise material control in images where we show that like we can fine tune stable diffusion to actually uh change material properties like metallic roughness or or the the transparency or even some complex properties like transparency like ashon in the ashon in the figure here so we get quite realistic results they may not be completely physically plausible but they are quite realistic so here again the technique is quite simple actually we actually generate some lot of synthetic data sets using existing uh using some cycle slenderer and some existing synthetic hdra maps and synthetic objects and by using the synthetic data sets and with this with these finra changes in the material properties we fine tuned the stable diffusion model here the instruct pix to piix model to actually output these precise changes in these material properties uh under work in this space uh is the Gest where we actually find you where we actually leverage stable diffusion in a zero short manner to transfer materials from one image to another of so it's kind of like a quite simple technique but it's kind of quite powerful where we show that like a given an input image and a material Exemplar we can actually apply these materials onto these images in 2D uh without any going into without any explicit estim estimation of 3D shapes or 3D materials or anything this is everything is done implicitly by stable diffusion here which is quite powerful uh so that concludes much of my talk so in summary uh we show that like these generated 3D Technologies can form a foundation for wide range of 3D applications despite not having a unified 3D generative model there are two emerging Technologies in generative 3D space one is direct 3D prediction and second one is multiview generation which both are both are quite promising and we are in very exciting time for both these directions and this for these multi generation these image and video models can be easily adapted to to actually get quite good results in general these 2D generative models can act as World models and can be adapted to different types of 3D generation tasks uh that concludes my talks and thanks and any questions or comments please come ahead thanks for the wonderful talk where for questions please come ahead to the mic that's right in the middle and come ahead thanks hi uh thanks for the great talk yeah so that uh actually I have played around s sp3d before so like I noticed that if sometimes I input images with some text then it may produce some uh consistency across the generated image uh what is the major challenge in the those F grand um uh fine detail multiv view consistency when generate multiv view images yeah I think I think yeah s3d provides a good multiv view consistency but still there are several uh issues to fix there and stuff it's just that like probably we did not see enough training data in in observers there is not enough textures and stuff with this text things so probably scaling up to more data sets and more in the wild multiview data sets might improve the results uh so uh a follow question so that that in general the shape the large shape will be uh looks multiv consistent but it just maybe like some texture a little bit detail looks bit inconsistent what do you think to be the major difference between these uh uh General course resolution consistency and those fine texture consistency what is the Tex technical challenge have that so that least course course multiv consistency cannot also guarantee like doy multiv consistency yeah that's that's actually good point so so there there are actually several techniques used in 3D graphics literatures and Gaming Community where they actually disambiguate disentangle course geometry with fine textures kind of thing let's say with UV mapping and stuff so we have some Works in progress where we actually also trying to disentangle this in for such the for such 3D generation things so yeah so so there actually multiple trade-offs so there is no s simple answer to that but yeah depending on the type of application we may have to disentangle fine grain textures with course geometries okay thanks good question have you considered generating a whole scene instead of individual objects sorry uh if I understand correctly the the the question is have you do you have you considered uh generating whole scenes in 3D instead of individual 3D objects yeah so the question here that like have we have we have we have we consider generating the scenes instead of objects yeah so far so far we have mostly focused on objects but yeah that is we have we have several Works in progress where we are also where where we also considering scenes uh the main challenge is that like the availability of data sets for scenes and but but but we are hopeful that like with the with the more powerful video generative models we can also go into scenes as well I mean do you just take individual objects and place them is it just a placement problem or you um it's like a is it like an object composition kind of problem or you use the whole scene for Generation uh it is not just an object composition problem I think scene has much more background things like walls and floor and other things as well so there is actually much more data set problem as well and it's a uh yeah one could tackle both like background generation and putting objects in the scene stuff uh yeah yeah there's no single right way to do it all right thank you thank you for the questions we'll be able to take two more questions thank you so much um thank you for the talk so um assuming that the 3D data sets are going to grow in the in the time what do you think have bigger potential for 3D generation the direct 3D prediction or the multi multiv generation style uh I think given enough 3D data that direct 3D prediction has a lot of potential because it's quite fast and it's actually seamless and stuff and in general it's but it's difficult to scale 3D data sets much unless we get lot of data from gaming companies and stuff uh my general hunch is that like we might need both probably one could directly predict 3D and then use mult use this multiv optimization based techniques for further optimize this 3D generation quality thank you yeah thanks hiun uh thanks for the great talk uh one question regarding direct 3D prediction how big what is the resolution of the occupancy field that you're predicting and uh how do you see uh the way forward in predicting high resolution uh 3D objects natively rather than doing multi view generation that seems to be the Paradigm in the recent past yeah I think I would not say multiv generation is the main Paradigm direct prediction is also is gaining lot of attention and stuff for this tripl planine resolutions I think for this work we use 64x 64 by 64 which is somewhat small uh we don't get high resolution stuff and again there is again some disentangling should we should do some disentangling of geometry and textures there geometry can be low low poly and low resolution but textures can be high resolution and stuff so one could go to higher triplan textures or one could do this disentangling to get high quality 3D how do you see the way forward to go from the 64 Cube limitation to let's say 512 or 1K uh large geometry Generations yeah there ways to scale up there are some several transform yeah for instance in this work we use these Transformer layers to predict these Tri planes and there are also several advances in Transformer attention layers one could leverage to directly predict uh 512 or 1024 things but I feel the main bottleneck is not the tri plan resolution but probably availability of such high quality data sets to train those than for to share and uh I've noticed that and the most uh popular generative stre model is about Point C and surface but uh that's is not quite friendly for edit editing in CAD software so and there's M shiity and poen which doing multistage to translate uh Point C to mesh but uh what do you think uh why it there so uh not not that large number of projects about the reverse engineering stuff what what's the major difficults uh do you think so sorry uh can you can you can actually be more specific on reverse engineering what uh I mean uh so mostly uh the the the generate 3D is in surface they generate surface and point clouds but not meshes uh but I believe MCH is better for uh industrial and uh for add yeah in general the current uh the current strategies for most of because because meses also have have this we also need to deal with this connectivity it is a actually a graph problem so so the so the usual strategy now is to actually either predit Point cloud or these triplanes or Nerf representation and then and then actually postprocess it to get some meses out in a differentiable marching cubes or or even some standard of the Shelf techniques kind of thing but I agree that like meses are much more suitable for industrial applications and lot of applications but maybe we will see more and more Works where that directly try to predict meses thank you thank you so much thanks so much Vin for the great talk and thank you for the questions we'll uh move on next to junan all right so our next speaker is junan who is an assistant professor with the robotics Institute at Carnegie melon University and prior to joining CMU he was a research scientist at Adobe as well uh junan has been involved in a lot of exciting early results uh from generative models for 2D image synthesis like gaan and pix topx and has done a significant work exploring how to best control these models especially for Generation uh he's also worked recently with 3D aware generative models and today he'll be telling us about controllable 3D generation so excited to hear that yeah you sharing screen zoom turn on your video too y hello yeah thanks for the introduction U I try to be quick so you can have your lunch earlier anyway and so I'm very excited to be here we will talk about some our resarch work on controllable 3D Generations um so we have seen on two types of um parad them when is uh such as you can you can modify the shape and edit the material in in blender or zbrush um they you get very precise control of what you you can get but the draw back is might take lots of expertise and workload and and so that's one one method the other method is go for the other ex let's just uh using uh text prompt since everything is about prompting now nowadays so you can write a simple text prompt and hopefully you can generate an asset um with some method like dream Fusion but other more recent works as well um so what you show with this kind of Paradigm is it's very hard to uh describe uh shape um even material uh using text prom for example we try to say a car with a long shape smooth curves and somehow you get something like this so um uh so it's it's very hard to describe these things in words and it's not sure even you can describe it the the model may may or may not works out well if you provide very detailed shap description okay U so we would like to uh find something uh in between uh these two uh paradigms okay uh so one work which has inspired our uh recent efforts is a very uh one of my uh favorite paper um called Teddy system by uh te at s 1999 which the he his work enables us to uh create create a 3D object with very simple uh 2D scribbles okay um and it's very hard to believe this work was produced like 25 years ago but what I encourage you to look at Key System U so we inspired by this work we would like to add special controls such as us scribbles into recent 3D generating models either G models or 3D diffusion models um so one the first thing we have tried uh that was also four four years ago yes we we wrote the one of the first paper on how to editing uh generative nerve or 3D G like that so A generative nerve looks like this is basically you're able to uh generate different types of nerve if you sample a different shape code or appearance code and this is how the standard Nerf looks like uh and so there has a very um well-known paper called graph which basically adding shape code and color code to the Nerf architecture and this allow you to uh generate a random random uh object with different shape and appearance uh in in in the test time uh so we have done some sight modification in 20 21 which we we introduce a shared branch which in codes the common shape of the cars or chairs and also the shape code will Feit with the uh it's controlling the instance specific shape variations okay um um then our message allows us to generate uh different types of chairs with different materials and different shapes as well um but the work this particular work was focusing on editing the editing the nerve rather than just generating random nerve so we would like to um modify the appearance and shape of for Nerf model um so so now the next question is which which part should we edit so for the shape editing uh we can actually just modify the weights of of the of the Nerf networks um and and by by maturing the the predicted depth um versus the user provided guidance and for uh for the color editing uh it's even easier uh it's we can just ask the Nerf to predict the color and we try to match that color uh to the user provided uh color scribbl uh only at the particular regions uh and importantly to keep the background color the same we also have some background color constraint so that you don't color the ening as the particular color and for for the color Ting it's much easier because we can keep the the rest of the of the geometric branches the same so the color editing is very efficient only takes um several seconds but the shape editing is more involved it takes you modify more layers and here is how how how the interface looks like um okay so try to play the video uh this is the render videos from from the Nerf okay and you can pick up a particular View and you can pick up a particular color your favorite color and you can apply very SP scribble onto the shape and then you need to and our model our M will update the weights of the nerve uh to reflect your edits and you also need to specify which which which color you would like to preserve uh called background strokes and now once you achieve that you can actually render the same nerve at different view point just like a regular nerve and here are some additional editing examples um for example here we can change the the color of the card and here we can remove PA of the object as well another changing the color of the chair okay iove parts of the the object as well yeah so this is our first way of adding the control through by adding uh the user scribbl as as constraints and try to optimize in the shape code appearance code as well as part of the layers uh to match the users uh constraints so what limitation we are facing here is since we are doing optimization based method uh is optimization is is is fair slow especially for the for the shape editing so it might take you several minutes uh per user edit to update the Nerf wids uh which is not a very uh good experience for if you want to achieve interactive editing experience uh and also we need to implement different types of uh optimization message for different type of user edits so we need to manually see how we can Implement uh each loss differently to for particular user edits so it's not very scalable um so so having working on this direction um but the next idea we have is maybe instead of adding uh do this kind of test time optimization test time training we can directly uh add in the spatial control like scos as the conditional input to the to to the generative nerve architectures U so this is the work um produced by by last year CPR done by student student can go and Sh as collaborating with Deva CMU um so there already prettyy lots of work on with along with similar ideas basically uh this a sketch MCH you can generate uh MCH G and user sketch and here is another work which try to EDI in face although they they only apply to the to the frontal views uh so in this paper we would like to have a framework which allow you to uh support all kinds of user controls and support all kind of subjects not just faes so the idea here is we would like to um um generate a Nerf objects given conditional on on a s a single 2D user sketch um but now the next question is once you generate objects maybe you are not happy with certain parts you would like to edit the object U maybe it's it's very hard to edit uh some some parts from a particular Viewpoint so when you edit the object you might want to also change the Viewpoint okay so to facilitate these functions we actually TR the model to predict both the 3D appearance and shape as well as the the 3D labels either can be a sketch label map also can be a 3D semantic label map okay and this allow us to uh uh modify so once you generate object you can change to a different Viewpoint and you can modify uh the the shape of the of the car make it more wrong from a different Viewpoint notice that this these edits is very hard to achieve if if if you uh do a left facing um like bu Point okay um so here is how how the how the interface looks like um user can create a scribble and we can generate a car and you can further edit here and we can further refunding the the edits okay um right not only works for the for the cars but also for work for like other objects like cats and faces uh and some other cats um so let's see how how how it works so the basic idea here is the input is a 2d label map and we would like to produce both the images and and three labels which encoded by a Nerf model okay um okay here is how the visualization of the SED label M okay um One requirement of for data is we do assume we um we have access to the images and uh we can learn a segmentation model or we have ground true stable map and we also assume that we know uh the camera po uh in the training time um the the PO can and label Maps can be obtained by the of the chef models for certain categories like like cats and cars and faces might be harder for some other categories um okay and we can sometimes we get the P labels by clustering the dyo features um so we can get Ed uh Edge labels by using long Edge detection method um so given this SC data let's see how it works uh we start with a very famous architecture by Eric Chan at all called EG 3D uh so the idea of EG 3D appreciate people are very familiar with is you can uh have positionings and condition on the L code it can produce density and color uh here okay and then you f your discriminator and you you apply a g loss to make sure the the render 2D view of generate images is the the Nerf is still realistic uh so one thing we can do is we can uh just simply add this conditioning to the h3d which we call Baseline called h3d d c c means conditional um so here the idea is uh you can um just encoder to encode your label map uh and to the L code and you and rest exactly the same as e3d okay um and now you can render The View as well uh this is our first attemp unfortunately uh it doesn't work doesn't work that well um uh I think it do generate some realistic samples but the one issue here is the alignment between the output and input uh is not very uh very good for example here you can see uh this forehead is different from uh doesn't does not respect the uh the hair labels in the input and sometimes you put a glasses it doesn't respect that as well um and and and it just does it can produce Rel samples but it does not respect the input and so the reason for that is one of the reason is um there might be two of two reasons one reason is we have a encoder and encoder all the spatial information into a a very narrow bottleneck so it lose lots of high frequency information in input and the second reason could be we are not explicitly designing a loss to um ask the network to respect the input and so we made some modification to the this baselines U the first thing we have been doing is we ask the network to also predict the semantic Logics like either the path labels or Edge labels or semantic labels so the network can render both the the image as well as the label map okay uh it's it's very it's very simple just add additional Branch uh to predict the semantic logics okay and then in the training time we ask the network to produce um both the cant logic and intensity um you can get semantic logic very easily by this is your standard nerve um formul but you just need to also ask net to predict the seman logic along the each point in the aray so it's it's very straightforward um and during the training time uh so there are two types of scenarios one scenarios we given the ground choose image uh since we already know uh this is the ground choose image we know the post of the ground choose image we can generate image and minimizing the Reconstruction loss between the render image as well as the as the ground ground choose image and since we can also derive the ground choose label map so we can also render this model in that particular view point and minimizing the Reconstruction loss in the semantic La map space um but uh but for other cases which we can also add a g LW to make sure this looks realistic um but for other viewpoints since which we are interested in generating this image in different view points we can sample a noal Poe and we generate all kinds of sematic label Maps we generate all kinds of images and we can ask um the network to uh produce these images but since we don't have access to the ground choose of the rendering at noval poses all we all we can do is we can enforce G loss to make sure the rendered label map look like a natural label map the render image looks like a natural cat images so we do that 50% time with noal poses and 50% time with ground choose postes and and here is uh our results you can see our results can respect the input label map much better then a baseline model by adding conditioning to IM 3D okay and additional things we have done to further improve the results especially for editing is we would like to make sure uh given uh the different uh given the the label map of the same person at different view points we should able to uh generate uh exactly the same 3D shape uh so we call this CL VI consistent loss so the idea here is we would like to start with from post one and then we can generate a a 3 label map we can render we can s a different post post two we can render this uh three label map at post two and then we we go the we go this apply the step back again and try to generate the 3D nerve object and then we can render uh the the the the the semantic label map at post one so this is kind of like cycle consistent loss but just with post one post two back to post one okay and this allow us to uh roughly keep the identity of the object uh building the user editing where if you user want to change different viewpoints and and this method Works U much better than some of the previous work as well and here I will show show more results um okay and please check our website for for more results okay and here we show visualization which we basically uh try different input layout as well as we sample different appearance code and this can give us to generate object um with with the the same input but with different appearance okay here is a video of it okay so this this work still has a boun limitation first it is if you if you if you input a very strange like sketch inputs it will tend to um generate something which are within the data set so it's very hard to generate a car with a very inusual shape it does it does not generate very well out of distribution user inputs um and the more importantly it doesn't you need to we are using EG 3D so you have to tr a separate EG 3D model uh for each categories right so if you TR a model on cars you cannot use it edit cats so it's very category specific and more recently people are more interested in like generic text to 3D generation rather than the category specific ones uh so there are lots of nice paper like dream Fusion magic 3D uh and there are lots of uh interesting uh followup papers and which um more very recently okay U so one issue we do observe from recent Tex to image to 3D method is this issue of the the baking lighting for example here is a results from very nice recent work of Fantasia 3D uh produce very nice results but you can see uh some of the lighting has been is Bing them in in the texture Maps so so uh so if you change the lighting you put it the same asset in different Li environment it will look the same so which is a little bit strange U so in in very recent work um we propos a new method uh again it's led by student Kango in collaboration with uh blocks as well as Stanford uh manage group so the the paper is called Fresh text so we try to uh generate Rel reliable um textures given uh given input mesh with light controler so we talk about what is what light me so here are some results is we given the same mesh we can uh we can type different kind of text promp and allow you to add different kind of texture uh to the input mesh okay um so here the input and output put is the input match provided by the user and user also need to specify text form uh and our goal is to uh to produce texture mesh which can be rendered uh in different lighting environments um okay um and there lots of PR work was trying to generate this kind of PBR material map uh for example here uh the fantasa 3D will try to generate the the diffuse term the rough term metallic term um so and and this P map but the map is not very um um this in tangle with the lightings um so and we we were wondering uh what has what was the issue here why the uh the dream Fusion type of approach always create a baking lighting um in in the in the results uh so for I'm pretty sure everyone is very familiar with sction loss but just give a quick recap you just you try to uh produce some material try to render you to add some noise uh you you ask diffusion model to prediction noise and Cate the difference and try to do the back compation uh to to the to the framework um but one issue here is to render a image you need to specify the lighting right so you in addition to the material and geometry to render image you need to specify the lighting now the question is which writing should we use um and okay maybe you can use some uh predefined environment map you can manually Define some lighting simple some lighting maybe it's okay but the the the the most serious issue here is uh you sample liting L star uh diffusion model might produce a image or produce a noise of this image and that imor noise may not correspond to the lighting use samples right so so so this is the biggest issue is you sample lighting but the fusion model may not respect the lighting you sampled and and we we are not aware of that and so and to and then all this the lighting difference between the the lighting the gener image versus lighting sample the difference will be distilled into the texture map so and so this is the one of the issue we have observed uh so our solution is try to adding controls to this called thetion loss the control can includes the shape but it can also include the lighting explicitly so we will teach the diffusion model which type of images we would like to generate which type of lighting we would like to generate okay so our main efforts is trying to make diffusion model uh be aware of the lighting um and we use a very standard approach called control net which hopefully we can TR control net conditional on the geometry as well as the lighting um okay um so our um proposal is to uh um to feed the control net the text prompt as well as C image under a specific lighting condition is L star and also with different materials uh and we can train this model using some synthetic image training set like object verse since we are the one who rendered the final image um so here how how it works we we sample lighting and we apply different type of materials can be non-metal non smooth half metal half smooth and pure metal uh smooth and then we can render uh three uh shade uh shaded image under three uh different materials with the same lightings and we can concatenate these three gray scale image as RGB image to to to to the KET so this is KET inut you see a a shading image at different with different materials as well as text prom and you the Contin is ask to generate uh this image which we render using object vers and now and we call our Network light control net and once you have this network uh given the same input you can render uh you can three different text prom and it will generate a image with different material but interestingly it will follow uh the input lighting conditions uh very precisely so even it has different material it follows the same input lighting conditions okay and but you can also render the same material with different lighting conditions and it still follow the lighting conditions okay there are some more details about how TR this network for example uh we need to come up with some n and Tech and we also need to extract some original texture material and then we need to uh try this three different types types of conditioning material and these are our tring inputs and once we T light control we can directly using this slide control Ed with a standard framework like like like SC distillation laws uh and and here um we use U particular um representation called texture Hash Hash grid so and we we sample random views and lightings and the and once we sample our liting we can feed this lighting can render the C images and we can feed this lighting conditionings to the light control net and we can still add the same noise and then we can predict the noise using the light control net subtract the difference and back up gate uh to this networks um but the interesting thing here is since we using a light control net the render um the light continer can give us uh the the correct noise prediction given the same lighting conditions which are follow the same lighting condition which we use to render this image in the first place um okay so this is the basic idea to initialize it to make it much much faster we have two tricks one trick is we can actually uh initialize um the network uh with light control as well using multi prompting uh this is our initialization and we can use this image as our to train the model uh with reconstruction LW okay so we we can we can just uh take uh feed forward pass and then we get these images and we can use these images our reference image using reconstruction laws to minim optimize the texture Maps Okay and this will enable us to get much faster results uh because SC distillation sampling is very slow compared to direct construction laws like lpip or HB lws and another thing we have observed is um the the to Compu this SDS laws you have to pass your network to uh V incoder in the diffusion model and eventually we found out actually this V encoder uh takes 50% of time of back appication so it's so most of time half of time you will spend your time on this VA encoders um so to to speed up this part uh we actually uh distill this V encoder uh which takes uh more than 100 milliseconds and 500 milliseconds uh to backage the gradients uh we distill it into a small Network remove some attention layers and our distilled encoder um is about 42 milliseconds for the forward and 81 milliseconds for for the backward and and the backward is we we do care about backward because we are using the SDS lws to optimize the texture M so by doing that we can actually uh speed up our method from one hour if you initialize the texture map using like control net you go to 10 minutes uh if you distal your encoder uh you can reduce the the the inference time from 10 minutes to to four minutes okay so again a quick recap of the pipeline uh we we used the our light control net with multi view prompting to generate a consistent multi view images and use this as reconstruction LA to initialize uh the texture uh has GD learning um and then we in then we further apply STDs LW again with the help of light control net to make sure we can disentangle the lighting and materials in the SDS laws okay uh and we use a very standard like PBR uh representation like which includes the whether it's metal or not the roughness bump map as well as well as the base diffuse color okay and here is some results um comparison um the first row is fantastic 3D and the second row is uh our results our result is not perfect but there's a better job than than the Baseline and here we show our M material rendered with different uh lighting conditions okay and and here a different environment Maps two additional environment Maps um here are more results again please check out the website for for additional results and comparisons great so again promise I will keep it short so in summary we U talk about three different ways of adding uh user conditioning like user scribbles to a ping uh 3D gener models either it's a game models or diffusion models um and there's different ways and we can con controls as constraints and do some optimization you can add constraints as conditioning inputs to network so it works much faster and also more recently we are interest in adding these controls uh to the score dation sampling law uh thank you yeah all right thank you for that nice talk uh anyone has questions you can use the microphone in the middle you have time for several questions maybe I can ask one while people line up um so you talked about this trade-off between using constraints for optimization and direct conditioning and ideally we'd like to do direct conditioning for a lot of this but for really fine grain control like the edge Maps you showed or maybe 3D constraints um it's much easier to do with optimization you get lot more accuracy I guess so do you think uh like moving forward we should be doing a trade-off of using both of those like conditioning first and then optimization on top of it or can we get conditioning to the point where it's accurate enough to just directly do a feed forward pass through a network and and hit constraints exactly yeah I think I think this is a great question um so I think one we have seen people doing in the 2D editing is you always apply the fef encoder and then you try to optimize for like 10 iterations so that's again depends on how good your encode is and how much iteration you need but I think a hybrid approach is very promising for 3D editing as well yeah thanks uh hello I have a question about Flash Texs uh in this work you train a control net that is condition on lighting right but I think uh you do not expect show the material properties to be consistent across different Generations so is it possible that uh the generated material properties uh are inconsistent so the optimization process may be slowed down um so the question is whether um we enforce the material to be 3D consistent right yes okay okay uh I mean we do use the the 3 uh like a hash grid right so uh so you need to U so given XYZ position it will produce uh this PBR material so it's it's it's kind of 3D consistent by by default and then you to project into a 2d so so we 3D but uh you your generated image May uh reflect uh the material properties right but the generated images the material properties in the generated image may be inconsistent is is it right so you you are using generated images to optimize the material properties yes and the material properties in the generated images may be inconsistent yes that's right um I I think the since we are using a rendering to to to render the the 2D image given the 3D asset so it's kind of uh it's consistent by default may not be correct maybe still have some light light baking it might not perfect I think it's consistent by default when you render like 3D asset with material into 2D image okay but may or may not be correct so you know okay thank you yeah thanks for question all right with that let's thank junan again and all of our speakers from this morning session thank you so now we're to take a lunch break until 1:30 where we'll resume with more keynote and Spotlight speakers thank you e possible e I think e my good e bu e you my no e e for e e e e who spe good e e okay spe back I e e th w for e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e try to up Microsoft like so you're just you're ining more heard would matter Happ you Jo think gu have I think e you know you have aent model they basically ch a and not that that's likeit I would be ask said was that's not what that's that e hour and they're like their system gets like % accuracy as e e I can anate all or I can the you're e e e e e e e e e e e e e there day like let e e for s been around e e e go out all right hello everyone we're going to get started with the afternoon session here uh our first speaker is Gordon Vin who's an associate professor of electrical engineering at Stanford where he leads the computational Imaging lab and co-directs the Stanford center for image systems engineering Gordon works on really an impressive range of topics including computer graphics and vision AI computational Optics as well as applied vision science and as part of this he's really been at the Forefront of work into modern 3D representations and neural Fields uh since he published scene representation networks back in 2019 and has really since been continuing to innovate on these 3D representations and generative models uh so today we're going to hear from him about fun with generative AI looking forward to it all right thank you for the introduction and welcome back to the afternoon session uh I'm Gordon wetstein my appointments are in dou and also in CS so traditionally I've been working on a lot of image and display systems engineering so examples like that include design and Fabrication of Novel types of VR and AR display plays or Imaging systems so for example just two weeks ago we published a paper in nature on full color 3D holographic AR displays with meta surface wave guides we're not going to talk about this but I'm going to use this as an introduction to tell you that typically I give talks to the Optics and physical sciences audience on why it's important to have a display embedded in your eyeglasses and all the cool experience that that uh enables but today I'm going to make the argument that for many cas is you don't actually need a display building a display into your eyeglasses is really important and really difficult also so but in the near term we might be able to deliver real value to Consumers uh using a stylish form factor by simply embedding a camera and an eye tracking system into a pair of ey frames and connected to a multimodal llm we call this Gaye GPT uh and it's a really fun thing at you can think about it as an interface between the user and the generative Ai and it's actually really useful so uh let me just introduce that so many of you probably know what a multimodal large language model is U GPT for V for example you give it an image and you ask it a question for example um you show these bananas and ask how many calories are in this and these multimodal llms are very good it tells you that you know mediumsized banana contains about 105 calories um cool that's great so now imagine you have a camera that looks into the world embedded in your eyeglasses it's going to see a lot more than just one banana that if I just look into the room they're about well the room isn't quite full but there's quite a few people in this room and lots of different things so if you ask the same question on a an image like this you know how is the llm supposed to know what you're actually referring to so this is where we uh dive into the topic of contextual AI contextual AI is this idea of knowing everything there is to know about the user through various means and then helping the user achieve a task faster or more efficiently that they want to do and there's a lot of gadgets out there that are fostering this for example meta and Ray bands have partnered to develop smart frames um there are some where like this AI pin for example and there are even dedicated smartphones now that are just meant for AI applications now how successful are any of these in the market um well I think time will tell but the basic idea is that each of them have a camera that sees what the user is seeing and they help the user understand their world around them but they use different interfaces for how the user can select an object and pay attention to something specifically so the frame frames are pretty intuitive uh the camera rotates with your head because the camera is basically attached to your frames which rotate with your head uh the wearable not so intuitive you basically have to rotate your body in order to select something and without visual feedback that's actually really hard to do if you've ever tried this a smartphone is really good at that because it has a viewfinder it shows you what it sees so you can naturally align the phone with whatever object you're trying to interact with so now going back to this example of the fruits you know if I asked the GPT for oh how many calories are in this it just has no way to know what I'm actually referring to and what I'm paying attention to so it's going to give me all the valuable answers there are and tell me about everything that it sees in this object uh in this scene and that's not really what I want uh so it lacks the specificity it lacks the understanding of my attention and so this is where the Gaze part of gaze GPD comes in gaze is you know or vision is our most important sense and the ey gaze direction is a super fasted natural interface for providing precise context about our attention a gaze tracking system basically measures the direction the ey is pointing and thus where the user gaze lands in the scene um so with the Gaze you can index into the world facing View and you know exactly what the user is looking at gaze tracking systems have become very small and uh performant for for example it's used in the Apple Vision Pro you've probably seen that there's two cameras per eye that actually track your gaze and they replace the mouse the keyboard many other aspects of the interface and replace it with this gaze tracking system and also hand gestures now uh we've been prototyping with these types of ideas for a couple years now and uh one thing that we put together was a prototype system that looks like a pair of frames that has a world facing camera it's basically just a high resolution webcam that is pretty wide field of view has a microphone it has speakers and has a binocular ey tracker in here and what this allows you to do is basically track the Gaze index it into the scene it has speech to text and text to speech embedded this was pre GPT 4 uh gbt 40 um but it's basically doing the same thing as gbt 40 and then you can basically just go around in the world and ask questions and I'll just show you a video of what that might look like hopefully you get the sound here do we get sound do I need to water this one frequently let me look that up for you no it does not requ water okay what is this called does it typically need a lot of maintenance is there something I need to do for sound sorry I don't shut that off do you need to share the sound I do need to share the sound oh okay cool so I think you get the idea this is real time it's not stage we this is a one shot type scenario where we just captured it walking past the person who's using it uh we didn't do it a 100 times we just did it once um you ask the question and you get an answer and the answer is very reasonable so if you ask about the plant it'll tell you how often to water it for example okay but how do you get sound on here oh I'm not the plant oh is a sansia commonly known as snake plant it is low maintenance and can thrive on minimal Water and Light this includes supporting multiple languages let me look that up for you I don't know if you have kids but they love Paw Patrol and translation of the user surroundings on command what does this say let me look that up for you the text says kood which translates to no entry except for authorized persons in English Okay so this is just a really fun thing it works in many different languages we tested it in Spanish Portuguese Romanian polish German English French also it's just baked into the LM right like we didn't do anything special for this but it's just such an intuitive interface to put this on your pair of eyeglasses and we run a whole bunch of user studies and we're able to show a few intuitive things so there's not anything surprising there really but uh it was good to quantify it and the thing is like how accurate can you select something well if you use a phone and you have visual feedback you can very accurately select something because you get the visual Fe feedback if you use your body without visual feedback it's horrible you don't know what you're pointing your body at it's not it's just not a natural way to select something if you use your head it's pretty good but as humans we use gaze as a as a as a visual selection mechanism within a maybe 35 degree cone or so so you can get a rough sense where what somebody's looking at but it's not super accurate and so gaze is very accurate as a way to select objects in the real world and that's reflected in in this scenario so the gist is that for no visual feedback type devices like eyeglass uh eyeglasses uh Gaye is the best selection mechanism and this works really well the selection time is uh actually really fast for Gaye because it's just so natural for phone is not so not so fast because you need to pull it out of your pocket you need to aim it at something and then start the app or maybe double tap some buttons and that all takes time uh and the accuracy is directly related to The Selection error so you know the accuracy isn't very good if the selection isn't very good because if there are a lot of objects in the scene then you're going to uh get the wrong one right so this part of the talk had nothing to do with generation and this is what this Workshop is about so I want to dive a little bit more into that topic but I just wanted to give you a sense of what my team is sometimes working on um and uh it's it's this really fun uh intersection of weable devices arvr and uh generative AI so let me talk a little bit about what the rest of my group is doing on 3D generation for example with camera control video um so all of us have probably seen the announce the recent announcements I mean Sora was the first Luma came last week uh Runway just came when was it this morning or yesterday or something I mean this is amazing like wow um we live in a time where we can generate photo realistic videos right I mean it's it's fantastic and the step from the last generation like anime div and so on to these Fusion Transformers is pretty large so I think we're starting to see these tools being really useful I think we're not quite there yet in terms of like having them being useful for creators quite yet because there's a lot of prompt engineering around it and prompt engineering is just not a natural way to create content necessarily so the question is like how do we take a a really capable model like this and make it useful for creators we're not necessarily super technical and who are not well wored in engineering prompts that are 100 lines long so one of the first things that you might want to control is the camera pose because imagine you want to have a certain shot and you want the camera to move in a certain way well text to video generation models don't enable that out of the box so we've been thinking a little bit about how to do that and you know one simple way to do it is to think about this as a conditioning uh signal so you could take the camera pose and then condition the video model based on the extrinsic or intrinsic parameters or combination of that um or you can use those and also some kind of a plucker embedding you just need to find a good way to uh encode the camera pose in a way that the video generation model can understand it and translate it so the way we prototype this was using a combination of plucker embeddings that are happening per pixel basically uh and the extrinsic parameters of the camera that goes through a an encoder uh that generates features that are then injected into a pre-trained video diffusion model in our case it was animate div so with that you can generate videos with a prescribed camera motion and the quality of these videos isn't exactly the same as we saw in Sor just now because it's limited by anime diff but the idea is directly applicable also to things like like diffusion Transformer so I think this is something that we're definitely going to see very soon and with that you can enable text to video Generation Um using a prescribed camera trajectory as you can see here looks pretty plausible I think the results at least qualitatively aren't hugely degenerated from the original anima um Quality if I don't have that as a reference here but uh we evaluate that quantitatively also on the paper and then you can also do things like image to video generation so you specify single image along with a camera trajectory like zoom into the image or move left right down rotate and so on and so forth and that works quite well and it also works with a lot of the uh customized video model so if you want to have something that looks like a Fango or some kind of a stylization it works pretty well so we did some initial experiments on this camera based conditioning on diffusion Transformers and it seems to work there as well so it's definitely something that we're going to see pretty soon okay so we can basically think about 3D generation as in one of two ways at least I do one way is we take a video model and we add camera control that gives us a 3D model because we can move the camera anywhere we want right but uh one of the challenges is really that the camera control with conditioning isn't super precise so the camera motion looks correct qualitatively but if you were to do some kind of a structure for motion on that you're not going to get the exact camera poses back right and in many cases is that's what you want you want to be able to precisely specify the cameras and that's not really something you get you're going to get guidance on where the camera might move but there's also no guarantee that it will so it it is a little bit challenging to control still so the other way to think about it is um well we could use a 3D generation model and add some type of an inductive bias that forces the network to generate something in 3D um and we could do that with a 2d diffusion model or a 3D diffusion model and I'll have some examples of both of these so when we first started thinking about this with uh my student Eric Chan we thought about it in this context of well we don't have any 3D training data we don't have any 3D diffusion models we really only have 2D diffusion model so back then back in the day in 2023 which was just a year ago this is a fast moving world right uh we thought about okay we're limited to 2D diffusion models and what we're going to do is we're going to try to be clever about how to force this this 2D diffusion model to be as multi view consistent as possible so what we did is we took an input View and this is single image condition novel view synthes so single view comes in we take a CNN extract some features we're going to lift these 2D features into 3D by using just a back projection we're smearing them along the Rays basically so there's nothing really clever about this we're going to take this 3D Volume within the frustum and reender it into our novel View and there we take these features and send them through we use them to condition a uh 2D unit decoder this is a diffusion model and everything is trained end to endend because Eric was doing an engine trip with Nvidia at the time so we were able to to do that with their gpus and it works pretty well we presented this last year at iccv and the basic idea is that you can take a 2D image like this fire hydrant and you can now control the camera explicitly and again it does that by exting the features lifting them reprojecting them and then injecting those into the 2D diffusion model so at the time we had View forer and pixel nerve which were slightly different approaches to this and our approach called gen Vias and gen Vias works very well it's very sharp and the main inside is that approaches like pixel nerve that are trained on in L2 lws will just gradually get more and more blurry as the target view moves away from the source view there's just nothing you can do about it because it basically averages over all the possible uh uh scenarios that could explain the single input View and gen vs basically samples from the posterior and gives you a very plausible answer we done a lot of engineering about trying to make it consistent to reduce the flickering because that's the main challenge here it's not that getting it to be multiview consistent actually to reduce the flicker um but it works reasonably well and we were able to do this also for room scale things by training it on matter Port 3D so again on the left you see the input view uh there's look outside the room which was the uh stateof the-art at the time and in the middle you see these generated views from genv they look good they look plausible they don't look like the reference so this is not a reconstruction task right this is an ill poost inverse problem ill poost inverse problem that means there are infinitely many solutions that are equally good we're not trying to find the original scene because it's impossible to see that there are many parts of the scene that are not part of the input image we're just trying to generate something that looks plausible that looks like it could have been there and that's exactly what we get for a limited sequence which is in this case fairly long so again we actually did this work about two years ago and then it was only published at iccv after uh a related idea is something we explored more recently where we focused a little bit more on humans and we wanted to extend this idea to 40 so when I say 4D I mean it's supposed to be 3D and also dynamic in time so we call this generative rendering because we're using a lot of ideas from the graphics community and what we're doing is we're rendering a template a very coarse template in this case a simple mesh with UV coordinates we're now using the simple conditioned single uh image generation approach stable diffusion in this case with a text prompt and we can generate well whatever you want to generate in this case a manga of the Joker dancing and now what we're trying to do is use the features from the diffusion model and reproject them using this template mesh because we have ground truth correspondences via this template mesh in 3D from one frame to another right so we don't even need to lift anything or learn this correspondence we actually have a a template that's pretty good and we can use the template to reproject our diffusion features so um works okay uh and then we can animate it uh it looks fine mostly consistent there's a little bit of flickering um I can run the same example with many different text prompts and it works okay so this was all done before any video diffusion model was really available right so this is all sort of like image generation model um with a video model you could do the same thing and probably get a lot more consistency but it looked pretty good this is actually a paper that will be presented later this week at cvpr by my student shangu Kai so if you're interested in this topic St by the poster now these are all small steps towards something that you know serves as a 3D Foundation model so what's actually a 3D Foundation model well we want to build a 3D model that is perfectly multiview consistent ideally and that enables a lot of different Downstream tasks and by Downstream tasks I mean things like text to 3D generation uh cat wearing eyeglasses a beagle in a detective's outfit so 3D objects that you never see in reality right but equally we want the foundation model to enable other tasks like single image to 3D generation and again this is something that you couldn't do with reconstruction I just want to stress that most of you will probably know this but you can't Nerf this or do any kind of 3D reconstruction simply because there's only a single input view there isn't enough information to know anything about how this Louis vuon purse looks from the back simply because you didn't see it right there could be a cat hiding behind this back and you wouldn't know it because you just don't see it so the best we can do is hallucinate it we need to hallucinate something that is plausible and because the model hopefully has seen many different Louis vuon bags from many different angles during training it will come up with something that looks plausible to you and this works pretty well these are all results from a paper called gaussian reconstruction model that I'll tell you more about in the next slide but uh works pretty well and another thing that we might want to do is a multiv view image to 3D reconstruction so we think about this as reconstruction but it's really a sparse view reconstruction so we're we're do we're thinking about maybe two views maybe four views and you still want to get the full 3d scene out of this so this would be more constrainted than the single image to 3D case but it's not as much constraint as a Nerf where you need hundreds of views of the scene to reconstruct it so it's not a reconstruction task it's it's really a constrainted hallucination task where the constrainted are the four input views you want to make sure that from those camera poses the scene looks like those four views but you have to make up what happens in between and so what uh we looked into and by we I mean mainly my post yinga and our collaborator Zan uh was this architecture that was using a pure Transformer uh takes the input views for input views uh it uses a standard Vision Transformer to tokenize them and then has a pretty standard Transformer architecture but with a upsampler architecture that's based on Pixel Shuffle and what it basically does is it's a transform that takes four views in and that still gives you four views but it translates the RGB values in each pixel to pixel align gaussians so it takes the RGB values and turns them into all the different attributes that you would expect from a 3D gaussian and so now you have 3D gaussians aligned with each pixel of the four input views they Define your 3D scene and you can change the camera as you please now importantly we really trained this sparse view to pixel and gaussian's module as a Transformer we did not do the single image to multiv viw generation or text to multiv viw generation because there are a bunch of tools online that already do that part uh so we just use them out of the box like instant 3D 012 3++ or tools like that they are fully compatible with this model so this was a really big step forward uh I think and um you can use this to understand 3D scenes too you just have to break them up into individual objects so this is an object Centric 3D generation model but for a complicated scene you can use Sam or your favorite segmentation technique identify individual objects segment them out and then do a single view reconstruction of those all right oh and I should mention that this happens in like a second or so so this is really fast uh compared to SDS based approaches that we've seen in the morning session they take a long time to optimize this is a single feed forward model is one example from a class of emerging feed forward 3D models so these are really fast and really good so um good stuff uh on the 3D side why are we not seeing results that look as good as Sora yet uh I mean it's partly because of the lack of available training data the internet is full of images uh Lon has five billion images I mean you know if you train a an image generation model on five billion images it's probably seen everything there is to see on this planet maybe not your cat or your dog but you can customize it with just a few images you think about videos there are not quite as many videos out there but there are a lot of videos and we can train video models based on this vast amount of data for 3D that's really not the case uh we have obra ver which is great and I congratulate the obers team on letting all of us work on really interesting projects on 3D generation but it's just not enough right if you look at the diversity these are all computer generated assets that look like computer generated assets they don't really look like Photo realistic content and we have about 10 million of them maybe a few more so 10 million 3D objects versus 5 billion images in Lon that's like almost two orders of magnitude of a difference so you know we need to close this Gap or be really smart about how to do it so this is a really great research topic and for all of you students out here in the audience on who are thinking about what to work on I think this is a pretty good topic because uh 3D is just not quite solved yet maybe maybe in a month it will be because we're moving so fast but uh knows there is again as I said a ton of training data out there that is photorealistic uh that is just single views of of a person for example ffq is a data set that Nvidia compiled a couple years ago already they trained all the Gans on this um that has 70,000 images of faces they're very diverse so it's not five five billion but for this specific category it's pretty good and we're starting to see large scale 3D data sets like dl3 DV coming out but these are also very small so one question is can we train a 3D generative model on 2D data um well I'm GNA go back in time just two years seems like an infinite amount of time by now but back then we worked on exactly that idea how do we learn train a 3D generative model on 2D data it doesn't seem easily possible but back then we had Gans we didn't have diffusion models yet and it actually worked really well we came up with this idea of a tri representation to represent a scene and it's actually pretty widely used by now for many different reasons uh but uh some of the reasons why we developed it in the first place in our paper called EG 3D was that we can use a 2d CNN based generator generate the features on these multiple planes we can treat the scene as multiple 2D planes we can rearrange these 2D planes into 3D uh and then we can render them using volume rendering and it's also much faster to query this you know sparse volume if you want or low rank volume that's how I think about it um then to query a new network so it's much faster than Nerf for example and we just have a very lightweight implicit decoder on top of it with an AG feature aggregation function so really fast um compatible with 2D generators and it allowed us to train 3D Gans on 2D data that were pretty multiview consistent so here's a couple examples probably seen the these already but we can explicitly control the camera this person doesn't exist in reality they're generated the Gan has never seen any depth maps during training it just learns that there is some depth that helps it to be more multiview consistent it's very diverse it's very fast can generate all of this in real time with more than 30 frames per second uh so that's different from most diffusion models it's category specific so it can generate anything but it looks really good and I think just two years ago this was sort of the state-ofthe-art and 3D generation I just wanted to show it because I think all the cleverness came from the design of the inductive bias in the form of this triplane representation and you know most of us don't use Gans that much anymore but I think being clever about how to design the architecture and how to combine you know generation architect architectures with inductive biases in a meaningful away can be interesting okay so here the cats I love this example back then because it was it showed you that this is an AI and not real because you can do interpolation in the latent space and you get semantically meaningful uh results so this shows 3D semantic interpolation uh and these gansters have a nice smoothly varying latent space which is very nice and most diffusion models don't have that so you know yes diffusion models are awesome but it's hard to make them well behaved and interpolate things in in the noise space we worked on many different follow-up projects like lumigan which was an idea of not generating the pixel colors directly but the normals the the normals come automatically from the from the density but the alido and the diffuse and specular components and then have an explicit uh lighting model on it in this case it was just a Fong model but uh if we train the Gan and introduce this physical bottleneck in the form of an explicit lighting model then we can edit these parts right just like we could edit the camera for the for EG 3D we can now edit the camera and also the lighting because that's the explicitly controlled part so during runtime once the model is trained we can use any environment map and relight these scenes in a reasonably convincing way so here's an example where we have the same identity we have this latent code interpolation and we're showing the same identity in different lighting conditions and you know it is pretty convincing I think and useful for many applications so we've seen good first steps towards enabling relighting too with diffusion models for example earlier I think junian was talking about it so uh this is great that we we're seeing this now also for diffusion models but 3D diffusion models with reling for photo realistic heads is in real time uh is not not quite there yet so again Gs are still interesting in some cases we've been thinking about generating full human bodies too uh here the challenge is that you know if you look at natural pictures of humans in the wild oh hold on they they are pretty ill behaved They Ride tricycles and bicycles and they wear purses and do have all kinds of other accessories so they're not really well aligned the body poses are very different in these images so if you look at faces they're pretty much the same most people have two eyes a nose a mouth and two ears and some hair um so it's fairly easy to align them well but for full bodies that's actually much harder um so one way of doing that is training a again a 3D Gan on these types of data was something that we explored in a project called gazan shell Maps which will also be presented later this week um and the idea here is that we again use a g framework we take a 2d generator it generates a whole bunch of textures these textures encode the attributes of 3D gaussians they texture a inflated and deflated variance of a simple template mesh which we can explicitly control the body pose for and then we can just render it and use a gan discriminator to to train it so this allows us to take large data sets of single view images train a 3D Gan that is now articulable so we can do the interpolation of these gaussians we can make the character walk or jump or swing or do really any other thing like we can explicitly control the body pose all right so here's just a couple of examples of different poses so it's getting better and better even with the Gans this brings me almost to the end of the talk I want to show one more Outlook of a really cool project that is going to come up at sikov this year it's not published yet but it's going to come out any day now uh this was a collaboration with noas nav team at Google research and the the idea was to train sort of like a 3D video model like towards 4D generation on street view data so my student boyang was working on this with uh partly at at an internship with Noah and the idea is that we have a map we know the height so we have proxy geometry boxes basically of all the different buildings we can now train an animate diff like video generation model just a few frames but we came up with some tricks to make it auto regressive so we can generate arbitrary long sequence and the nice thing is that we can also control the camera using a camera control mechanism uh we can do text prompting all kinds of things we can run Nerf on top of it and uh it's uh really multiview consistent it has a tiny little amount of flickering in the video generation model but that totally goes away once the you look at the 3DC and then what you can do with that is you can edit the weather the day of time of day uh the geometry we train it on a couple of different cities and you can make New York look like London or Barcelona and so on and so forth so it's it's a little bit difficult to appreciate that for a small video so here's the result on the left you see the map car is driving around we render the proxy geometry and some other additional information into a g buffer use that to condition the video generation model in the Middle where you see the output there's a tiny amount of flickering still but it's good enough for us to Nerf it and that gives us a really nice 3D reconstruction so these are large scale 3D generators SC basically all right uh shamelessly I'd like to advertise uh a couple of things one is our state-of-the-art report on diffusion models for visual Computing uh we wrote this with a whole bunch of different uh co-authors which is a really good survey paper that just came out um a few months ago it's on archive it's been on archive for a while and then um I Le the computational Imaging lab so I sort of started this talk talking about wearables a little bit but we are working a lot on new scen representations rendering and generation but we are also working on AR VR webble Computing uh actually a whole bunch of projects on generative AI for proteins and some computational Optics and imaging I'm also advising a startup called apparate labs and we just released our very very first uh model very recently just a couple days ago just want to play that teaser real quick controlled this is awesome how can I test this out interact with our AI avatars directly on Twitch are you g okay so the sound is not synchronized actually with this video right now but you can test this live on Twitch we have these twitch streams you can actually chat with the AI and it's it's a visual embodiment of a of an AI but basically um if you're interested in this technology let us know again real time uh or if you are looking for a job we're hiring um we'd love to talk to you and that brings me to the end of my talk I'd like to thank all the people who did all the work including Eric Conor boang shangu Rin po yingo Zan the camera control team gay gbt team and the apparates team so thanks for your attention and enjoy the rest of the week right thank you for the talk uh if anyone has questions we have time for a few you can use the microphone here in the middle I have one question for you while people are thinking of theirs uh you talked about this camera control ability I was wondering how you collected data for that like did you use automatic camera annotations for the pose or can you use noisy estimations from video and that kind of thing to get extra control controlability or is it too noisy to use those yeah I think we use these noisy uh sort of conditioning uh information from existing videos which are okay but not great and it's part of the reason why it's not perfectly controllable but also the mechanism itself doesn't really guarantee uh any accuracy right it's just like more like a guidance signal more moreover if you use these plugger embeddings it's really object motion and is also not really part of that so there's actually a couple of problems with this camera control mechanism and there's a lot of room for improvement there both on the training data on the mechanism itself and on other aspects of it all right thank you all right there's no more questions we'll move on thank you Gord all right next up we have uh Jun GA here for a talk so for people working on 3D representations or ISO surface extraction uh you've definitely heard of jun Jun is a research scientist at Nvidia in Toronto and he's also wrapping up his PhD at the University of Toronto he will be joining University of Michigan as an assistant professor in 2025 so for all you new PhD students looking for an advisor uh jun's looking for students and with that I'll hand it over to Jun very excited for your talkk you um thank you oh oh sure one second so okay so hi everyone so it's really great it's really my honor to be here and thanks everyone for joining so today I'm I'm going to talk about the S representations for sedy cond creation in the past few years we have seen amazing progress on using machine learning to create digital content such as language image or video however the world that we human lives in is fundamentally a 3D World we study and learn in the sedy environment since childhood we perceive this very beautiful sedy animals and Forest we communicate with each other in a sweety environment such as in this um in this room we also perform some actions like driving a car in the sweey environment as well so therefore to broaden the impact of generative AI to our life I Envision in the future the generative AI system will help will help us to create realistic 3D virtual world and enable the immersive interactions between the human and the simulated virtual environment for example in this movie the main character could simulus interact with this Virtual City such City generative a assistance will revolutionize many applications Beyond entertainment such as the VR robotics self-driving education or digital Factory so this is going to be a very exciting future that's being said the future is not here yet generating sweety content with AI faces fundamental challenges that that are very different from the generating images or language first as we have been talking about a lot in this uh in this Workshop the size of the data set that we have in sedi is much smaller compared with image data set even until very recently the size of the largest 3D data set that we have is still around two orders of magnitude smaller compared with image data set at the same time while while we have much less data the complexity that we need to deal with Inu is much higher than the 2D image and this additional complexity is not just we have an additional dimension in s it is also the complexity in terms of the S geometry materials and the final lighting effects we need to create all of this information when generating the sedy content for realism and the immersive experience before we go to the technical part let's take a step back and zoom out zoom out a little bit actually the problem of creating 3D content itself is not a new problem it has a very long history from the computer Graphics back to 1960s the computer Graphics Community has studied many techniques from modeling the S geometry to animating the created s content from modeling the appearance to finally rendering and showing a photo realistic image to the user this is very cool because we do have some tools that can create realistic sedy virtual words and these tools provide a lot of domain knowledge on how we should model the sedy content such as the swey geometry appearance Etc however many of these tools are often designed for experts and therefore it is very hard to scale up and allow much more people to create 3D content so therefore to push toward the future and tackle the challenges that we have my area of research has been developing the generative air system for 3D content by incorporating the domain Knowledge from the computer graphics and this is very multidisiplinary area in between Vision graphics and machine learning more specifically we have been working on two AES along this line over the past few years first to tackle the challenge of the sedy complexity we study how we could incorporate the sedy modeling techniques from the computer Graphics into the S generative a system such that we are able to regularize the generation behavior and reduce the S complex through this approach we are able to improve the efficiency Fidelity and the usability when creating the 3D content second to alleviate the issue of data set scarcity we study how we could leverage the foundation models and extract the 3D structures from these Foundation models by incorporating the inverse Graphics in this approach we are able to improve the diversity and the realism when creating the 3D content by combining the approach we have a 3D modeling and also the the algorithms to leverage the foundation models we are able to Turbo trust the applications in the whole spectrum of 3 generative AI including SD generation from single view image SD Reconstruction from multiv view image building a 3D generating model or finally Tex to SD generation um and and in this talk I will mainly highlights our SD modeling techniques because the algorithm we do for this um labing Foundation models are mainly bu uh building on top of our SD modeling approach okay so with this motivation let's look at the s model approach that we have in s computer vision so s computer vision Community has witnessed a really great success with neon field based approach such as using a neon Network to model assign distance Fields neon field based approach has inspired a lot of work in s computer vision such as neon Radiance field nerve and many others this approach is very suitable for machine learning because it has a Contin continuous definition in the sedy space and such a continuous definition makes the optimization of a s generation very easy it can also represent very complex s geometry such as a complex shape that we can see from the nerve however such a continuous mod approach is also very redundant for example suppose we want to model this bunny then we need to not only model what's inside of this object but also what's outside all of this empty region of this bunny and such redundancy further makes the Rend of neon field very inefficient more importantly in many applications such as if I want to render this object with lighting or if I want to touch this object with a hand what matters most is the location of the actual s surface however since that s surface is only implicitly encoded in the parameters of neuron Network such explicit reasoning on the S geometry is much harder from the other side of the computer Graphics actually we do have one representation that specifically Focus on the sedi surface and this representation is sedy mesh mesh is super efficient it is super compact in modeling the SED geometry because it is specifically localized where the surface is and because of such compact needs r 3D MH is very fast however the key challenge of incorporating the mesh into the generative a framework is that generating generating the mesh is not trivial especially if we want to change the topology of the mesh because this will involve learning the discre connectivities between the mesh vertices so to bring the mesh into the generative AI framework and incorporate the key rationale that we should focus on the actual 3D surface we developed a differentiable ISO surfacing approach for 3D modeling and this is the Jal work with our my fantastic collaborator Frank on the um on the right side so specifically in the forward pass during training we hope to differentiably extract a new uh we hope friend should convert a neon field into a SD mesh and this SD mesh is the iso surface of this neon field with this operation we are then able to define a lot of objectives that are directly evaluated on the 3 surface such objectives could range from a lot um a loss function from the by comparing with visual observations to more complex geometric energy functions that are defined on the 3D surface it could even be a loss function from physical simulator in the back pass since the operation is different ible we could then back propagate the gradient from this objectives into the newon field to generate the 3D content that we really want by minimizing this different objectives in this approach we could regularize the neural network to specifically on the S surface and further unlocks new capabilities by allowing us to Define new objectives that could be well defined on the 3D surface such operation can all can also gain efficiency by leveraging the hardware accelerated rendering and solvers that that are deved Ved for the swe mesh so with the time constraint I will not go to technical part of how we achieve this differential isosurfacing so if you are interested please check um check our paper for more details and I will talk to I'll talk like show some applications that our model is able to achieve with this differential ISO surfacing so let's first look at a vision uh based objectives which means in this case given a set of images with the camera camera parameters of this images we hope to extract the SD information from this image such as the 3y geometry or the other um appearance which is always called multiv reconstruction and we can achieve this application by simply using our differentiable ISO surfacing to model the online 3 geometry and differenti render this 3D geometry into the 3D mesh the loss function is then computed by comparing the rendered image and the visual observation since our operation is differentiable we can then back propagate the gradient from this loss function into the 3D geometry through differentiable render and our differentiable ISO surfacing as we can see in this animation after training we are able to reconstruct 3D shapes that aligns well with the visual observation compared with other baselines which are using the newon field to model the online sedy geometry our method is able to achieve better alignment with the sedy surface because theen ISO surfacing allows us to specifically focus and optimize for the 3D surface that aligns with the visual observation we could even optimize a soft body simul we can even optimize with a soft body simulator to reconstruct a deform object by minimizing a physical objective in this case we optimize both the 3D geometry and the physical pares that control the simulation of this object and this is this is uh this this is achieved by combining our differential isos surfacing to model the online 3D geometry with a differenti sof soft body simulator to simulate the uh the 3D geom r as we can see in this animation we can also reconstruct um a de object that aligns well with video observation we could further use this operation to train a 3D generating model for 3D MH here I'm showing some of the generated 3D assets from our model our model is able to generate diverse and high quality 3D outputs especially on the 3D geometry and this work is actually building on top of the tri plan representation introduced by Golden before um we could also turbocharge the Tex s generation with our representation so to give a very brief uh recap of a typical pipeline for Tex 3 generation with with SC distillation sampling L functions we can first have a NE Network to model the online 3D content on the left site and render this 3D content into the 2D image we can then fit this this 2D images into the diffusion model and get a grading direction that can help us uh update this to the image and this the the meaning of this gradient direction is basically like if I walk through my image along this direction the image will be more realistic um and then we can back propagate the gradient from this diffusion model into our neon Fields the key challenge here is a computational cost because in this case the whole comput the whole computational cost for one pass will be the number of images we rendered for newon field times number of p number of pixel to render one image and further times number number of point that we will sample per R to render one pixel and typically to render a nerve we probably need to sample hundreds of a point along the r which means the last item in this equation will be hundreds furthermore because of the high such high computational cost we could only render a very low resolution image and such low resolution rendering will miss a lot of details on Geometry and appearance for example in this video I'm showing some of the result from the prior workor which contain or which does not have enough geometric details on the other side one thing we do know is that run a 3D match can be super efficient for example suppose we have a 3D mesh to render this mesh into an image we can shoot a ray from the camera and this Ray will intersect with one point on this mesh collecting the colors of this point will immediately give her the color of the corresponding pixel and therefore suppose we have the match in the loop the whole computational cost will be the number of images times number of pixel for one image and times one and this is equation is almost two orders of magnitud smaller compared with the previous equation furthermore once we have the MH we could even render the image at a much higher resolution and such high resolution rendering will allow us to capture a lot of details on Geometry and appearance such high uh such efficiency and and the quality enhancement can be easily achieved by incorporating our differentiable ISO surfacing in this generation process in this case the gradient from this diffusion model will be back propagated into the 3D or into the 3D Shape through differentiable rendering and our differentiable ISO surfacing here I'm showing some of the more uh more examples of the generated result for our method our model is able to produce fine grain details and there are two reasons why we can produce such details the first is theal isos surfacing as I I as I was mentioned before it allows us to focus on the actual 3D surface and optimize for that 3D surface and second we can also render the image at a very high resolution and such high resolution rendering provide a sufficient supervision signals of how such high frequency details will behave beyond the application that we have demonstrated we are also very excited to see the broader adaptations of our method in other applications such as articulated object generation single image to 3 generation human generation Etc we have also open source our code in um in a media coding coding library and on the right side there's also very wonderful Cuda implementations from UCSD Okay so until now we have talked about the advantages of having a mesh and incorporating incorporating the mesh into this generative AI framework through differentiable ISO surfacing however I'm also not here to say that we should definitely focus on one particular representation such as such as a match for sweet con creation because mesh itself also has the limitations for example in the first image I took this picture yesterday under the Space Needle um it's very it is very hard to use the match to represent the cloud of this um of the sky and in and in the second image on the right side it's also very hard to use a match to model the fur of my cat so to analyze this let's consider this human model as an example suppose we want to render this human model and then there are two different types of regions that deserves our attention first The Hil region of this um of this human for example the The Hil region here The Hil region here contains both the fuzzy and semi-transparent patterns and then this this complex regions actually will benefit a lot with neon field based approach for example we can useing neuron neuron rendering to render the hell region and we can get really great render out of that but if we use a simple if if we use a surface based Rend with a mesh then it will likely produce a lot of artifacts because it's super hard like this regions is not possible it's super hard to represent it with a 3D mesh on the other side if we focus on the region such as the surface or such as the face of this human this location will be actually benefits a lot with surface bace approach for example in this case we probably only need to sample one point to render this uh to render the face of this human head but if we want to use roal rendering to render this this pH then we will sacrifice a lot of Unis points there and this these two different typ of regions on one particular object reminds us we should probably need different representations for different regions of a particular scene or maybe different representations for differen SC so in our new representation adaptive shell here we aim to model such spal value complexities across different SC and distribute the and and distribute the computational cost on Surface on surfaces um depends on the complex depends on the complexities of the local surface regions and this is also a joint work with my collaborators ZM friend R and Merling so specifically we hope to derive we hope to automatically derive derive the region such as the H surface like the face of this the face of this human head and also the F region such as the such as the hair of this human head with this uh with this different region uh region notations then we we hope to derive we hope to Define two shells to specially regularize the geometry specifically in this figure blue means the blue means the outer shell and yellow means the inner shell and we assume the whole s content will be B will be specially bonded Within These two shells and then during the inference we will only render the content with Within These two shells the key the key benefit of such bonding shells is that we can specially change the WID of this spand according to the complexities of the surface for example for the hot surface on the face of this human head we can have very narrow band and this very narrow band will be cross bond to a surface based rry such as a mesh and for the fuz region of this head we can have much wider band and this wider band will be more closer to the surface based volume rendering in this way we are able to achieve a very smooth transition and transition between volume rendering between volume based representation and surface based representation so during the inference as rendering during the inference to render to render the neon shell is actually very uh very efficient for example supp we have two shells one is the outer shell and the other is the um is the inner shell and we should one Ray if this Ray doesn't hit any shell then this means like it will hit the hit the background we will it does not need to sample any points here and then if this Ray has a hit a very if the two intersecting point of these two uh two shells is very narrow then that means this is a very hard regions then we probably only need to sample one points along this R and this S one point will equivalent to a surface rendering if the inter uh if the intersection interval is very wide then we'll try to mimic the uh the volume rendering by sample multiple points along this R and the key benefit of our representation is everything can be achieved within one framework so everything is done in one pass in this video I'm showing some of the like runtime comparison with instant NGP so in the S with single object our model is able to achieve four like five to six times speed up compared to instant NGP we can also extend our works into unbounded like 360 SC and our methods maintains real-time performance as a like competitive qualities with other interactive methods we can further export our representations with the animation so in this case even with the presence of the deformation the rending process still benefits a lot with our adaptive shell representation to efficient sample the underlying volumes okay um so with all of this like I want to kind of looking slightly looking ahead of the future so this is like um this is still the beginning of my chapter there's a long way to go to achieve the final goal of creating realistic 3D virtual world for immersive interactions with a human to achieve these goals there are two direction I think we need to push the first is how we are able to generate artist level 3D content for quality and the realism and second is how we are able to generate interactive functional and dynamic 4D content for example for for the first part when we look at the object in our real life we could really zoom in and discover super fine grain details such as a rink code on the face of this man to generate such levels of details we need to study both the 3D modeling techniques that are able to model such F details and also the learning learning algorithms that can provide the supervision such for such fer details along this direction actually although we talk a lot talk a lot about that we don't we do not have enough s data but actually there exist a lot of 3 information within the current existing 3 data sets such as the MH isolation or the UV ma ping many current 3D generative assistance neglect this information however such information is particularly important when exporting the generated 3D asset into the graphic engines for animation or simulation so like I hope we should we can also explore more on how we could explore this uh different uh different properties within the SED data sets so the second one is that how we are able to produce 40 generative a for interaction and and dynamic 40 content the swey virtual world that we are building should be able to support interactions with human in a way similar to how we would interact with this real world for example in this video the human could interact with with this puppy dog and the puppy dog will will react to the interactions of this human and this interaction will also require more Explorations on how we could model the physics uh and dynamic body content so with all of this I'd like to thank to all of my collaborators and thanks you all for listening to my presentation I'm happy to take any questions thank you thanks a lot for that wonderful talk if uh people have questions please come up to the microphone right in the middle please come ahead you could use the microphone that's right there we have time for a few questions thank you for for your presentation was a um very interesting uh I I liked a lot um also the your your especially the your work on the Adaptive shell and I was wondering uh how would you uh you didn't mention Point clouds right would you integrate Point clouds as a mesh representation and when when do you think it's a 3D representation and when do you think it could be useful to do so yeah I think Point call is also actually very wonderful representation actually and especially is the benefit of of the point se s GAA is like it's very very fast and it also represent then represent the Dynamics and actually there's also one follow-ups of our of combining the attap sh with the S Gan such as I didn't show in the in the talk it's basically this Gia for thing which is basically we can also regularize the the point coud using our adapt shell which means like the Gan point is restricted within the shell so I see there's also Mutual benefit between this smesh and the point clouds thank you thank you so much for the talk I just have small question um sorry I didn't follow what's the exact name of the paper for differentiable ISO surface extraction oh yeah sorry I didn't mention that I I did very quick so let me go to that part so so so actually we have a line of work along this direction so the first paper I would say I would recommend to check is like uh the marching T we call it DM T and the second paper is in s last year is called fxy Cube and you you can also go to my web page like I have that in my web page thank you thank you right let's uh thank Jun again for this wonderful talk and uh we'll move on to the next one all right so it's my great pleasure to introduce our next speaker and a great friend of mine Alex Kolinski so uh Alex got his PhD from udab here in Seattle where he worked with Steve zeit Brian Kess and Rick zisi uh since then he has been jointly at Google research and now Google Deep Mind as research scientist and a postto at UC Berkeley together we working with ANUK kanava and alosa EOS and uh the oh the most recent news of course as of last week is that Alex will be joining uh Columbia University as an assistant professor in the recent future which is exciting and uh yes Alex work has been recognized with the best student paper award award last year in iccv uh he has one more paper that is best paper candidate this year and I think that with this space regardless I can see him it would be it won't be long until he would be on that stage again and Alex has done impressive work in the space of uh 3D generative Ai and I think that today he will uh cover some of his recent cut cut 3D work which uh turned a few heads uh last month that when it came out so yeah Alex we're very excited you can take it away whenever you're ready yes you can wow what an intro can everyone hear me yeah all right cool all right so uh today I'm going to be presenting our paper a very recent paper called CAP 3D create anything in 3D with multi viw diffusion models uh and this was uh joint very close work with raie GW and Ben p and a handful of other very wonderful uh collaborators at Google research and Google deepmind so uh first I thought it might be good to start by offering uh a little bit of motivation for why we even want 3D generation and you know this might be obvious to a lot of you because you know well this is a workshop for 3D generation but I do often hear people saying things like what's the point you know video models have already been shown to be useful for a bunch of things that we use 3D 4 nowadays uh like uh movie quality CGI so maybe we don't even need uh 3D models anymore maybe we can just do everything with with a video model well you know I get this argument I really do but you really need to think about you know some of these hot new VR AR and mixed reality devices you know with these and and really with any other kind of interactive 3D medium the most natural way of consuming content simply isn't on a 2d plane it's an actual 3D entity that you can walk around and uh explore and see from different viewpoints and for for for what it's worth I think that this is also true for more uh traditional forms of consuming cont content like phones and and laptops you know if you generate a video um if you get you know if you run it through a generative model you only really just get that one video as output but people often want to like explore control or interact with the worlds that are being generated by by these models and with a video model how do you do that well you can certainly train these models to take in new signals so that every time say you make a gesture you get a new different sample that respects the user inputs uh or shows the scene from a new view point and we've definitely seen how to train uh image models to respect camera viewpoints so we know that this is doable but practically speaking you're really not going to be wanting to run a video model on these Edge devices anytime soon and even if you can is probably just not a great idea you know they're not going to be fast enough to generate new high quality videos 60 times a second on a phone uh or they're just going to be too power hungry to actually be useful so really a better solution here is a 3D model you know 3D content checks all these boxes you send over a single 3D model and a user can interactively move the camera around and explore the scene by just rendering that model from new viewpoints uh which is something that we've known how to do efficiently for some decades now and how do we create 3D content well traditionally this has been done through uh you know processes with trained 3D art artists and this process can take sometimes hours for a single asset which means that creating a full 3d room a scene can take up to days of work depending on how detailed it needs to be but fortunately you know over the past few years we've also gotten new tools for replicating real 3D uh content things like you know Nerfs and Nerfs are great because they let you capture and reconstruct real 3D scenes in great detail which lets you render arbitrary new viewpoints you know fly around the scene interactively and basically all the things you'd want to do with a 3D model but one thing that you don't often people talking about is that capturing a Nerf requires a really careful capture process you know you need to take hundreds to thousands of images of the scene in a pretty deliberate way that covers all the structure of the scene from from multiple viewpoints so here you can see this little Lego bulldozer has been reconstructed pretty well but it's taken over 80 images captured in a ring around the object and in the absence of this number of images Nerf really just doesn't work all that well so let's say instead instead of 80 images we only have three well you'll probably end up with degenerate reconstructions of the scene like this that don't look good at novel viewpoints and don't have reasonable geometry and this is really the result of an under constrain system there's some degenerate Solutions like simply pasting the captured images on random geometry that satisfy all the constraints and optimization and so uh a few months ago we put out this paper called Recon Fusion that we'll be presenting on on Friday morning in the poster session that was aimed at making reconstruction more reliable in the cases where you have very few captured views and so if we look back at that failed few view reconstruction we just saw once we add in some some magic from Recon Fusion we can get a pretty reasonable uh reconstruction of the scene which again is from only three input images so how does it work well the core idea is to Simply try to simulate the existence of additional views in the Nerf optimization process so typical Nerf optimization works by rendering out views and then supervising those renders against pixels from real captured images but if we don't have enough views we also don't have enough constraints to guide our optimization process towards a reasonable solution so in Recon Fusion uh in addition to supervising the captured views we can additionally sample new unseen views views that weren't captured of the scene and at those viewpoints we can query a generative model for an estimate of what the scene might have looked like from that Viewpoint conditioned on all the other information that we've been given and what I'm describing here is a generative model that's not the standard textto image models that we've all come to know and love but rather a new kind of generative model that takes in multi view observations of a 3D scene as well as a Target pose and produces samples of the 3D scene from that new Viewpoint you know effectively this is a novel view synthesis diffusion model uh and here you can see an example uh you know it takes a few images as input as well as their poses and takes in a novel Target camera pose and it produces a novel view synthesized on the right so the big challenge of this particular paper was figuring out how to build uh this model you know from choosing the architecture and figuring out how exactly to condition the model on the put information of image and POS pairs uh to also the data set where we needed posed multi view images that covered a sufficiently wide range of scene types as well as an even coverage of potential novel viewpoints so we ultimately hear settled on a mixture of real and synthetic data from a handful of different sources like real estate 10K uh c3d and uh renderers from from obers and by the end we managed to get a pretty decent model you know that could create plausible novel views of the scene given as few as three input images and this was great but there was still one big problem to be solved if you sampled the model you know a handful of different times from different viewpoints or even from the same Viewpoint with different seeds you'd get super inconsistent estimates because the model was really only trained to produce a single image at a time so we had to figure out the right strategy for actually integrating these in consistance signals into the Nerf optimization process and our solution to this was uh something not too different from dream Fusion uh in which you know in each iteration you render an image from the 3D model and then you pass a partially noised version of that render into the diffusion model and ask it to produce a clean image and then you simply guide the 3D model towards that clean image that the diffusion model produced and effectively this iterative partial noising and denoising process leverages the guaranteed consistency of the 3D representation to encourage the sampled images to be more and more consistent with each other and you know our version of this is slightly different in ways that I won't really get into now but basically this ends up being pretty effective for observed parts of the scene so here you can see some pretty complex scenes you know reconstructed at the top with very few views so this is anything from like three to nine images but while this is very impressive you've probably also noticed a few few key issues so far so first of all if you know anything about these large diffusion models you'll know that this iterative optimization process I just described is really slow because it requires calling a diffusion model multiple times for each Nerf optimization step which you know turns what would otherwise be a minute long process into something that's like 20 or 30 minutes and this is fine you know as long as we're getting good results that would otherwise be impossible to get but even the quality isn't perfect sometimes because while Recon Fusion worked very well for observed images or regions where the input images offer a strong enough signal for what should be there uh for unobserved Regions like uded or entirely uncaptured parts of the scene the model would basically produce entirely inconsistent content which is very hard to consolidate even through our iterative optimization process and you get to see this most prominently in our single image results so these results here are showing what happens when you feed in only a single input image to the system you know most of the scene is entirely unobserved so the samples are very disperate and optimization becomes really hard you know most of the unobserved parts of the scene end up being just reconstructed as sort of blurry or or gray blobs so uh here's where Cap 3D comes in so we first started this project as a followup trying to solve some of the problems that we had with Recon Fusion and the first problem that we realized that we needed to solve was to make sampled images from the diffusion model uh consistent with each other since as we just saw if we have samples that are very inconsistent we end up with reconstructions that are blurry or or otherwise don't look plausible from novel viewpoints so the core reason why samples were inconsistent in Recon Fusion is because it was built as an image model you know in other words it was really only trained to produce one image at a time so there's really no surprise that multiple images weren't consistent but we've also seen that video models are possible and we've seen that these can generate consistent content across many many frames so we can borrow some of the design decisions to build the corresponding multi-image or video novel view synthesis model and this multi view latent model uh is the core innovation in cap 3D that makes most of the difference you know this is building on a lot of the learning over the past year in video model architectures where information is shared between the different frames that are being generated and uh this model here is sort of like a special case of a video model and I'll walk you through a bit of of how it works so it takes in as input uh two different types of inputs so the first is captured or otherwise observed views which contain an input image and the corresponding camera parameters uh of of pose and intrinsics and we encode all of this before passing it to the diffusion mod so uh the image gets encoded by a pre-trained vae into a set of latents uh since our model is built on a base Laten defusion model uh and the pose and the camera information is encoded into per pixel Ray coordinate Maps which is basically just contains uh each pixel's Ray information so simply it's just the the the ray origin and and Direction per pixel and for novel views we don't have an input image uh so we simply feed in the same set of information but set the input image the input latent image to be Pure Noise and to help distinguish between these two different types of inputs to the model we also feed in a mask that tells the model which views are observed and which views are not observed and so crucially the input information to the model can be any mixture of these two different types of inputs you know they're treated exactly the same by the model just as sort of a large set of views that may be you know variably known or or unknown and although probably already seems a lot like a a video model a key distinction here is that these inputs don't have any temporal information and they interact only through attention operations without any kind of you know temporal positional positioning betting so they are like almost truly an an unordered set of of posed frames and so the model takes in all this information and simply produces a set of uh generated novel views for the specified uh Target camera poses and and of course we train this with sets of posed multiview images so it's supervised to produce multiple consistent frames at once and this uses exactly the same training data set that I just showed before uh for for Recon Fusion so it's you know obverse MV image net CO3 and real estate 10K and so uh within the model you know there there attention interactions between these different frames that are being generated so you can actually get you know really consistent samples out and and here are a few of those so you can see you know simply modeling sets of frames instead of a single frame at once the samples already look a lot better you know almost as consistent as some of the Reconstruction results that we were looking at earlier and so all of this brings us back to the question of speed you know seeing how consistent our samples were this made us realize that maybe we didn't need to do all of that fancy iterative optimization stuff that we were doing in Recon fusion and instead uh since our sampled novel views are consistent enough we can just decouple these two parts of the process you know first we can pass our inputs into the model and generate a whole bunch of Novel views synthesized novel views and then uh we can simply reconstruct those views using a standard you know relatively standard Nerf pipeline uh and while these samples are not entirely 3D consistent they're consistent enough that some of the standard robust Nerf reconstruction tricks like perceptual losses and so on are are enough to consolidate them into a consistent 3D mode and this decoupling that I'm showing here also basically solves the speed problem from before because we don't need to repeatedly sample from the model and we instead only do it once at the very beginning of the process so we just sample a set of frames and we reconstruct them and in the case of Recon Fusion where before it would take an hour to reconstruct a decent scene from three images Cat 3D now only takes a couple minutes and produces reconstructions that are notably sharper and more coherent so this lets us produce full 3d scenes in as little as a minute from any number of inputs uh like three images shown here uh or now since we made such an improvement to the consistency problem even just a single image and that single image can be a real captured photo like this photo of a dog or uh it can be a generated image that was created with a text image model so you can really get you know creative and create a 3D scene out of more or less anything that you want uh so I'll scrub through a couple more of these examples just to give you an idea of what the results look like but definitely do check out the website where we have a full gallery of of results including some you know synthetic scenes that we came up with prompts ourselves as well as some uh real captured photos of our own uh and here's what that Gallery looks like so at the top you've got a search bar and you can search for whatever thing you're interested in seeing and running on such a large collection oh okay sorry Zoom problems uh is that good yeah all right okay so you know running on a large collection of examples like we have here in the gallery was really only possible because you know the method is as fast as it is you know it it takes only about a minute of processing time total per per sequence per per model that we want to generate um and you know on the website we also have an interactive viewer where I just recorded this you know minutes ago sitting in the audience so this is the as slow as the loading time gets uh and you know we've reconstructed some some of the scenes with gausian Splats and we're rening rendering them in real time on the browser so you can play around with all of these models you know interactively on your phone or on your laptop so definitely give it a try uh and so to close out I've got a couple slides on what our learnings were throughout this process and then also what some of the failure modes are so let me take you through some of those so first of all uh you know what are some of the things that we learned well we learned that you know robust and perceptual losses are super effective at reconstructing sets of slightly inconsistent images but they don't really do much if the images are too too inconsistent meaning if they describe entirely different scenes uh we also learned that you know camera trajectories are really important uh since we're sampling the scene at novel uncaptured viewpoints these need to be valid viewpoints and the same virtual camera trajectory might not necessarily apply to all different kinds of scenes you know some scenes might need more orbital object Centric views uh some might need more forward-facing uh camera dollies uh we also learned that the the way that you pass in conditioning information to the diffusion model makes a very big difference so the choice of passing in Ray information a as we did worked way better than a lot of the Alternatives that we tried and and lastly we found that certain Advanced sampling techniques can be used to encourage more views to be consistent like you know autor regress sampling and anchoring uh and I'll leave this for for you guys to read about a little bit more in the paper uh okay so finally there are a lot of things left to be done here like this this is this problem is far from solved and this points us towards you know directions that might be interesting for for future work so you know first of all you know geometry sometimes just isn't great and this is largely because of ambiguity in photometric only reconstruction so maybe adding things like depth priors might be useful here and uh also as I mentioned mentioned before you know not every scene is compatible with the same camera trajectories and our our current Mo here is to just try a couple different camera trajectories and pick the one that worked best but the correct way is to actually somehow predict uh which set of poses would be reasonable to sample the scat uh given a particular input image uh also you know our training data here is super biased you know we've used A small collection of data sets that definitely don't cou the full spectrum of possible scenes uh camera poses or even intrinsics so either coming up with better data sets or better ways of generalizing would be very helpful to the eventual uh reconstruction quality and finally uh and this is the obvious one which is that you know our method requires you know static observations of a seene so it doesn't work well if you pass in multiple observations of a scene that were captured e e e e e e e e e reason why you see more generated images here is because it's much easier for us to run text to image on a thousand images it's much harder for us to collect you know thousands of images that we might want to release on our website thank you thank you for the presentation uh my question is for the few view set few view settings uh does your method depends on the text prompt and if true uh how carefully it should be picked uh so we we've actually so both these models the rec confusion model I showed at the beginning and then Cat 3D both of them are fine-tuned from a base text to image model that does take text as as input uh for Recon Fusion we removed that text dependency by replacing the clip embedding as input with a clip image embedding so it's fine-tuned to sort of not need text as input uh for Cat 3D we remove that embedding altoe in the fine-tuning process although I do expect that adding it back in or keeping it altogether might improve results even if you were to say just caption the input images uh that would probably help uh preserve some of the prior from the base model that you started training with thank you uh we will need to take the rest of the questions uh offline because we are 10 minutes behind I'm sorry okay sorry and uh we can can continue with the uh the next speaker so uh yes next the next speaker is the our the uh legendary Alex youu so Alex is with Luma AI with where he's he and his team are responsible for some of the amazing amazing things that uh you see out of the company Alex did his undergrad at UC Berkley uh where he worked with a group of anguk kanava and uh during that time he was uh the responsible in the strongest force behind one of the uh strongest and most memorable trinities of papers in this space pixel Nerf pleno and penoxal and after after that after he joined lumay ey and I hope that he can lift a bit the curtain when it comes to all the amazing things that they are doing with his team including the latest uh uh dream machine uh release that uh took Twitter by storm last week so as soon as we resolve the technical issues this good okay so Alex you can take it away okay thank you for the introduction yeah and yes my name is Alex hu and I'm co-founder and CTO of luma Ai and today I will be talking about our recently released uh video model which is dream machine and also connected with 3D generation e video generation including the Imagine and J um so but there was always some gap between the two streams like then it is feels like like to get past the reliability problems of Nerfs we need to add in the generative priors that we able to learn from large data sets um oh no okay however as has been brought up multiple times at this Workshop 3D generative models are quite Bott necked by the availab of data uh the foundation of recent generative AI boom is these massive internet scale data sets of text images and videos and despite considerable efforts to expand 3D data we still only have like millions of them of objects and captures which uh is a lot more than a few years ago but still tiny compared to the images and videos and the videos videos are something that people in general take and that they're like they have intrinsic level of quality since they usually reflect the real world meanwhile 3D models are not as abundant or high quality and they come in like various formats um and what's more like even if we do have a lot of these data like what what is really important for 3D are things like topology UV materials articulations and so on and it is very difficult to collect data with these additional properties and more generally I think there are also like limits to how much we can use computer Graphics based priers for solving uh 3D problems with with like static 3D objects and scenes 3D methods like a Nerf and Goan Splats have been successfully applied in many problems but they are already like quite quite slow and and sometimes require a lot of Uris stics and in 4D this becomes a bit more challenging and and but maybe it's still like reasonable however like what people are really interested in is still like all these additional properties of like light transport and also like simulating physics in various ways but like to be able to code in all kinds of physics we have to handle various uh Graphics problems like um articulated objects cloth simulations and so so on and by introducing many degrees of freedom um the problem becomes more and more osed and more difficulties arise so like in this direction ultimately you would need to build a game engine and somehow differentiate other features so I think that is not very scalable if we want to be able to solve the problem of physic simulating physics and things like that so recently the direction that seems promising is using these by by using the 2D data sets for which we have a lot of 3D information in them and like as alexinsky just presented the work Cat 3D uh and and also like several Works last year like MV dream and instant 3D uh doing these multiv view diffusion approaches but over the last several months at Luma we have been focusing on the first part of this problem which is to build a video Foundation model um and this uh video is a generation that I made of a drwn shot of I statue and here are some um more examples of amazing generations with 3D camera movements this one is the left one is a generation from a Japanese Creator and this one has some uh something that one of the team members made for the release stick uh and here are some more examples of 3D trajectories around around this like skull and also a pick riding a motorcycle um and one of the things that ways to test 3D consistency is by running these outputs through a Nerf or Gan Pipeline and in this case we uploaded it to the Lumas interactive scenes and generated this gion from the video to to produce like a posible 3D scene to some EX so uh so another interesting sort of experiment I tried was giving some popular Nerf data sets to the image to three to video pipeline so this is actually a screenshot from the zip Nerf paper from John Baron and as you can see it generates some drone trajectory through the house and here are some more results from the MN 360 data set that's like you see all the time in the Nerf literature right what uh and and I also tried these like lff scenes which are taken from phosy in breley but like going Beyond just these PR scenes um one problem that was always very challenging in Nerfs was actually handling specularity and reflection despite despite initially seeming to promise to Sol this problems and also works like R Nerf like the specularities are just like very high frequency to model with just the view dependency on or spal harmonics and as you can see like well it it sort of models the reflections it has some artifacts your hands it's well like I guess with the video model it can somewhat Sol of these like light effects but um and here are some more examples of generations uh from the model there's a lot of Reflections and refractions in the car and here's like a some cinematic shots from a creator of a Sci-Fi setting um so you can in this one the bear teddy bear swimming in the ocean you can see the reflection of the bear in the waves and like this is a from from a longtime Luma user and you can see there's the complex refracturing effects in the bulb on the plant um and this one is actually using our recent extend feature to make the video uh longer than by three times and furthermore like Beyond just materials and light transport the model can Al also have some idea of dynamic like 4D um this is like uh also just a fluffy teddy bear swimming in the ocean you can see like the mod handles the movement forur and water and this one's actually a photo of my one of my cuts from and there's like a giraffe roller skating in a park just some additional examples Beyond just animals and humans we find that the model can also somewhat simulate physical interactions here's a beautiful example with debris fing around and the fire smoke and explosion at the end and this this one showcases some human and some cloth and another one with clothing and uh show chose the some 3D camera movement we find that also the model is like kind kind of able to reason across Cuts sometimes like this scene is a little bit like horror but uh just to illustrate the point you can see that at the beginning there's like ey and then the girl looks horrified and in this case there's a monster and then can see the monster from the back so another capability that like that the model has is generating very stylized videos so be Beyond realistic effects often times creators want are interested in making videos with various specific Styles and this is like a short film from the monster Library called Monster camp that um and throughout they're able to make an entire short film with this kind of style uh and there are like others Styles like this anime and uh this a kind of oil oil painting just you can see like the whole scene is quite consistent I guess finally I will discuss some Li limitations since this is a academic Workshop um and the current model still has like a number of limitations for example it's not always consistent like with the shapes and there's actually this one with kind of genus effect which is something from the 3D generation literature but when the the Bear's head goes under the water the you can see there's another head um just and then there's like also sometimes it does strange physics like in this case I give it the lk99 picture and well it adds a rock on top of it and then the rock somehow becomes long and then becomes electric so um maybe it interpreted that there's a cut in the Middle where suddenly the rock becomes a long one and this is like a I I I gave it a image from the clever which is like a vqa data set that's also used in some 3D project and while it has some like cool shadow effects it's like doing something very strange like the objects merged and kind of clip past each other and something to be improved is the the camera control so like currently like we didn't actually train with the kind of precise camera controls you can somewhat prompt it with the text but I think it will just give like some trajectory that looks reasonable and like for for the of course for the 3D um and 4D generation like it'll be very important to add camera precise camera control but in any case we still find the results quite interesting encouraging and some future directions and that not not necessarily like for our products but just I find it interesting or things like uh to make the camera controllable and maybe like train on entric data you can you can have like a game engine maybe kind of a generative and um also recently there has been work in using the video models for robotics which is pretty related um and I think um as a kind of like Universal imagination World model I think these are very exciting directions um that may have like a large impact on extending advances to the physical world and I guess finally special thanks to all of the Luma team basically everyone contributed to some part of the project either the model development data info or like the product so like we are also hiring if anyone if you're interested please check out this link great thank you very much Alex if you have questions you can come at the mic here at the center hi Alex uh first of all congratulations on your successful release of Dre machine and so you talk you first mentioned that they are serious uh data constraint for the 3D generation and also I noticed that um so the uh Luma geni hasn't updated for a long time quite a long time and it doesn't mean that you guys abandon the product line of lumini and how would you invion the future of 3D gen thank you all right so like as I kind of explained in this talk like our our view is that we kind of need this to to to be able to leverage videos and images to like further Advanced 3D because like just doing this using the existing or like asset data sets it's it's difficult to make the model much better all right so so then do you have a plan for I mean future release of next version of the Luma Genie like yeah I mean like I think we we we are planning to like uh use these like video models to release more 3D products all right yeah thank you not congratulations for you great talk and all the demo pretty amazing so I have one question so I will assume uh Dre machine were using 3D models like Nerf or 3D GS so can can give more details about how uh do you do text to 3D like uh do you first using uh like text to video to generate video then transform to 3D or like you directly generate 3D parameters like Goen spting you directly generate the Goen points like with is in end to end model like all that would be great if you give me more details yeah I mean I think this is not entirely resolved but the C 3D approach or something like this maybe like where you fine tune on just 3D multiv view data after training on lots of videos something like this could be okay cool thanks hi yeah thanks for the talk so I guess I'm still trying to understand what is the what is the differentiator here in terms of other video diffusion models like I couldn't understand where the 3D part was involved if you can little elaborate a bit on that uh like while creating the 3D like the dream machine what I mean I don't think you need dream the 3D for dream machine like I'm just trying to say like you need something like this for 3D okay and then like in in the case of most of the other models like currently I don't think the public can access them and like I guess we have to build it ourself for thank you okay so let's thank uh Alex again and this was the last talk of this session we will continue with the the poster session that's happening right next door and we come back at uh 4:30 for last session of the workshop thank you e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e spee spe um e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e forign spe e e e e e e e e e e e e e e e e e e e e e e e f e for e e e I should um so we can I start sharing okay looks like it's mirroring your screen I don't know that's okay e e e e e e e e e all right hello everyone uh we're going to get started with the last Talk of the day here for the workshop on AI for 3D generation uh our grand finale is Sergey tul aov who is a principal research scientist at snap research uh where he also heads the Creative Vision team in general his work focuses on creating methods for manipulating uh the world via computer vision and machine learning and he was really an early Trailblazer in problems like animated 2D images video generation and motion retargeting and Sergey has also worked on generative models for 3D objects and scenes especially focused on controlability so I think could be a great talk to wrap up the day with that I'll let you get started thanks David Davis so yeah I mean it took me some time you know to find this room also it's a pretty big room but I mean I went in the book it was written the other building so got some exercise uh anyway so very excited to talk here today and I imagine this is the last Talk of the day and people are tired so I'll try to add some maybe a bit of a humor somewhere so please laugh will make me feel better so uh you know we talk today about generation and generation of 2D generation of 3D generation of 4D I don't know if we can generate 5D but um maybe if we look into the past have have people generated 3D in the past or is it something very recent that happened this year in computer vision community and graphics and so the answer is clearly uh no people have been doing lots of things with 3D and uh maybe here we just quickly go through um uh like some generations of 3D in the past so the first first ever 3D I don't know sculpture or generation that I was able to find is this Venus so is a essentially a pebble on found on a beach uh somewhere and uh there were traces of human processing on it and you can see that humans were really not skilled at doing that uh and also it's a woman and if you don't recognize that it's a woman maybe there are many reasons it's very old uh and also maybe people were not skilled or people looked differently back then or wanted to look different so then we have this which is roughly 40,000 years ago and this is not yet a Spiderman but it's already a lion man and this one is interesting because it adds some creativity to the process combining some two different entities like a lion and a man and also you see better uh better Fidelity also it survived 40,000 years and still looks okay and then maybe 800 years ago we see some pottery and uh this is much more developed and also this one had some use it was useful for some daily life maybe and we arrive at the Pinnacle of Renaissance art which is considered one of the most notable statues uh and everyone knows about it and people are probably we cannot do better than this even today and so these were Generations or some sort of 3D sculptures and you know how they were created clearly they didn't have generative networks the tools were quite primitive and for the pebble to create a pebble you actually maybe used another Pebble to create this Pebble then uh for the lion man maybe the tools were a bit more advanced and developed and then to do the uh Pottery you need a pottery wheel which is already quite uh sophisticated device and finally to do uh to to have a statue done you actually need not only skill but also you have to have a lot of experience training and uh some talent and so this clearly shows that uh 3D creation has seen a really great development in the last couple like 250,000 years uh how does it look today and today it seems to be simple so this video supposedly shows that hey learning in blender in just 30 minutes I made this video 60 times faster to show you that it's really simple you know it's nothing particularly you cannot do so you move things here and there you just create some stuff uh yeah so well in fact what I want to say with this video that it's actually not so easy and you still need to have a lot of experience and after you spend all this time you get this I mean I have nothing against that uh but maybe you can do something better than this and still uh it was just 30 minutes of blender and to to get something more creative you need to spend probably way more time but the good thing is that it doesn't have to be that hard and uh today with the advantages that or advances that we have with uh okay generative AI technology or generative modeling we can generate uh from images to 3D we can go from text to 3D we can go from text to 4D if we have a scene we can texturize or have an object we can texturize it uh easily and it looks uh really good so creating something like that would take some time if you know Photoshop and if you don't know it probably will take forever then we can also manipulate different uh rooms so for example here this is actually a cvpr paper that will be presented at uh the main conference so you can with just a prompt uh generate the texture for the entire room and so how is this how did this become possible so there were a couple of things that happened uh roughly at the same time the first thing that happened was neural Radiance field and uh what happened there and why is it important is that there were Graphics there was Vision there was machine learning and now with uh the use of neural radiance field they are all combined together and people from different field can you know introduce methods from Graphics to ml and and vice versa so that's one thing and you know I want you to really understand how things work so I show you this example how Nerf can be understood so you can consider Nerf as two things one is learning the volume which is the laser on the right on the let me see on the left according to if if you look from there so the laser leaves some footprint in the volume which is some neural rendering that you learn trace the Rays and learn what's the radiance in that point and then when the volume is ready you can rotate it from different ways and uh generate novel views which is on the right I think it's a cool uh example I use sometimes when I uh show show what's nerves to to kids you know in the meantime a lot of stuff happened in the generative domain and from the G from first gains and uh W GS we got uh Progressive G and at this point six years ago you cannot really tell if this face is real or this face is generated and then roughly in uh 2019 two parallel tracks started one was with diffusion models one was with sty Gan and uh fortunately or unfortunately uh there is no more well there are some Gans but not too many so and from 2020 we have diffusion and Beyond maybe we'll have a to regressive models but what I want to say here is that there was a tremendous progress done in uh generative domain combined with advances in the 3 domain but then these things appeared and these things are trained on huge amounts of data billions and billions of images from the internet and this is really exciting the question is are we able to to do something like that for 3D or maybe even for 4D and to to get the answer uh let's look at the data or or or lack of the data so there is really a lot of data in 2D in videos and it's extremely simple to take your phone and take a picture of you nice wonderful people but if I want to reconstruct this entire thing in 3D it's it's not possible because I first don't see what's behind you so I can do depth but getting 3D is hardly possible and so that's why we have you know we have large 3D data sets such as obers or we have some multi view data sets but these are orders orders of magnitude lower than the number of objects there is lower than in any good 2D or video data set because of the difficulty of creating 3D so we can maybe assume that Civilization hasn't just created enough 3D data so there's just not enough data and maybe the path to 3D and 4D generation maybe it lies through the value of 2D generation and I think in the last year this was discussed at CPR and this year we see a lot of evidence so how about this this is dream Boo and dream Boo has capabilities that are quite interesting from the 3D standpoint you can take images that are on the left and you can fine tune your model so that it can generate this object and then you provide some prompting and it seems that it can generate this object from viewpoints that haven't been seen in those images and the question is can we use these capabilities for 3D and 4D generation so maybe just think about it if diffusion model was trained on all the images that exist in the world it have seen your cat from from behind it has seen everything so and we saw many works like uh for example dream Fusion that can can do this they introduce a score distillation sampling loss that can reduce the gradients uh of the den noising process and generate images or sorry generate 3D objects um and this is a very exciting didn't work because first it doesn't use any 3D just 3D representation to generate 3D and then use Tod images and there are of course some problems with uh with this first there is yanos this is a Greek ganos if you don't know he has two faces this squirrel has more problem it has three faces and that's called like a yanos problem for for those non-3d aware uh 3D generation methods and there is another problem that generation is actually pretty slow so it's 30 minutes per object so you know you type your prompt you get get a coffee maybe another coffee and maybe after that another coffee uh maybe a couple of more coffee and then you get the squirrel with uh three faces and then you need to do it again so I mean it's very exciting but we probably can do better and so some of the works that appeared in the community was this interesting instance 3D for example they diffuse four views at the same time and so to train this model they require multiv view data so how can they get it they take this existing data set OBS verse that they can render from multiple views and then can take a pre-trained diffusion model and Def and and fine tune it to generate these four views so still it still uses a lot of Prior from 2D training but it also uses something from the uh 3D and then it doesn't have this issues with um doesn't have this issues with uh yanos problem and then the second step is to take these for views and reconstruct 3D out of those uh for views which can be done through you know take uh this four views r a Transformer or any other model that reconstructs some sort of triplanes then you can do volumetric rendering uh and and that works and recently I think we just submitted this to Archive last week we introduced a work called GTR which improves geometry and texture um through refinement and I will just show some you know some ideas clearly the paper has many more of them but we take not only the images but we also take the masks and also uh rise encoded as plucker coordinates uh you see those rainbowish kind of uh maps and we can train a large transform relatively large and learn some triplane tokens reconstruct high resolution triplanes and decode them into differentiable measures and through this we can also use some 3D uh 3D data to um to fine-tune and train this approach and we saw that we can just from a couple of images we can have much improved uh texture quality and geometry quality so what's interesting you see this SC and on this SC you see that you you actually see what's written on the texture which is a difficult problem and it can also go from just one single image these images on the left were generated by uh by a diffusion model and then we can lift them to 3D space with high resolution so you know this advances and I'm going to announce something like that so I think it's tomorrow that snap will release snap ml kit where you could uh generate different uh different meeses from prompts and use them uh well hopefully you will use them in Snapchat uh but probably maybe there are ways to use them somewhere else and then these are some of the examples that we can get so it's just from one image or from text so it seems that 3D creation has become simple maybe it's still not very good maybe we still have a long way to go but it's already you know not like a couple of years or not 30 minutes to Lear blender so now one can say okay cool how about scenes can we generate scenes with different objects as easy as we are generating uh objects uh clearly since I asked this question the answer should be yes and if you check out this work this is also CPR paper uh this year this work can generate different uh scenes with different objects and not only that it can move objects around it can add objects to the scene it can be different number of objects so how does it work and and here I will go a little bit more deeper in detail so bear with me I know it's late so I'll try to make it quick so we have a prompt for example we want to generate a scene and there are a couple of things we can do for example we can say an astronaut in in space and then there are two ways we can get uh can get a mesh we can uh you know use an llm that will understand that the prompt contains this kind of objects and it will extract those objects and then we can use a 3D generator to generate those objects for example if we have a bed and a chair we can uh use llm to understand that the prompt requires us to generate a bed or a chair and then or the user can provide a b or chair so it can be either generated or it can be provided by the user so allows for some flexibility so then we can represent this bed or chair as uh sign distance functions and the now the question is uh we want to put them in in uh in some context so we have a hybrid representation here so chair and a bed are represented as uh SDF so explicitly or can be extracted meshes while environment is represented as a Radiance field and so what we want to do there we want to find good places for those objects in uh in this environment assume we can do this and if we can then we can uh train this system by using you know some we can generate an images and back probe through depth models through some uh diffusion models or through some perception losses we also fine-tuned a panoramic rgbd model to to train it but the key question how do we uh how do we generate layouts so this is really difficult and I will go a bit deeper in detail here so layouts are difficult so what we can do we need to find size rotation position for each foreground objects so what we can do we can put an object somewhere in the scene and then we can maximize try to maximize clip similarity or uh clip score between the rendered images and the text promt that we used so how to do this we can first thing we can try we can use back propagation to do that so we can optimize the parameters of the layout using clip similarity and so a simpler case of that is quite interesting so imagine we want to we have this plate and we want want to change the position of the burger so that it dropped into the plate or similarly we can get glasses and try to optimize the position of the glasses so that they get where they should so if we compute clip similarity we see that the most heat map position of of where they should be is actually located in the plate or uh near the near the eyes but if we do gradient descent or back prop we see that it kind of diverges uh so it doesn't work so that's because optimization landscape is uh non-trivial and has multiple Minima and so it's very hard to optimize so what do we do so you know these days is all about deep learning learning things from the data so we might have forgotten some of the uh old school stuff but think about this this is might be very well landscape that we want to optimize it has multiple minimum depending where you initialize you will get into different place how about some more old school methods for example particles warm optimization so this is our Global minimum this is where we want to get and so this is our cost function from the top so what we can do we can initialize randomly multiple points in this location and for each point we also have uh Direction so we have location direction and speed so these points move and we randomly initialize the location direction and speed so the next step we analyze those points and find the one that has the lowest loss so what we do then we update all the particles such that they try to get into this best position that we have just identified and then we run another step and we see that particles currently converge to uh better location and we find even better location so we do this iterations couple of times and we see that we arrived at convergence so this is a relatively simple algorithm that has been forgotten maybe but if you look how it how it looks it's actually like that and uh it's quite efficient and why it is called particles War optimization I hope this image uh this video gives you an idea so actually birds do something like that it's not only Birds it's also fishes but I didn't find a good image with fish so uh now this happens if you do gradient descent and this happens if you do uh particles warm optimization so we can put a burger where it belongs on a plate and so we can do this in 3D um and now we can generate diverse Styles we can generate different rooms uh you know it depends on what you train so we can generate all sorts of things uh different styles we can also move objects generate people multiple different cars but you can say all right but it's not realistic uh now I have a question if you were you know not very tired and attended to my talk you might have noticed that I showed this videos in the beginning and how do you think I got it how do you think I generated those videos clearly it's not me doing these things I have zero skills and also I'm not bold like this gentleman who is doing uh sculpting but these are generated by a snap video model that is also paper here CPR so um please stop by it at our poster and now you can say but okay how does this video generation how it is related to 3D and um I'm very glad you asked that because I have an answer so you could actually provide a prompt which would be uh a camera rotates around an object and you get a very nice 3D and very consistent and so the question is can we use this to uh generate 4D which would be a volumetric video and since I asked this question clearly there is also an answer and imagine this is is video generated uh this is a bulldog where in the PS had 18 candy generated but not video and now with our recent work that was uh think on archive last week uh we can generate this thing happening from multiple views and you would ask why and how it's a Gauss and splat representation so we can gener this is this is where I show just gaussian SPL representation for one frame so it's not this one is not Dynamic but there are many frames like that and in the same way like we became experts on nerves after seeing this slide I want you to become experts in gaussian Splats by seeing this one so a gentle introduction to gaussian Splats this is a triangle right this is a key element of Graphics this is a gaussin these are three gaussin these are 7 million gin and these are 7 million G rendered using all the parameters such as opacity scale and also other tricks that are used in gaan plats so now when we're experts in Gan Splats we can a bit move forward so uh I want to maybe first explain a bit how snap video works so in snap video we have we can it it's a cascaded model it has two uh Cascades one generates videos at low resolution the second one generates videos at higher resolution so imagine we have a prompt we say two pandas sitting at a table playing cards and blah blah blah and a lot other uh prompt based engineering so we pick a frame we pick a frame take this Frame and uh use S to generate a different video we take the frame and ask snid to generate a video in which camera will be S circling around these two pandas this is what you get it's a mask model so it can do kind of conditional or mask generation then we can upsample all this stuff so when we upsample this stuff we get higher resolution we upsample the frame and also upsample this somewhat supposed to be freeze Time video but you can see that it's not freeze time it's it has still some some movement there so we want to ideally decompose the movement of the camera from the movement of the object but you can still see that there is some movement so how do we build gon plats on top of that and also these are four dimensional gon plats so we have freeze Time video and we have uh reference video so in freeze Time video we have camera moving around an object but we assume that the object is not moving although it's not true the object is still moving but just a little bit and in the reference video we have camera which is fixed but the object is moving so what we can do first thing we can do we we can fit some per frame deformations that we can optimize we know the camera poses because we can run some sort of call map uh call map is still not accurate on those videos because there are some inconsistencies so we also need to optimize slightly the camera poses and so and this gives us canonical uh 3D gon plats and then given we have a reference video so we can now uh do the motion part so we have a reference camera the refence camera doesn't move so we know that whatever happens in that video is related to motion and then we can learn temporal deformations here so then how to train this we can apply you know uh temporal and uh multiv view SDS loss so in temporal multiv view SDS loss we assume that the camera is fixed uh and we sample different time in multiv SDS laws we sample the camera and fix the time so and we can uh do this to uh to learn canonical gas and Splats and temporal deformations so this is how we get Ford Splats so we have canonical 3D Splats and temporal deformations and now we can generate uh multiv view for different time steps for example you can see that this baby is a different stage of eating the ice cream so on one frame the mouse is open on the the other one is closed so they're different time and we can render it from different viewpoints also we can uh kind of choose two cameras and change the time basically and play the video so we are generating multiv view videos in that sense so there are many more we can generate this is for example a blue monster is read in a book and these are two uh I don't know raccoons on Time Square WR in a book or something like that what what's interesting that it works actually with several objects because it's it's it's a bit difficult and then if you wanted you know to see how an alligator playing drums uh looks from different viewpoints you can do this you if you wanted if you wanted to have Corgi uh doing playing with a ball or panda eating ice cream or your favorite flower growing uh you can now do this and maybe one other point I wanted to make is that you know 4D generation is not a new topic and what interest enough so we have another paper here at CPR which is called 4 DFI and we compare here at 4 DFI and interesting thing here is that you know papers before they even presented they can get outdated before they even were presented because our field moves so fast uh but that's also very exciting time to live in and uh maybe you know I have convinced you in this talk that uh you know 3D and 4D generation is not a new thing people have been doing this even 250,000 years ago so nothing is novel so reviewers to is right and then maybe the the road to generate in 3D and 4D lies through reusing the priors the rich priors used in uh learned by 2D image and video models and I think with this um I think that's the last slide I we have time can take some questions all right thanks for the talk if anyone has questions you can ask at the microphone there we've got time for a few thanks sge for the talk uh regarding the for real work it's based on a video diffusion model I guess right that's right uh could you elaborate a bit on how that was trained how video diffusion was trained well there is a poster uh session I don't know which day uh but essentially is a large for billion parameters uh Transformer model I see that uh generates realistic videos that we built at the time of submission just a followup question so I think for real is basically a adding on top of that so that you can create some consistent 4D videos from that that's correct but the way you say it it sounds simple and it was pretty hard to do yeah but that's that's really amazing like uh thanks for that work I mean yeah this this is pretty hard to do the question is how do we build methods that can reuse the reach prior and uh it takes some time yeah please uh yeah I had a question about using diffusion to do 3D and 4D editing because like in this case we kind of saw generation and I feel like there are like two research line right now like doing generation and like doing editing and like I'm kind of seeing that doing the editing is like kind of harder in a way to do because it's like getting the multi view of the editing is like kind of more difficult than for the generation it's like are you trying to kind of go into also the editing uh 3D stuff using like diffusion models or is like so what would you call editing would animation be editing I mean let's say I have I have a a table and I want like to change the texture of it or like add something on top of it without like kind of losing the uh the meaning of the scene right like not generating something is different but it's like I want just to change it in a way and just the table not the rest of the scene yeah that's a very good question I think and people have done a lot of work showing how to do this in images and in 3D using diffusion models uh we haven't done it for for videos and for 4D I mean not sure if it's possible I think it's possible it's just maybe hard and someone needs to to see how to do it but I think that eventually I'm a big believer that maybe they will be just a video generation model and it will be able to do everything I think okay thank you yeah yeah great talk um I just have one sort of detail oriented question but why only generate the multi view video of the scene from one of your frames um yeah was was there a reason or you asking what would be the alternative uh I mean generate it from every frame or or multiple frames to sort of like ensure like what if what if you mess you're kind of picking a canonical frame to generate a 3D scene from what if that's yeah I mean so I mean it looks like that so you have imagine you represent uh space and time as a matrix right and you go from first frame to the last frame you go down and your camera doesn't change but the time changes right and when you go uh to the right or to the left according to where you're standing you change uh the camera but don't change uh time right but you can start at any time frame and you can use the full Matrix while what we explored we explored only kind of the column and some row there is no real reason why we couldn't explore the entire Matrix I mean maybe the reason is that you know there is always that much time before deadline and uh also I mean we should leave some room for future work yeah sounds great thanks yeah great presentation thank you I have one question so what do you feel is the most challenging when we go from 2D generation to 3D and 40 is it because is it uh come from the data set part or the model design part of the real inference part how do you this is no real time so clearly um uh this optimization base so it's pretty slow uh it's roughly I think one hour and a half per per scene so it's a pretty slow we can make it faster I think the just maintaining the quality is pretty hard because certain aspects can be considered like if things moved is it because of the camera or is it because of the object so was that Move Motion caused by like that I just move my hand or it's just a New Perspective and I think this ambiguity is actually a different one and it's not we're not the first to see it it has been always like that how to decompose style from content how to decompose albita from shading how to decompose like motion and uh and the scene so I think this is probably difficult and as diffusion models become better uh I think this will go away and this and then we'll see probably other problems that we don't see right now okay thank you all right well hope you kind of that you made it till the end of the day you know okay let's thanks Serge once again uh yeah so that concludes the workshop uh thanks everyone for for being here we'll have the presentations up on the website eventually and hopefully some slides too but uh yeah thanks all for making it a great day e I you could that something e e e e e e e e e e e e e e for

