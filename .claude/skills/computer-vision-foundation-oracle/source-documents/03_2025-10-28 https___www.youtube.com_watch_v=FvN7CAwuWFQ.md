---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=FvN7CAwuWFQ"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:00.888Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=FvN7CAwuWFQ

d101ab4e-b05c-429c-a137-f05d9c8cb155

2025-10-28 https://www.youtube.com/watch?v=FvN7CAwuWFQ

71ab3772-e1cd-4161-92c0-35ca3d53509b

https://www.youtube.com/watch?v=FvN7CAwuWFQ

FvN7CAwuWFQ

## ComputerVisionFoundation Videos

okay great I think uh we're on um so good afterno everyone and welcome to the third monocular depth estimation Workshop um so this is the workshop which is following uh the past two versions one at the winter uh computer vision uh workshop and then at cvpr last year um through this Workshop we'll have a group of uh invited speakers who will be talking about uh their work in monocular depth estimation and uh we'll also be discussing the challenge winners for monocular depth um the depth estimation has become a prettyy crucial uh problem for the robotics uh Community to solve and it's been uh it's been one of the primary problems that a lot of us what a lot of us have been focusing on um over the last couple of years monocular depth estimation um has advanced substantially uh due to the availability of compute as well as uh a lot lot of data both labeled and unlabeled um as well as new technologies and new algorithms and models coming into picture uh that are making this even more exciting to work in this field at this moment um this Workshop is basically going to be that platform which we thought would be interesting to get like-minded people together and and discuss uh the monocular estimation overall uh we've been fortunate to be uh interacting with a lot of experts in the field uh and a few of them will be presenting their research here today um we also have a challenge uh just to give everyone a brief on the challenge uh the challenge is basically a b benchmarking uh problem uh which was carefully curated for natural scenes uh what we observed is a lot of the current monocular depth estimation challenges are focused on autonomy so we wanted to give another perspective uh using natural images uh which would have indoor and outdoor scenes um so with that U let me introduce the organiz organizing team um so I'm ripudaman uh Aurora uh I am a principal machine learning researcher at the blue R technology uh Jamie Spencer uh he is uh he was U the first um person who was leading the first two versions of this U monoc challenge um Fabi Tosi um mat pogy um Chris Russell Simon Hatfield as well as Richard U bden uh this is our organizing team and U everyone's put a lot of effort to putting this together uh today we'll be hearing from mattio uh VT and Eric uh about uh their work and research in the field um the schedule just to go over the schedule a little bit so um we start with the first keynote speaker so FMA couldn't make it so we're having Mato substitute um for for yeah a substitute for her uh then we'll have the second keynote speaker uh VOR he's already here um there'll be a 15minute break uh after that um be a good time to like talk to people socialize with the uh Keynote speakers um then we get dive into the challenge the monocular depth estimation challenge results so mat will be running that virtually um and uh yeah eventually uh Eric Brockman will be giving his talk at 4:30 and then we'll conclude by around 5:15 with that uh let me hand it over to Mato pH can you hear me yes can you hear me can you hear me ripo give me one second I can hear you I just connected the audio so you should be good okay thank you very much so I'm going to share my screen okay can you see my my slides so looks good okay so thank you very much repo for the introduction sorry Matthew one second we're still sharing the uh slide here give me one sec okay uh Matthew we're good to go sorry I couldn't hear you we are good to go okay okay thank you very much thank you repo for the introduction and hello everyone Welcome to our monoc dep estimation challenge I will open this workshop with this talk with a provocative title which is monocular depth estimation are we done and you will uh soon figure out why I gave this title to this uh to this talk and uh here I'm showing you a very quick overview about what we will see during this talk we will have a very quick introduction about how everything started uh concerning monocular depth estimation in particular we will have a little focus on self-supervised monocular depth estimation which was the very the very first topic which ignited the monocular de estimation challenge that we are actually taking that is taking place today then we will look at how monocular depth estimation is going on today and we will have a little focus on uh more recent Works dealing with aining variant zero shot monocular depth estimation as well as as with the very most the very most recent foundational model that we you will probably heard about and you will probably see around at cvpr in the next days and finally I will show you how monoc depth estimation is still far from being solved in particular even the very latest Foundation model can still fail in some very challenging condition so before starting just a very quick note and about how it's going on I will just talk about a a f variant monocular depth estimation models but another very ODOT topic uh nowadays is zero shot metric depth estimation and probably VOR will give you a look about this during his talk and then a second disclaimer uh unfortunately this was an emergency talk so I prepared it very very quickly and uh often I mixed up different coloring codings for the da maps that I will show during the talk so I'm very sorry for this so let's start very with a very naive definition of what uh what actually is depth what is a that map so let's assume we have any picture like those we have here on the top and when we want to estimate depth for those pictures we are actually estimating a new image in which each pixel on this new image contains the actual distance of any point that we have seen in the real image with respect to the camera that collected the original picture so we will have a new image that we call a that map and each pixel encodes this distance of course to have a more pleasant visualization for the human High usually we encode these dep Maps According to some color scheme and in this slide for instance we are using a color scheme varying from red to blue to encode those pixels that are the the closer and the farther from the camera and what can we do with a Dev map of course we can recover the 3D information about the scene that we captured in our picture so we can take all of the P all of the pixels in the single picture we can use the depth to project them back in the 3D space thus reverting the image formation process that we carried out when we collected the picture and we obtain some kind of 3D representation of the scene in this specific case we obtained a point Cloud by projecting the pixels into 3D points so let's have a look at how this started so monoc depth estimation is getting uh quite long standing because I would say we are working on this for more than 10 years and while at the very beginning the very first approaches were training some basic convolution neur Network on very few annotated data I think the very first um the very first advance that made this topic explode was the introd the introduction of self-supervised monocular depth estimation and in particular in 2016 2017 we had the very first approach that we're capable of learning to estimate depth out of a single input image without any kind of uh depth annotation on the images themselves so in this slide I'm showing you some of the results from one of the very first approaches appearing in 2017 and yeah I know that looking at these images right now might feel a little disappointing but I can assure you that for those working on this Topic in 2017 these results were Unthinkable because despite being not that accurate here we can perceive the 3D structure of the scene and the very surprising point was that everything was learned without any kind of supervision in terms of that labels but of course nowadays the thingss are very very different uh in the in particular through the years the increasing availability of annotated training data made self-supervised depth estimation going a little in the background in the overall research Panorama and nowadays one of the most standard approach is to combine as many data set we have available of course with as many uh ground TR depth annotation or pseudo ground TR dep annotation we have in order to train the more the most robust neural network we can and here in this example I'm showing the results by depth anything that is a foundational model that you will see many many times today during this workshop and probably also in the next days during the main conference and as you can as you can notice of course the results by depth anything are much much better with respect to what we could get by means of self-supervision at the very beginning of the monocular depth estimation era so nowadays we have much much better results nonetheless even if we have some very strong models like dep anything and a couple more that we will see doing this talk there are still some very challenging condition in which in presence of which these accurate models can still fail here I'm showing you some uh pictures taken from the booster Benchmark that I will introduce in the reminder and here we can notice we have some bottles being characterized by some transparent surfaces both at the top and at the bottom and what happens if we run depth anything on these images well we can notice how dep anything cannot deal at all with transpar with transparent materials indeed we can notice that the bottles tends to disappear in uh in the predicted dep map meaning that the current model has no knowledge about this kind of objects of course for those of you that are more familiar with the topic probably you know four days ago that anything V2 came out on archive so today while I was uh preparing the talk uh I made the prediction the depth prediction for these images with that anything being pretty sure that that anything V1 was failing and then I said okay let's see how it goes with that anything V2 because as we will see in a in a few minutes this model is even better than the previous one nonetheless we can see that in some cases the perception of the bottles is slightly improved nonetheless we still have some critical failure cases in which actually the bottle is not perceive at all so this is just an anticipation for telling you that no we are not done with molecular depth estimation so now we will go a little more in detail into these three uh topics that we will cover with this with this talk and we will start in by diving a little more into how uh everything started so I mentioned uh 2016 and 2017 as the years uh during which self-supervised monocular Dept estimation appeared and then exploded making this topic very very popular and the rational Behind these uh these these kind of approaches is to replace the ground tro depth annotation with some form of self-supervision that we can obtain in alternative Manner and the preferred alternative manner is the use of multiple images indeed uh as soon as we have multiple images of the same scene even if we make our neural network process a single image we can use the remaining images to obtain some uh self-supervision by exploiting multiview geometry and in the literature we had two main families of approaches to deal with this kind of self-supervised framework the first one was proposed in 2016 by gar at all and the choice was to use stereo images in this case so given a a data set in which we have multiple stereo pairs for each left image in the steer repairs we we assume it as the input to our monocular depth estimation Network so so our network is trying to predict a dep map for this input image or an inverse dep map and according to this dep map we retrieve the corresponding Right image for the input image we use image warping according to the estimated depth to reproject the right image over the left image this means that if our dep map is accurate we will be capable of reproducing a warped image that will match the left image except near the occlusions of course the more accurate the D map is the more precise will be the Warped image and to figure out out to figure out how accurate is our predicted that map we can simply computed the risk construction error as the difference between this warped image and the original input image given this error then we can simply use this as a form of loss function to make our uh monocular Network learn how to predict depth out of the single image and the main properties of this framework is of course we just need to replace ground through depth with a second image and since we are dealing with the stereo images for for which we assume we actually know the parameters of the stereo camera that we used for collecting them we are capable of uh learning a that map that is in metric scale the second approach is a little less constrained because it still assumes the availability of multiple images of the same scene but this time these multiple images are collected by a single camera so as in this example we have what we call the target view which is the image that will be processed by the neural network and we have some nearby views so the molecular network will again predict a dep map and this dep map will be used used once again for projecting the nearby views over the target image in this framework the main difference is that we don't know the relative position between the images and we need to estimate that to estimate such a relative transformation and for this purpose we use a parallel neural network that is usually known as the postet to basically process both images uh together and estimate the relative transformation between the two then using this relative pose and the predicted depth we can reproject the images and we can obtain the supervision for both the depth Network and the post Network conversely to what happens when we have stereo images for which we know the exact distance between the two cameras in this case we cannot uh properly estimate the metric distance between the two images this means that also the predicted that map will be by its own nature up to an unknown scale Factor so in order to retrieve the metric scale for the scene we will need to actually estimate this kind of scale by means of different strategies despite this little drawback of course uh you might as you might imagine scaling up monocular videos is much simpler than scaling up stereo images just because we can possibly uh collect any video we can Source from the web and any of this video is poten Al a source of training data for learning to estimate depth out of a single image and indeed Jame and his team at iccv last year proposed the slow TV framework that was basically scaling up the amount of training data and together with some very nice implementation tricks they were capable of training a self-supervised monoc dep estimator that is much better than everything that was proposed just before of course even if we are capable of scaling up the data that is in the Deep learning era what really matters for having an accurate neural network some weaknesses Still Remains indeed the self-supervised losses we can get for from image from image repr projection are indeed weaker with respect to the use of explicit depth labels and a second uh a second issue uh appears when we use monocular videos indeed if we have any independent Mo independently moving object appearing in this video These object will violate the multiv view uh geometry assumption that are at the basis of the self-supervised losses and so we won't be able to properly learn the depth for those objects and uh yeah this supervised approaches become very very popular and this research Trend exploded uh however at the very beginning all of the approaches were focused on uh very limited data sets for instance most of them were focusing only on the kitty data set and this could lead to some biases in the evaluation and could also limit the advancement in the research on this field and this is why the very first edition of the monocular depth estimation challenge was proposed in wacv 2023 by Jame and his team and for this challenge a new data set that we will introduce later during the workshop was proposed to as a more variated Benchmark with both indoor and outdoor scenes in order to get rid of any bias during the evaluation process and in the first edition of the monocular depth estimation challenge what was quite evident is that self-supervised approaches despite being to some extent accurate they were still suffering of several limit ations indeed from the dep map that we can see here we can notice how self-supervised approaches are not really capable of reproducing the fine details that we might have in a scene like this that is taken in a forest and often they also tend to reproduce the texture from the input image in the predicted that map as well so this is how everything started so we had three to four years during which self supervised approaches were the uh most popular strategies for conducting research in on this topic then three years ago something changed uh and a new direct research Direction emerged in particular with the increasing availability of data sets and uh either ground root or Pudo ground root depth annotations uh some research teams started inquiring about the possibility of mixing up as as many data set as we can in order to train monocular depth estimation model that could be possibly uh be capable of generalizing on any unseen domain and this is what the Midas paper did in 2020 and Midas was the very first work following this direction and uh in this work the the basic assumption was to train a convolutional neural network to predict a relative depth in a fixed range for instance let's say between 0 and one and this relative depth was optimized over a variegate set of uh data sets and corresponding ground TR labels having very different depth distributions one with respect to the other and to make this work uh Theos proposed uh to basically learn a dep representation that is invariant up to a scale and a shift factor with respect to the real depth so basically authors admitted that uh learning metric depth at that time was not sufficient for for having a generalizable estimator and so they proposed to learn this aining variant uh depth estimation with Midas and this made very simple to combining up many data sets and this way you know the more the data you have for training your model the more generalizable will be your final solution and uh yeah with the Advent of Midas and let's say uh more recent Works focusing on this kind of task rather than self-supervised depth estimation the second edition of the monocular depth estimation challenge also opens the door open the door to supervised approaches and indeed from the results uh that were presented at cvpr 2023 we figured out that supervised approaches can are slightly better at retaining the details uh that we have in the input image and of course given enough training data they are also more accurate with respect to self-supervised Solutions and nonetheless Midas basically was followed up by several more approaches until now until cvpr 2024 and in particular in the very latest month we had some very nice uh works extending this idea of learning and aining variant uh depth representation and some of them basically scaled up this kind of approach to exploit even more data with respect to those that were used by the original Midas and in particular in this talk I will introduce you uh very quickly very very quickly two foundational models that you will see around at cvpr in the next days and one of these models is known as marold marold is a depth a mular depth estimation framework derived by a latent diffusion model and specifically the authors uh claimed that these very large models that have been trained on billion of images they will know for sure much stronger priors with respect to anything that we can make them learn on the data set that we have available for uh depth estimation so their idea is we just take an off-the-shelf latent diffusion model for instance they took stable diffusion V2 and they repurpose the stable diffusion V2 uh model for estim for denoising actually a that map rather than an image and so for training this kind of approach they took the latent encoder from stable diffusion V2 and they kept this Frozen a training time they took the input image and its corresponding ground through dep map they extracted the embeddings with the latent diffusion incoder then out of the depth embedding they add some random noise and then they start the den noising of such a noisy Vector conditioned by the image embedding that that is provided uh to the latent diffusion unit concatenated to the noise so the latent diffusion unit is fine Ted to denoise such an embedding and of course this happens iteratively as it happens for any latent diffusion model so at any iteration of the unit some noise is removed from the latent vector and this noise is compared with the ground tro noise that we generated and this difference is used to fine-tune the unit so this way we can fine-tune the stable diffusion V2 for estimating depth and and at difference time of course we won't have the ground TR depth we will have only our single image but that's enough because we will use the latent encoder to get the guidance for the diffusion process then we will sample some random noise and concatenated with the image encoding of course we will proceed the noising the noisy uh embedding once we have the noise dis embedding we will use the latent the stable diffusion decoder to obtain the final that map and according to this framework the authors were capable of fine-tuning stable diffusion for very few iterations indeed their training was just something like two days long and they used only synthetic data sets for training marle this means they used only a few hundred thousand synthetic images and of course the important the importance of synthetic images is that they provide dense ground true depth that is necessary for making everything work during the lated encoder processing and here I'm showing you some results just to make you uh figure out how good the results by merry are and in particular the use of the prior knowledge from stable diffusion as well as as the uh use of perfect ground tro that is very fine details made available from synthetic data sets uh allow marle to provide much finer uh that maps with respect to everything that was proposed just before marold and this idea has been uh extended recently by other following up Works uh some papers used this idea for estimating both depth and normal but we won't we won't talk about this today in the interest of time and another more recent approach uh used the idea of marold uh to uh predict depth out of a monocular uh video sequence and in particular the idea was to replace of course the stable diffusion V2 model with a stable video diffusion model and to fine tune it over video sequences and this allows to obtain a depth estimator that is capable of both predicting accurate dep Maps as well as specially as well as sorry temporally consistent dep Maps indeed the temporal consistence of the different frames predicted by a monocular depth estimator is usually a concern but as you can notice in the videos that is playing on the background this kind of approach that is called Chrono depth is capable of predicting depth that remains consistent over time in particular with respect to marold and to the depth en model that we will introduce in a while here on the top you can see as Crown dep is much more temporally consistent and uh of course yeah now we talk about depth anything I've already anticipated this as one of the most popular model cing out uh in the last months you have probably heard about this you will for sure ear more about this in particular when we will we will look at the results of this edition of the molecular depth estimation Challenge and uh the main uh the main idea behind that tening is to take everything we can in terms of training data to make monocular depth estimation scale up at the best we can and in particular the authors were very smart at mixing up a few millions of images with provided depth labels together with many more millions of images not provided with depth labels and the main idea is to exploit a teacher student framework in particular a first version version of depth anything is trained as the teacher on the on the few million of images with the provided dep labels then once the teacher is trained uh a new network the student starts starts from scratch a new training on both the labeled images as well as on the unlabeled images for which the teacher can provide some seog gral depth labels and both the networks use an encoder that starts from Dino V2 and during the teacher student training the features extracted from the teacher and the student are compared and a loss is um and a loss constraints the true to Remains the Same and according to this strategy dep anything was capable of generalizing much much better with respect to any Midas version and any existing approach pursuing a fining variant depth estimation but of course as I told you uh just a four four days ago that anything V2 came out and by the same auor of dep anything and basically the they were capable of improving the results even further and they followed up a similar strategy they already used for depth anything so they scaled up the training data but this time they learned the lesson from marold that was published in parallel with them so they replaced the training data they were using with dep anything V1 which were real images but having noisy depth labels and they replace them with synthetic images having perfect ground tro so these way they were capable of training a teacher that is much more robust and capable of predicting much finer details in its own dep maps and of course the better the teacher the better will be the final student that you will have in the end and just to give you an idea here there's a comparison between depth anything V1 and that anything in V2 taken from the paper you can notice how both uh that maps are very good and they can generalize in very different environments nonetheless we can appreciate how that tening V2 is capable of preserving the details much much better with respect to the original version and here there's a comp between marle and DEP anything Vu and again we can appreciate how this latter model is more robust with respect to marle to some very challenging conditions for instance here in the middle row we have some we have a foggy image in which marle to some extent struggle while that anything be true is much more robust to this kind of challenging condition so uh after these two part these two sections of my talk you might might think okay we are done with monocular depth estimation but as I anticipated you at the very beginning we are not entirely done so we can figure out that even these foundational models like marold and DEP anything V can still struggle in some very challenging uh situations and this very challenging situation often uh corresponds to some um some objects some elements that belongs to the very long t of the training data in particular all those objects that are non lambertian so that are transparent or that are reflective may represent a challenge even for the foundational models from now on I will refer to this kind of objects as Tom objects so transparent or mirroring objects so if I take these two pictures and I process them with depth anding V1 this is what I get so as we have already seen at the very beginning of the talk the transparent objects tends to disappear in the predictions just because the monocular depth estimator uh probably has never seen them in the data distribution so basically what we need to make a foundational model or any molecular depth estimator uh be capable of predicting the the depth for this kind of objects is just to enrich its own training data and for this purpose a CP couple of years ago uh my team and I proposed the booster data set that is uh of course A Benchmark tailored for boosting the research on depth estimation in presence of transparent and mirroring objects at the very first the booster data set was proposed for the stereo matching task and we collected several scenes like the one I'm showing here for which we were capable of providing very accurate D maps that could be used at the ground tro and of course this ground tro compared to the prediction that we could get with a state-of-the-art stereo networks is very very different indeed we can easily perceive how the stereo model cannot properly um identify the mirror as a planner surface just because of course the reflection in the mirror violates the basic assumption behind the stereom mattion task so what we did was we set up uh some uh some scenes I would say something like 60 scenes in which we add some uh Tom objects so both transparent or either transparent or reflective then we set up our camera we collect a set of images with varying illumination condition and then we started our annotation pipeline so uh to in order to get the depth for the transparent surfaces we painted all of the non lambertian surface and then we projected some random patterns and collected multiple images over time why did we do this because by collecting multiple images with some random patterns we can use a space-time stereo to get some accurate depth labels and in particular we use the SpaceTime stereo together with a deep neural network to obtain some initial uh depth labels that were of course not perfect but they were indeed a very strong guidance then starting from this guidance we had the funny part we had some manual filtering in particular Fabio uh had a lot of work to clean up all of these uh all of these depth annotations and then we add our final data set and as we can notice from this point clouds the uh annotation we could get are very very accurate while what we could get by running a stereo Network on the original passive images is something very very noisy so with this data set we organized a couple of challenges one took place yesterday at the entire Workshop so just as a reference for future editions because we are counting to uh have a new challenge next year but another challenge is still taking place and will be presented at the tricky 2024 Workshop in eccv in Milan this autum and this challenge is still open so if any of you is interest is interested into participating and is willing to figure out how to estimate depth on transparent objects feel free to participate here I'm showing you some references and of course at the end of the talk I will share again these references with you and uh I'm going to conclude the talk with some lessons we learned while collecting the booster data sets uh of course the most important lesson we learned while collecting this data set is that annotating no lumbert and surfaces is extremely time consuming in this this is a picture just behind the scene uh when collecting the ground tro for the motorcycle and here there's Fabio painting his own motorcycle because this was his motorcycle and I think this took something like maybe a couple of hours to full paint the motorcycle then of course we started collecting the images we we postprocessed the images and in parallel of course Fabio was willing to use back again his motorcycle so he also had to clean everything up so you can imagine how collecting the ground TR for a single scene is extremely time consuming and thus this approach is not that simple for scaling up the training data so uh Having learned this lesson we started thinking about some possible Alternatives so how can we get some cheaper annotations some Pudo ground trots with very fewer efforts in particular can we do this with possibly any uh image we could Source from the web so can we download any image with the transparent or mirroring objects and obtain some pseudo ground TR from these images and uh well actually we came out with a simple solution and the solution was exploiting monocular depth estimations monocular depth estimators themselves so we started by studying how the single image depth estimators fail in presence of this kind of image so here we have a picture in which we have a tiny mirror that is reflecting the content that we can notice here in the image so if we take this picture and we process it with DPT this is what we get so we have this context that is quite large in the image that is influencing DPT and in it's making it believe there is something just behind this kind of object so since we we feel that morular depth estimation is mostly based on the context in the image we we asked ourself what happens if we simply take a mask and we simply paint with a Conant color the mirror so we did this we processed this with DPT and this is the result we got so basically we could get some reasonably good dep map that in which we could have a planner surface in place of the mirror of course a testing time we cannot be there and just paint our image so uh we cannot assume this as the final solution but maybe we can use this to get some pseudo label for training a moare depth estimator and this is how we proposed the depth for Tom at iccv 2023 so we took a data set with transparent and mirroring objects and some segmentation masks that could be even that could be either annotated manually or by some kind of segmentation Network then we for each of the image we took the mask and we imp painted the image with some random colors then we took an offthe shelf monocular dep estimation Network to estimate some pseudo labels of course since we are using random colors we might have some ambiguities with respect to the background so we repeat this multiple times to be sure that at least a few times we are not having ambiguities and then we predict the depth for any of these images then once we have these multiple images we combine them by means of a simple average or the median of the the the depth over each pixel and by repeating this over all the images in our in our data set we can produce a training set over which we can fine-tune any uh single image depth estimator even the one that we used for creating the pseudo labels themselves and how do we do this well when we start training of course the monocular depth estimator will process the original image over which it will fail it will predict some kind of dep map and then we will use the virtual depth labels that we processed before to obtain a loss function and to fine-tune our monocular dep estimator and here I'm showing you some results here we have two pictures from the booster data sets with a very large mirror and transparent bottle and at the bottom we have an Hoven with a very reflective surface here in the middle column we show the results obtained by DPT using the original weights made available by the authors while on the right we show the results that we have obtained by fine-tuning DPT on a completely different data set that has been annotated through our death for Tom pipeline so once again the images of course were not part of this training data set and even if the images that we use for fine-tuning were were very very different because most of those images were uh top views on some tables with some transparent objects nonetheless those priors from those data set were enough for fine-tuning the model and make it aware of some larger context for predicting the depth and these are some references for for all of the papers I mentioned during the talk and thank you very much for your attention feel free to ask any question and I'm sorry if the talk was a little messed up but but yeah that was a last minute talk so I'm sorry about that thank you for your attention uh ripo can you hear us Matthew can you hear us yeah yeah I can yep so can you use the technique that you went over to oh wait a second I can't hear you anymore maybe my I maybe my has some issues okay try again can you hear us okay now I can hear you yeah so the question is can you can you use the techniques that you shared for metric depth estimation okay actually you mean which technique the the last one or any of those techniques that we mentioned before um just in general just in general any just in general okay uh let's say I focus mostly on techniques estimating relative depth this means that the network itself is not targeting metric depth as the final predict ition but of course you could fine tune all of these models for predicting metric depth indeed something I didn't mention in the interest of time is that sorry I'm going just back that anything V2 made an experiment like this so they first train a relative depth estimator and they also fine-tuned it for metric depth estimation uh while there's also there exist also an entire new research Trend that concern Le uh predict directly predicting metric depth and there's a couple of works at cvpr and I also think I think also probably vtor will talk about one of his work on Metric depth estimation so usually we have these two different approaches but in principle you could fine tune all of these also for metric dep so on the meth to actually detect dep in the reflective surfaces so for training did you actually like on the image create the mass manually and then like train that as it is and then that's why the depth was estimated currently for those refective surfaces Matthew I'll just repeat the question so the question is depth for Tom in that data set did you manually create all the M okay actually we did both we tried both with manual we tried both with manual annotations because the data set we used for training was providing the segmentation masks which were of course accurate but we also tried with uh the prediction by a segmentation Network indeed there are a couple of works I can't remember the name maybe one was trans toag something like this uh in which the authors were training a segmentation model for for identifying the transparent and mirroring surfaces so of course in principle you could have a neural network predicting the mask for you of course this mask will be likely to be less accurate than those that you can annual that you can annotate yourself but in our paper you we showed that the results are almost comparable and any change in the like the field of view like right now show Imes are close to the camera but what if the field of view is larger if there's like aor the back while you're seeing the Indo would would that you the would that account for the view change as well um ask because you start a video scene where somebody scanning the room can start from small distance and then you can move back further and then scan the room so in that kind of like a streaming scenario how would like would would you could see any issues Matthew do you want me to repeat the question or you got it h yeah because it's yeah we can't hear very well from Zoom or actually I can't cool so so the question is that when you change the field of view so for example you're taking a video of a uh of a scene and there's a there's a mirror in in the center of the scene but you go backwards so the field of VI changes and therefore there's like a lot more context in the scene and the reflective mirror becomes comes a small part of the scene rather than the whole scene which is there in the depth for Tom data set what happens in that case well actually I don't know because in the data set we used let's say the mirrors were covering a large part of the pictures but yeah I would assume I think the depth anything V2 paper showed I can't tell for sure but I think at some point they have a qualitative example that was something like this so they have something like a um a truck that on his back was uh showing what was in front of the truck so basically you had this truck showing you the car in front of of the truck itself and the basic monocular depth estimators were actually predicting a a hole in the truck because even if the context is not that small maybe it is sufficient more question if I have a stereo rate does it ever make any sense for me use or should I always useth so Matthew the question is if there is a stereo if I have a stereo rig does it make sense to use monocular depth estimation or should I go for deep stereo methods okay yeah uh from my personal perspective I still I think you still should go with stereo because uh of course with stereo you have metric depth if you know the parameters of your camera and of course it's a matching problem so with the very latest state-ofthe-art models I think you should go for stereo I don't know how long this will last because with that anything V we are getting better and better so probably at some point there will be some use case in which stereo probably won't be that convenient with respect to this kind of foundation models all right uh thank you Mato um we'll move on to our next uh keynote speaker thank you very much right so our next speaker is uh WI G Al California uh his interests include self-supervised learning monocular depth estimation uh unsupervised domain adaptation uh implicit visual representation camera calibration and vision based machine learning in general um he has close to 100 Publications in this related area in addition to the large number of patents in different uh stages of filing uh and applications in various products he is the creator of uh Creator and currently maintains the packet net sfm and wiar repositories to of the most widely used depth estimation code bases in the scientific Community he co-organized uh the first three editions of Frontier for monocular 3D depth perception workshop and has also given several talks in the top tier conferences so welcome thank you you connect could be connected but I don't now it's it's here get you can see it you see it yeah can you hear me can you hear me on Zoom uh yes I think that's yeah sorry for this technical difficulty yes so thank you for the introduction and thank you all for coming to this wonderful Workshop uh my name is Victor I'm my sta scientist at the to research institute and this is the name of my talk for today uh an old to mod deps so for those who don't know an old is a type of P used for celebration to elevate a person or an idea and this is what I'm planning to do here today I've been at the for the better part of five years already and doing this time my research has always been centered around dep estimation in one way or another and other geometric based tasks as we like to say so five years is a lot of time uh there were lots of publication lots of patents and I had the pleasure to see the field evolving just as Mato just was talking about now and with all challenges being conquered as they always do and new ones rising up to take their place so and by no means I was the only one there were many other brave people taking apart to face those challenges sometimes finding solutions that I could never have thought of and sometimes making me wonder how that paper got accepted so today I'll talk about my journey how I started how I got to am today my decisions along the way that were kind of shaping the fields but also being shaped by the fields that it was evolving and what I think will happen next so hopefully you find this useful see a little bit of your own Journey or get inspired to go on your own adventure uh and thus from my first verse I'll start with with pet so it was a nvpr 2020 and it touch upon a very interesting aspect that is unique to this geometry based tasks which is sised learning so I'll briefly touch upon that even though Mato did a pretty good job on his talk previously uh most people are familiar with self Supervision in the sense of mascot encoders contrastive learning and so forth but here there are some additional priors that we can use to train an actual dep Network without ground truth of any kind just a sequence of images usually videos but I'll expand on that later if you have two such images you can run one of them through your dep Network and un project information to 3D Point clouds another Network estimates their relative Transformations across the two images and it can use to warp the 3D Point cloud from one frame of reference to the other frame of reference before project it back to produce a syn a synthesized version of the uh first image of the second image using only information from the first image and that's our training loss give me some assumptions this loss will be minimized if and only if your depth and post networks are correct that's how you train a depth Network without any labels uh and because our our train objective is photometric it's easy to see that resolution plays a big role the more detail your image is the more accurate your reprojection has to be in order to minimize the photomatic error but back in the day and even by today standards honestly processing high resolution images is very timec consuming taking up a lot of uh uh a lot of time in memory as well so our main contribution here was a novel architecture designed to preserve spatial information during the encoding and decoding stages so instead of down sampling using Max pooling and upsampling with bilinear interpolation we propos packing and unpacking operations that use 3D convolutions to switch between spatial and channel Dimensions so they are preserved all the way from the beginning to the end as expected this network uh was much larger that what was available at the time reaching upwards of 120 128 million parameters which was quite a lot back in the day so times indeed change uh but we show the pack net scales better than other networks relative to the number of parameters that it benefits more from uh higher resolution input images and also scales better with range with a gaping performance increasing as you consider longer distances which is expected since preserving high resolution details is even more important as we consider regions further away where one single Pixel can uh cover an area of several meters so you need indeed need F find Grand details on those cases so a little bit into the metric realm that was going to talk L today another contribution of this paper is weak velocity supervision so one key problem as we all know uh with sales provis learning scale ambiguity meaning that the predicted depth and eom motion will not be metrically accurate it's going to be up to scale uh makes sense since there is no way to disambiguate between uh this the size of objects purely from images so I driving through a city or through a toy model of a city everything's correct up to scale so to to address this issue we show that it's possible to inject metric information into the PO Network by supervising the estimated translation with instantaneous velocity measurements and in doing so the depth Network also becomes metric because it needs to be consistent with the post estimate uh with the post estimates because everything is learned jointly and as you see this metric monocular depth estimation idea become a reur part of our research Direction at TI and it's currently a very hot topic as I'm sure you're all very aware of so we be doing work on this but take it took us a while to get there and took the field a while to get there as well uh our third and final contribution was a new data set called DDS or ddad as some people like to say it was collected with Toyota's vehicle fleets covering us and Japan cities and is actually a very very small snapshot of our own internal data sets that we use to train our models internally as they make their way to deployment and production it contains six synchronized cameras in high density ground Truth uh lighter with ranges up to 250 meters and it and it enables us to expore also cross camera consistency as I'll talk about later uh and also over the years made its way to become one of the standard uh bench marks for depth estimation which is very nice to see um here's some pet uh results on the uh D data sets uh those were pretty amazing back in the day so this purely self-supervised and scale aware so these are actually metric because we have the Velocity measurements and it was pretty amazing back in the day I still find it quite amazing and gave us the indication that this was a very good direction to keep pursuing so what to do next uh the way I like to see that we started from the hardest problem which is learning from un label videos uh turns out it wasn't the hardest Problem by far as I'll talk about in a bit but the idea here is how do we have a stable way to solve a problem without any supervision uh what if you had some labels even if not the ones that you are looking for weak velocity supervision was a step towards that and this followup paper explore semantic segmentation as another alternative so still no ground truth those are generated from a frozen semantic Network and the key idea is to use uh the features from the semantic Network to guide the generation of better depth features so you are add information that is not available in the image itself because it's semantic so we had to come from somewhere else uh to achieve that use pixel adaptive convolutions to to generate semantically guided kernels for the convolutions operations of the dep Network so they have more information in addition to the RO pixel colors and by supervising the network at a feature level um you preserve this information every step of the way just like we did for the packing and unpacking operations from from the previous paper and leading to better results than just providing the semantic uh they predict the semantic map at the input level and plus you get to run those two networks in parallel because the sematic network is frozen and never change so you still get that and you can run both in parallel which is quite useful for Real Time applications so there is no latency and another key limitation we of sales provis learning we address in this paper is what I like to call the infinite depth problem if you have an object moving at the same speed as the camera there will be no relative motion so your network will learn uh there will be no relative motion and that's what you expect to learn from something that is infinitely far away so there is no motion if you're infinitely far away so your network will learn to map those objects to very large depth values creating this object shaped holes in your depth map at the ends which is kind of dangerous if you are applying that to something that is practical and try to avoid obstacles so our solution here was a two two stage training procedure first you train a model use all the available data and accept that we have this problem so that's the first rle uh then you use this model to filter out any problematic frames uh by detecting the estimated 3 3D points that fall below the ground plane so why is that so if you have an object on the ground that is very far away and shouldn't be it's Point Cloud will go through the grounds can you can see that with some geometry and that can be automatically detected by just finding pixel that fall below the main ground plane assumption and then you you remove these pixels and you you remove the frames that contain those failure cases and then you train a second model and just like M was saying if the model was not uh uh was not exposed to these sorts of failures it will not learn those failures like he did for the mirrors if you remove the mirrors in your training data set you will not learn how how mirrors behave so we go through whatever else is available in your training data sets and the INF dep problem kind of goes away just by treating the INF dep as a bias problem in your data sets and the bias in your data sets to to be more realistic um moving on to next one another sort of source of supervision we explore in this other paper was depth itself so how SPS you can go while still extracting use information to improve self-supervised learning and of course you produce metric estimates so this is semisupervised learning because are doing self supervision and supervision joint with uh increasingly sparse depth maps as the supervision so the goal here was to produce uh a supervised objective that is more aligned with the photometric self supervised loss that we use that operates by warp information across frames and at a pixel level so we created a supervised loss that also operates at pixel level by reprojecting the supervised error onto the context image so instead of warping the pixels we warp the range error between prediction and ground truth and minimize that as well joint with the automatic aror so there is a pixel level and there is a range error level of losses but they both work at the image plane we experimented with with various levels of uh sparity in our data sets range from Full supervision around 20,000 pixels to less than 100 and our proposed uh reprojected distance loss consistently outperform the other traditional supervised losses especially lower density settings and now taking one step further what do we try to do next was investigate what happens if you actually have depth labels at T time so this is a slightly different problem than monoc depth estimation which try to predict the depth map and that's why you call it depth completion because you're trying to go from a sparse depth map to a Danse depth map with the help of your image here propose a novel module called s so sparse auxiliary networks that uses sparse convolutions to process the input depth maps that you have available and inject the information into the skip connections of a traditional monocular dep estimation that does prediction uh one very interesting benefit of that is that you can turn the same module on and off so if you don't have any input depth you can use this network for prediction but if you do you can just turn it on and switch to depth completion and getting all the benefits that come from this additional source of inputs uh on this plot here you can see the effects or different amounts of spars that they doing training uh as few as 1% is very good enough to to see improvements and you can see that depth completion improves by quite a lot as you provide more input depth data uh during training and doing evaluation as well but the surprising thing here is that we also observe improvements in depth prediction uh showing that this joint training leads to improvements even if you don't have put depths at test time so essentially this we get these improvements for free by just changing How We Do training to leverage input depths as an inputs as well rather than just as a source of supervision for our Network plus you can get the benefit of turning this on and off in case you have this information available um another very important aspects of monocular dep information that we studied and we are still studying I think I think this is a very exciting topic is it used as a pre-training tool so to serve as a feature structure for Downstream tasks uh sales provision gives you depth for free but even ground Tru depth maps are much easier to obtain doing compar to labels for some other tasks such as cement segmentation or through the object detection in this case so what we did was modify a monocular depth estimation Network a monocular 3D detection Network to include the depth estimation heads that share as many parameters as possible with the other outputs in this case the confidence heads and the 3D B box head we then pre-train this network using different amounts of depth data from our larger ddad internal data sets think we end upwards of 15 million samples in this case and as you can see on the right uh pre-ra your more depth data consistently increase 3D detection performance even though we are still fine tuning on the same amount of 3D bounding box data which is much harder to label than that data in this case unfortunately we could not release these data sets but we did release the pre-trained d3d backbone from this paper and for a while pretty much all of the uh leaderboard methods were using our backbone which was pretty cool to see as well so in our follow-up work presented iic last year we showed that salesvision can also serve as a pre-training uh tool for Downstream tasks so leading to improvements over our original DD 3D uh backbone across the board by injecting even more un label data which is even easier to obtain than label data in this case so now there's no limits to how much data you can use to improve the downstream task and now that we show that dep esor can be used to improve Downstream tasks the next one we explore was semantic segmentation so now not as additional inputs but as the outputs but instead of pre-training we focus on uh unsupervised domain adaptation where the gos to improve performance of cement segmentation Target domain without having access to any labels from that domain uh that sound very similar to what we're doing for self provis learning where the whole point is to train without labels so what we did was multitask learning with a single encoder that takes us inputs an image and two decoders one for depth and one for semantic segmentation for training you had a combination of synthetic data where labels are fully available and they always are and real world data with only videos uh considering a mixed batch from these two domains for synthetic data we supervised for depth and semantics so because it was there and for reward data we only sell supervised for depth on the videos that's all we could do so there's no way to Sal provide semantics without any other any other assumptions so because there was only one in quarter the self supervised domain adaptation from a depth perspective actually led to improvements on unsupervised domain adaptation for semantic segmentation even though we NE we never actually train this task at all so the geometric prior in self supervised learning for depth estimation were being used to improve a task that had nothing to do with geometry it everything was happening at a feature level like we did for pnets and like we did for the semantically guided work as well so that's a very interesting aspect is what are you learning at a feature level for depth estimation so that was like the underlying strategy that we're going for here as a result we achieve State of-the-art performance without any bells and whistles no pseudo labeling self trining only multitask self supervision so this is the semantically uh maped Point clouds from the same network just run for both tasks simultaneously and as I discussed before the introduction of cement segmentation uh cement information doing training also L improvements for depth estimation with sharper and more powerly consistent frames again because we have this additional source of information at a feature level so now for our next paper uh a little bit away from Bondo depth but still depth we address a very common question in self-supervised depth estimation as well if you're training with multiple images why don't you use multiple frames at that time as well so this a very good question and to put it simply back to what mat was saying as well sales provision is not perfect it relies on some assumptions that most of the time just are not true real world environments are almost never fully static you have non laboration surfaces uh different viewpoints will lead to changes in brightness occlusions textureless areas and so forth so when training those failure Cades tend to average over time and can and you can still expect your model to have enough correct information to properly learned and even then there are some failure cases like the infinite depth problem that I was talking about earlier but during inference uh those failure case tend to be catastrophic when the model needs to rely on a proper pixel correlation across image so that's also a potential shortcoming from using stereo as someone was asking in the audience so our solution was to improve the feature matching so we have a a novel feature matching module so for each Target they uses attention values as appr proxy to matching candidates so that was perhaps our first our first foray into Transformers uh for each Target pixel on your target image over there on the left you discretize the viewing Ray across the different depth values and you project those 3D points onto the context frame to generate a series of matching candidates to produce a cost volume in other words and features from this target pixel image features are cross attended with each one of those potential candidates followed by self attention uh between the candidates themselves we do this several times and the resulting attention uh values are used to select the most probable match and that's how you're going to do the the correlation that leads you to the depth that we're looking for here's a very interesting plot kind of hard to explain but I'll try uh so what you're seeing here is a cost volume between two frames containing the matching probabilities on the right the metric that we're seeing is uh image similarity ssim and on the right uh you have our feature matching module as you can see our probabilities are much sharper with picks only around the actual True Value while the ssim has a very spread out distribution you can see that on the bottom plots the blue lines show our SS uh shows the ssim probabilities the blue lines the red lines show our matching probabilities and the dots show the actual ground truth value uh visually it's very hard to see where the blue lines speak but it's very very clear from our matching module so that's why on the top you can actually see with with their own eyes the predicted Point cloud from the cost volume itself while for while the ssim cost volume is very blurry and you need some heavy Pro processing in order to extract something useful from that right putting all of this together we able to improve upon single frame s provis depth by leveraging Mook frames at test time as well even in Dynamic and outdoor scams uh it's worth noting that we're still learning the poses jointly so the epipolar reprojection to produce the matching candidates is a free variable that is being optimized alongside everything else so this still uh a joint learning problem so now remember when I said that we had started from the hardest problem which was learning from unlabel videos turns out that was not completely true since there is a hidden label that we've been relying on this whole time which is the camera model now without it you cannot unpro your death map to 3D Point clouds and you cannot project the war Point cloud back back to the context image to produce the synthesiz uh image for the photometric loss so if you truly want to train unlabeled data from any Source you should be able to use data sets where this information is not available so YouTube videos or whatever Source you want to you care about and you should be able to and I think more importantly you should be able to use data set that are not pingold data sets that had distorted and all sorts of strange geometries because that's data that's available should be used in principle so here our solution was to predict a ray surface alongside with a depth estimat so we are learning depths pose and Ray surfaces which are essentially pixelwise viewing Rays uh pointing from the center of the camera to the direction where the 3D Point actually is in space so the UN projection operations trivial just multiply your predicted viewing rate with the predicted depth but the UN projection operation is not so trivial because now there is no way to know where each 3D point is going to fall in your image plane for the grid sampling to address this we propose a coign similarity base projection operation that compares the vector generated by uh a pinhole projection to all viewing Rays uh in a patch around that point so we do patches for efficiency purposes here and the closest Vector according to this cosign similarity will be the one most likely to be pointing to that 3D points so that's where the projected point will fall on the image plane and then you can do the grid sampling then you can pay the loss and everything works out just fine as you can see this is a very very overparameterized uh solution but even so it managed to train depth e motion and Ray surfaces joint through with very different camera geometry so here are some examples of pric Ray surface we using p hole and Coptic models only directional models uh the modeling the training protocol is exactly the same just providing different data and that leads to very different camera geometries while still leading to Accurate uh depth and ego motion regardless here are some examples of our uh 3D uh 3D Point clouds reconstructed in very distorted settings uh they were on the right actually huge fan of this video it shows that our model Works underwater which is a very distorted domain uh even worse this Distortion uh changes at a frame to- frame basis since the water is constantly flowing so everything changes all the time I don't think any sing any single camera model will be able to handle this data set long term because the calibration parameters change every time but because we produce uh V we produce V rays and surface uh Ray surfaces for every frame it's a frame specific calibration Tool uh we can successfully uh process this data set along the way and still produce U accurate depth and eom motion estimate so this is accumulated over time and you see that things really fall nicely throughout the entire run uh sure our near Ray surface approach worked and gave us some amazing results but there were still some several drawbacks uh INF was fast but training was quite slow because the projection operation had no close form solution so we had to calculate this whole similarity time which is quite demanding and the over parameterization also L to some instability in our training so we set up to find a more practical solution and here on this paper uh from micro 2022 we show that the the UniFi camera model can also work as a very simple alternative to a wide range of camera geometries so it had a close Farm solution and could could approximate any Central Camera with a single additional Alpha parameter it works by first projecting the 3D Point onto a sphere ofet of set by Alpha and then projecting this onto the image plane using an naturual pinhole model so it's a pinhole plus Alpha that gives you this wide range of camera geometries so by using this camera model we are able to train on all sorts of non pinhole data sets and even across multiple data sets with independent camera geometries by learning a different model for each and still sharing the depth and po networks so we just switch the the camera model and we're still learning One camera model for each data sets so uh and also it serves as a self- calibration tool because our goal is not to produce pixel level race surfaces now we are actually producing a compact camera model that can be deployed or compared to traditional Target based gos we actually showed that our methods um was able to achieve sub pixel accuracy compared to these tools uh just by training your videos so no need to go around with your calibration checker board and and calibrator camera every few hours or so and again these are some some in the wild uh results from a cyclist with a GoPro camera and from a robot navigating supermarkets on the second one you can see the poses are actually very accurate as well and the point clouds as well they still produce like straight walls even though they input images very distorted so we are bypassing the Distortion and producing real 3D Point clouds that match um in space so now that we got intrinsics out of the way the next Target we set up to solve was extrinsics uh up to this point you've been using only temporal context for self supervision as usual we have video but if you have a data set with multiple cameras with some overlap between those cameras it's also possible to self superise use a spatial context by taking two cameras from the same time step and warp information between them so that's called the generalized stereo case because you have cameras that could be anywhere but you know the relative transformation so you can warp information and even better as I sh this paper you can use different cameras from different time steps in what we call spatial temp power contexts um as we show here this approach leads to pairs of frames with more shared information since now we can combine frames across space and time to increase the amount of overlap you can see that on the right image over there temporal context has a lot of overlap as it should have the the camera is not moving that much uh Spa spatial temporal has way less overlap because mostly focus on the left and right portions of the image and with spatial temporal Contex you can pretty much double the amount of overlap that you have and also increases the amount of information that you have for uh standard tricks such as Auto masking and minimum reprojected error that's why the photomatic was actually Deemer there because you have less errors that you can so you can use to produce a better proxy for your self-supervision we're still producing a single ego motion estimation since the camera rig is assumed to be static and because we know the extrinsics uh we can propagate estimate across all the other cameras so everything is is moving jointly and even better because the exchanges are metric we can propagate this to depth network and to the post networks doing training so everything is still scale aware so that's similar to the weak velocity supervision but uh less restrictive because now you don't need a constant measurements for velocity you can just you just need a pair of cross calibrated cameras on the left those are depth maps from all six scammers of the ads and you can see how well they overlap without any sort of post-processing which makes sense because now they have one single scale that is share across all cameras The Matrix scale rather than each camera converting to its own uh arbitrary scale as I'm showing you the top right there so there is no way they're going to convert to the same thing because they are not constrained by anything the extrinsics constraints then uh there are many follow-ups building on top of our full surround monod depth and our own follow-up work presented iros last year we focus on removing the need for calibrated extrem as well so trying to remove that constraint again we propos a a multistage training pipeline starting with with the standard temporal monod deps model that is shared across all cameras but each one is treated individually we then freeze this model and and use it to estimate cross camera rotation and then the full extrinsics and then we optimize everything jointly to further fine tun results uh plus if you still want metric estimates you can inject velocity supervision as before or provide a true extrinsics for any single camera pair so the MC information is available to flow to all the different predictions that we are we care about here so we can still now have several ways of providing metric for our model here are some results compared to co map uh in Easy sequences where the environment is static with reasonable motion uh we are comparable with it in the that sequences and KY sequences however in more challenging cases with lots of dynamic objects or very high and low speeds we exhibit a lot fewer failure cases and we outperform combat by a large margin overall and you can see the cameras being optimized on the on the video to the right over there in another spiritual follow up to this work uh that just hit archive a couple of weeks ago we look to self-supervised depth and eom motion estimation as a source of geometric priors for a bundle adjustment uh we focus on dlam and now as our Baseline and investigated some of its failure cases uh showing that they are very similar to what we observed in our previous paper so Dynamic environments sequence has a very high or low speeds so that's wasn't Droid Droid slam and other slam based methods were not really working well with that setting so what we did we kept this L model Frozen and you train a self-supervised module aiming at refining the optical flow that were being used as input to the bundle adjustment procedure so we're improving the optical Flow by using the same procedure that we've been using for self-supervised dep and motion so that's a different way of uh optimizing the same problem and turns out this was enough to greatly reduce failure cases and improve performance overall without ever having to find this L model itself just by improving What It Takes as inputs by leveraging svision again and now speaking of optical flow one issue with Sal provision that I mention a lot is its inability to deal with Dynamic objects uh makes sense if you only have depth and E motion predictions there is no way a 3D points will move in time before it's reprojected onto the context frame so it doesn't account for external motion and a straightforward solution would be to estimate sin flow as well so there is one additional 3D Vector that moves each point in 3D before it's reprojected but uh self supervising depth and see flow jointly in the monocular setting is an even more upos problem because now there are infinite combinations for any points to fall anywhere and there is no way to constrain this problem so in this paper we explore similar setting as the hour supervised domain adaptation work and now we can use self-supervised depth estimation to transfer information across domains one synthetic with full supervision and one unlabeled where only depth self supervision is available uh following rafts we started by estimating features from each image and building a correlation pyramids that allows us to match pixels across frames and we sequentially estimated the optical Flow by quaring from this correlation pyramids at each iteration we use the previous estimate to work pixels across images just by adding the optical flow in this case and find a closest match around that area once you have your final Optical flow estimates you can triangulate it to produce an initial depth map which will have a lot of failures just like I said in our multif framework because it doesn't take to account any Dynamic objects and doesn't take account any of the standard failure case of self supervision but that's good enough as a starting point and now we encod this depth map and combine with different image features and start a second rounds of interative refinements but this time the working doesn't happen uh with Optical flow it's done with depth and SC flow we un project the estimated depth we add the estimat inflow and project back to find a closest match around that area uh and a key Point here is that we start from the estimated Optical flow from the previous rounds and keep refining just now with a different warping procedure so we don't lose what was done in the optical for Rounds just keep refining it but now adding these additional tasks that we're trying to learn uh this approach combined with uh s real and supervised do adaptation enables us to as far as I know be the first ones to successfully self-supervised depth and sin flow jointly in a monocular setting and kind of get rid of the uh inability to deal with Dynamic objects so now off on a little tangent this is a very interesting site projects that we explore along the way in all of our monoc depth estimation applications so far we focus on outdoor scenes or indoor room level environments and all this paper focus on a very de very different setting what if you what what if you only have an image from inside inside a tectile sensor with a range of just a few millimeters uh these sensors they aim to estimate physical properties of such as Force pressure object shapes based on the deformation gener ated when they touch something in this case we had a range sensor inside the bubble to detect this deformation and the goal is to replace the range sensor with a monocular depth network if it works we would be able to remove the range sensor and bring down overall costs and dimensions which is quite appealing for us and turns out it worked quite well in this setting here are some examples of input images ground truth depth maps and predicted depth maps as you can see the images are not very expressive it's only a gray scale camera pointing at a DOT pattern but even so you're able to accurately um approximate the ren sensor and remove the need for a whole additional sensor in the tactile bubble that we're using for our robots internally and I think we have a VI papers on that as well and on the right you can see the profile created by the point clouds the estimated Point clouds and the Align object there for uh shape and pose estimation so that's another success of monob setting that I don't think anybody had explored before so and finally getting to more recent times in this new line of work that we've been exploring over the past couple of years the goal should look into implicit representations for depth estimation uh so far I talked about explicit representations where one image goes in and a depth map comes out so with a onetoone mapping any sort of getic Prior or constraints will have to be applied at some stage either at the loss level so self provision adaptation and so forth or at the architecture level so cost volumes the people projections and such and the model is struggle if any of those are not true they do not hold in the representation that we're exploring here the goal is to learn implicit geometry without any baked in assumptions so we provide the geometric information at an input level combined with image information using this case a perceiv iio style architecture this information is cross attended with a learn latent representation and further process with self attention to produce a condition latent representation uh what this means that it combines the global information from the latent representation and that is Shar across all data points and stores the priors that we're trying to learn with the local information from each specific scene that gives context to This Global prior uh when this is done you can decodes from this condition uh representation using only geomatic information that tells you from which viewpoints we want to extract ract predictions in this case depth maps this is not volumetric this is a single query that def that gives you the pixel prediction so it's a depth field Network that's why I call it Define depth field network uh another important contribution of this paper was uh some data argumentation techniques designed to essentially make the problem harder but still geometrically consistent so the network has to learn the proper geometric priors instead of just overfitting and finding short cuts that don't lead to any meaningful prior being learned which happens a lot with Transformer um architectures so those two are virtual camera augmentation that creates new viewpoints so we have this point Cloud available from the available viewpoints we can project those to novel camera novel cameras uh located anywhere in the scene and creates uh sparse depth maps and sparse images correspond to that location and and the they they will be sparse but we don't really care about that because we only care about a geometry doing the coding so we can just throw away any invalid pixels second one is canonic to approximate equivariance in our learn in our lat representation so if the entire setup is translated or if it's rotated the resulting prediction should be the same so back then we couldn't enforce that in any way other than that data level and that actually help results with quite a lot and our results other than state-ofthe-art uh performance in generalized stereo and video depth estimation we have now this geometrically consistent latent space that we can be decode from any viewpoints including from outside of the encoder cameras as I'm showing here so the red camera is virtual and can be moved around this is real time by the way so it's actually a very fast network and on the bottom left you can see the available Point clouds explicitly projected to that new viewpoints and the render depth map now does a very good job both interpolating and extrapolating from the available use so this is all du to the geometric prior that are learned and can be transferred to scene specific uh tests so not going to get into details this paper still hasn't hit archive yet so I cannot really talk that much but we've been doing some followup work to extend the perceiv architecture to be actually equivariant so we're not approximating anymore with dat augmentation we are enforcing by Des L that it should be equivalent uh in Broad terms we use spherical harmonics to encod camera information and go from this equivalent embeddings to an invariant latent space and from there you can use standard decoders to produce exactly the same outputs when translating and rotating the entire visual setup as well as switching between canonical cameras in your views so hopefully anytime now you'll be able to read more about it so how I'm doing your time okay that's should be fine question yes yes I'm almost done so in this official uh followup to Define so already published and everything uh we expore ways to remove the need for depth supervision and come back to the self provide setting and what we came up with was the joint training of multiple solutions to the same problem so one volumetric are radiance Fields there with sample and composite and one single query with specific depth and light Fields so they're all decoded from the same representation using different heads and that's why the name deyra so depth lights and Radiance Fields uh there are two main contributions here the first one we use multiframe self-supervision in addition to the standard single frame n synthesis loss because you have multiple images it seem like a waste to not leverage this multiv view consistency and that's something most Nerf papers weren't doing by back then I don't think any was was doing that back then and this was quite use useful as a way to to regular regularize your learn geometry in the F view setting so addressing the shape Radiance ambiguity problem so now Earth doesn't have a way to cheat and produce very high quality images without a proper underlying geometry and the second one was ways to synergize between those two representations so volet r renderings are used to produce virtual views for single query data augmentation like we did for Define but now without the need for supervised apps we have this implicit um structure that we can decode produce the ground to that the they augmented depth maps and use that to supervise your single query and at the same time we can use the single query depth the code single query depth to improve important sampling for voltic rendering so we can refine it by only quering from point clouds around these initial estimates putting this contributions together we are able to achieve stere Arts few view no synthesis outperforming other methods that rely on sparse depth supervision and even prior on pre-train networks so this has no additional networks no additional priors everything is learned from three four five images from a scene and that gives you pretty useful 3D reconstructions and yeah and you also W improve improved volet rendering because now we can qu we can sample way fewer uh points because we have this initial estimate from the single query that head so the previous Works focus on multiv view uh implicit multiv view geometry and now here on this third work uh we go back to the monocular setting and explore another very important geometric prior which is scale priors as we all know by now no estimation is z pose because there is no way to in first scale from a single image so if you're supervising you are going you can introduce metric by via your label dat that you use to to train and you explore several other ways so velocity extrinsic Sy real Etc but even if you get metric depth for data sets uh it will most likely not transfer to other data sets because the camera model is different which change the underlying projective geometry so there is not just an appearance domain Gap in this case there is also geometric domain Gap that we need to over come before we can truly zero shot transfer metric depth across domains so in this work we propose to use our input level geometric embeddings to introduce information about the camera model to the network in addition to the image information and let you learn global scale priors and those can be zero shot transfer across data sets uh and that's why I call it zero depth that was our first Network that focus on zero shot metric depth estimation as you can see here the pipeline is very similar to what we had before with one main difference the variational space is the the latent space is variational that means that it's story an actual distribution uh each weight is broke into two values one for mean and one for standard deviation and once it's conditioned you can sample from this uh distribution and decode from this sample Laten space generating a depth map or any other task that you might care about uh these allow usch St the out uh results on zero shot metric depth estimation or performing other methods that start to pop up at the same time uh here are some reconstructed Point clouds alongside the ground truth with the little points uh Heights colors so we can visually observe scale accuracy and another very useful aspect here is that we can use this to Bas the uncertainty of your point clouds of your depth maps in your point clouds we just sample a few times from the same condition latent space and calculate the mean the standard deviation and turns out that it actually approximates the uncertainty you would expect to get from a depth map so things around object Borders or far away are more uncertain and you can use this uncertainty to filter out your point clouds so just focusing on the points that are more certain and remove the ones that are less certain uh on the bottom you can see the impacts of doing this filtering approach uh by keeping only the 50% % lower uncertainty points you can bring our MSC down by more than half so if you care more about about if you care more about quality than quantity this is a very useful way to sparsify your point clouds and improving quality as we're doing that so you can Target the more useless pixels and end up with a smaller more compact Point Cloud that is even more accurate than the one started with as I said before we were state-ofthe-art for a while but somehow this whole zero shot metric depth estimation topic exploded over the past year as Mato was saying and there were lots of papers papers focusing on the same tasks so we've been keeping up with them as well and again I won't be able to get into details because this paper is not in archive yet sadly but uh we have improved over zero depths by quite a lot uh and I think we're still stateof thee art given the depth and things that are coming up even though definitely is not really metric but uh the idea is to come with a diffusion model that can be properly trained on sparse data which is something most people are running away from let's train synthetic data let's avoid sparse reward data we are going to another way and how can we properly leverage all this untapped potential that are the available Spar St Maps w't get into details hopefully soon you'll be able to hear more about it and now for my last verse uh I'll talk about one of our papers that we present at CB PR uh I think on Thursday a poster is not exactly monoc dep estimation but the goal here is to generate range images that follow realistic glider patterns so to achieve that we propose a novel Laten out encoder that is designed to preserve lighter properties during the encoding decoding stages in terms of patterns geometry and object shapes our main goal is unconditional lider generation so after training on specific data sets you can generate Point clouds that approximates the lighter patterns found in that data set we also explore conditional lighter generation a little bit by providing different types of condition information uh we experimented with images which kind of makes it similar to monod deps I don't know semantic Maps Bing boxes and even texts and have some very promising results showing that all of those indeed work so this is an example of semantic map conditioning and if you're curious uh I want to know more you can please stop by our poster and thus my old monod dep uh comes to a conclusion at least for now I'm sure there will be more verses some are coming up some are being written as we speak as I was saying just now and hopefully in a couple of years I'll be able to come back with a new chapter or true uh sorry there were no deep Dives and in depth discussions I think now they go the go here is talk more more about the journey not the destination and I think uh some people say that that's what matters the most uh so have Tak questions now and I'll be around so please reach out to me if you want to know more about any specific topics thank you for a couple of questions sure I'm curious about mon from the perspective of speed and efficiency Hardware is that a consideration at all for yes not for any of those papers but we are currently exploring several transfer efforts towards bringing that to actual vehicles or robots and um that becomes a huge problem that kind of invalidates a lot of the new architectures and a lot of the latest models but we have had some success with like tensor RT Jets and all the other tools that actually speed up of networks by quite a lot surprising amount so we can get realtime performance with a Transformer Network even maybe not a full on diffusion one billion parameters but yeah that's a huge that's a huge Topic in itself how we can improve performance in these axis as well imagine you use these models as like a teacher model for yeah yes yeah usually we have these two thrusts one is the offline learning that produces the sudo labels and you have the online learning which are the actual distill networks that we apply but yeah this is a this is a good idea um you got a question about the self supervised camera calibration so you mentioned that you do you mention that you calculate the calibration parameters on the for the first paper yes for the new array surfaces yes the second one is per data sets yeah we are learning one jointly across all the data all the images from that data sets but we can switch to other data sets and learn a different model for that other data set cool you train with li but when doing inference you do need to use Li is that yeah that's when you are supervising not self-sup supervising then you use ligher as ground truth and you can train your network in that in that case so yeah and you can do both if you do the semisupervised routes you have one portion of your training loss is self supervised the other one is supervised and you combine those two question the U color information does that play a role in in training it all uh would work with the mon camera we were surprised they work that well so usually when you are training those networks you do some sort of uh color jittering so you increase robustness to changing brightness and things like this but I would imagine Network cares about about color because that's all it learns from an image so you're surprised that it works in such low expressivity setting but yeah thank you we haven't done any specific study uh about what it learns but I imagine that that this is the only way that is going to be able to do this useful transfer across across domains because now the the difference from what it learns in the traditional aining variant or supervised s model is that those only care about the image properties so they are constrained by the camera model that you're using so if you have a wide wide F View Camera the car will be more be larger but the car is not larger the image makes the car larger so if you provide image plus geometry you are learning 3D priors so the car is the same regardless of how it's observed so I I imagine that it's learning some sort of object shape priors a car is this size and a chair is this size I'm imagining that it won't work then we need like a larger data set and more diverse to be able to go in that direction yes yeah that's always possible that's also yeah that's very useful actually the the zero depth was it trained on a specific kind of data set or you train so we train on a combination of indoor data sets and outdoor data sets so that was already back then back then so I think our main competitor at that time was Zep that kind of has two different heads one for indoor domains one for outdoor domains so it didn't need camera geometry at all and I really don't see how that works but they are still kind of bridging there there's still breaking those into two problems one with a different depth range this one doesn't really have that we just combine everything we need the camera intrinsics as one additional source of inputs but it's one model for everything and we try on tabletop settings we tried on uh know outdoor scenes or people walking around that it works quite well in all those settings but this is still a very small data set we are currently scaled up by quite a lot especially for the new paper so the as is it is Rel like landscape close to yes yes so what happens if you it's it works just the same so when you're training you actually resize the image by quite a lot as one additional dat augmentation so it could be landscape could be portraits could be any sort of we do a lot of gravity that like direction of gravity like portra me like landscape is like yeah you take a landscape but then not like that's true I don't know how it works if you provide an image rotate by 90 degrees but if you provide an image that is on the right Axis but it's portraits that of landscape it should work just fine how about we break uh because there's a like 15 minute break but witer is around uh yeah we can have follow off questions around for next 10 15 minutes so thank you so much thank you hey Ru can you hear me I think they might have headed for the break so I think yeah yeah tell maybe we can have the break until 3:40 because we are a little later so we can have the break a little longer ah right uh yeah I guess if we can get in touch with him we can check I guess it depends on what's going on in person because I think we're aligned it so that there's the cvpr break as well yeah yeah yeah so yeah we'll see I'm just going to pop away for a few minutes while The brak's Happening e e e e e e e e e e e e e e e e e e e e M can you hear me yes can you hear me yeah are you ready yeah start in one minute okay I will share the screen in we e e okay let me know when to start e e all right um so we're going to start with the second half of our Workshop so Matthew FY he's going to share uh more about our monocular depth estimation Challenge and then we'll have the challenge winners uh come and present in uh in the workshop so Matthew you want to get started okay thank you R for the introduction so in the next 15 to 20 minutes we will talk about this third edition of the monocular depth estimation Challenge and we will start by briefly recalling why it is important to have a good Benchmark for molecular depth estimation and in general for any task in computer vision we will uh take a look at the main components that are important for having a reliable and a solid Benchmark then we will introduce the actual monocular depth estimation challenge bench Mark and we will look at the results for this year edition with a very quick overview of all of the participants and the main trends that have been followed by any of the teams and finally we will conclude with some take home messages that we can take from this year edition so before we start it is worth mentioning that in the past years in particular for the self-supervised literature that we uh discussed before both me and vtor uh let's say we had some issues with the initial uh the initial way we had for benchmarking the results indeed uh most of the evaluation were um uh limited to a single data set which was the kitty data set and the evaluation protocol itself was affected by several failure points and among them we will U appreciate in the next minute how the ground tro that was used most of the time was actually often not entirely correct and we will also see how many of the metrics that are common in the papers and are still used very often nowadays are sometimes uninformative or saturated by the kitty Benchmark itself as well as we will alll light also some um not correct practices like r ablation studies or hyper parameters tuning on the test set as well as uh comparing with very different methodologies when actually the methodologies themselves are not fairly comparable so we will start by having a look at the ground troot and of course uh this is why we why we started from the ground troot because actually the ground troot is the core of a benchmark and if we don't have an accurate ground troot as a consequence the entire evaluation we will carry out will be of course not correct and uh at the very beginning of the self-supervised literature the kitty the row Kitty data set was used as proposed by the hen paper uh as the standard Benchmark for evaluating single image depth estimation and the so-called ground TR depth for this data set was obtained by means of the row ligher scans made available by the data set itself however these scans uh despite being quite accurate to some extent they expose some critical weaknesses and we can resume them into two main uh pain points the first one is that the Viewpoint at which the ligher collects collects the depth labels is not perfectly aligned with the image Viewpoint this means that the lighter scans are projected over the camera plane and this produces some occlusions that we will look a little more in detail in a while further more the lighter scans as you can appreciate here are quite sparse so our ground rout is indeed sparse and this will produce some very inaccurate boundaries at the objects so these very first issues often make devaluation on the kitty Benchmark to some extent stand quite biased now I'm going to zoom a little more into this this is a detail from the previous lighter map and probably you can figure out this is a car parked on the side of a road and these small points here being both yellow and blue are basically rder lighter measurement from the background and the foreground that are mixed up together and this happens when you reproject the lighter points over the camera plane and this effect is actually caused by the occlusion of the car over the background indeed since these ligher scans are indeed sparse the the over the final result will be having them merged together uh in between the foreground and the background of course your monocular depth Network by seeing a car will predict some kind of consistent depth for the car but when you evaluate it you will have also these outliers coming for the background that will give you an higher error this even despite if your uh prediction is actually correct this is another example for the left side of the road and as you can also see uh in addition to the occlusion we don't have clear boundaries between the car and the background so our accuracy the the accuracy of our prediction uh and how good our model is at dividing the foreground from the background can not actually be properly assessed on this kind of sparse labels and for these reasons uh a good Benchmark should take into account better ground tro annotations and for this purpose Jame and this team for the previous edition of the monocular depth estimation challenge used the supto York natural scene data set also known as Sin which consist of several ligher scans being collected into panoramas and being collected in a very dense manner indeed here we can notice how for a single panoramic image we can obtain very dense uh gral depth labels and uh these these labels are obtained mostly in outdoor environments and are very very dense however there are a few pain points even for this kind of Benchmark indeed to collect that Danse annotations we need to scan the scenes multiple times and this means that actually uh if we have some moving object appearing while we are scanning of course we will have some inconsistent depth being accumulated over time nonetheless uh Jame and his team sampled some portions of this data set and generated the so-called sin patches data set and they simply took the panoramic images and they sampled every 20 at the high level to obtain some patches like this so here we have an example from a forest and we can appreciate how The annotation for the trees are very very accurate and they are far beyond what we could get from the kitty data set on the right we also have an example for a scene uh in which we had a moving object indeed we can see uh the the color image on the top and at the bottom we can see some kind of silhouette that actually not corres respond to a car so this is what I meant before for moving object coming into the scene while the scanning is taking place and for this kind of scenes of course manual filtering was uh of course necessary necessary for removing the errors in the annotations and this is a zoom into the detail I say I just say before and uh from this data set together with extracting color images and Danse ground through dep Maps it was also possible to obtain some Edge boundaries for the objects in the scene and this is particularly important for assessing the accuracy of the monocular depth estimator in particular at the de at the depth discontinuities so uh so far we have talked about the ground troot which is of course uh the very basic of any Benchmark but also the metrics are very very important indeed for having an accurate evaluation of any of the methods that we are going to to Benchmark on our data set we need to also have meaningful measure uh meaningful metrics and for what concerns the kitty data set and the standard protocol used by many papers Through The Years uh of course we had some we had some issues also with the metrics indeed some of them were not entirely meaningful some of them were also not entirely correct for for instance for the squared relative difference we were missing some kind of square term at the derminator and more importantly some of these metrics despite being informative at the very beginning of the research on this field nowadays are quite saturated so they are no longer meaningful of the differences between the state-of-the-art approaching coming out year by year so for this purpose uh Jame and his team uh went proposing The Benchmark for the molecular depth estimation challenge they faved only those uh metrics that are more meaningful and more importantly also more interpret interpretable like for instance the mean average error the root mean square error and so on and so on and so forth furthermore uh another Legacy from the kitty Benchmark is that uh most of the evaluation were conducted directly on the dep map that means that basically the error was computed with respect to the single Pixel depth rather than Computing some errors on the the overall 3D reconstruction and since estimating the depth is instrumental for actually 3D applications probably estimating some more meaningful for more meaningful metrics on the 3D space might be a better choice and indeed oral almost concurrently with the first edition of the monocular depth estimation challenged tried to revise the standard protocol and proposed to use some point cloud based reconstruction metrics in particular they were used using the F score and the interception of a union union based on the Precision and recall computed with a 10 cm accuracy threshold and following this strategy uh J and this team also uh decided to uh evaluate the accuracy of the estimated dep at the object boundaries uh by using the depth boundaries extracted from the dense ground Trot labels and in particular following the high BMS uh Benchmark they propos to use the Cher distance between the predicted and the ground tro boundaries this way uh they could have a good measure of how how precise was the division between for instance the foreground objects and the background so uh after talking about both the ground truth and the metric now let's uh introduce the monoc depth estimation challenge which is built over these two main components so uh this challenge takes place over the SC patches data set by selecting some only a subset of the most informative metrics and together with the challenge Jame and his team also implemented uh an entire framework allowing for a fair comparison between all of the existing approaches from the literature as well as any newcoming state-of-the-art approach and one key property of this Benchmark is that the evaluation is completely centralized indeed uh the The Benchmark provides a validation and test uh split and for both of them uh the ground tro is completely hidden to the developers these ways the developer this way the developers are forced to submit to the online Benchmark and they have access at all to this kind of information so it is not possible to cheat it is not possible to uh fine tune the model or to tune the hyper parameters of your method and these are some examples from the in patches data set here we can appreciate the verifying details in the ground tro Maps as well as the depth boundaries extracted from the ground tro dep themselves and we can also notice the mixture of different environments that are uh that character ciz this kind of Benchmark being both a mixture of indoor and outdoor environments and uh with the development kit Jame was also very active at improving the existing baselines and in particular he implemented up to 16 state-of-the-art algorithms in 2022 and he also revised most of these uh uh Frameworks with some improved design strategies that of course out through the years and just to give an example the very Baseline of this framework is the very first self-supervised paper I introduced a couple of hours ago that is the one proposed by gar atal and Jame basically just revised the common backbone used by gar and slightly improved the supervision tricks used to train the model so this basine represents the very bottom point from which we start the evaluation of this challenge so now let's give a look at the results and uh also draw some conclusions about the outcome of this third edition of the mon depth estimation challenge this year we got an explosion of the participation to our Challenge and in particular we had up to 20 teams participating into the final development in into the final testing stage among these 20 teams only 10 of them uh actually submitted a description for their method and they are reported in this slide and among these 10 methods we will figure out in a few minutes that most of them share uh one very same approach that we have already introduced several times today and indeed here I'm listing the 10 methods and I might like think in blue one common point that has been exploited by most of the approaches indeed six out of 10 basically started from the depth aning model provided by the the original authors and they extended the depth aning model with some different techniques in particular the winning team used some stereo proxy labels so they obtain some uh thisp party maps from stereo images in particular on cityscapes to fine tune the depth anything model and any of the other had just some small variation on this team so the second team used an auxiliary Network and self and used the self-supervision on the kitty data set while the third one used some Metric module for estimating metric depth and they run a a fully supervised fine-tuning on NYU and kitty data set and so on and so forth we would also like nonetheless to uh um highlight that four of the participants also proposed uh new techniques in particular we have the fifth team that deployed a custom model that has been published in parallel to the main conference cvpr so probably you will find out this paper in the next days uh through the main conference but we also add some other additional approaches using some ZN Transformer uh for learn learning outo estimate depth and finally we have a very peculiar approach based on a segmentation model and some statistical models that starting from the segmentation masks and some priors uh learns to extrapolate some depth information uh by using the cityscapes data set as the training data set among these participants we selected four uh invited speakers that will take their talk just after this presentation and to favor the diversity among the participants basically we selected of course the winning team as well as we selected uh the evpp uh plus pass method which achieved very good results then we favored for favoring diversity we invited the team using the EOD depth model and finally we also invited the Elder lab team which used this very particular approach which which is completely different from any of the above approaches and before leaving the word to our invited speakers uh I'm just reporting here the results by The Challenge so picr is the absolute winner of the challenge uh since the main metric over which the winner is selected is actually uh the one on which they achieve the best results and then they are followed by uh three more teams using the pre-train depth en model and this is already suggesting that using this kind of foundational model represents a very strong U prior for facing this Challenge and then so on so forth we go through the bottom and in particular we had the evpp Plus+ method that was very very strong on the depth based metrics at the expenses of uh the point Cloud B once so we decided also to have them as the invited speakers because of their very uh exceptional results on uh the dep Maps accuracy and to draw some conclusions we can notice how the F score achieved by the absolute winner is uh has been has improved by 62% over the original Baseline and already by at 35% with respect to the previous winner of the second edition for what concerns the absolute Rel relative error on the accuracy of the D Maps we can observe nearly a 60% Improvement by evpp Plus+ over the Baseline and still uh more than 27% improvement with respect to of course the winner of the second edition so we can appreciate how the introduction of this uh foundational model such as depth anything allowed for boosting the accuracy on this badge mark by a very large margin and here we are showing some qualitative results about any of the meos participating into the challenge we will focus on the top two sorry on the top three and despite the use of the strong priors by that Ting we can still figure out that we are still missing something of course in the forest we are recovering a little better the details with respect to self-supervised approaches or the supervised approaches from the second edition but we can still figure out some artifacts in particular also both the the first and the second method are showing some grid artifacts that probably came out from the fine tuning they conducted over the dep model and of course we can still figure out that most of the edges at the object boundaries are still far from being very sharp as they are supposed to be here are some additional examples and again we can notice how okay we can figure out a little better uh details but yeah I think there's still a lot of room for further improving the results and this concludes this talk before again leaving the word to our uh invited speakers uh from the among the challenging teams I would like to draw some personal conclusion so yeah into in this year edition we witnessed a a completely uh conquering of the stage by the foundation model in particular by depth ening and we feel that the web scale data set used for training that tening played a crucial role into making it the standard Choice by six out of the 10 participants and we also observed that despite six of them used depth tening we also came out with very different fine tuning strategy so I I would say none of the uh six teams used the the very same fine tuning strategy but we can say that this fine tuning strategy plays a secondary role yet it makes some little difference in the in the final Le leaderboard and to conclude we can figure out as well from the as we have seen from the qualitative results that the scam details and the depth boundaries REM challenging even if we are using some foundational models so that's all for this presentation uh feel feel free to ask any question and of course thanks for your attention right are there any questions regarding the uh challenge okay I think we can move on to our present presenters okay perfect so according to our schedule the first two presenters should be connected online the first one is mola lauk sorry for my pronunciation okay thank you very much feel free to start uh whenever you're ready okay uh do you hear me well and uh see my screen yeah loud and clear and yeah we see both the screen and the pointer okay good uh so um hello everyone my name is mola lauk and uh um and today I would like to share with you my like competition Journey from the start point and to the um results that I get so uh the Baseline model that I choose it was zeps model because I have the experience working with with it and it is very easy to use and uh provides good results uh so it consists of encoders that is um pretrained on uh a lot of data sets and the specific decoder that uh uh solves the problem of uh um metric depths estimation uh not like a regression but more like a classification uh using the metric beans approach so uh using this Baseline model I uh I overpass the Baseline that was provided by the organ iers and from that point start exploring the alternative models to uh to improve accuracy more uh so what I uh start checking it is the best models on the Kitt uh data set so I selected the self-supervised uh model for uh modp estimation called SQL deps and it was like on the moment I have checked it was like state of the art on the uh K data set um and um and the main idea is uh as was uh uh mentioned on previous presentation so it consists of a depth Network and post Network that jointly uh train together on the uh unlabeled data set and after that when this model uh trained the author fine tune in it on the uh depth M maps from the Ki and um I tried this model uh for the competition but for for my like frustration I found that it has uh lower result comparing to zo adeps and even when I tune this model and uh uh get better results on Kitty than original auor head it is still lower than Zep so I uh moving forward uh the other models that I looked at is it was VPD models that is uh for my knowledge it is the first model who utilized um stable diffusion model to solve the depth estimation and other task like semantic segmentation and so on but the main problem with this model is that uh it is utilizing the text encoder and the author of this model they used uh text templates and for this you need to have a class name so it is uh it could applies for uh NYU data set but it could not apply it for KY or sins data set um that is why it is uh like our Uh custom model called EVP uh so uh what we did we uh we solve and Tackle this problem with this uh uh classes uh with two different like approaches one is to utilize some model for example like blep uh version two to generate the text description from the model and after that just use this uh uh text description to put it to text encoder and um and make this pipeline working uh also introduce additional model uh based on the multi attention uh refinement refinement of the features to boost the accuracy and alternative uh approach is to use directly without text encoder and without any text just directly use clip encoder to get uh embeddings from the input image and this embeddings uh thanks to clip are very close to embeddings of the text and uh using this it could produce um the results for the depths Maps uh very good results and without need to have some class or some text on the data set that is why it called like EVP Plus+ so with this um model I have get like much better result and um and uh still looking for new model so after that I found like dep any so I will not be uh stopping here for long time it is the same code base as zeps and uh they change only the encoder from bait to V encoder that Ono V2 and utilized a lot of data for training uh and this model uh provides me like a better result comparing to EVP Plus+ but the difference was not very big but uh if but when it is but uh in general it is better so I decided to stop on this model and after that I was uh uh trying to do local detail enhancement and try to to investigate uh some approaches for this so I started uh working on boosting depths it is a model for uh for enhancement of the details like edges you use uh like deps model for whole image and after that use your deps model using slide sliding window technique and after that using the uh gun model you merge the results together but um this model did not provide me better results so the results are pretty looking but not uh improving the metrix after that I tried like state of the AR patch Fusion model that uh is uh new one and um it uh do in the similar way but like uh different models uh for course and uh Fine Results and after that like Fusion model for combining them uh it is it provides much better results comparing to the previous one boosting deps but the result was still lower than EV Plus+ without it and depth anything without it so uh my final solution that I get the best result was um uh use deps and with with encoder and zeps Metric beans and fine tune it on uh indoor and outdoor data set so it is provided the list of them with standard loss and augmentation the best training option that I could found is provided and the other tricks that I use I used just sliding window so without any Fusion uh model Neal Network I found that it is improving the result from um dep much better than comparing to to any like previously mentioned methodology also I turn off the TTA that is U by default turning on in depth anything because with TTA it and the same for the uh model soup and I aggregate like several different models I get much better not much but some somehow better accuracies for like rmsa or like other but not for F1 score so I decided to turn it off and also turn off the pting of the input image so what did not work for me is as I mentioned like some models that are much higher accurac it has much higher accuracies on Kitty comparing to anys or to even VP but they are failed here I think it is main reason is due to uh transfer of domain and this two models like EVP and DEP any they have pre-trained on very large amount of data and that that is the main reason also large larger image resolution and ler longer training did not help me also some Advanced augmentation like prop or cut flip from uh specific depths paper uh did not work as I mentioned some dedicated models for Edge or local details enhancement failed uh and I finally used only like sliding window without any other post processing TTA and model also improve like other metrics but not F1 score so I uh uh just rearrange uh on the table and to add the column like total rank to summarize all the places so if you get like here 4 + 3 + 1 1 + 3 + 5 I get like this column and uh shows that this approach is not like perfect in terms of fscore but uh in general if we are like combining the methodology using all like Matrix it is uh in the first place and all also it is the only one who has like more than one uh more than one top one places and uh also all the metrics are at least fifth place and higher yeah according to qualitative results when I compare in like deps and and my results and for example patch Fusion so uh for like visually the best results was for me this patch Fusion you could hear like very good details comparing to these two models and for example here this model found the details like grid uh sell on the windows but both models like failed and it is not found but when you are looking on some other parts like here's the pole but this model did not found it and here you could see like more clear the structure of tree trees and uh so it is not U in every case it is obviously better but in general for like a lot of images it is better but when I comparing the metrix uh it is uh a lot lower comparing to like any deps or EVP Plus+ model yeah so thank you very much and if you are interested I publish the code um a final solution you could found it like here right we may time have have time for one question if anyone has okay then I think we can move to the uh second presenter okay the second presenter should be online as well should be gangu sorry for my pronunciation perfect we see your screen can you hear me yes perfect but we see the presenter mode how change this perfect okay hello everyone I will introduce our work high quality data makes great progress I'm from PMA um first of all we are looking at existing Works um in general there are two men directions relative depths and the metric depth exmination relative depth exmination it need diverse deps data set um then it will use skill shift Environ loss in there are many Works us uh Sim Sim P plan uh the first one is Minas and the Laris then the Z deps and the dep the most latest is merod it use a diffusion based model but this this kind of method have some problems uh they are limited in use for they they have limited use for in reality because it has no physical distance um then it it's metric dep exmination the training and the test on Sim data set uh like the first figure it has po generalization but it perform high quality on 3D Matrix structure um to solve the problems the metrix sending consider the camera parameters into the network TR also the zero depths it input the camera parameters to send to the network um but this type of this kind of method just consider the camera difference it ignore sense difference uh it means uh if we use the same camera we just take the picture of Y uh it's difficult to transfer to the out do sense so the mrix extend work mric s we to use more data set then we will look at our method just like I said before there are two difficulty the first one is the different sense the second one is the different cameras how to solve these two problems is also the key to other mod deps exmination um for the test Benchmark sense P it include indors sense and aor a aor sense include natural Urban and agriculture Etc sense it's very difficult to solve the differ sense together the second one is the camera difference in the open data set the camera parameters is different and is also different from the test test data set s Pates so how to transfer the open data set to the test data set is also key problems our framework is based Z deps but we change the backbone from from Midas to depth anything it's just a little little change the same the rest is the same like dep anything um the backbone we use the B uh three 300 84 large backbone and the head we use dep Bas the metric dep metric head and then we use Pro proximity labels the rtion we keep a bigger also we just keep one 1 24 and 2 48 which is different from input image set in indeed we also do experiment S 3 84 plus the 7,668 but the result is worse than the bigger resolution superation uh indeed we select the the data set we have three steps the first step is quality evaluation the second step is diverse data set selection the last one is necessary data selection for first one the quality evaluation we should consider uh three factors the first one is the accuracy the second one is coverage the last one is density the accuracy it means the labels should access method I will introduce in the in the after okay under the M shift we will use the data selecting because it has many data set uh how toate many data set has a S sim sense so we need to filter the unuseful data set the last one is just used to select necessary data set it's a inser from BL language model we just apply it to the to the image process the second one is the gradient angle loss we design a new modu loss it is very use for for the for planner superation because U it has a big superation around zero traditionally we use gradient gradient loss but it just supervise the gradient of x axis and y axis but the Ang Ang Los it ignore the ignore the skill and the gradient of y axis and gradient of x-axis they will div which will um how eliminate the scale it's so it's useful then we will look at a result um to select a best model we evaluate the result on C on fil benchmarks K we to C skes the and2 we use six data set to select the best data set perhaps um then we find the C SKS has the mean best r one and F score then C also it has more informations because ccps has more density it's it access me is sgm and it coverage is larg than other other data set because City scps have 50 50 cities and in different country the are data set just use less cities and in one country also the numbers have much so which to C SKS then we also do the experiment of mixed data set with that only your C scps have a better result than C scps with T A and C scps with K so we just choose C SCS as the Baseline of our me also we do the experiment of different backbone we use Z dep based backbone is unstable to training and your depth anything based back B it enhance overall performance especially the F score also we do the experiment of loss function we use four loss function C log SSR as option and the great angle loss the C plus SSR is also demonstrated by Metric 3D V2 we also have a same conclusion it is Ed for and option error as used before we will see use for loss it has a strong enhancement in F score so we concentrate more F score but the Delta one is is is worse in the end we our result get the first place and the parameter of great angle loss we set fail have a better result okay thank you and questions all right in the interest of time uh we'll just skip to our next uh presenter okay we're running a little lower so uh if you can keep it up to 10 minutes for us yeah video right do I need to connect it I can just share it on Zoom yeah oh sure you can unmute yourself so it's not visible yep okay so hi everyone thanks for being here um I'm going to present I'm going to present our method called EOD depth which uh which got us the seventh rank on the third monocular depth estimation challenge so um uh this uh we we we represent the vision Lab at I Delhi hence the name Vision itd uh this this is the team comprising of myself uh my co-author surj partney and uh Professor Jan right um so uh this is a quick demonstration of uh of our method so you can see the RGB image on the right hand side of the uh on the right side and the corresponding depth map on the left side so one striking difference between um this R method and the other methods that okay there's some uh I see yeah it's not that uh it's not that smooth uh so one striking difference between our method and the other methods that uh most of all which seem to use depth anything is that our method captures um very fine grained information which is absent in other methods as was highlighted by Matthew in his talk uh right and uh so our method was our method was um published in uh uh cvpr 24 Main track and uh you you you uh welcome to come to our poster session six in um Arc 4 AE right um so yeah so uh quite a lot of methods that we saw today use uh diffusion use a unit as the information extractor and uh we also follow so so so some some examples of such methods are tadp and VPD and the reason why diffusion models are seem to be so popular with dense prediction tasks is that they have been pre-trained on a large number of image text pairs which um which Pro which uh which instill in them a strong prior for uh visual tasks in particular depth prediction tasks such as depth estimation uh so these are uh the tadp and BPD papers which um which inspired our work you can check them out the links are below uh so so the question which comes up uh is what is the best way to condition uh the unit uh in the absence of ground truth text so um so uh so the in in diffusion in in stable diffusion you start with a you start with text and an optional initial image and then um and then you you produce the final image but uh in our case we only have the initial image and not the and we don't have the ground Ruth text so we carried out an ablation on the um different on the different types of embeddings that we can pass into the uh unit so um one one uh one method was to pass the scene label embeddings uh the second one was pass text captions so so this this is similar to what uh uh the uh I think the EVP uh uh EVP participants were doing that they initially although not in their final method so they they were passing the text from the blip to uh passing the image into the blip 2 captioning system and then uh passing the generated text into uh the unit but uh and and I noticed that they later also uh mentioned that they directly wanted to um compute the embeddings and pass it into the uh into the unit so so our method is also similar but we don't use clip we uh we directly pass the image into the vit and use an ml use series of MLPs to U align the align the embeddings to the clip space uh to generate the proposed scene embeddings so one question that you may have is why do you want to align it to the clip space uh well that's because uh the the in the pre-training Paradigm uh the text is passed into the clip uh into the clip model and um and then the embedding is passed into the unit so so so there's a strong prior for having the embedding in the clip space uh yeah so that's why aligning it is important and some things to notice over here is that there is no clip so in in TP you can see um DDP on the left you can see there's a clip uh step and in the um in VPD also uh although it doesn't seem to be shown over here but uh they they have uh they have clip inside it uh but we we directly pass the vi embeddings into uh the unit yeah and Alignment to clip space happens through MLPs as I already mentioned uh there is no intermediate text so uh there's no loss of information uh which happens due to um due to uh passing the image through a captioning system which generates text which are discrete labels and everything happens in the latent space so there is no uh as such loss of information U right and and U as I show showed in the uh video before our model seems to capture very fine TR information so this this might be due to the uh continuous due to the uh uh no intermediate texts being uh used so this is the overall model architecture uh we we use the uh we use the side module which which is which I just showed over here is called the side module uh we pass the uh you pass the image through the side module and to generate the context the condition and the rest of the pipeline is just um is uh is motivated is motivated from uh VPD uh I have attached the source so these are some of the qualitative results uh this is the NYU depth V2 data set this is an indoor data set uh this is the kitty outdoor data set and these are the zero short generalization results so Sun rgbd uh iims one diode hypersim so so U so so some questions u in in our uh GitHub repository came up about uh comparison with depth anything so it seems that um on on uh on tasks like NYU and kitty our model seems to work at par with depth anything but on zero shot generalization depth anything works better uh possibly because of large scale pre-training uh yeah uh so in our case we don't do uh pre-training we just use the stable diffusions pre-trained information and F tune on the uh uh on the nvu and kitty data set separately right and these are the results on the nvu depth video data set Kitty zero short generalization uh I'm skipping over these uh in the interest of time so this is the MDC challenge so as I mentioned earlier uh you can see the fine details captured in the so so uh the First Column is the RGB image the second is the ground truth depth the third is the predicted depth from our method and you can see the like the how how intricate the predicted depth is which does not seem to be the case for depth anything based models so this is the standing our standing We Stand uh at the seventh place but uh as you may see that there is no dagger uh uh against our uh entries where where a dagger stands for pre-trained pre-training using depth anything so I think that that is that makes our model special uh unique so right and thank you so any questions we have one more presenter we have uh is the last presenter there repo because actually for the information I got the fourth presenter should be there in person but I see also one of the participants being online which is oan right e uh I think you're muted or at least we can't hear anyone yeah talking yeah we can't hear you if you are talking in Seattle we can't hear you I think somehow they can't hear us either I've I've mess hello can you hear me now yeah there was a bit of echo for a moment but can you try again yeah whose microphones are on if both of you have it on it might be echoing now we're back to not being able to hear yeah me neither yeah we're just figuring out the echo so can I turn trying to do AIO settings are can you hear me something they can't hear me uh we can hear you but again there's still lot of echo e uh maybe you can speak from your microphone okay we'll try this can you yeah you can go ahead I'll just take the laptop here it should not Echo I think that's better is it sounding okay yeah that's better thank you thank you sorry about that umor okay sorry for the delays okay so um this is really the work of my graduate student of s year unfortunately and um as mentioned some very different approach our goal is now really to to click on the other screen really to onar rather to explore um some ideas and maybe will provoke some new kinds of research and mod dep so I first give a bit of background um so the instigation of this project was partially this um this paper that we worked on couple years ago student um Sou University Anderson very student um and conventionally in human perception it's been thought that um spatial understanding kind of proceeds and forms semantic understanding um so we you know compute kind of spatial gist of a scene and that gives us sort of 3D spatial a and then semantics follow and in this work like phical work we found some interesting results suggesting that actually to a great degree it's the opposite that semantics were informed our perception of space so that's one of the key ideas is to what degree semantics Mon and the Other M idea is an old idea that sure everyone is aware of which is that the ground cus is extremely important and determin perception um so if we have Ober with a horizontal optic axis then there's a relationship between dep on the ground plane terrain image Right image away um and that's a very strong CU this is shown byot right here um so this um turns out to be used um really profoundly by humans this is shown um in the plot on the left here finding the correlation of human judgments in blue with IRS for que and we're also plotting a bunch of mon networks these are some old now showing that they also use this CU this is not a new idea that's been back to Century ground the so we're going to call this um this talk ground Theory talk the question is academic question of to what degree can mon death be reduced to semantics and brood now obviously it cannot be entirely reduced to these things but we think it's useful to try to extract as much out of these two things as we can in in an effort to understand how that works so to do that um we've produced a simple pipeline um that makes some pretty strong assumptions first of all assume that the C is know orely estimated meths we doing that um we'll make the gross approximation that object surfaces are vertical and we'll make an assumption that if you see an object that's in contact with the ground image that it's resting on the ground in the SC these are all things that are okay so I'll quickly walk through method um so we're going to um use not the shell semantic segmentation system in our case image um and um we're going to aggregate a set of classes into a super category of ground classes so then we'll use that super category to segment ground surface from the best soon and this is the end of deep learning in our framework so we really in from that semantic segmentation how much get sort of using of retro computer okay so um the other thing we do is with this mation is use it to practition scenes this turns out to be fa easy with a Max lik M classifier based upon the semantic classes that works very well so now we can consider person SE um so the first goal is to um estimate the depth of the pixels under ground so we can do this using just simple projective geometry um we find it's a bit better to use a learned crial impression profile just reads off that from image so that fills in dep on ground down surface okay so then um the next assumption was that was the contact assumption so that semantic segments that are in touch with the ground segment will inherit um the ground up so it's assume that they're resting floating so that expands the number of pixels which we have t okay there is a problem with objects that are very close to the camera so grounds not visible below them so for these we currently learn class additional expectations of the steps okay now actually um one quite important feature we use um in this method are line segments so we are using a state ofthe art um method for detecting Manhattan and non Manhattan segments um and um there two cases so the the method we're using of um is is a pretty good estimate of the reliability having of course in s there are you know Urban scenes but also scenes of forests and and LS and so forth where you frame is really not reliable so if the M frame is deemed reliable then the algorithm is for the segments if they overlap pixel we have you can use those estimates to scale design you already know the 3D orientation and frame we can use it to scale the lines um for the non lines we can use those Che estimates scale and rotate Lance in uh if the Manhattan frame is unreliable then we'll just use the existing deps to both scale and Orient those lines and you can see at the Bott there that that extends the pixels which we have estim okay so that's pretty much it and then it's a matter of completion so we use a simple class completion so all the pixels of which we have dep estimates sping terms result heat sources um the uh semantic segmentation is important here in um that we use the boundaries of the semantic segments as reflection conditions as well as the frame of the quite important okay so that produces a completion okay so that's very straight forward it's all vintage computer vision aside from the CTIC segmentation so how well does it do okay so here's some qualitative results we're just comparing to Z here um you can see that we call the ground the me is on top it produces something reasonable um definitely you can see less detail of course um because of the simplifying assumptions but of course when we see we always have to be careful when we look at detail depth maps you don't know if the detail is correct but our brain is kind of interpreting um it as as better but you know it's not Neally better um okay here's two more examples um sense the left okay what about quantitatively so um first let's look at the ground surface so we broken it down into outdoor and indoor here and um as I mentioned we tested two methods for estimating the dep of ground pixels just thetive GE method we learned aggression model AGR model looks better we is um we do that the Zoe depth on the ground terrain but we're not 3D on ground this point um we Indo summer story um we are much better than sodia but um comp okay so then what about the whole scene so here um we we te of it we're not competitive s dep but we'll see um to because of the we making the services that are on top of the ground um but still you know I know it depends on if you're last half full person I kind of think this is basically um one thing I did want to stress here is what we found that was interesting was the lines are very helpful so if we if we don't use these line segs our uh ma dumps from 37.6% to 52 1% pretty shocking degradation so suggest that line SS quite useful okay so that's it um so summize um we were surprised that we could get fairly reasonable metric dep maps with Tye of learning uh the ingredients were good semantics ination ground surface estimation of L segments and um semantic and we hope that this will just generate some some and for us useful for better explainability generalization models this could be for example you know starting okay thank you right apologies we're running about 20 minutes late uh so our next speaker uh and our last keynote speaker for the day is Eric um Eric is a staff scientist at NCH uh the Pokemon company uh working on the light sh visual uh positioning system uh he works at the intersection of machine learning and computer vision uh 3D Vision in particular his research uh involves uh uh topics around visual relocalization posst estimation end to end learning uh robust optimization and feature matching uh he publishes his research in top conferences in Vision where he's also an active reviewer with several outstanding reviewer mentions uh he has co-organized several tutorials and workshops on visual Rec relocalization and object POS estimation so welcome Eric thank you so much for I think I sorry you have audio no right is it not recording from this or I'm saying uh yes we can hear you we can we can no no no no no no sorry is you hearing me or not they should be can you have you unmuted yourself I think so can someone check inum whether someone says that they can hear me so we can hear you but I think it was through repo's mic and is this thing can you hear us yeah I can hear you zoom can you say something again I think so should let's assume they can hear me can you hear me as well yeah loud and clear yeah yeah now is this no yeah now now it is okay so um thanks for the introduction my talk is called metric depth for instant AR I'm yeah it's too far no of course yeah that's that's okay yeah now this thing is also working okay now we're set up sorry um I'm Eric I'm a stuff scientist at Niantic you might know Niantic from the games like Pokemon Monster Hunter Pikmin and so on but it's an AR tech company and also the games they combine elements um or they include AR uh and try to combine elements of the real world and the virtual world and games are a good opportunity to do this today but we are generally interested of pushing uh forward what's possible with um to do with ar today um and as such Niantic has a sizable research team and parts of these researchers they work on depth estimation so some of these Works maybe you know monod dep uh Isis v19 single view um depth estimation which was later extended to multiview depth estimation many depth and cvpr 21 and then going to full 3d reconstruction with simple Recon here is this v22 and maybe you can understand the relation to AR because if you if you want to place virtual content in a scene you have to understand the 3D geometry of the scene to um place it in in plausible locations um so for example what you can do is um if I want to place a virtual object I first need to understand where I can place it where is the free space and then I can use depth estimation to render plausible occlusions which is important for immersion so my talk was called instant AR is this instant yes I can do this immediately I can drop in my in my phone I can place something I can render realistic occlusions perfect thank you very much talk is done no um when if you if you watched closely to the paper titles which were on the slides I was not involved in any of these brilliant works unfortunately because I work on something else so when I talk about instant AR what I actually mean is instant shared AR what does that mean well imagine you go out with a friend you take a walk and then maybe you go to a beautiful place like this and then uh your friends said says oh I want to show you something but I want to show it to you in AR so your friend places some content in his view or their View and um now we want to show this content in your view so what do we need to do well we need to uh project this virtual content into your view but obviously you or your friend you're standing in different locations so we have to estimate the relative pose between these two views this of course is a longstanding problem in computer vision it's called visual relocalization usually it consists of two stages you have a mapping stage and you have a relocalization stage mapping stage means you collect many images of the scene where you want to relocalize in and then you build a visual map of this scene when you have done that the mapping is done you can relocalize so given a new view of the same scene you can uh register this new image to the existing map and this is what we want to do right and you can solve this with traditional methods we were able we are able to do this for 20 years now uh to with very good accuracy so for example on your mapping data you can run Co map create a sparse Point cloud with high dimensional descriptors and you can use a feature matching to register a new query image this is what it looks like if you use feature matching so you have your query image first thing you have to do is you you have to Fig so you map many places of the world you first have to figure out which map is relevant to you you can use image retrieval or maybe GPS signal to figure out which map do you want to relocalize to and then you can use feature matching so you extract features you match them across the images and because you have already reconstructed your mapping data the 3D points in you know the 3D uh coordinance of the feature points in the mapping image which gives you 2D 3D correspondences of the query to the map and using ranch and post solver you can estimate the pose this is what we want but this is instant well n not really because if you have a lot of mapping images and you run coal map that takes quite a bit of time or can take quite a bit of time so we can look at maybe other recipes for visual relocalization and see how they would perform so one that I want to talk about in coordinate regression the pipeline looks very similar the only thing that we change is that this feature extraction matching step has been merged into correspondence regression so we start again with a query image this time here's in this example it's an indoor um scene and uh we have a network the network is the has is the scene representation it is an implicit representation of the scene as no explicit 3D Point Cloud but the network has been trained with your mapping data so with mapping images and Crown proof poses and it learns an implicit representation of your scene what the network predicts is a dense image of scene coordinate coordinates it means that for every 2D pixel of the input image it predicts the 3D location in scene space which means the prediction itself is a 2D 3D correspondence field you can apply PNP and ransack or post over and ransack as before which gives you the pose is this instant well h two years ago the answer would have been no no no no no you have to train a network per scene which takes hours or days right in cvpr last year we were able to show that you can do this actually quite quickly you can train a network in a few minutes and uh still achieve very good accuracy this despite the need of to train this network per scene you can just do it very quickly this assumes that you have the mapping poses of your mapping images already sometimes that is the case when you do uh realtime slam on the phone for example but sometimes it's not the case and very recently we were able to show that this Ace Rel localizer that I just showed can also be trained in a self-supervised fashion so you start with a single image and a dep estimate you train the relocalize you use it to lock on more views you add this to your training data of the relocalize and you retrain and you do this in a loop until you have mapped your complete mapping images this is quite exciting because this turns the ace relocal into a learning based sfm model basically an alternative to co map which um does not do any imageo image matching it Direct ly learns to regress image to scen correspondences which has some nice properties for example it scales very well in the number of images you can uh reconstruct 10,000 images in an hour on a single GPU which is nice but is it instant well let's assume we have the mapping uh poses so we can train in a few minutes and maybe you can train it even faster so here you see a mapping run of the A3 localizer in 10 seconds so this is really real time what you see here real time mapping and still we can relocalize with high accuracy 10 second is it instant yes but I would say still I would say no it's not instant it's not what we want but because imagine you are out there with your friend what do you have to do you have to take the mapping images you have to go around with your phone you have to record all the data and you have to take the scan itself which makes it not instant okay so we are back to our two images and back to our problem we go through the literature again and see what else can we do um we traditional computer vision has still something for us we can do Post triangulation so we can do feature matching 2D 2D correspondences we can estimate the essential Matrix decompos it which gives us the relative pose exactly exactly what we want uh unfortunately it's only up to scale so we don't know the distance between the two cameras is this important ah yes it is important because in our example it does not only the the true distance of the cameras it does not only influence the scale of the asset that we want to draw it only it also influences the the position of it so the balloons could be anywhere if you don't handle the scale ambiguity so one way out of it is to assume well if we have a third image we can estimate a second relative pose which gives us two directions of the translation vector and then we can triangulate the pose which resolves the scale ambiguity the problem is that it comes more robust and more accurate depending on how many images you have so the minimum is three it will work in theory but in practice it will be extremely gittle and inaccurate so the more images you have the more practical it becomes but then you're back to well you have to take a scan of the place you have to collect many images so different approach and now learning comes into play we take a scale metric depth estim uh depth estimation model which gives us um a metric depth estimate for our two images if we then do feature matching we don't have 2D 2D correspondences anymore but 3D 3D correspondences and we can estimate the relative pose without the QT right and this actually also works in practice so here you have super clue feature matches between uh the reference and query images here you see the corresponding depth estimates of DPT in this case and if we combine the two we actually can estimate the relative pose between these two images including scale and I want to appreciate this fact we have a single image of a place and we have a seen agnostic learned model which conditioned on the single image induces a scale metric space around it and this allows us to register a different a query image an unseen image to the reference space and that's pretty exciting from a real localization perspective and we were so excited about it that we gave it a name that we rebranded this type of relative post estimation and called it map free relocalization this was a eccv 2022 paper and quite some people were involved in this uh work why did we need so many people well uh I've shown you one example where it works right but we wanted to know how well does it work does it really work in practice so we collected the data set we looked for places where this type of relocalization could work places where a single image is um enough to represent this this place well and we asked users to go out and scan such places for us and they did they scanned a few places for us a few 100 scenes each of them were scanned twice um we ran Co map to reconstruct these images and registered the two scans together everything is in scam metric space because we have IM information from the phones so if nothing of this is interesting to you we have 460 sfm models online and SC metric units maybe that at least is useful to some of you so each scan uh each place was scanned twice by potentially different users which who do different things so here on the left we see a user who um scanned close to the Statue and did a full 360 Circle whereas the user on the right scan from further away we split the data set into 460 scenes for training 30 scenes for testing the test CR proof is private we have an online evaluation service so that you can tune your hyperparameters still we have a validation set of 65 scenes where the ground proof is also available for the training scenes you get a full two scans so you can sample pairs of images across the two scans for test and EV validation since we selected from the first scan one image which is the reference image of this place and the second image uh the second scan you can use to uh um to pick query images the data set is quite diverse um because the locations have been scanned throughout the world and we have some nice statistics so um if you look at the time between scans roughly 25% of scans were taken on the same day maybe by the same user um 25% were taken within the same week 25% within the same month 25% uh within half a year so we have some season changes for example in the data set although these are Urban environments where the season change is not so dramatic uh in practice we have the nice distribution over visual overlap so we have some easy image pairs where the overlap between the two images very high but we also have very difficult image pairs where the overlap is um very low and you see in the diagram there's a Peak at zero overlap so we have seemingly impossible image pairs as well and I will talk about this uh in a bit so why does it happen because if you fix one reference parts of the query scan just don't see the same structure as the reference anymore and then the overlap is zero and in terms of um trans ation and rotation difference between the two images in the data set um here's a graph one way to read this graph is to look for the median so 50% here and I would say 50% or median in Translation difference is like 3 m and in rotation difference 40 I would say so 50% of pairs are have more and 50% of pairs have less and here we see one example where the reference is very close up and the query images are very far away so quite scale change uh to to bridge here okay that's the data set let's see uh some resides I already showed you how we can use super glue and DPT to enable this capability and uh here we see one more example where it actually works so we have the reference image the query uh uh image is here shown in blue and the estimate of super clue and DPT we see it's quite close to the crun roof and it traces out a nice trajectory um so it looks very plausible but this is a very easy scene of this data set they are much more difficult uh cases and some nice challenges challenge number one large VI changes so here we see uh in the query scan that we take a 360 circle around the statue and at some point the feature meure breaks down you don't find correspondence correspondences anymore and the uh dep the post estimate in yellow here spiral is out of control tough Challenge number two scale prediction this time the death estimation model fails because this is pedestrian data back then in 22 we didn't find many metric death estimation models that work on this type of data and in some of these scans there are really just very very subtle cues about what the scale could be in this case here if you look at the yellow um process CIT the orientation is fine the feature measure works but uh the translation Vector is uh the translation is incorrectly scaled because the depth estimation model fails to estimate the scale of this the scene Challenge number three higher level reasoning this is one of my favorite examples of the data set you have a sign we start at the front of the sign okay we go around it as we go around it there's a nasty surprise because the back of the sign has the same text as the front of the sign so any feature matching method will happily latch on to this text it's the same so it will tell you yeah yeah you are the front of the sign but no you are not and uh in order to figure it out you have you can for example look at the background to figure out it cannot be the same side and you have to do some reasoning right cool and last challenge challenge number four we need a reliable confidence what do I mean well let's look at let's look at this scan here we have the reference image and now the query appears and the user of the query image was just exploring the environment so in times they are looking completely apart uh it will be very hard if not impossible for any method to estimate the correct POS here right so should we include these images from the data set I would say no because this of course can happen in practice if we deploy such a capability then um the two users might look completely different directions the worst thing that can happen is that the system just predicts a random pose right the system has to tell the user I cannot do anything sorry you cannot uh cannot estimate anything so the method not only has to predict a pose it also has to predict a confidence in its own prediction so we are very excited about this problem we deployed some baselines but we are also interested in how far we can push it and how far you can push it so we are organizing a workshop and a challenge in eccb um 2024 and we invite you to participate the information uh you will find here U when you follow this um this um QR code there is an online leader board this is what it looks like we have some entries already so why should you care of the depth estimation challenge well there are instances here in this data set where just coupling a existing feature meure with better depth estimates gains your ranks on the leaderboard so for example you have here lofter when you couple it with dpd you have you have a certain Rank and then you gain one rank if you just couple it with KBR Plus+ instead and a more extreme example um DPT with Roma is rank eight on this leader board but if you couple it with depth maps of a method which is called Mickey then you jump to the second place of this leaderboard and as an extra motivation you created a second leaderboard now there's a second track which is called multiframe you still have one reference image but instead of a single query image you have a burst of frames as a query with relative tracking poses between them um so if you work on multiframe dep estimation this leader board is for you and as you see it's empty we just provided a single Baseline which is single view the Baseline so it's very very easy to achieve something on this leadle board cool so back then in 2022 what we did when we created the data set we tried some baselines we took off the shelf models we taped them together and see how saw how well they worked but of course that's not ideal because if you couple feature matching with depth estimation they don't know each other right they were not what what were not built for each other um they don't know what the other model is doing so we tried to come up with a solution where um dep estimation of feature matching are integrated within one Pipeline and trained jointly and this is Micki which is an oral here at cvpr uh so uh Axel can give you a great talk tomorrow in the oros set and now I share my perspective on this work here in a few minutes so the pipeline is inspired by feature matching 2D 2D feature matching it looks very similar we start with two images and we predict key points but the key points are 3D key points they live in metric space they are metric and for each 3D key point we have a high dimensional descriptor so we can do feature matching these correspondences are now 3D 3D so we can apply rans and a post Ser to get a scaled metric pose and because our key points are metric so are our relative poses okay cool how do we train this network we can look at 2D metes and see how are they trained they are usually trained by uh strong supervision using Crown proof correspondences because we have 3D 3D correspondences we need Crown proof 3D 3D correspondences how do we get them well first thing is we need matchable IM image pairs right we need some images where we know we can establish CR roof correspondences good then we can start with okay we do the same as other methods we establish CR roof 2D 2D correspondences this usually works by taking key points or points in the first image and projecting them to the other image for this projection we need the relative camera pose and we need metric depth here is where the trouble starts because we don't have depth for these images these were collected with with phones without lighter so H we can use maybe a strong teacher model somehow a good Dev estimation model which we then use as pseudo CR roof but not ideal another issue are difficult image pairs like this these are image pairs which we would like to solve it's possible right you would look at this image and would say yeah it's probably the same statue one is from the front and one's from the back but how do you define Crown roof correspondences here because the front of the statue is not visible in the other image it's self occluded by the back of the statue and the last difficulty here is well common metrix of visual overlap would probably fail here they would predict a low overlap and we would probably dismiss this image pair so we would have to come up with a new visual overlap metric which tells us yeah this is a solvable image pair so our solution to all of these question marks was oh no no no no no we don't do any of that no we have to make do with the relative poses this has to be enough how does does it work or first question does it work yes it does work the at the end of the of what I explain next this is what comes out of it and it's quite interesting because with only this relatively weak high level supervision of relative poses what Mickey learns to do is to reason about the shape of the prominent object in the scene which recognizes oh there's a statue and I try to match the top of the head of the statue and the left shoulder to left shoulder right shoulder to the right shoulder if you compare this to 2D 2D Mees because of how they have been trained it's very different they uh ledge on to the co visible surface which is here in the background and they try to figure out which symbol matches which symbol between the two uh frames I think that's quite interesting this different emergent behavior of the two models okay how does it work well I presented this the forward pipeline to you and the end of this pipeline is the relative pose right and we have relative pose Crown roof so what's the problem we can just apply a loss right and then differenti so propagate the gradients back through the pipeline this is how we could train it it's not so easy because um have ransac here and then there's feature matching is all discrete but it's not impossible right there have been works and I was involved in some of them uh to show how this can work differentiable ransack differentiable feature matching these methods exist and now we um we use them I will explain the gist of how this works uh here on the slide the real system is a bit more complicated um here is a simplified version to to show how what the idea is so we have discrete matches and we want to train them problem is that they are discrete so if I have a key point and I want to match to key point a or I want to match to key point B there's no easy way to make it soft it's either one a or either b or B right so the idea behind it here is to assume you have an exhaustive matching all matches exist you match everything to everything all matches are there they only have different probabilities so you define a probability distribution over possible matches the probability is derived from the descriptor distance if two descriptors are close then the match probability is high if they are far apart then the descriptor probability is low and this is how you can make it soft switching from matching a to matching B just means reducing the probability to matching a and increasing the probability to matching B so how can we now train this using post supervision well we just sample matches so from this probability distribution we discretely select matches and have a concrete sample of or a complete matching sample from which we can calculate a post and we do it again we sample again we get a different estimate of the pose and we do it again get a different estimate of the pose and we do this many many times we have many samples of poses and now we can apply our loss and the loss pulls all the samples towards the crown Troth and what it will H what it will do with the matching probabilities is for um matchings which resided in a good pose you want to increase the probability of these matches and for matches which resided in a bad POS you want to decrease the probability of these matches which then shapes the descriptor space bad descriptors are pulled apart good descriptors are pulled together and this is the main idea does it work yes it works so this is the result that we get and we see that we indeed can predict key points in 3D and match key points in 3D and the model is also able to bridge quite extreme Viewpoint changes also it uh does relatively good in um scale changes so if you if you start from further away and you go closer to the to the statute that's also working nicely and here's a very extreme example um two images if you stare at them long enough you would probably figure out well it's opposing views opposing shots of the same structure and Mickey is able to figure it out right other methods they struggle so they because they only do 2D 2D matching they collapse to the same side or they fail completely and one nice property of all these um feature matching Solutions is that you can count the inliers and this gives you a confidence and in the Mickey um differentiable post optimization framework there's also a signal going to the inlier count uh to help train it in the procedure and we see that as we go behind this sculpture or whatever it is Mickey struggles to predict a good pose but at least it tells you right there's a low confidence to reinforce the uh the justification to be here sending here in the mo depth uh estimation Challenge and uh Workshop here is depth estimation resite of Mickey this is now a bit awkward because I've seen in all the talks I have seen amazing crisp and detailed depth estimates and you might look at this and say well dude to be honest it looks quite crap but remember this was trained without any direct supervision of uh on the depth maps and it was trained in a way to say we don't need correct depth everywhere we just need correct depth where we want to sample keep points and that through the differentiation of feature matching and ret we still see this plausible and nice death Maps emerging I find it quite exciting so how do we do with our initial vision of enabling instant AR everywhere I think we we are doing pretty good so here we see again an example of the balloons and here are Mickey estimates where we use Mickey to project this AR asset into different views there's no dragging right these are individual estimates and it's quite decent it's nice and especially I want to emphasize the progress that we have made so the best method of eccv 2022 is on the left the best method of cvpr 2024 is on the right and quite some progress that we made in basically one year maybe one and a half and as we speak there's another method throning on the leaderboard um a paper from neor which was just the archive came out just before this uh cvpr and it basically in terms of numbers the the step forward that we see here they made another step with the same magnitude and um these results are also very exciting and we are very excited to see whether you can help us push this even further again I invite you to participate in this challenge ECC 24 that's the end of the talk thank you so much we might have time for questions or you catch me at the conference demro very question uh so what's your like let's say you have some data set like partially you are like good deps and maybe you can add this provisory signal but on the other hand like good Force only like things which you're are not interested in because they're like not using this like good key Point me and so on so like would it help in your Paradigm if you add like another for part data set or it's actually not and it's better go fully like poised uh I think it would help so the problem the the the way this is trained is reinforcement learning right um we have been able to train it from scratch like this on a relatively small scale data set I mean the me data set is nice right but it's still it's just a few hundred scenes and there are larger data sets out there I think the way to go is um to harest these larger data sets and The annotation when you have it pre-train it like this and then use the reinforcement learning approach just to fine unit I think that will give you the best uh approach there's always the danger that with pre-training you you sy in the local Minima that you can't escape anymore but I think ultimately my intuition would be in this case probably it works out better if you do it like this how are the key points to lifted and how many is this is a question for the main author which is also in the audience uh I think it's like 1,000 2,000 key points which are randomly sampled is that correct axel 2000 yeah okay so we select 2,000 key points they are randomly selected according to the confidence which is predicted by the network and then from these use sample 500 correspondences to to calculate the C the post or yeah first from the audience I think you this on larger scenes like landmarks uh no no so we for this paper we really just stti stick to the um to the methew data set uh scanner data set but again Axel did we try on larger data sets no even more so we haven't tried uh this this new leaderboard is really just went up a few weeks ago for the challenge um I think so you have multiple frames and you also have the tracking poses from the FR phone for these frames which should help a lot with the scale estimate of the scene but it will not help you to bridge these extreme Viewpoint changes so I'm personally looking I'm I'm curious to see how how how how much it helps I think it will help to some part but it will not help you solve it because there are lot of challenges that you still have to address uh so were you able to train Mickey on data sets that were usually people don't train on because they don't have that you would think that it is possible but we haven't tried it um I think we should we should just scale it up we you know from yeah we we focused on the mefree data set but the thing to do is to scale it up massively we know we have many many uh like observations like this scale it up it will work better so there's no reason to not do it so you have a many to many Association which you call a soft Association which is based on distance how do you prune between a high confidence uh distance which is farther away but a low confidence distance uh match between which is like closer I'm not sure I understand the question so we just randomly sample and then if the I mean if the if the matches are bad then the sample will be bad so or is this going in the direction of your question or so you have a set of matches from both images right and then you look at the match and you calculate the distance between those matches which is proportional to to some probability because their father give it a lower probability is that right the way I understood it so um we calculate the descript of distance between two key points that we match and then it's a soft Max over all matches but this is how we get the probability so sorry that I understood it as a 3D distance between those two but it's basically a key Point distance it's a keyo distance yes uh so for for Nikki and and the 3D key SE should we really be thinking about the key points as basically you know like a death map AR with u with with some feature or are the KE ones kind of free to appear you know somewhere in space as long as it maximizes basically the you know or I should say minimizes the during training I love the question because um in a sense I personally I was a bit disappointed to see the depth maps emerging because I was hoping that the network is free to do something like uh latent or semantic key points almost right to not restrict itself to the surface but to um do anything right to but the Mickey um the Mickey architecture does not have cross attention it's a purely single image Network which predicts the script and key points and so on so I think for this to happen what you explain it would need cross attention when we started the project we decided against cross attention models because we thought well if you want to use it for sfm then it's better if it doesn't have have cross attention because you can just uh apply it to images but then we didn't have really time to explore it we were restrict or we restricted us ourselves in the paper to the map free problem where you could have used cross attention but we didn't so I mean for the challenge you would be a good opportunity to um explore that thank you okay I wonder to to scale up the train for the mick upline because currently I'm the lead the top one master is probably use a lot more data during training it was trained on a lot more data and it has cross attention so it would be really interesting to see if if you level it out between the two approaches and train Mick on more and maybe add cross attention to it what comes out on top what we don't know okay I think we don't have questions anymore so thank you again for having me was fun great talk to run you guys are the new wave of feature thank you I selected it put occasion yeah good to meet you in person yeah same here same here I'm a big fan of your Twitter tweet keep tweeting I will thank you all right all right so let's wrap online as well uh thank you everyone for attending thanks everyone probably see you next year great thank you everyone see you thank you everyone

