---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=wrpjBuObomA"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:04.826Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=wrpjBuObomA

b21ac740-43d3-4b58-a30e-08fc140323c8

2025-10-28 https://www.youtube.com/watch?v=wrpjBuObomA

13c68eec-f20a-4613-938b-858bc5b1da1d

https://www.youtube.com/watch?v=wrpjBuObomA

wrpjBuObomA

## ComputerVisionFoundation Videos

okay so we should be ready to start soon um just before we start properly can I just check that people in the zoom can hear me if someone can just send a message that's great right okay so we will start now then right so welcome everyone to the fifth annual workshop on uh new architecture search when my slides decide to update we go and this Workshop is run in conjunction with Newcastle University Durham University the University of Edinburgh and Buu um my name is David Towers I'm going to be the organizer and the runner for today um how however we do have a lot of people behind the scenes who helped with scheduling stuff um so this year for this Workshop we have um a brief tutorial to introduce Nas we use this because people in the past have came see like our keynote speakers or presenters and a lot of them are a bit uh information dense with keywords that people outside the field aren't necessarily certain so we're going to start with a roughly 35 minute introduction to that um we also have r maralescu as keynote speaker who's going to talk about zero shot s and we have six accepted papers each with a presentation and an optional poster um what I have found out uh today is that cvpr has for some reason decided that while the workshop is here the poster session is over in the other building so we'll see what we do about that and so before we get in i' would like to um ask you at if you don't mind if you can uh scan QR codes and fill in our online form we use that to see uh who to use that to show um our supervisors and Speers that people do actually come this is worth for us to running it on and our mailing list will keep you up to date for other workshops that we've run and uh Nas competitions which we are which I will talk about towards the end of uh the workshop as we are running one at another conference later in the year that's we go for our schedule today um so we have uh our welcome which is just going to be over another three minutes once we get through the schedule and as said we have our brief introduction to NAS uh we have r maralescu as our keynote with zero CH neural architecture challenges Solutions and opportunities um I said we have our post session at 3 until 3:50 as I said this is in the other rooms this is in the arch building don't ask me why all posters are over there and then they put some of the workshops over here but our posters are free free 8 to 355 although uh so some of our some of our presenters have as have got their posters online and uh some opted not to do a poster and said just give their talk um but it's as you go in it's on the left hand uh you have like as you go in there's a row of um boards on the right and AOW then you've got the middle section of board if you go up the aisle in between it's around in the middle and after that we will have hopefully we'll have most of yous back here for our six favor presentations which would be about 10 minutes each we about five minutes for questions and then finally we'll have a 10-minute section for closing remarks um so without um without further uh Ado we'll get on to the brief introduction to Ness and so as a brief overview to this um presentation it's as said it's going to be about 35 minutes and as an introduction we use it because people uh sometimes come in and end up being too complicated or not necessarily complicated but just too unfamiliar with them and this can lead to confusion so we want to try and get people onto a similar page that uh the workshop can be enjoyed by as many people as possible um this presentation was designed with image classification uh Nas in mind that is uh after all we are at computer Vision conference and that is mostly where my research figures but Nas can be applied in other areas the original Nas paper itself worked on text prediction um a lot of what I said will apply but just there might be some specific examples or bits that are for the image classification side of NES and and basic understanding of machine learning is expected although we do briefly talk about what a neural network is that's kind of fund uh fundamental for neural architecture search and so without further adue what is an newal Network and if you look online you'll see the definition is it's just a it's a mathematic model which we use to approximate uh a function rather than having to make it ourselves um instead of having to hard code something we can just give it to the computer to figure it out and and this is the main concept of neur networks and pretty much why machine learning as field exists and we're seeing the rise of it is we can use it instead of having to co hardcode something for example uh with self-driving cars Tesla pioneered this although there may be sort of slacking in recent years compared to other competitors but it's very hard to take an image and say oh there's a human on the road there but it's very easy to draw a box around a human say there is a human here figure out what human in this image is or at least it's or at least it's easier than I said hard coding it so when we talk about anual Network we need talk about anual architecture this is how the network is made up this is what like we this is what um the computer uh this is what we give the computer and tell it on how to connect everything together and so very often we have layers of neurons which are all connected by edges so in the example on the book uh on the screen here we have like a very simple U multi- perceptron uh very basic form of machine Learning Network which has five layers an input and output and three hidden layers and a fully connected mesh topology and basically each neuron takes input from the previous layer and performs a weighted sum and then we apply an activation function which is the output for the next neurons and conventional uh NE this is mostly what the computer figures out it takes it tries to figure out what weights make the optimal um make the optimal sum which applies the activation function and then which makes an optimal output and so we use a backward propagation to Cascade backwards in order to uh find an optimal netk or find or train the network problem is is there are an infinite number of different network architectures so this is is a image of Google net which is uh one of like the off-the-shelf ones that you can download from the tense flow of the P torch libraries and then train uh then fine tune for your own problem and you can see it's a very different structure to the one that I showed earlier the m perceptron and it's not just that it's got It's got a different shape it's got different like con connection topology it's just also got different types of um layers in between them different topologies different layer types the different High parameters as you use all these affect the final outcome and all these are part of your neural architecture and different architectures perform better on different data sets which leads uh leads us to ask is what is the best network to construct for any given data set and that's a hard problem to solve so if you have the knowledge you might be able to transfer techniques from one data set to another if you work on uh imag net and you make a really good uh Network for that for Cipher 10 which is you which is a small data set of like a similar complexity you might be able to transfer some techniques that you found or network that you or um Network decisions that you used and apply it to that and there are very there are many very good new networks available uh off the shelf which you can find you Google Nets one of them res Nets n one VG Inception but the question you that we end up asking is do they give you the best result and for a lot of cases you don't need the best result you just need a pretty good result so at least the next question is are they within 5% of the best result and there's no really any way to know unless you know what the best result is but as we said there's an infinite like there's an infinite space of possible architectures so we don't really know what the best is or we don't really even really know what the best weights are for uh an off the shelf off the shelf model and so find all this out it's it takes time consuming trial and error you try and network see what results that give you then you need to find you might change the hyp parameters find out what find out how they affect the result and do that again and again and this is where new search comes in so instead of using an experts time and knowledge or in my case my time and knowledge why don't we automate it and so neural architecture search is a way to automatically F search for good neural network architectures instead of manually editing the model and testing it we can use the same principle of a newal network itself where we give it an input and an output and it finds the best weights where we give um our neur AR search algorithm an input and an output until I find the best network which gives the best uh output and this allows the spok ne networks to be generated without needing an expert's time or knowledge now before we go a bit further into talking about new AR search we want to uh want to talk about what we have identified as the three basic parts of Nas this is uh pretty much every NAS network we um can split into these fre Parts uh there's the search space and so this is all the possible networks that your algorithm will consider uh modern day Nas algorithms like to use uh Benchmark search Bases uh that is stuff like uh Nas bench to or one net spch or any other benchmarks uh but uh the more interesting part of Nas or the part which people like to um people do write more papers about is the search policies this is how you search through n through the search base so this is like the original Nas algorithm itself or the more modern ones like uh beta dos or uh boss Ness or eness although that one's not too modern and we've uh can break you can break the search policy into two phases so that's the search phase which is the part which actually searches through the search space that finds the architectures and then there's the evaluation phase this is the part where you take your searched architecture you f you uh fully train it and you get your test result and you use that and say okay this is our results and the final part although we don't won't go too much into this uh in in this presentation because it's very general uh amongst machine learning itself is the data policy what sort of augmentation you do of the data before you feed it into your network so stuff like cut out or Auto augment it's very important in machine learning in in as a whole but as I said it's not very specific to Nest so we won't cover it too much in this talk so finding at this point is probably a good idea to talk about the original neural architecture search algorithm which was uh made by zof and Lee in 2017 and basically the main idea of it was that they realized that neural network architectures can be encoded into Strings and this is the image they given in their paper where they show that uh where they encode um where they tokenize the number of filters the filter height The Stride height stri etc for each layer um and feed that into a uh a a recurrent newal Network which is a newal network that's uh very good at doing text prediction and therefore using reinforcement learning we can generate a architecture using the tokenized inputs poly train it get its result feed that back into the controller RNN RNN as a reward signal and then that will generate a new one and we repeat this uh we repeat this cycle until we get uh well the best architecture that the RNN can give us and so effectively we can use machine learning to optimized machine learning the problem with this method was it wasn't efficient the original Mass algorithm took 28 days on 800 gpus which were or about 61 GPU years and since that's not viable for most people probably won't shock you and know that this was done by uh the team at Google um it's just not a reasonable time scale to remove the need for manually assigned networks if you can't afford a network a network engineer or a machine learning expert then I don't think can afford the cloud time of 61 GPU years especially in 2017 where a bit more expensive um but in later in later experiments they use several techniques such as a micro search base which we'll talk about in a few slides time and reduced it to five GPU years which is still too much for me but many more modern Nas techniques have now only use a single GPU for less than a day and as we'll talk later some are even quicker than that so before we can talk about what a microarch Bas is this this is a very key technique which is used to um reduce the training time of an neural architecture search when you talk about what CNN blocks are because this is where the idea comes from so if we look again at what Google at Google net um you can see that it's made up of multiple Inception blocks which I have highlighted here using um those brackets and if you look in the bottom right you can see that uh that is basically the Inception block which is madees up the majority of their Network um many uh off the shelf cnns use similar structures such as reset with their residual blocks effectively they find very an optimal um structure and then they repeat it again and again use that train the N uh train the network and so the micro search Bas is very similar in concept as uh instead of searching for a whole network we instead search what we call a convolutional cell and this massively reduces search base as these are then chained together uh so instead of um having a search for an effectively infinite search Bas as you can always go deeper you can always add another layer you can always change a layer we now have a very small defined structure and for example this cell I believe is off darts um there's a different uh if we look at there's a different very similarly shaped cell which I think we'll look at later from the nas bench 2011 uh search bace which only has about 16,000 architectures or maybe closer 15,000 which is a lot smaller than an infinite number of them and makes the training time a lot better that's a very another very key um I'm saving uh technique was called weight sharing so weight sharing is a tool which again we use to reduce the training time and the idea is that similar architectures can share weights across a search base so for example if you have these two uh cells which I've cop with the is roughly the same shape as the nas bench 21 cells uh if you imagine each color is a different sort of operation uh between the four nodes these two um these two cells are quite next to each other differing only by a single edge the question that weight sharing asks is can although I point to the green uh operation it can be any of the operations that are the same color can we share weights across these uh cells and by if we say yes and accept that we can massively reduce the training time as instead of having to train uh 15,000 cells we actually only need to train about 30 edges before we can just start copying and copying the weights over so this is a few problems that come from the original versions of all the early Nas methods most of these were found by a single paper called uh called evaluating the search phase by utl and one of them that they found is probably not surprisingly here because it is in Computing is uh seeds and reproducibility many early Nas methods neglected to mention the seeds using their experiments and this prevent the results being reprodu reproducible or verifiable um this goes even further in some cases where the code is broken or you don't know the code is missing and dependency lists are sometimes not given or the files don't exist anymore um when I've tried to reproduce certain works and I've gone gone looking I will the I sometimes have to go through the GitHub issues to find what the dependencies are or find a dependency which doesn't exist anymore unless you're on Mac for some reason um this problem however isn't Nas specific it's everywhere on Computing and luckily more modern nasw Works tend to give the seeds of what the of their experiments dependency lists are a lot more common so we are going in the right direction but as always it's important I believe uh to strive for more reproducible Works uh another issue that uh the paper found was restricted search spaces so we talked about search spaces however they can affect the training time because we've use weight sharing to M educe how many spaces you actually go through but they also affect the results you can get and it was found that many of the early Nas methods had two constrained search Bases and this basically meant that most of the architectures were good they would Prov they produce good results which meant a randomly sample Network often gave competitive performance as a search for network and so many of the search methods didn't significantly uh outperform random sampling and in the case of Nao if I remember correctly or new architecture optimization random sampling actually outperformed that uh search method and this also ties in with an unevaluated search of the unevaluated search phase where many algorithms would present their results and state they were because of their search method however they often also prevented a new search space in the same paper so it wouldn't just be oh this is our search method using differentiable architecture differenti differentiable equations but they would also include obvious is our new search bace and both these factors affect your result if the ures in the search bace are good you're going to find good architectures all of them are bad they're going to find bad ones and so we can't really say if the results are because of the search method or because of the search base of themselves without oblation studies that also meant that methods weren't directly comparable sure your method might get at 6% more but if my search space doesn't allow you to get that 6% more that doesn't necessarily mean that your search method is better than mine um so Lu at both these problems Nas benchmarks are a lot more common now um Nas bench 2011 is a popular search space which contains both good and bad networks as you can see from these graphs here the bottom one is um a version of en's um search base and although it's the validation accuracy you can see that most of the architectures give above 80% accuracy while in the nas bench to a one search base it's a lot more varied from about just under 50% so there's obviously some bad networks in that search base and since they use this search base since a lot of networks at least will use that if conjunction with another search base in ablation studies we can now directly compare if you're use if you're producing one result on um if you're producing one result on your method and I'm producing a different result on my method we can actually also look at the true Optimum see how far away we are from it and we can also see where are where each of our methods rank in comparison so we actually know how how many architectures uh better than mine yours would find the final main point that uh you Al found in their paper which was a problem is with weight sharing itself as a quick reminder so weight sharing as said is a technique which we use to speed up Nas and this involves similar networks using uh sharing weights across similar edges to reduce the amount of time training the problem of this is they found that algorithms that used weight sharing produced uh produced architectures or ranked them as the best and the true ranking didn't correlate at all so I find this graph a bit confusing but it's from the paper but essenti effectively the right hand column uh the numbers indicate the disparity so if you can see the architecture which would be ranked one with the ground truth was actually ranked eight and had seven positions of disparity and we have some networks which have 17 positions of disparity 14 it just doesn't correlate that's kind of how it just doesn't correlate so we found that weight sharing actually had a negative effect on it and in fact they found that the more weight sharing you had the more disparity there was and this is because small differences still require different weights it is a very helpful idea to reduce time but it does affect the actual results but despite the problems it's still a massive time Improvement so I have seen some methods that still use it we don't necessarily need the best um architecture in every case we just we can need a good architecture as long as it still find a good one that should it's still a usable method um some methods um try and mitigate its effect using other techniques there's a NES algorithm called f Ness which uh weight shares only in blocks so it doesn't weight share across the whole network which is to mitigate the effect as we saw in the paper wait the problem the disparity generated by weight sharing is correlated to the amount of weight sharing itself and meth is that avoid weight sharing all together and use other techniques but they each have their pros and cons so now just want to talk about um two concepts that might come up later in with people's presentations so we just want to make sure that s and the first one is oneshot n and basically this means that we it means new Lo set search um and we do everything in one training run so if we have this uh mini Network where we've got three nodes and two edges between them and we don't know which operation to use uh in traditional machine learning or in traditional Nas you would say okay well we're going to have the first layer be a 1ex one convolution and then we're going to have a 3X3 convolution for the second Edge and we're going to train that then we'll train the next one and so you in this case you'd do nine total training runs instead uh in one shot Nas you'd create a supernet where you mix all the operations together and train them all in the same run which usually takes longer than a single run but is often quicker than running on nine and then afterwards you just deactivate the edges that you don't want uh enable the ones that you do and you do that nine times and just evaluate it which is a lot quicker than training and then you do fully train the best one out of the nine that you get and next uh so another concept is very similar one shot Nest is zero shot Nas and this effectively means Nas about training you don't run a training Loop or at least not a full one instead proxies are used to estimate the performance of a network without fully training it now this proxy can be generated from some sort of algorithm uh which score the untrained networks uh this is the example in the uh paper called Nas without training funnny enough and other methods sometimes perform short runs on Mini batches of the data set only for a couple of epoch rather than fully training it um as effectively is just try a way to perform Nas without having to do a full training time because that is the main cost of Nas so I've talk about problems in N but I wanted to now focus on the future and problems for Nas so these are things that Pro these are things that we want to work towards or that I think Nas would be a very good thing to use um and the first one has been probably surprising it's the search Time versus accuracy ever since the original NES um paper we've tried to make it faster and faster um we want Nas to be as fast as possible but we do need it to be accurate and this tends to be a tradeoff the high accuracy we want of being the best network the uh slower it takes to actually train it if Nas is to remove the need of manually designed networks as the original paper suggest it could or off the shelf networks we need to be able to generate networks from nas uh quickly because if uh even if we have methods which take a day I can for many problems I can just download resnet and retrain it within an hour um we talked about the micro search base problem that uh some people have identified is the micro versus macro um approach so many Nas methods as I said are evaluated using npch to a one or other a similar uh search Bases and these are usually micro search Bases you search for a convolutional cell which is chained together rather than a full architecture and if we agree if we agree on a point from earlier that different data sets uh have different uh different data sets require different networks to find their best performance then we have to make the Assumption this that a micro search bace is not going to be the best network for every uh data set as such we need efficient methods of exploring full networks not only in terms of depth as I said you can always add one more layer but structure Nas bench 2011 only looks at cells with four nodes and six edges now they do use skip connects which could argue does alter the structure as it has a smaller size as a smaller cell size but we want bigger we want to have a look at bigger networks with weirder topologies uh so an interesting one that I found I heard at a different conference was if everyone remembers a few years ago uh Transformers were all the rage there were many papers that had new Trans that were talking about Transformers Transformers were the new big thing and they were designed by humans or rather the architecture was found by humans but if NASA is supposed to be about automating architecture construction why wasn't it employed to find Why didn't it find the Transformer Nas currently looks at optimizing specific structures and topologies as I said we use a very specific search base for a very specific structure but there's no necessar reason necessarily that it should be limited to that if NASA is about automating automating Network design if we apply a sort of more infinite or less less so infinite a bigger and more fluid search space why can't Nas be used to find what the next big architecture will be why couldn't Nas find the Transformer why couldn't Ness find the ne whatever is going to replace the Transformer another big problem with uh NES is accessibility although it's not as big of a problem anymore as usually it just requires a single GPU and a day uh and some other tools not everyone has access to these tools you don't need to be Google anymore but if businesses are incorporating AI into their business plan more and more as we see more and more smaller businesses which can't are simply going to struggle Nas removes the cost of an expert but question is can we go further can we develop Nas which anyone can use I can can we make Nas that I can generate off a laptop without a GPU just off the CPU can I generate a Nas can I use Nas to generate a network on my phone that suit my business needs um there are defin works at look into this and is very interesting uh area of the research because it's a opportunity to help smaller businesses compete with the likes of Google who have effectively infinite resources uh the final um problem or challenge I want to talk about Nas for is accuracy overall so very often Nas is evaluated in terms of test accuracy you run your Nas algorithm you get your Network you test it on your data set and then you say okay my Nas algorithm reduced a network of x% however the problem is actually isn't the most important network uh metric all the time in medical settings Precision is often more valuable you don't want to have any false positives if you are diagnosing a certain type of disease or condition you don't want to tell someone they have it when they don't and in mobile settings as I said when talking about making it more accessible multiply accumulate operations are also important you want to reduce those as much as possible uh there so a a challenge fors is to shift away from accuracy being at the Forefront and develop algorithms that can match a user's need and we do see that in some algorithms where they will return a portfolio of different models along a gretto front or tradeoff of different metrics where you'd have a model which optimizes for accuracy or optimizes for in this in a mobile case multiply accumulate operation um so that is the introduction to NAS thank you for listening um we have got 10 minutes before our keynote so if anyone does have any questions uh feel free to ask if not we can move on okay um so so I'll go back to the page um so just for people on Zoom I was asked of um what does the rank when it was coming out incorrect um to this image here um so this was back this was uh basically in this paper they about they took 32 architectures from a search bace and fully trained them to find what the ground proof of them was in terms of accuracy so then they could rank them saying Oh this was the highest actually this was the and it wasn't necessarily that they were uh that the NASA algorithms were incorrectly ident rather were worse or better it was just the actual what the um Nas algorithm would actually say is the best network would have no correlation to its actual position so for example as said in this they found uh that uh an architecture which was um actually ranked six the nas method was saying you know this is number one the architecture was number one they actually said that was number eight there wasn't a correlation between what the NASA algorithm placed them and their true placement no problem um there are no other questions then um Ru are you fine to uh start earlier y so to introduce R marasu from the University of Texas at Austin and to give his keynote and we're now going to swap over e e e e e e e e but let me not I'm here some okay so I should go in M can I get three drag it to the top it should then and then press that button to turn not working okay wait just want to make sure nothing Falls in yeah okay so uh sorry about the weight there I think hopefully we're set up now and i' would like to again introduce R Mar kesu from the University of Texas at Austin to give his uh keynote on zero shot new architecture search challenges Solutions and OPP unities all right um thank thank you very much David for your nice introduction and thank you everyone for uh coming to this session and to uh show interest in U zero shot Nas so uh just to start a little bit from the title um luckily we had this introduction so the neural architecture search is essentially about solving a optimization problem uh where we have a search space typically a very large search space we have constraints that the most well known is the accuracy but we also can have other constraints like ly it's too far yeah yeah accuracy but also power and latency and the idea is to find the best solution that's uh fits into a certain computational budget that's the idea why was Nas uh such an interesting topic to both industry in Academia was because it promised to move from this kind of manual or atog design like was shown in that Google net example uh to systematic solutions that can lead to uh optimal designs that can be used for different applications now the the zero shot m in particular promises to get rid of of the training all together so therefore to improve significantly the time uh for finding these Solutions so it's a Hot Topic within Nas so to speak and the basic ideas at the end of the day is to use some sort of proxy or some sort of metric that can capture very well the behavior of the network and the performance metric that you're trying to optimize let's say accuracy and uh in this talk I'm going to uh go uh comprehensively over various classes of um um such proes and see how they can be used to uh design better architectures better networks and what are the limitations as well as the promising directions for future research so okay uh as I suspect that there is a broad range of expert is in this room uh when it comes to uh zero shot Nas what I'm going to do I'm going to structure this talk in three main parts so the first one is essentially meant to answer the why question why zero shotas is important why do we care about it and hopefully motivate many of you if not most of you all of you to do research in this area uh then I'm going to get into the how in other words what's what approaches or what methods can be used and what do they mean particularly for zero short now so this is the proxy part third I'm going to uh answer the what question so in other words what can we get what kind of results can we obtain and I'm going to go uh into uh many results that or many evaluations that will show a broad range of solutions and a broad range of applications that can benefit from it and finally closing back to the why in other words what can we do to improve to mitigate the limitations that I'm going to mention and um get better Solutions or better results for the future okay all right so now to set up the scene um given my particular expertise in this area I'm going to talk about uh Nas in the context of edge AI in other words edging intelligence where the idea is to put to make all these uous devices intelligent typical for applications like internet of things and uh in this context the classical solution when we take the the the models from the cloud uh and we just deploy them on devices has some limitations in other words we have to deal with issues related to the computational to the communication issues uh to the connectivity issues the connection may not always work uh we may want to keep the privacy of different models for different users so there there is a lot of stuff going on uh particularly given the wide class of devices that we can have in this situation ranging from your phones to your Fitbit to other such devices some of them being even medical wearables that have very private information uh about our health so in this uh context taking the model as they are um from from the cloud it may be problematic because typically these models are big and don't fit easily into these kind of devices in these kind of devices we have limitations when it comes to computational processing we have limitations coming from the memory space that we can use to save these models we have limitations when it comes to the battery level that we can afford to use with these devices all of them are limited by the amount of battery that we can use before getting back to charging them again so in this case the idea that one can develop Nas approaches that considers the hardware limitations we can call this Hardware where Nas is very appealing okay and U in this uh in this uh kind of situation I'm sorry in this kind of situation the fundamental research question that one can ask is uh how to design this efficient networks that can be used to run this kind of applications directly on edge devices and when I mean applications I I mean anything from home and smart home Automotive autonomous driving mobile wibles things of that nature so there is a broad range of applications that can benefit from such a solution so uh just to get a little bit into the basics um there are many approaches that can be used for Hardware awareness I'm going to only skip skim over the important concept here and get to the heart of the which is the zero shot Nas so uh in general we start in this problem space with uh a Target device a Target device meaning anything that can be used to run this application which could be a very powerful phone could be a uh fpga could be a microcontroller so it's a wide variety of devices and in this case we characterize these devices and we have some sort of idea about their performance metrics then we have the search space which is uh broad a very vast space where various architectures can belong to and then uh we Define what we call a search algorith so an algorith that will go systematically over all these possibilities and choose hopefully the best one that will fit our application so in general there are three categories of uh device of U approaches that have been developed so the the multishot methods that uh were mentioned earlier as being based primarily reinforcement learning or some sort of evolutionary algorithm then there is the one shot learning which was again mentioned as being more efficient than the multi-shot and finally the focus of my talk which is the zero shot approaches which are the most promising approaches among all because they promise to get away with this important step which is very timec consuming and it's the training step so just to go once over lightly the the m te shot approaches we have typically some sort of controller so we call this a Nas controller that generates a description of the circuit the description is converted into an actual circuit having real operations as shown here uh this one is trained and some performas Metric is extracted and it's fed back into the controller over multiple steps such that we get in the end what we want so through successive refinement the architecture becomes better and better and hopefully we get a reasonable solution in the end the problem with this is that it needs a lot of time okay and therefore uh it's kind of limited in terms of applicability to anything but small examples one shot Nas okay uh it's a significant Evolution over multishot Nas because in this case we simply build a hyper Network which consists as shown here into notes that are feature maps and to Ed that are different types of operations uh and basically we train only this hyper Network and during this training we kind of uh emphasize the important operations and in this representation the important operations are U are shown with thicker lines okay so then in the final stage which is essentially the one that generat architecture we basically get only go ahead only with the thick one ones and therefore these are the most important ones that will by important it means they affect the most the performance of the network and uh in this case is a significant Improvement compared to previous approach uh since was mentioned I want to bring again the weight sharing approach that can even uh bring even more improvements over this uh one shot learning but at the end of the day the fundamental step which is the training step remains there hence the desire to develop time effic time efficient and Hardware whereare approaches uh that will uh eliminate this step Al together so last my last bit in terms of motivating the interest for these zero shot uh approaches comes from a sustainability angle here so we talked about getting time efficiency getting various things but if we think in terms of the search the cost of search in terms of CO2 that is consumed and this could be a serious problem in the long run uh and the accuracy that we get you see here different levels of accuracy and different classes of approaches the ones shown in blue here uh are the multi-shot ones which are obviously the most expensive ones uh the the the orange ones here are the ones that belong to the one shot approach and you see many circles that means many many versions of this one shot approaches have been proposed some of them having more or less computational requirements you see here the the um computational in terms of Mac operations multiply accumulate the the smaller ones take less the the bigger ones take more but we also see the promise of the green ones which are far less dense compared to the others but in this case you see for instance an approach like this you have gives a very high level of accuracy yet at a fraction of a cost compared to one of the approaches here or here uh let alone these ones which have a lot less so this is another way to think about the importance of having such approaches that um uh can provide us a good solution without uh having a big impact on uh sustainability now there is another angle to this problem and I'm hopefully I'm going to convince you by the end of the of my talk that this is an important one despite all these things all these improvements that we can get through zero shot last there is one more angle which is this um having this approaches developed can deepen our understanding our theoretical understanding of of why certain architectures work or give better results than others okay and this is an important thing as we as long as we think of having the next generation of classes of I don't know super Transformers or whatever that will do things very efficiently so this is another thing to to keep in mind so I hope that by now I convinced you why it's important to get this sh zero shot proxies um and now I want to get into the the how part so to speak what are the methods what are the approaches that we can use uh but I'll start by uh mentioning two um proxies that probably most of you if not all of you have used or have seen uh in any kind of problems of that of this nature and these are specifically the number of parameters okay and the number of flops so people have used and still use U number of parameters to compare different architectures so they said oh this one has fewer parameters and you see this reported everywhere oh one billion or whatever or this is 100 100 million or whatever or this is few Millions parameters it's better than this because obviously it can fit smaller um uh devices right so this is still very much of use I don't brag about it a lot but I'll keep constantly in the presentation and the second one is the number of flops T typically the number of flops relates to the complexity the computational complexity involved so this is also very important and I want to make this clear from the very beginning because in many of my comparisons shown later you will see this brought up constantly the number of parameters and the number of flops and actually a little bit counterintuitively they are extremely good uh and extremely competitive with this kind kind of quite refined or quite complex uh proxies that are based on gradient accuracy Proxes or gradient free accuracy Proxes so the big picture um means that a good Network can be designed when all these three criteria shown here in these three bubbles are satisfied and by this three I mean uh the capacity of expressiveness so basically the the model that we have the proxy that we use to design that should be able to capture subtle patterns that correlate the network Behavior with the the the objective at hand with the application at hand then should be should have a good gener generalization capability in other words should be what whatever we do and we derive with known data should work well with unseen or out of distribution data and finally um it should be um trainable and converge pretty fast to the result that we want because that shows a very competitive result so all these things somehow together can bring a very good solution a very good uh Network and it's important to keep this in mind as I'll go to the details of this uh because this impacts actually the quality quite a bit so let me start with one of the simplest uh proxy so the gradient Norm the gradient Norm is pretty basic so basically measure the sum of the L2 Norms of the gradients across each layer's parameters so here is a Formula that shows the you have here the gradients of the parameters thet I are the parameters I I here is the layer okay so you have layer from one to D where D is the depth of the network and you have the loss function shown here and is a Norm too so the square um and basically this is what is a mathematical formula that can be used to understand the propagation the gradient propagation through the N Network given all these layers that made up the network so the idea is that by knowing this or by using this during the design process we can have a good idea on the behavior and how that will behave in terms of will impact the quality of the uh metric that we're interested in let's say the accuracy for the sake of Simplicity the problem with this is that it captures the gradient propagation properly but does not consider the parameter importance during inference and this brings me to the second one so I'm going to build on this you know uh in terms of sophistication starting from the very basic to the most sophisticated one a few slides later and each and every time I'm going to add something new in terms of complexity or sophistication in terms of uh that specific proxy so this one snip single shot Network pruning is okay is the one that brings the importance into the picture so uh to jointly measure the parameter importance for both forward inference and backward gradient propagation uh this considers uh a more sophisticated for formula this is called selency which tells a lot and in this case it's basically the product between the you think like for video generation there will be a CL like model like training and all those kind of kind of complicated more like engineer instead of a uh research kind of pro I don't is this for me that is weird Okay so clearly we or I cannot cover them because they do the mind so I can keep going yeah keep going happens again I'll go the room mask okay the question is which one okay yeah so I was seeing in this case we this is the saliency which essentially looks at the product between the parameter magnitude itself you see that ey here the gradient and very cool is this because of Zoom right it comes anyway right the funny thing is that it comes loud and clear right we can listen to that h so in this case I was saying that um this saleny gives a sense of importance of the parameters so what it means it shows that some parameters are more important than others and therefore if we think about the importance of those we can maintain those and eliminate the ones that have a lower uh importance lower saliency score such that we can prune them and therefore Focus the training only on the important ones so the training Still Remains part of the picture but with fewer parameters therefore the efficiency increases over the previous one uh and hence this is a significant step compared to the the the previous proxy that I showed so uh in in uh in this case oh in case you wonder this one and the brackets here show here this is the inner product sign that I used for this representation so compared to the gradient Norm this is a step forward so this is uh supposed to give good results and uh of course it has the limitations as to what exactly um uh how exactly the effectiveness of this uh relies on the assumption that the initial gradient importance uh of weights correlates with the actual Network Behavior the important thing for for for for for you to understand from this presentation at this point is these are mathematical constructs that can be used to give some importance of some weights versus another and therefore we once you set a threshold and you say okay this is the threshold that matters to me you can eliminate others and train with this so basically the training goes with a simplified circuit because the number of parameters is fewer and therefore the approach is faster and hopefully leads to the best results but there are still still things how do you decide that threshold how well the uh the uh proxy itself correlates to the actual behavior of the network so there is no Silver Bullet but there is progress as we get in terms of sophistication and hopefully um best results in the end now okay now I'm going to move to one that is even more complicated so this is the Jacobian covariant or Jacobian covariant uh in which we uh do a lot more compared to the previous one so basically uh this uh addresses the issue um over the input data to reflect the network expressivity so for instance if you look at this formula shown here J is the ja Jovian um it's basically um a matrix that does that contains all these gradients of the output y for a particular uh batch B uh with inputs X1 X2 XB which is the the collects all the um gradients with respect to all the inputs so why is this great it's great because it gives you a sense of the sensitivity of the output with respect to the inputs so this is why is interesting and whatever uh so the way you build this is essentially by uh building this covariance Matrix doing this gra collecting all these gradient calculations finding a normal normalized version of it then Computing the igon values and then these igon values are used to compute this proxy the ja Jacobian covariant or covariant which cap has this Lambda as the EM values and some uh small value added here for numerical uh stability so in general there are many subtle aspects here from a mathematical standpoint of view but what's important for us here is that this reflects the network expressivity uh remember those three bubbles that I showed earlier and in general is a complicated approach in terms of uh uh computational resources that's needed because we move from scalar that I just talked earlier to Matrix type of uh information processes which is this J Jacobian uh um okay one more level so this is an approach called ziko which we proposed um at I clear last year uh and this one is even more sophisticated uh but is a significant change um as it introduces a new type of metric that combines the mean you see here the mean and the standard variation um uh of the GR across batches to evaluate the network um architecture so the formula that is computed here is based on what we call the inverse coefficient variation okay of these gradients which serves as a metric to assess the potential performance of different architectures so what is this is essentially the ratio between is essentially the ratio of the mean to the standard uh deviation of the gradients and in general High uh uh icv or inverse coefficient variation values indicate uh stable and significant gradients therefore more important coefficients so this is what is used so the intuition behind this is that the parameters with stable and large gradients are likely to be more important for the network or the architecture of the network to obtain High uh performance results so in other words these are the ones that should be considered during training and basically uh get get rid of the rest so um it's it's again it looks into uh the importance to finding um the importance of of certain parameters compared to others in order to do this uh just to clarify this it doesn't involve any training this B batch that you see here is just the initialization one that is used to uh do these derivations and after you eliminate the the parameters that you don't need and you keep the ones that matter the most you can do just regular training which will be far more efficient it does have uh so it's easy to implement but it does have a overhead because essentially of this calculations so this gradient should be calculated twice once for the U once for the uh denominator and one for the variance of the the um denominator and the problem is that um they cannot be comput in one single step so you do two steps to compute them because otherwise you will be a Jacobian right if you do it at once as a matrix but overall uh it's a interesting approach is an interesting twist and you'll see uh in the evaluation part it gives quite consistent results so the question is um everything I presented in the uh last few slides were having this approach this proxies that are based on gradient calculations let's see now what can we do about the gradient free calculations okay what how how will uh this uh this work and here I have um a couple of approaches I want I want to mention there are others that can be mentioned I'm not exhaustive in this presentation but I just highlight the ones that have a significant shift in terms of um intellectual sophistication compared to the general Trend so one of this is based on the number of linear regions okay so the basic idea in this case is that um the linear regions correspond to Unique activation patterns of the neurons in the network so for instance if you think about this multi-layer Petron and you have this input and you have this output all of them assuming the binary case when everything is shown with zero non-activated one being activated uh the input space can be partitioned and for each activation as you see here a certain region corresponds to a linear behavior of the network that's the basic idea so once you get this type of thing for the input then you can create some codes as you see here so you associate codes to different regions and in the end by putting all this together and you get a matrix here the more uh uh this Matrix resembles to a diagonal matrix the better for the performance of the netork in this case the the um formula that we use or the um proxy that is used is shown in this formula where you have the number of linear regions here and you have this uh Matrix type of representation where DH is the Heming distance and these are the codes that correspond to uh these different representations and the basic idea is that the the smaller the Heming distance between two different codes the harder will be for those networks to differentiate between those two types of behaviors so once you have this log that type of formulation you can get an idea on what matters and what doesn't matter again is the same idea uh is the same idea that has to be used and the basic idea is that the larger the value the better the behavior of the network of course there is the issue of the exponential growth of this uh um linear regions particularly for complex or deep networks because that the problem can clearly increase in complexity very fast but nevertheless the the results that um uh have been obtained and the results that have been proposed um are quite interesting and you will see later in the experimental results another class of approaches so in this case we saw a change from the mathematical formulation in the sense that is not just gradient but also involves something like the topological uh concept of the linear region as well as the coding through those codes as Associated as well as the Heming distance in this case we go in a different direction we still increase the sophistication and in this case we bring together uh the concepts in the uh relationship between the gradient Behavior and the gradient properties and the network behavior on one hand with uh Network science on the other hand in particularly small world networks for those of you who are not familiar with the small world networks those are the class of networks where uh distant nodes may look very close in fact in terms of behavior due to the long range links that connect them okay so for instance typically in epidemics um the the effects are a lot worse than would be in a gridlike structure if all of us will be in a grid because precisely we have this long range links and therefore the small world effect comes into the picture in this case we can look at the regular you see you have a network that is wide and deep you see the values here and you can see that we can identify two types of connections so so we can identify cell to cell connection this is a cell so we have these networks that are called short range links shown with black but you also have these long range links that skip multiple levels shown with this purple or this red that create this kind of small effects so in other words in this case we take a topological or a structural view on the network and we try to see if we can identify any proxy or any metric that will reflect that in terms of quality of the network in terms of achieving higher level of accuracy and in this case we Define two metrics one of them is so-call density of a block which is the ratio between the number of shortcut links in the block and the total possible of no of shortcut links in that block and this is the row b the density and then the more the most important one which is the NN Mass shown in this formula which incorporates this row b you see it here uh which essentially gives us the idea of the mass of the network so the the analogy here is that in physics if you think about density is a mass over a volume unit of volume right and the idea in this is to associate to the neural networks a certain Mass which is density and you can think this as an equivalent of a volume which comes from the product of WID and uh and this so we don't look precisely at the topology like an fency Matrix but we look at this through the statistical properties that are important through this U density and through this NN M so this is the fundamental change in the understanding and the idea is that the networks that have higher n and M are better from quality so what it means is that similar to you know I'm just giving a everyday example if you go to a market and you see okay this watermelon is heavier than this well this may be better for me because I want more to E the same thing so if this has an NN mass that is higher than this this will give me a better result so I can say that a prior before uh training or doing anything with that Network so this is the advantage of this and this is why this is a zero shot uh type of approach last but not least is the variation of this it's called NN degree which relaxes a little bit the width requirement that I mentioned earlier um and in this case the view is similar but slightly different so what do we do here we do two things first we look at any regular Network shown here okay as an overlap that's why we say this network equals this plus this so essentially is this is the real Network which is an overlap between a regular uh Network and an irregular one that has only this long range link so essentially is an overlap between a Lattis type of network and a random type of network that is based on these long range links okay and this is the idea so again we look at the structural properties of the network but through a statistical lens that gives as this kind of metrics I'm not going to get into the details there are many mathematical details and theorems proof that are interesting to to understand this kind of approach but the the final result which is kind of uh deceptively simple somehow shows that we can do the NN Mass the NN degree I'm sorry of this cell as the sum between this and this and then the NN degree of a given network is just the sum over all the cells so a cell is the one that has a certain the same width and a certain depth okay and we when you sum all these you can get a g an N degree of the network that is here so the greatest thing about this is that is extremely simple it needs only three parameters so essentially it can WC the width the total number of skip links and the the DC which is the dep so these are things that can be observed that sort of mcro scale and you can get an idea of a network how good or how bad it is so the interesting thing also from a slightly different perspective here is to acknowledge that this um this type of approaches uh look at um uh macroscale properties of the network although they characterize the structure at the fine scale somehow so the very fabric of the network comes into the picture and it's interesting because these kind of uh approaches again uh enable to do good results to obtain good results with uh very few parameters you may ask me why do we need this why were the previous ones not enough okay well well it's interesting uh that uh these kind of networks are um relying on these statistics to get properties for specialized architectures so these were shown results to help for the dense net class of networks res net type of networks as well as mobile net mobile net class of networks what's common among all of them all of them have this kind of long range links so a property that can be exploited for particular class of architectures but motivationally speaking why is this important because because the previous ones although they are the same if even not more sophisticated as you'll see shortly they don't necessarily perform better than number of parameters and number um and number of uh flops so why why is this angle needed is because you you cannot see one size fits all type of metric and these one are metrics that are try to be specialized a little bit to certain classes of architectures in the hope that they will give better results compared to the general ones in other words the ones that are purely mathematically Justified okay and we'll see this in a a short while uh to summarize okay uh these are approaches that I mentioned in terms of um proxy gra Norm SN didn't mention a a lot of them then Jacobian Co variant then ziko and then from the second category I mentioned Only log that and then mass and N then degree so with respect to the three qualities that I mentioned earlier meaning the the the three bubbles that I mentioned that will make a great proxy you could see you could make three or four observations here the first one probably the most obvious is that this class is significantly imbalanced compared to this one so you have a lot more in this category compared to this category so the gradient approach is dominant obviously the gradient approaches are more complex as I said simply because you need to compute gradients as opposed to the gradient three which you don't so the efficiency of this just from the GetGo will be significantly more compared to this so one possible direction will be to focus to develop on this one and develop better approaches for this another observation even in this class alone you can see that the vast majority of the approaches focus on trainability very few focus on expressivity or on gener generalization capability so the ability to work with unseen data or out of distribution data just two all of others are doomed to fail from the G last but not least if you look look at this class gradient PR the vast majority follows on expressivity none of them focus on generalization capability and last but not least there is not a single approach that covers all three of them so this shows you in principle fundamental limitations of all the Proxes that have been published to date in dealing with any general case why because they either good here or good here and here or good here alone or good here and here so in principle is like you need three points to determine a plane and you know you have only two at most in most most cases one so this probably should be as a general guideline as to what should follow next in terms of uh good uh proxies so I'm going to go to the last step of my presentation so essentially evaluation uh David covered some of the benchmarks that makes life a lot easier so uh uh I'm going to focus just on the second part um the benchmarks are specifically uh Benchmark developed for Nas evaluations in other words they are providing you certain data sets like Cipher 10 100 or whatever but also certain model architectures to deal with and probably some of the most known one is like N St 101 2011 so on so forth so as mentioned earlier what's great or What's um interesting about it each each of them is built by stacking multiple s cells each cell will have six six operations you see the edges again I'm talking about the nas bench 2011 here and five candidate operations show different colors and the nas bench 101 have the same structure just more examples the interesting part to me at least is this the hardware aware cases when you have in addition to these operations and this kind of things that we discussed we have also results that have been provided specifically for different Target architectures for gpus for microcontrollers for whatever and in this case typically we're talking about latency or uh energy figures um and you may wonder what do you do with it how do you connect and I'm going to show you just one connection which takes the very last one that I mentioned meaning the NN degree which look like nothing probably you have seen before but yet is can be used in a very concrete framework that consists of two steps one in red uh which is about uh building Hardware performance models for a given architecture and then uh the second blue box the the bigger one which shows how this can be used to get the optimal architecture so we have performance metrics here we have a general search space here and then basically you can do this proxies you can do this estimations and you iterate multiple times such that at the end this very D star architecture that you get is the optimal one what means optimal one highest accuracy yet satisfying the energy budget or the latency budget that was given to it and this is new um and I'm going to show you hopefully this will work uh a short video function Al we after that we use theile perance cre so the cost is the reg Ali and finally our okay so you may wonder what why did I show this you see all these parameters BC TC that I mentioned earlier and this comparison is very detailed unless you know the paper the only reason I showed you this is because of this time three seconds on Raspberry Pi to do a full Nas uh uh run you may say where is this useful where do I care about 3 seconds whatever well just imagine you sent something on moon or more recently on Mars This people want and something goes wrong and you need to resynthesize something and to be able to run the application there where you cannot go and replace it yourself or use uh websites to upload a model or whatever so this will will be able to run to run a new model find a new model deploy it and run on it in three seconds on a Raspberry p three not even four okay this is the so this shows that as sophisticated and as alien those Concepts where with small ones and whatever they can lead to very complete results where we have actual number me in terms of Energy Efficiency cost of the search and so on so forth okay so uh coming back to the more General evaluations I want to show a few of them that are based on two metrics Spearman and kle Spearman spr here the raw parameters and K St for both all case when all the networks in The Benchmark are consider or top 5% when we look at the top five Solutions David mentioned this earlier so so uh you see this is the comprehensive evaluation of cyer 10 for Nas bench 2 2011 where you can see the the correlation in general these are between minus one and one to is typically more robust but one is best correlation minus one is the worst so you can see if we look at this orange and green meaning the the U um these two for old architecture clearly these points dominate so which one gra sign uh ziko and the number of parameters so basically the number of parameters that is such a naive approach used by everyone performs very well in in fact a lot better than far more complicated things like Fisher Jacobian covariant and whatever so this shows that this effort should be um realigned with this kind of evaluations so to see which ones to use okay uh if you look at the red and the blue one which are the uh top five% architectures you see that the Zen score the one shown in here in red performs the the the best so others perform a lot worse like you see over here so first of all there is not no claim that can cover everything depending on what you're looking for and also many of them either gradient based or non-gradient based ones perform better worse in different cases um so in this particular case as I said these are the two conclusions that cover this white range this was again for cyar 100 data set but similar results have been obtained for other Benchmark sets another comparison here involves uh image net okay for nuts Benchmark and U in this case you could see that again um all orange and green these are the best meaning the highest value 6 and8 and this one here so again number of parameters ziko and Z for perform the best if you look for top 5% uh these were probably yeah these are the best so uh you see again similar conclusions so a number of parameters in Zig also perform much better than other proxy for 5% so this is sort of the first longitudinal type of evaluation that we done and we could see where different uh of these Proxes therefore different approaches stand when compared against this U this U uh Nas benchmarks now everything I talked about here is for CNN and for this type of regular architectures in terms of here we evaluated over 100 VIPs you see and if you look at these results without too much attention even you could see that these two are the winners so in other words the number of parameters and the number of flops perform better than than all these indicators uh for the image data set as shown here so these are pretty you know uh interesting results because it's hard to say that the simplest ones in the end do better or best sometimes for um V Vision Transformers right so the there are many explanations that can be found behind it probably one of them being the fact that none of these proxies cover all three characteristics some of them have limitations because of the underlying assumptions anyway the the result here uh is interesting and tells an interesting story I think last but not least we have these results on uh two uh Hardware so the first one or unconstrained this one is constrainted with Hardware parameters so this one is an Edge GPU that we used we uh use the Jetson Nano PX to uh and you see in this results so you have the test accuracy on the Y and different uh uh uh approaches so this is the ja Jacobian covariance this is the S flow number of parameters so on so forth and the interesting thing is to see if we set a certain limit so we make a problem very constrainted we put a very tight constraint on energy okay say uh 10 Mill here so you see in this case this is the only one that fails you see this is the ground through and none of the data points here you see gazillions of them in the gray area here none of them get even close in terms of quality compared to the ground througho all others perform decent decent see over here over here over here but if you relax a little bit energy con you move it to 20 it's only three approaches that are able to satisfy the constraints meaning to get the blue points as close as possible to the red limit and these are again the the uh number of flops the number of parameters and ziko so this is interesting so if you have a problem with a real constraint no matter what point you'll get will be somewhere in the gray area only this tree will be able to satisfy that constraint last but not least uh on a different GPU so different figures Al together GPX 1080 uh you see the the story is pretty similar so we in this case is for latency uh if the problem if the limit is around 50 milliseconds only these are very close to the par Optima only this and only this so again this Z approach seems to be generally the best for this kind of Hardware oriented um Nas If we think about the future what do we what do what do you think in terms of applicability of Nas approaches to these kind of things I think a very general direction that will cover a lot of research potentially is this geni type of optimization so in this case the automated architecture optimization is a big asset so it can help identify optimal architectures for generative models such as Gans or vies or Transformers and the process can lead to better architectures improve improved performance obviously resource efficiency and in all these cases the zero shot can provide a very quick evaluation and you don't need to spend a lot of time in training in order to find out that your architecture is actually not good enough to get the best result uh efficient fine-tuning is another Direction a lot of stuff going on these days follows this Paradigm you have a foundation model or whatever and then which is fully trained and very well verifi and then you only find unit as needed again um we we can benefit from um uh using this zero sh proces to to see the importance of different parameters in this case and um only the fine-tuning that are is done for the parameters that are important are going to matter in terms of quality in the end um there is another one uh data quality that can be used with in conjunction with this approaches to determine which one has what kind of impact on the accuracy but I would like to just take the last couple of slides to re reiterate over the important things that are promised to this so is the time efficiency is the interpretability and is the effectiveness that we can get from this Pro proxies that can and should matter the most um in our results we have seen and I think this is a significant mindset change that we have to be aware aware of is a difference between the unconstrained Nas when we look at all architectures and the constrained Nas when we look specifically for some uh Hardware constraints either energy or latency because um in this case the solutions might be quite different as we see in the preliminary evaluations and there is obviously the good news is there is a plenty of room for improvement this can be used to uh task in including image classification object detection language processing and many more so this is great in fact if I remember correctly there are even today some papers that will get into this new applications uh that will be presented last but not least just acknowledge my contribution contributors so we have the um most direct contributors to this former students that uh work on this problem space one of them is here uh and then for more details on the stuff that I presented and primarily more results in in terms of experiments that we did are in the T pami paper that app appeared this April and this is the title you can check it out thank you uh thanks thank you r that was a a great keynote and we are a bit over time but I would like to give people an opportunity to ask R any questions and yeah me too are best way the end you will up that you need toare what you using so you need a benchmark unless you start sear space and but my question would be what in is in your opinion speaking about realization the best have right now uh so just before R an that I'm just going to try and repeat the gist of it for the zoom basically the question was regarding uh zero shot ask for video use and what's the best uh Benchmark for generalization in that case if I got that um it's a very good question certainly a question that keeps uh working on this problem um I don't know a benchmark for this so the ziko that I mentioned and has a good U results for generalization is basically because of this kind of uh robustness to to to the parameter variation um but there of Benchmark to use I really don't know and uh I'll be happy to keep in touch to find but it's a very good Pro very good question and problems space work on yeah I have another question about discussed several proxies I've been considered using multiple proxies to create a better predictor of performance um yes and no not directly for this type of applications we try to use for bioimaging um and the results are mixed and and um my explanation for that is because we didn't know how to use them uh but it's again a very good question yeah and just um because we do have a microphone then it might be easier for have question please go to the microphone it doesn't seem not seem to be working repeat question okay uh my question is about concept of zero shot and gradient based meals basically zero shot means we don't have training we don't have training we don't have weight if we don't have weight how no you have weights because you take a mini batch or you can take the the a couple of pairs of the at the very input so you assume that is the initialization step you don't have training per se the full-blown training but that's how you compute it's just initialization that use yeah tfy but you can um correct me if I'm wrong it's one zero shot it's training in the search phase find find theit you do train at the end but if you just don't train my your surf so you only train one Al one network and you find it and so when you train you know you have a reason to train you know it's good and SE like also involv optimization of hyper parameters like loing rates R regularization etc etc like uh how works with that because sometimes like all the hyper may impact the results much more than if you just yeah change number of channels yeah well we don't have a silver bullet for that so to speak so we don't have a uh approach to say exactly this is the best rate the learning rate to use for that so it's essentially an iterative process for that but again the nas for S focuses more on uh getting the most promising architecture and after that the actual uh result that you get with it say during inference or whatever is a separate story because that is part of the training process say again please you said I could not understand what you said one like normaliz performs and is kind of like normaliz right normaliz uh is the ratio between the the yes yes yes and theed Vana that we can see that that used in the first exp so I'm curious we can combine another I think that in comparing Z with the snip yeah well result wise yeah we did and yeah qualitatively I don't know probably because this is captures second order effects better would be another explanation but result wise there is no question it it's it's state of the art it that clarifies well I'm for sure snip has been used for pruning I'm not aware Z being used for PR pruning I think was just proposed here yeah yeah we have to take it offline and so because we I think I said a bit over time I'm going to uh thank R once again for a great keynote thank you and I'd like to remind everyone that PO session is starting now um it will be until 10 to it will finish at 10: to 4 unfortunately post session is in the other building in 4E uh I think it's 338 e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e um as we are getting as it is now 102 and we are still we are still waiting for people to return from the post session uh since we did uh run a bit over going to wait a few more minutes and hope that uh people find their way back and don't get lost e e e e e e um constanti are you in the zoom hi uh yes I am can you hear me yes can hear you great I'm going to stop sharing and then hopefully you'll be able to take over okay give me a second can you see the slides uh yep but if you just give me a second and swap the screens around uh okay so I think that should be fine um so this is our first paper which is uh the devil is in the discreation discrepancy robustifying different Nas with single stage searching protocol by uh constanti saoto um take it away uh okay uh just to make sure can you hear me um yeah sorry canar you away from the mic okay so hi uh my name is constant botco and today I'm going to present our paper titled the devil is in the discretization discrepancy ifying differentiable n with single stage searching protocol I'm sorry for such a long title um yeah okay so in the in the in in the differentiable nas we construct a differentiable search space uh so that we can optimize it using gradient best methods such as SGD uh and to simplify the terminology we divide uh we decompose daa framework in to three stages uh the searching stage where we optimize over this differentiable search space and we try to find an optimal continuous architecture uh then we have the decoding stage where we discretize a continuous architecture and then we have the retraining stage where we take this this architecture and train it from scratch for a longer time yeah and uh indas we associate a single SC scolar par parameter with each architectural choice and if we group them all together uh we can form a so-called weight sharing super net that David has already mentioned and um the supernet contains all possible architecture candidates as its sub networks and we want this supernet to be differentiable with respect to both operation weights and architectural parameters yeah and dasos in general is very efficient but suffers from a few issues uh the issue that we study in this paper is the discretization error and this occurs when we try to discretize a supernet that has a high entropy of architectural parameters uh this issue has been already mentioned and talked about in the literature and here we show a example uh where we try to discretize a high entropy uh architecture and it leads to some um errors yeah and ideally we would like our uh supernet to have a low entropy architecture so that this procedure does not induce any additional errors yeah um so we have a few contributions um so first we we demonstrate and quantify the the discretization error uh and um in particular we we uh demonstrate its its relation and uh to the entropy of a supernet architecture and then to to eliminate the uh the issue we introduce a no single stage searching protocol uh we have some other contributions but we do not have enough time in this presentation to cover them so if you're interested we ask you to check our paper okay so we have a bit unusual approach in a sense that uh as opposed to other methods we conduct our experiments in the semantic cementation task on the cityscape uh data set and uh the reason is that um our method is suitable only for a particular design of the space and so uh yes and we found such a design in the sematics meditation task but our approach might not work as well uh that well for example on the image classification um yeah uh so first we measure first we measure uh the entropy of AR architecture at the beginning at at the end at the end of the searching stage across different methods and tasks and here we can observe that this entropy does not change much from the beginning to the end uh of the searching stage which means that it can cause a discretization error yeah and then we uh then we try to then we try to quantify the discretization error by uh measuring performance drop after removing a a certain number of least important architectural parameters and you can think of it as a simple proxy of the discretization error and um we conducted these experiments for super Nets trained with different entropy regularizations and actually these different curves on both plots correspond to supernets trained with different entropy regularization yeah and then in ideal scenario if our supernet has zero entropy architecture then we wouldn't see any performance drop uh but in this case we actually observe uh a sudden performance drop and additionally we also observe that is correlated with with the uh entropy of the architecture uh of a superet yeah so um then we uh considered uh penalizing High entropy explicitly so here we did experiments with adding uh adding an entropy term to the L function and people have already tried pnik entropy many times in the literature in this form or or another uh so we we are not the first to do that to do it of course but here we as you saw before for lower entropy architectures uh suffer less from the dis from the discretization error but here we can actually observe that um strong entropy regularization actually leads to worse results in the searching stage and um you can think of it intuitively as harming exploration in the seing stage in favor of having a more discrete architecture yeah so so how to get rid of the discreation error completely well our idea is simple it's it's to just not discretize anything at all and so in the standard dinas framework we would train a continuous supernet then we would uh discretize this continuous architecture to uh to a lightweight disc architecture and then we would change this discrete architecture from scratch uh here we instead uh we we do not discretize u a supernet and instead we just freeze its parameters and find tune of operation weights so here we treat um super net optimize super net as our final model and we will use this model in the inference um can think of it also as as having a continuous architecture instead of uh projecting it into into the space of discret architectures and um so this approach um is seemingly um naive and overly simple but in fact uh we we uh we found that under certain conditions it can be actually pretty efficient and namely for a particular design of the side space uh we observe that there's there's not much difference in terms of fting Point operations and number of parameters when you go from a dense continous supernet to a discrete architecture so it means that um we that this actually makes usage of supernet in the inference feasible uh in this paper we follow the the nondense variant of the sh space introduced by dcas and this set space does not include long range connections uh for those of you who don't know DCS is arguably the most successful application of this of DS to the sematic cation task um so of course our approach has a slightly higher inference cost because we do not discretize anything and we have ADD superet but as we will show later we save a lot of competition Resources by by by by funing weights and not training it from scratch so uh as you can see we also considered a few different variants varying in size of our model okay so um now let's look at the uh results so here we comp our method to to the state-ofthe-art U dinas approaches in the cementation task um first let's look at the computational efficiency of these methods uh for all methods except ours we provide the number of lops for their final discrete lightweight architectures and in our case we provide the number of lops of a supernet because we treat supernet as our fin architecture and unfortunately we do not have an exact number of lops for the non- variant of DC and we couldn't um couldn't comput it but given the search space of DC it should be roughly the same as the dens variant of dcas um yeah so here we can actually see that our that instead of using a den superet with the say space uh introduced by DCS our supernet is still computationally efficient and there are some small differences but it's not an order of magnitude difference yeah and um then then we compare uh the nond variant of DC with our medium model it's a fair comparison because both models were trained on very similar or basically the same set space but also they have the same heavy parameters and here we observe that our supernet actually outperforms the final architecture of of n variant of DCs uh and we believe that it shows a potential of our method and that is capable of achieving competitive results uh unfortunately we did not have enough time nor compute to um to to conduct more experiments or to implement train and tune the den varant of dcas so this is something to work on uh maybe in the future yeah and as I noted before we save a lot of time but by not uh training a discret architecture from scratch so uh we just reuse already trained weights of a superet and we just fine tune them for a few more EO and additionally in the standard dinas framework uh due to because of the discretization error there's usually a lower correlation between searching and retraining uh results and uh and because of that if you want to actually get the optimal architecture because of this low correlation you would in the vanilla denas framework you would usually have to run this whole searching retraining pipeline a few times to find uh the absolutely best architecture but in our case we do not have this correlation issue because we do not discreet anything at all yeah so um we we contacted like a few novel digesting experiments which provided more understanding on thetion error and in particular we showed its relation to the entropy of supernet and then we proposed a novel searching protocol which which uh which eliminates this issue completely and although our method does not outperform state-of-the earthart uh methods we believe that it is a good starting point for a further study on efficient uh single stage proxyless uh techniques yeah and uh and we will probably publish our code very soon and thank you for your attention thank you very much constanti um so I'm going to open the floor to if anyone has any questions or if anyone has any questions in Zoom uh a question that I have uh first is um so you said that you used the DC NES um search space uh did you investigate any other search spaces for your uh paper um yes so so we start actually from the how to De sh space and then we considered this uh this like this goal of training a network in a proxil way by using just a single stage and we we did not find any other suitable set spaces other than DCS but maybe there are some uh we just decided to use the DCS because it's it's it's a good comparison for us because it's state of the art method okay thank you uh does anyone else have any questions oh maybe maybe like following F following up on this question for example like the dcas does not um we can the idea is that we can train supernet with roughly the same parameters as we would train the final Network and in the case of like Auto deeplab uh we would have to use like much lower image C and much lower Network to be able to fit it on the GPU so um the so the same uh training protocol shed across searching and retraining makes it actually viable for us okay thank you as I said any more any other questions okay thank you once again uh constanti um we will move on to our next uh presentation if um you can give me that control okay thank you so how to give your b control uh if you just um stop streaming and then mute I'll eventually figure it out cuz it's on the other monitor this thanks once again to constanti hopefully now my screen is shared again um my name is and I'm going to present our oh s that's the video that I was sent which has been in if it's going to up there we go um so next our next presenter is tan and you're here and that's great and let me just get your slides up oh why is it doing that do% PDF sorry there are some technical difficulties uh should hopefully be a few seconds e e new sh sh now the technical difficulties are done uh I like present t with cycle ganas which is new architect search for cycle Gan it should be recording itself so hello my name is tonan from Kore University and today I'm going to present about the psychness which is a neur architect search for psych so this is the presenter it's me and I have a long hair at the time I'm from Kore University and interested in Automation in machine learning and this is the homepage I guess the C code is expired um so let's go on the motivation so actually n become more s more and more successful so for the CNS and RNs and in Gans and even for the Transformers and recently for lrms right so and at the same time the neural networks are become more complex so there are become multiple models or branches for the multimod learning and their objectives are also become so different because they use various like losses or techniques and more importantly they are all handcrafted so we start with Gans that Gans have uh two neural networks which is a presented in 2014 I guess and there are some nwor that called Auto or aers NOS that deals with the architectures of Gans so to uses RNN controllers Anders no uses some differentiable NOS approaches so basically there are some NOS for Gans but not for the cycle Gans yet so there are some more neuron networks because cyan is basically a two paired Gans and the task is harder because Gan uses an unconditional image generation but cyan targets an image to image translation so let's go on the methods so before go on our method I will explain about the cyan briefly so cyan is for the unpaired image to image translation and you see that there's there are images A and B from belong to the finite set a script A and script B so note that they are finite unlike the Gans because Gans semper the noise from the gaan noise because it means that their inputs are infinite and cycle G has three roses called cycle consistency and one is adversar like in Gans and one is for identity so it has triple more objective than G and we choose and search strategy called differentiable neural architecture differentiable architecture search so techical biev optimization in G are proposed but it's not directly applicable to the cycle Gans because dividing the data set for the Gans is like easy because for the G tesk and Nest test they can just sample the noise but Cy Gans has finite inputs so it's difficult to divide where like if you just give half and half to the Nas and cyan TK or maybe you can more give more data to the cych like so it's important to dividing the DAT data set so we are forced to use like single level optimation because it's necessary and also powerful you can see that the architecture and neur Network weights are simultaneously optimized and it's shown to be working with CNN but not in more complex uh Frameworks like Gans cycle Gans so the main concern is that Gans are known to be unstable and even with the single Lev optimization it will be more unstable so we decided to use the differentiable no we design our search space as follows so we avoided to use complex structures because it is widely adapted and people are um giving many efforts on designing the complex CS but we don't want to do that and it makes uh building super networks more inflexible so we adopted simple cell design um based on the resonet reset cells like residual cells so basically there are some residual connections and we have two operations o1 and O2 which has some candidate operations and yeah can few candidate operations I explain about it in next slides so each candidate operation is consist of um following this pullings or convolutions and interpolations according to its role because Cy again is a image to image translation Network it gives given an image it outputs an image so basically they the generator has an encod cell and residual cells and decoder cells but discriminator should put a probability so just has in color CS so let's go on to the experiments uh we compared cycan and cyan in various data sets so basically the original assort of cyan provided some six or seven data set and we tested cyanus basically conducted Nas on those data set and evaluated on those data sets so we vared the number of channels of convolutions you can see the xaxis is the number of channels and Y XIs is the feeds which is evaluation metric for the quality of image so the feeds is the low the better and green one is the psychness and the red one is the original cyan you can see that in most data sets cyanus outperforms the cyer again so let's see more details in the uh in the mod ratio so you can in this table or graph you can see that for the channel number of C 2 and 64 We compare the ratio of moders so basically the left side is the generator a and the right side is the generator B so generator a translates A to B and Generator B translate B to a so you can see that in most data sets the size of models are similar but in the horse to JRA and iPhone to DSLR there are their model size are different so note that the green one is the cycus you can see that the difference are clearly visible so we analyze about it it seems that yeah there is a test imbalance so in first Jabra it is reported that there is a imbalance in the data set so basically cyan is a image to image translation and it uses two unpaired image data sets and if you have horse data set and Java data set and if you have more like number of images in horse then translating maybe the difficult uh the difficult may be different so we conclude that jbra to horse is harder than horse to jbra and it coherent it's cently reported in the the cyan this so you can see that jbra appears to be more closer pictures than horses so translating jbra TOS requires more effort and similarly in I ion to dsrr we can see that there is an unbalanced model so we can guess that um yeah increasing resolution seems to be more harder than decreasing the resolution so we see that the model ratio affects the performance so we adjusted the model size of cyan because actually the if the model size is the only thing that important then the architecture like convulsion filter size and these kind of things may not be important so we see the effects so basically the brown one is brown one is the model size adjusted Cy again and green one is our San architecture and red one is the they have the same size for the generator A and B so you can see that adjusting only the ratio ratio is not important like it's not enough to deal with the data imbalance you need to search for the filter sizes and Etc so in conclusion we build a cyanus framework that differentiable n for a complex framework called cyan and we used a simple residual cell design design and we use single level optimization for finite data set of the cyle so we see the performance improvements and it's Cali with the number of channels and more importantly we see that cyes the architectures are able to deal with data set in Balance so we observe that it horse tobra in ion TSR and especially in horse tobra we found that the previous reports on data imbalance which is the which is published by the author of Cy again so thank you for listening and yeah this is my presentation uh thank you very much for the talk um I'm going to open the floor to any questions and any questions that were from Zoom uh first a question I wanted to ask is um as you mentioned that Nas hasn't been used in cycle Gans before and a question I want to have is how did is how did you adapt the current Nas search spaces and Nas methods to a search space that works for cyclean models yeah actually I first tried some previous n method so like using R controllers or other verion n the SE search spaces but um it seems that cyan is quite heavy and requires a lots of computations so yeah I yeah um I see some quotes but I basically build from the bottom yeah uh so do we have any other questions okay uh well thank you once again and we will move on to uh our next presenter e e e e e e uh sorry we do not hear anything right I'm sorry uh I keep going right okay so predictor based algorithms I uh algorithms that train a proxy proxy uh gcn for example a graph convolutional Network to predict the validation accuracy just given the topology and they are then used to S to narrow down the number of architectures uh in the search space during the years they were very much used because they are super fast but usually they are used uh under a single data set and mostly this data set is cipher 10 assuming that the found architecture will gener as well for imag net later or recently some papers focused on improving the generalization ability of these predictors by focusing on different tasks as in the first paper Arch graph or on different data sets differently from these two last papers that I mentioned we um we propose again a a method to improve the generalization abilities but we do not deploy an encoder set as in The Meta Meta d2a paper because although being super well performing uh using an encoder set to provide information about the data set you're are working with limits the applicability to only images and not videos for example we rather instead use a very simple method which is providing the vertex shapes information as additional input to the predictor uh then uh Arch graph instead requires the tuning of the predictor to move from one data set to the other and rather than fine tuning the predictor we decided to deploy the validation accuracy of networks not training until convergence because we show that this helps a lot finally as a contribution we Define a new uh search space or better we Define a new way of restricting the search Space by exploiting the clal product going to the search space definition okay so um we created a data set first of all because uh the existing benchmarks do have architecture stain in different data set but usually they're all data set that are down sample version of imag net and we wanted to include also something out of distribution which is fashion amist so we took the randomly wire search space and decided to restrict it in a meaningful way by exploiting the chronical product which allows you to Define two skeleton matrices that you can multiply to with random Matrix to have a reset like uh structure we sample then 2,000 architectures and trained them for the four data sets to get all the measures uh in our search space we did not choose to look for a fixed number of channels because this would have limited scalability but we kept instead a more flexible approach by defining layers that doubles or half the number of channels we then trained the networks with the hyperparameters we found for reset 18 um before presenting the result of the gcn that we proposed I would like to show you some results in the data set because they give you an idea of the ground truth and what a predictor can do when dealing with different data sets so the first question we asked ourself is are the data the data set that we use representative of the suring they induce it's easy to say that imet is a complex is more complex as a data set than Cypher 10 but does it hold the same for the architectur trained over a specific data set and to answer this question we um analyze the ranking correlation through the normalized discounted communative gain that is a ranking measures that penalizes more if you rank wrongly highly relevant objects and key message from the table and from the plot is that um if you use the ranking of more complex data set you will generalize better than simpler data set which goes in the opposite direction of the common assumption the second message from the plot is instead if you employ the ranking of fashion Mist architectures you go out of distribution which justifies the need for benchmarks that involve also different data sets architectures stained and different data sets uh second question is do the networks that we have specialized during training so um we again check the ranking correlation of the networks as training was evolving and key message from this slide is that we notice that the top performing architectures which are which is the black the dark purple PL stop changing their rank way before convergence and at a specific moment which is where we dro the First Time The Learning rate then we notice from the middle plot that the average performing networks keep changing their rength until the end and they are very influenced by the IP parameters and the worst one are such since the very beginning which means that in a Nas pipeline you can use in this context the validation accuracy of networks not trained until convergence and as you can see in the third plot the predictor will still be able to rank correctly the networks so given this I will show you some quickly some results so we trained our gcn in a binary Fashions meaning that we do not predict the validation URS but we rank the networks and we gave as input the topology of the networks the um encoding of the layers as usual and as in addition the vertex shape so the shape of each layer normalized with respect to the maximum shape that we had and we decided to use floating numbers and not one hot en coding to have a sense of distance between vertex shapes and then we trained the predictors with the validation accuracy that we had at EPO 40 and not until convergence and basically we show we saw huge improvements especially when fashion amness was involved and also when without distribution actually without distribution shift compared to other approaches uh we again saw big improvements in terms of fashion amist and cyppher 10 especially and before concluding I will also to spam a little bit the another paper that is in the main conference that if you're interested with new architecture search you can pass on Wednesday to M posters and thank you thank you Sophia for a great talk um again I'm going to open the floors to any questions a question I want to ask first vot is you mentioned um the use of different data sets apart from the typical standards of Cipher 10 and image net um a problem that to me and some colleagues have identified re uh over um the past few years is sort of Reliance on these two data sets uh do you it seems that in your work here youve seem to have identified a similar problem where using data sets but do you think there needs to be more of a push for in more data sets uh absolutely I think that there should be more Benchmark for Nas in general not only related to images but also as I was mentioning before to videos for example because most of the approaches are related on these benchmarks and you you need them to be fully comparable so yes I guess that that would be a good direction um so do we have any more questions I think thought I saw hand earlier so uh the predict we use a three layer graph convolutional network uh so the gcn layer was defined mathematically in a in a paper from Welling and we simply Stu three layers it's super fast to train and takes like for 2,000 architectures roughly two minutes okay uh thanks again sfia for your presentation and'll move on to our next presenter add which okay so I'd now like to introduce Chon Chen uh with the his paper up M yeah uh thank you and uh good afternoon everyone and the title of this paper is uh unified proxy for new architecture search not doing should okay thank you yeah okay uh so uh existing us approaches can generally be divided into three categories including multishot one shot and zero shot uh however the Multi Shot and one shot n would be take a longer time for training and so the zero shot approach uh however uh it can lessen the computational burden uh but um each pracy performs often well for only specific data sets so uh in this work uh we hope to uh unify uh multiple zero cost process yeah uh since a single zero process and to uh accurately calculate the quality of architecture without the need of training uh existing proces can be roughly divided into two cases uh gradient base and GR free um in our work uh we uh propose a general method to combining multiple uh zero cost proces uh to form a new uh zero cost uh uh approximation and uh uh in the current study we implementation of zero Course prices by Nas live including the following kinds of zero cost process and uh but our approach can be done for uh other kinds of zero cost prices and our UPN um start from the issue that no single cost price is suitable for all Benchmark data sets and so how do we can combine existing zero cost prices U and E the more powerful zero cost Nas zero Shan Nas and uh our method is simply combin different zero cost prices by adding them with a set of a fixed combination we that is a linear combination and the approach is easily scalable and uh for example when we combination of a m zero cost pris the combination coefficients LDA one to LDA M are learned based on a small existing Nas Benchmark uh for example we only used the nas bench U to 2 uh 2001's U c41 subset for learning such kind of uh linear combination and then allow to be used for other unseen architecture spaces and uh our approach is called a NP miple proxy estimation an estimator uh for multi-way zero cost process and the method consists of an architecture to Vector act to V encoder uh that encoder network uh requires no uh no uh uh ground truth accuracies but can simply uh perform in architecture to V to Vector um using only the topology of the network and then we have a multi simple M perception to estimate the scores based on the m prais and our steps includeing following three and we will introduce them in more detail so for the architecture to Vector part we adopt the implementation of the VGA approach yeah this approach can transform the architecture search space into a continuous embedding space and uh however currently this method is only uh operable for sale based approaches so our approach is also sale based and uh it represents it as a dag and denote a architecture with a topology graph of verticals and edges and it will use the upper triangular adjacent matric a and one hard operator Matrix X to encode a seal based neural architecture and so by using this approach uh uh the objective of VJ is to um optimize by maximize the variational lower Bound in uh below okay so more details can be found in this paper so use this kind uh use this method uh we can get the architecture embedding here uh it consistent with architecture encoder and a decoder then we simply use this encoder um to end the embedding and per and creating asent search over the embeding space and then finally we uh trans um will convert back our F invading to the uh using the decoder to the search architecture okay so uh in our approach we only require zero cost pry scores to train our multi proxy estimator and another pre precise accuracy of the model architecture so in our training phase we assume that the architecture encoder trained by VGA are available and then so for any architecture it can uh leverage this encoder to convert the architecture into an embedding and so in this embedding and we design a multi simple multi layer perception only uh three three layers three layer perceptron and uh this m u aims to uh learn the predict scores that approximate the zero cost procy scores for example here we are many zero Coy proces that have been proposed uh by many previous papers and U so uh we use this practice uh zero cost practice scores of region and learn our uh MLP and so that we hope that the for the input architecture the output predict score can be uh highly approximately to the zero cost this zero cost process and in our implementation um for training the m PE multi estimator for the nas bench uh two um 2001 Space we take uh 50% of the architecture together with their pracy scores zero cost prce scores for that space we generate U 1,000 uh 10,000 samples for training the MP and uh here we also have the uh how to linearly combine the zero cost proces we as we have mentioned we will provide a fixed weight LDA one to LDA M to combine uh this uh M uh zero cost pris we used and uh how to find this m uh parameters uh as mentioned uh the combination parameters uh we are learned by using uh uh B are only a very small subset of The Benchmark here we use the nas bench uh 2001's C5 10 subset only and uh the optimization criteria we used is candle top that is an order pre preserving loss yeah to find such kind of a weight and then once the weight are fin they are fixed and uh uh then so these weights are fixed and uh finally uh when those are all done we can then perform the architecture search on the continuous architecture embeding space simply through gradient as learning so that's this part so our entire goal is to uh maximize to find architecture uh in the embeding space to maximize the that that kind of weight has been computed and fixed to maximize uh these are the uh zero cost proces we they are totally and zero cost proces we we use and then we want to maximize this this goal simply through uh in fact this simply through gradient as yeah so it's very fast yeah after find ing the architecture in the continuous embeding space we use the uh decoder part uh of the architecture to Vector algorithm to convert it back to an architecture okay so the details of the proxy weight Lambda uh as we mentioned we use the uh tpe to search for the weights maximize the rank correlation uh using the nas bench as uh 2011 c51 validation accuracy and that that's the ways we found we also do some quantization and have four kinds of such kind of combination weights and they are then fixed and then uh we have also used our MLP Spectrum to find an approximation of a many of a many zero cost proces existing zero cost proces so uh by combining that we do the uh grading asent yeah to find the architecture in the uh embeding space okay uh here are the rank correlation we found uh by our up uh n method with several kinds of U uh combination weights and as as can be seen uh the rank correlation uh are all higher yeah than uh the original uh zero cost prices that have been combined yeah and uh to evaluate our performance uh we use the nas bench 2001 search space and also the do search space uh the above uh the above one has the uh ground truths for every architecture but in the dark space uh there does not have the uh exact scores for all the architectures because the space is too large and first we evaluate our Nas bench uh our methods performance on the nas bench 2001 and the left part is the comparison of our approach to the zero cost procy or trainingfree approaches uh as can be seen our approach uh is competitive that can find uh a little better uh architecture than the other uh proy zero cost proy or training free approaches and uh for the comparison of a non- trining free approaches yeah our approach is uh also so very competitive uh although uh a little worse but uh the approach uh is training free our approach is um belongs to the zero cost pry approach uh without using the uh model uh uh uh without using the uh uh training and uh later we uh have done our approach and the for the search on the image net and to do so because image net is too large so we simply perform our grading asent search uh uh this one has been done in the Art Space Art Space is much larger than a Nas bench 2001 okay and so to perform uh the results on the image data data set we first search on the do space based on the ca 10 data sets at first and then uh the architecture the best architecture fund based on CA 10 are then applied to the image net data set and the performance on the image net Set uh uh is here and uh in fact it achieves uh uh very well yeah results on the parts search space for the image net uh the here are our results yeah on the uh internet okay and uh the search time is uh also uh very uh very short and that's architecture we found the fold the do search base because uh in such kind of space um there are two kinds of sales a normal sale and reduction sale that's the architecture we find uh from based on the uh C 10 data set and appli to the image net okay and uh the appliation study of the search strategy if we uh on We performed the ablation study on the C 10 based on the Dodge search space and uh if we use gradient as uh continually search in the embedding space the performance is is the best uh compared to using the evolutionary search or random search yeah and the time is also shorter yeah so the conclusion is that to our knowledge no single procy works for the best of different task and scenario uh with do proposed UniFi processing for Nas uh is a generalized scoring function and uh realized by architecture encoder uh that's simply based on the um uh Network topology yeah uh such kind of decoder and uh design multi perception learning to predict multiple zero cost process and then unify them okay and we show how to find new architect cure through gradient asent yeah in the experiments uh the matter achieves competitive performance and the significant reduce the computation resource required okay yeah so that's our abas work yeah thank you for your decent uh thank you for your talk um so uh we are running a bit behind time but we should have enough time for a question or two uh one that I did want to ask is um your fixed um weights of the proxies you in the presentation you said you learned them from Cipher 10 will those weight will those weights um that were learned from the cipher Cipher 10 apply to the imag net experiments or would you train would you do a short train to find the weights for imag net okay uh we we do not do another training uh but the fix ways we have original ways we have quantized them to one or zero so there are four different kinds of virgins yeah we we do it by using two virgins yeah and one achieve the uh so far maybe the best performance on the image net for the D sear space yeah and the other also very very high performance yeah okay okay thank you uh does anyone else have a question okay now in which case thank you once again and we'll move on to our second off last uh presenter is when canut okay so I would like to uh introduce tonang uh with his paper on CM thank you David so uh hi everyone so uh today I'm going to present uh my work the conect uh connectivity search of convolution operators and uh uh this is a work that that I completed uh when I was doing the PHD at Duke University and uh this is also a collaborative effort of The asena Institute the Duke University and also the University of holon so oh sure thank you so before we dive into the detail approaches uh let's try to figure out the background uh information and also the motivation to do this work so we know that the C architectes they enriched by the dense connectivity of different cusion operators such as the one by one cusions or the 3 by or or even the uh St size uh convolution families uh we can see as the directy graphs can be the most effective tool to reflect the structure of the same architectures because you can simply say that a same architecture is composed of various direct Circ graphs whereas each uh each direct CC graph also know as a d has a building cell with like different connectivity and also different topologies of the uh conal operators so in this work we are going to study how to effectively search through a direct a c and how to use a search algorithm to see better architectures what while we are going to scale this deck bigger and bigger and we are going to scale the search algorithm to more like data efficient and also search efficient scenarios so our approach is based upon the predictor based uh search and let's first try to revisit the predictor search mechanisms so the predictor Bas search algorithm has two phases the first phase is that we are going to sample different architectures to to map the architectures to Performance using a predictor and then we are going to use the predictor as a surate model to try to uh Pro the whole surface and to identify the best architecture so there are actually two phases the first phase is a sample phase and the the second phase is is a search phace however upon the search Within These two phases there are a few challenges is within the predictor search uh the predictable search mechanisms the first challenge is the insuff efficiency where uh uh when we are going to grab the architecture performance pairs to try to train the predictor because each time you're going to S architecture you you will have to trade them either on a proxy or you are going to use the fiful training pipeline this is going to introduce some extra cost if you're are going to run it on Hardware the Second Challenge is that the sub optimal ad hog exploration using the predictor during the search phace because if because if we are going to scale our dat to compos of like multiple notes let's say 12 or like even 16 notes compared to the four notes paradig in the in the D you will say that even if you even if you have a perfect predictor it's very hard for you to utilize this predictor and and try to find the most like the most effective sub architectures within the search space because uh even even you can try to curate performance very quickly you have like too many candidates to choose from and it's very uh very difficult for us to use existing algorithm to reach the local Optimal Solutions so to address these two challenges uh we have actually proposed our uh Paradigm to try to uh utilize the connectivity search and try to uh to introduce two technque approaches to uh make better efforts and try to leverage uh and try to address all of these challenges the first app is the graph isor so this is actually inated by the by by the observation that's the isomorphic which means that the the the graph that represents the same same architecture uh in reality they actually will always end up with the same performance when they are going to evaluate on like on the same training pipeline so ined by this observation we can see that within our search space they are like multile nodes representing the same operators such as the one by one convolution such as the Sur that convolutions and if you're going to convert the the single graph towards an isomorphic graphs like using the swap or using the permutation of Matrix is you can actually end up with the with the same isomorphic graph without the need to try to train the same architecture on the hardware and try to obtain all of the metrix so in this way you can try you save the computation cost by trying to introduce the ismism as a data argumentation for the predictor and the when you're going to uh do this you can achieve better sample efficiency during the sample phase of the predictor baseness and you can use this to improve like the ranking correlations of candidate architectures within the search space during the architecture search algorithm the second technique is the me Metropol F Evolution search which is an improved version of the regular Evolution the idea behind the mhes is that during the regress Evolution we will typically accept the stronger Evolutions or the stronger computations because we de that this is actually better and we are going to keep it like within the whole population to try to evolve towards better uh better Solutions however in in the mhes we Define a more quantive approach to try to accept the weaker Solutions within a certain probability and try to use the temperature to gradually ening the C process and and try to EV local local op solution for better candid in Discovery know that the prop the propose i h will generalized the local search which means that you're going to perform a Rand work like along the whole like SE trajectory when the t is going to achieve zero and is going to achieve the same level of performance as as the evolution research when T achieves Infinity so actually aing algorithm on will guarantee you a better performance compared to existing search resolutions on the like very large scale de search problems so these are the two techniques that we propose to try to address the challenges in cre based mass for the large scale de sear space and we perform very detailed experiments on image 1K benchmarks and and the try to compartive results with like all of the D family architectures and we can see some like uh very noticeable gains uh on the IM 1K top one and and also the top f with a very plain training pipeline so when we are going to uh perform our ablation studies on the silver which means that we are going to evaluate the effect of the MH and also to compare with different SE like different search strategies we can see as the combination of the graph isomorphism and also the MS will give us the best results on the Ser whereas here we are using a piz res 20 will achieve around 92 and another thing that we would like to note is that we seen this very large scale deace random ROM search does not have a very satisfactory performance see due to skill due toity this techniques we can we can find something better that we can more this scope and try to so for theking to this session I think for me really the main thing is that video generation probably Le is very different from video do need to have some understanding of motion understanding ofal I think that is why probably see develop very consistent understanding how generalizable what do you think your method would be to other data sets uh thank you for question I think that for the generalizer prity of the method I currently do not see any constraint because we can simply replace the node to any kind of operators that you can think of and you and once you figure out how to do the projections how to do the concatenations I think that this method can be generalizable to not even image that but also you can try to do some other Vision task like the segmentation like the like the C estimation or something else I I see the only limitation is that because on S or image that the proxy has is quite mature because for example if you don't sample 100 classes or if you reduce the number of epoch you can typically get a very good correlation with the ground choose pipeline if you're going to scale to like more complex task maybe we need some efforts to find a find uh the D proxy and try to improve the search efficiency because the scale actually requires many many samples and even with the augmentations sometimes you cannot resolve like such like one magnitude larger examples in argumentation yeah okay thank you uh do we have any other questions no in which case thank you once again and we'll move to our um final uh our final uh talk of the session um I'll just get that up uh now so uh but unfortunately for our final talk the uh speaker is unable to uh make it today however they have uh provided a v uh a video uh recording of their presentation so uh we'll play that now where your architecture search for efficient of device networks are mobile investing however mobile devices us have limited resources okay so I mean apart from content creation I think content creation is probably the most obvious application right now I think probably it's also going to come down to I would really like this I don't know if somebody will make it hopefully somebody does is like when you want to you like I think today if you want to something breaks so I don't know your refrigerator breaks or your washing machine breaks and you want to repair it you go to YouTube you try to find out okay LG model sk3 is whatever 9129 how to fix the door I really hope that basically we get to a point where right now chat GPT if I ask it like hey I want to do this or I ask any other L it's very it'll give me a bunch of steps but it's not really grounded into what I really want there I hope that we're basically able to get to a point where media generation models are so seamless that if I basically ask it a question that click a picture or I have the Rayband glasses uh whatever ask it that how do I particularly like fix this it's actually able to walk me and generate all the video of how I'm supposed to go and fix it for that particular location where I am or if I'm stuck at a particular thing that I I can't find the SC or I don't know which like which particular B to move or so on it actually helps me do that I hope that fin so for me that is really apart from content creation that is really the thing that that sort of melts together the understanding piece of it because it really needs to understand what I'm trying to solve and what the object in front of me is and then sort of is really generating to a Fidelity where actually can reason about different types of bolts and like how they need to move or like moving on like other applications of this sort here we more clearly demonstrate the differences between our method and the existing methods when sh the comp networks operators areed to simulate competion existing composition awareness only contest The Improv and of the operator with large blocks but hasion otherization that so only the layers of sub can per intake culations other still is floating on calulations that's why the existing Meers result in limited latency reduction for all dur super training we consider the optimization during reir such as C be unfolding and we ensure all the layers are fly compex so the comp being there can be simplified to a contest C during deployment and the whole perform in culations however simulating the deployment behavior is challeng a standard to simulate diffusion ofay in Stand Alone contact network is to merge of C with the moving Aver statistic of B layer training this will bring two problems firstly we pl the cosign similarity of moving SD from 100 random sample Subs as can be seen for different Subs the moving St where it is impossible to update moving separately for billions of St if we only update one new UNP of this superet across all sub it will misad the direction of optimization and result in non secondly the scale value s that mass the broken point value into six SP values an important role in improving the accuracy of comp model due to the differences removing between different STS in ction is required to rec the performance if gr as across subnets means that the F rest of each subnet is different so the optim scale of the same of each is also meanwhile we cannot sh SC value for different subnets because the mon of subnet is large if we sh across all subet the is on a MH scale will lead to nonoptimal results to tackle the problems of non and optimal sear results we propos of the propose of joint the super and there a bch statistic Based training strategy during search the scale of each sub is calibrate with after calibration improve the performance in this manner theose from work isable to find the optimum can be directly and resour constraints now we our B is in detail of those simulate behavior of St which me we need to findy of your CH here ration train and show the similarity between fash of super and the moving of in situation that of is simar the of of B this investigation St bistic based strategy for instead ofing the of we F the of the between the sample RS the same compensation takes the folded asut and is shared layer us to AER to simulate the the proposed Rao the put activation X and output activation y before Fusion can be formulate at this equ supposing each of Sigma is Sig Z and each ofma Z then the scale the origin scale the value of each element being Sigma or GMA is in the network therefore we predict the optimal scale is a linear transformation OFA eval as sh in this fure and Bas reg for a well trained compa Network than to the scale S and G are St so we convert the scale by merging gamma and S into parameters seta the converion SC predictor can be directly at the SPD statistics into the optimal scale in this matter the proposed scale predictor can learn a more flexible transformation yeah so I think um my perspective on this is very similar to like when I remember I was maybe a few years ago and alexnet had just come out maybe it's like now old but at that point it was like okay academics cannot train Alex net on imet this is like too large scale this basically can only happen in four places in the world or three places in the world and so there's absolutely I don't know what are what are PhD students supposed to do I think it's the same thing right then the progression was that okay there were a few people who trained at Berkeley uh the cafe original Cafe like Alex net or Cafe net models that came out and then basically everybody was trying to F tune them get a grip on understanding what these models are but really people were not pre-training on imet now it's common and now like when junan saying that hey we can just do image net but like yes 10 years ago that was not the thing right so I think it's going to be the same thing where right now yes I think most people are not able to train it from scratch because it's a very resour intensive process large models of today are tiny models of like 5 years from now so I think like that's basically going to be the situation anyway so in general that does mean that we should not stop basically working on it just because industry has the resources to train it at scale today it does not mean that's going to be the only thing to do like 5 years from now 5 years from now everybody will have the resources to basically train at least some version of these models and I think the second thing to Jan's Point yes where I think a lot of the Innovations I think industry is really good at scaling things but it's not it's hard typically to like go back to the drawing board and rethink and basically like in like invented thing completely from scratch which has no evidence at all of working so essentially Pie in the Sky ideas I think that absolutely has to come from Academia that's actually where like most of the industrial ideas do come from I think even taking these large models that have been released today and understanding why they're working uh it's a huge challenge I think there's a lot of theory about these models generative models either continuous space or discrete space models which we don't really understand understand which I think it means even at a small scale you can actually do a lot of like work to understand why these models are working or what is really making them tick so I the scale aspect is really to just like create these impressively amazing but I understanding aspect doesn't really need the scale to goip questions yes if you work on it now like five years from now you can actually the scale aspect because that model actually reachable thank all of our speakers again for presenting their work as uh also our keynote uh speaker um before we close I'd like to um uh remind everyone that we have an upcoming competition in previous years we've run an unseen data competition uh getting Nas methods uh having people design Nas methods about knowing what data they're going to be evaluated on and this year we're going to be running it at automl in September um we're expecting to move into phase two in the next two weeks where uh you submit your algorithms to us to make sure that it works on our Benchmark um there's still time to take part so if you're interested please join our mailing list or go to the website to download the uh starting kit um thanks for attending if you want to talk to us about Nest please get in touch uh you can contact me at d. toow to NCL I will also be at the conference for the rest of the week um and stay up to date with future workshops or uncoming competition sign up to our main list or complete our form uh thank you

