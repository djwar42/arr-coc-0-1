---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=GEaSNJOpfsU"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:01.165Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=GEaSNJOpfsU

6633ea64-d14f-49da-8c3e-a964a2bbc9cf

2025-10-28 https://www.youtube.com/watch?v=GEaSNJOpfsU

9c43e170-3873-4278-b670-ac0958cb510c

https://www.youtube.com/watch?v=GEaSNJOpfsU

## GEaSNJOpfsU

## ComputerVisionFoundation Videos

good we're going to get uh started welcome uh to the second workshop on generative models for computer vision last year's Workshop was actually highly successful so we thought why not do it again and uh judging from the crowd that is here and the 45 50 peoples that are online seems to be a lot of interest so that's good um we are let me first thank uh the organizers here uh for helping us uh to set this up yeah our seniors for giving very valuable advice and then pangang helping me to organize this also taking care of the website Ling actually take taking care of the invitations of the seniors of many of the seniors and then to Michaels who actually did a tremendous job in in helping me with the reviews which was actually a lot of work this year because we had uh many submissions on on on expectedly many submissions so yep thanks to everyone um and of course why is this slide not showing sorry I need to show the slide because it's so important yeah it is here yeah um also share the screen again thanks a lot to all of the people who helped us review as well this was as I said yeah lots of work we had like I think around 70 80 papers I don't remember anymore um exactly um short papers long papers so yep thanks to everyone for reviewing I thought a little bit about also putting up a list of all of the reviewers who didn't do their job uh um you know the sounds a little funny but uh I think you know even on this rather small scale with you know it actually created a lot of issues and um at the scale of of cvpr you know I I think as I heard in in the area chair meeting we have this at a much much larger scale so I think our community kind of needs to think about how to how to resolve this and I'm a big fan of public shaming I haven't done this year but uh yeah maybe next year uh let's see you know like it would be actually interesting to discuss this with some of you over the coffee breaks yeah so but thanks a lot to everyone who actually did their job very well yeah so let me give a little bit of a motivation of of why we are here and why we organizing this this Workshop I mean we've seen a lot of progress even like compared to last year right if you if you would compare the current state to exactly the state that we had last year at this time it's like amazing to see what actually happened on the way right I mean for example single single view reconstruction but also video generation I mean it's crazy what we can do right now and so this like you know we've seen purely learning based modelss doing a crazy good job and then on the other side we've actually also had a lot of advances in in doing in in like really fully controlled uh 3D synthesis right with a lot of induct FES so really like mostly on the other spectrum of image generation I full control over you know human avatars 3D phes right even uh even combination with with semantics semantic fields so so we have like a lot of advances in the full spectrum of generative modeling and uh the question that arises with this is how does this now relate to classical whatever what we could call like classical computer vision so semantic understanding of the of the real world right so we have these like kind of classical tasks some of them obviously outdated right but you know where's this intersection between generative modeling predicting the next frame and uh and understanding right in order to elaborate on this question we've invited eight experts today to give their view on this where they are in the spectrum of fully learning fully uh 3D inductive biases I hope to see a lot of interesting discussions here um and uh we also have 40 posters which is uh which is like I think double as much as we had last year let's see if we can continue this exponential growth over the years um for those of you who are interest like I hope everyone of you is interested and for those of you who are presenting so the poster session is actually not in this building right it's in the ARC building over there which means it's 15 minutes uh 10 15 minutes it's in the in the top floor right um yeah we didn't I was not aware of this so there is you know a little bit of effort that you have to in order to get to the poster session we have the poster boards 1 to 40 so make sure to go there this is short papers long papers the short papers are actually um mostly papers that are presented here at cvpr on generative models right so um I think what this will evolve to is to to be like some form of of smaller venue Focus venue for generative modeling where actually people who publish at the main conference in the context of generative models can actually present their their posters in a sense twice right once for a focused community and then for the general community at the main conference so I hope this this will evolve in this direction so for those of you who present posters please set this up during the session hang it up during the session and then take it down afterwards again so that no one has to clean up after and uh because we need to walk over there 15 minutes and then come back 15 minutes we've extended the poster session by half an hour um meaning that it starts at 12:30 and it ends at around 2 p.m so 2m we'll start here again this gives you enough time to talk and to discuss about your work right so meaning you can after 12 when the last session the last morning session you can go down get your lunchbox and then walk over and have discussions over lunch then come back here at two good so that's the schedule of today we are going to end the opening soon um and then we have John luuka and cuu presenting we have a short coffee break and then signing and Alan closing the the morning session we'll have this two-hour lunch break which uh you should uh mostly use also for discussions at the posters then we have Jun and Katrina coffee break and we'll close with Andrea and federo so eight top talks super excited to see this and then we'll have like short closing and then yeah so and with this we have still five minutes to go um and uh I think we should actually start on time rather than so we'll have like a a short break of of four minutes JN Luka can can set up and then I'll introduce him and and we'll start on time at 9: I'm okay so I will I will let you know like yeah this direction okay yeah what case I let me check I know when I join e oh right yeah than I to know okay good all right test no does not work fantastic test not that's not working how do we do this now I mean either you need to shout I mean we can hear you very well online yeah I go get someone okay online but you know I mean here it doesn't test what I go get someone let me check if it's just yeah I think it selected my MacBook Pro microphone maybe no but I mean you know here you're not now it works can you see on the mic no right well anyway think yeah there's no way I will find out what this is okay I can just yeah I think just shout I'll shout if okay test yeah um so I mean so this can go on and off I hope F yeah I mean we're going to shout now for the first talk I guess I'm super happy to have here with us scientist at wave you know I think the autonomous company in the UK for sure in the world as well super or the of the Gaia project right so I think it was in some sense you know really pioneering work that showed what what is possible terms of world modeling and I hope we can you know hear also some uh background Secrets here on on on what we can do with World modeling yes so please welcome thanks thanks Adam and yeah thanks for having me I'm gonna try to like be as loud as I can if you can't hear me at the back wave I'll try to you can't hear me no okay I I'll be louder okay today I'm here to talk about generative modeling of course but more General about embodied Ai and autonomous driving um I always like to start with like this picture when I think about embodied Ai and uh all right great and so what is like an agent or an intelligence that can be out in the world and when I think about the word I think about this picture this is an actual intersection in central London and yes a car can drive there um and so yeah when I think about autonomous driving I really think about this is an embod eye problem we need to figure out how to take intelligence put it in a robot that will interact with the world um in a safe way so so that's just an example of what it means to do that um this is some actual footage of wave car driving in London during traffic and there are Road Works going on cyclists coming and we need to merge into the traffic so I think this this is just one example of the complexity of the real world embodiment for AI and what I what I'd like to show today is a little bit of of high level introduction to how wave is approaching this problem uh I think historically uh the AV industry approached the the problem of Soul field s driving through this AV 1.0 approach which has very clear separation between a perception stock a planning stock each of these stocks are like full of components each component is highly engineered U sometimes rule based uh high wave in 2017 uh we decided to change that and we decided that probably this embodying bed AI is an end to endend problem what that means is it's just one stack it's an end to- endend Network that can learn to go from images straight to actions and why why did we think that that's the way to go so I think the promises of hand to-end driving is are very very appealing to solve this problem when we think about being just computationally homogeneous like this network can be highly portable it's just a neural network in order to use it you just have to be able to do a forward pass on some Hardware uh it's agnostic to vehicles and sensors none of these stocks are highly engineered around a particular sensor type they're just images and videos going into a network and actions coming out from the other end um it allows a little bit to be more agile when it's time to develop these networks because it's just one network or let's say few modules and what we believe is this have will have and has today Superior performance than just human coding uh with human coding I mean writing rules on how to approach uh all these edge cases and uh and so on and again just think about the video I just showed you how would you hand code the rules to solve that situation um and so it becomes increasingly complex to get into detail and we think end to end driving is the way to go so here again some more examples of what do we think is like starting to see that you know it generalizes the longtail scenarios like a car backing for several yards and we're just understanding that this car is not coming towards us and we're just able to follow and keep driving while this car finds a parking spot in busy London we go around it uh and yeah we just keep going uh and another great example of why and driving is very powerful is it generalizes to different Vehicles so some time ago uh we managed to prove that we could train just one network to drive different vehicles and these different vehicles are passenger car and as delivery one and what you can see in the video on the right are actually the one driving behind the car and it's the same network driving both models executing correctly actions and driving around London and it's just one network it's been trained on data collected from both vehicles and it can just generalize from one platform to the other and this is very important because if you want to embody autonomy you can just take it there's a new vehicle you just put the network in and it drives very like humans right if you change your car you can still drive if you if you jump on a van you can drive um so yeah that was like a brief introduction of What wave is how we're approaching this problem and today I want to dive a little bit deeper into some uh research areas that we're focusing on that we think are important to develop embodied AI for autonomous driving so but first let's all let's start with like some open research challenges right uh for autonomous driving explainability will be important we will ultimately have to prove or convince people that the decisions that we're making are sensible and so what's one big challenge is like to open that black box that is a large neural network and make it like more explainable interpretable uh another thing is how does this scale to very long memory uh when you think about scaling an end to end Network and you want to have reasoning over I don't know last 30 minutes of what you have observed uh this is very hard to scale to that kind of uh memory so how do you add how do you add that kind of reasoning and prediction into the network how do you how do you even change the rules of how you drive without retraining the network from scratch and you know you can think about uh rules of the road changing and what do you do you just go and retrain your old Network you recollect your data you can't really do that so how do you extend that intelligence to be reprogrammable and of course end to end requires a lot of data and it's very expensive to get edge cases and these are just some examples of things you might encounter out there like novel combinations like a reflective surface and and a large truck or a zebra in the middle of the road or again this is just a fun uh video of I think a bachelor part in London but you know people sometimes get created and decide to addess as traffic on um and so we think that the solution is to use generative text and video models to supercharge AV 2.0 and this will allow us to solve a lot of our data problems uh add reasoning into the models and as well as explainability so today I'm I want to touch on three main points one is like what happens when language meets driving uh what happens when you have access to a generative word model uh and I would also like to talk about like neural replay so let's start with language M driving so what does it mean when language meets driving it's just when a language model that can take sensory inputs from a robot in this case a car and you can interact it with text uh can really help in terms of like explainability the intelligence that is in in that Ai and also like boost trust and safety um so for this reason like around a year ago well we we launched lingo one lingo one is a grounded video vqa model which took video nipits and then you could converse with this agent uh this was fully uh offline so you could just upload a video footage of driving and what lingo could do is for example do some commentary here there is some footage of driving and there lingo commenting on what's going on and it's clearly like able to explain the reasoning behind why a driver would be doing certain things um at the same time we went with a full vqa demo where we could just converse with lingo about driving and maybe here uh you can't read but uh it's too small but we're asking you know if you see any cyclists or what do you see around you um and it's pointing out at the delivery one on the left uh etc etc and also last year this was extended to full referential segmentation where lingo could point at things in the image while so here is saying what you're are paying attention to uh and what lingo can do is say I'm focusing on the driveable area which is the part a uh to ensure that we stay within the lane and I'm also uh monitoring the traffic etc etc so lingo now can point at things and so this is the level of intelligence that you would expect someone that is driving your car to have and you know we were very pleased to have it and so what we did next is we fully grounded Vision language and action models into a single model and that's what we launched recently it's called lingo 2 this is like fully grounded it's a video text and action model which means that this model can with drive and explain its own action while driving and so this is just I'm going to show like a little bit of this video it's a very long video of actual footage of lingo to driving in London and explaining its own actions and you know if you if you read you can see that it's paying attention to the writings it's able to explain what he doing and why it is doing it um yeah here like you know we're just turning to follow out why we're turning because we want to follow the root uh and I mean here there's not a lot more happening but I can probably skip ahead and see what we get to right so here says I'm accelerating changing lanes um and so on and so forth so I think we believe this is like the first language model is actually driving a car in a real embodiment um other things that we could do with lingo is we can even prompt the model have certain behaviors this is the footage I'm about to show you is from our Clos Loop simulator we actually didn't do this in the real world but basically is the same scenario which is we start behind a bus a stopped bus and in one case we can ask lingo we want you to stop behind that bus and what we can see is that lingo can respect our prompt and actually safely stop behind that bus and leaving space for the bus to merge in the other case we're more aggressive and we just say please overtake that bus and we can see that from the start uh the policy has much higher speed and it just overtakes that bus and this is all done same policy just different conditioning through text so what's next uh what's next is we we believe that multimodal training will be very important because just look at the image on the left yes you can learn something about you can learn that maybe you have to be stopped there's a zebra Crossing there are pedestrians there when you read the text on the right that text might be worth, videos because that text explains you how to behave in this kind of scenario so what we believe is that merging like video and language training into a single model will really boost the kind of intelligence that we can put in our vehicles um next up next generative World modeling um this is Project I'm very close to and uh I'll try to be unbiased but I really like it so I always start with this okaye if you just look at the image on the top it's just a static frame you don't really know what's going on as soon as you watch the video you understand that there is a car that is to our left that is stopped and you're just following the white car in front of us so what does this tell us is in order to drive well you need to be able to predict the future and be able to like reason about the possibilities until you saw that video you didn't know if that car was stopped or not but somehow you knew that those are the possibilities it could be stopped it could not be stopped U not much more right and what we notice is that since 2022 maybe the end of 2022 there's been like a huge increase in Publications in the world space of world modeling uh specifically for like autonomous driving um and yeah I just like to point out that that way we've been thinking about world modeling for a very long time where I think the first work where we demonstrated the use of word models in a real robots dated back at 2018 uh where on our old vehicle a small tweezy we managed to train a policy fully in imagination to drive that M Twizy down a country road in Cambridge and from there we went through like fiery which was a paper published in 2021 it's a world model that can predict uh the location of all the other vehicles in Birds ey view in end of 2022 we published a paper called mile where it's the first time that we merg World modeling uh with a a driving policy on a larger scale and in 2023 like a year ago cvpr we unveiled Gaia which is the most ambitious World modeling project project so far that we've done a w and this is an auto regressive Transformer model that can uh generate and learn from uh videos it can be prompted with text we can control uh the ego agent Behavior Uh with actions um so here are just some examples of uh what we saw after we train guy like we started to see the kind of emerging behaviors where Gaia could understand that starting from the same point there are multiple plausible uh things that you can do this Cas so here in this case like you're roundabout you can go straight you can go right and G can just imagine both other things that we so interactions with other agents can be different and this is similar to the case that I show before when it's real world this is fully generated so Gia understands that there is a car is sticking out from a side road that car could just decide to back or just merge and both options are valid and Gia knows about it um then what we can try what we try to do next is like even simulate some edge cases so what we did here is because we can force the ego agent Behavior we forced our car to go on in the opposite lane and what guy imagine is that the car the oncoming traffic would react to this to try to avoid the Collision that's that's plausible that's what I would expect the other car to do um and none of this is scripted is just fully generated and Guy understands all these Concepts other things we could do with guy control the generation and generate samples under different weather conditions or time of the day all this done by prompting with text um and you know we can do snow in London which it's not that common or all of these things and ultimately what really impressed us is like we could generate minute long videos of plausible driving this is a 2X speed uh and this is fully unprompted unprompted we just as G throw draw as a sample and we kept going for like minutes and minutes of generation and yeah it keeps going it's everything is plausible and there's always something interesting happening so we were very happy with it and yeah last but not least we can condition with language we can condition with action when you start combining this you start to get close to a place where you can control what you generate quite well like for example you would just say you want to accelerate and steer a little bit to the right and there's a bus 15 MERS in front of you and when you do that you generate a scenario where you overtake a bus um one thing I haven't we we haven't shown in the ga one launch but it's something that we've been working on more recently is also like controlling other Dynamic agents because the same way we control our Behavior we want to control other agents behavior and so here are like just some quick examples of what we can do with it which is we can just Place agents in different parts uh of the image and everything else around them is generated according what we can also do with this and that's the top right corner is that we can actually take a real image and we can decide to imp paint agents in it and this is the example like there we just have we would like to have a vehicle in there and that's the result there is a there is a car added to the scene everything else is the same the car is there now and how does that look when you extend this to like videos and multiple cameras and it looks like this so what you can what you're seeing here is every everything that is starting in that scene it's from a real video uh and all the agents pedestrians cars that you can see been fully injected by guy they weren't there and now they are um and we were really impressed to see that even extending this to like the multicamera setting you look at that black sedan it's correctly placed in the word in the three cameras and none of these has been there are no inductive biases not 3 inductive B is just fully learned from video so what can we do with this now uh and we use this in conjunction to lingo 2 uh to basically prove that lingo two can reason about the correct things so what you can see on the top is like some original uh scenario that we collected driving in London and that's the explanation that lingot to will give and the action that lingu would like to have right so in top case like says we're just seeing increasing the speed to match the speed limit Pro is clear ahead of us what we do is then we inject a car in front uh now lingo wants to slow down and says I'm slowing down to match the lead uh vehicle speed so this is allowing us to do this counterfactual reasoning where we convince ourselves that this policy like the lingo model can correctly drive and also reason about it similar example there in one case we say we're stopping because there's a red light red light ahead as soon as we put a c in front the red light is still there but of course now we're stopping because actually here it says we're stopping because there is stationary traffic in front of us um last but not least again we can go even more towards like generating safety critical scenarios here are just a couple of examples where uh we set up the scenario where we are overtaking some large vehicle like in one case it's a bus in the other is a van and there's J Walker on the street this is fully generated uh by Guy so what are the opportunities ahead is like again guy is very intelligent understands a lot of things about the world how it works and you know integrating this with our onroad driving is is really important um we're going to think about further scaling up going to multiview generation I mean I've show some sneak peeks of some of our three camera generation but you know we want to extend that uh go multisensor and you know learn this use this learn Simulator for training and evaluation of our driving models um so trying to wrap this up uh but basically that's what we believe is like if we just collect data in the real world that's how we're going to scale at collecting edge cases right we're going to become Raider and Raider and Raider what we think is to really supercharge this we can use generative II and something that is neural rendering so together when we merge generative AI with neural rendering we can really boost the number of edge cases that go into our training set so let me briefly touch on neural replay and novel view syntheses uh because yeah I'm really really glad to like huge congratulations to the team that and build yesterday the new model it's called prism one and this is like a new model that we use a wave to do sin reconstruction for dynamic sins and these are just some examples of dynamic scenes reconstructed uh from a London data and we similarly reconstruct sces in the Silicon Valley all from the same model and what we can do with this new we can start doing for example novel view synthesis and here are just some example of a reconstructed scene where we can freeze the position of the camera and we can let time roll forward and backward uh here's another example exle where instead we freeze time well you see in all videos sometimes we're freezing time sometimes we're freezing the position and letting the other one roll but then just to demonstrate like how good novel view synthesis can be once we have these scenes and show you some more examples and all of these models are just learned from cameras only like this is camera image like surrounding cameras and you can notice like in this example like we freeze the position but time is rolling and traffic light is changing color and similarly here if you pay attention to the blue car you can see the wipers are still going uh so it's like the attention to detail is really important here and we're mind blown by these results and here are some more examples where we can even that we can even fit in the scene some of um geometry and motion prediction so yeah just to wrap up together with prism one uh wave also launched wave scenes 101 this is a public available data set and that we think and this is a public available data set that we came up with when we were developing this model the reason why we did that is because we couldn't find a public available data set that would really fit our needs and what this data set uh this data set is special because we think it's the first data set that has a full evaluation camera and the camera rig is positioned in a way that the two two side cameras contain all the information that you need to reconstruct the middle camera so that camera can be used as as a test set for your novel view synthesis so we decided to release this data set it's 101 driving scenarios from the UK and the US uh the link is there uh or just go on the way website uh you'll find the link if you're interested in like neural reconstruction please play with this data set and let us know what you think so finally yeah unlocking embod it scale so here we touched on a lot of things right what happens when language M Driving what happens when uh you learn models of the World by predicting the future what happens when you use neural reconstruction so what's next is like we believe that all these things can come together into a single large Foundation model and what that means is we can train this model on diverse data sets that have access to diverse sens inputs that solve diverse Downstream tasks and even in diverse embodiments canot just be limited to uh driving vehicles and just pre-train this large Foundation model and that's what we really believe will unlock the full potential of a a 2.0 once all these things come together and I think a way we're like kind of leading the revolution of embodied AI for autonomous driving uh with like language word model neural replay yeah really supercharging av2 point know and Foundation models again they will unlock the full potential of it so stay tuned and to finish my talk on time H yeah way we're hiring uh there's a link for some open roles if you're interested in like having a conversation please come meet the team we have a boot at the Expo that is starting tomorrow so come say hi I'll be there a lot of the wav team is here this week so come find us have a chat with us and yeah we're always glad to talk thank you very much fantastic talk U we have plenty of so we have resered 10 minutes for questions plenty of time um let me start maybe with one so um how fine grade do you think could you make this control also in terms of generating edge cases like you know upside down cars or cars opening doors or kids running in Superman costumes across the street is this like is this something that you will get easily to or yeah I think uh I mean easily is maybe a strong word uh but I think again when you start combining the power of language and supervised video modeling I think all these edge cases become less Edge casy right because what you can do is at the moment we can place a car here and there once you merge this for driving you can ask for this car is upside down and when you train this Foundation model or like when you start combining all these Tas and you train them like an internet uh scale we've seen plenty of demonstrations of this kind of Next Level generalization and compositionality of Concepts just coming out of nowhere and so yeah I I mean I'm not going to say it's easy but we do believe it's that's that that's exactly the way to go so scale will actually all the scale and and multimodal training because I think once you combin video training and text training should be a lot easier for these models to generalize to these New Concept and these edge cases okay thanks more questions so you presented that in your models you use you know flip FP like encoders and other language models so there has a lot of work which shows that like models and then models then they don't really have a very goodal understanding of things right they don't really understand uhal relationships and things as B words so especially in the application where these things have real adverse effect you think these models that form um yeah thank you that's great question um I'm just I don't know if online they could hear us I'm just going to try to summarize but yeah we've been discussing about like the use of clip models and the fact that they've been shown to just basically learn a bag of objects and whether these are appropriate for like real world embodiments and yeah I will say short answer yeah it's no um and let me try to quickly go back in fact uh maybe it's just a small detail but yeah we use clip in lingo one and this model is was fully offline so that was just grounded video question answering model and that worked very well but once we move to like lingo two you can see that it's kind of Disappearing and the lingo two is the model we actually embodied so yeah I think you're right um we've seen that other pre-train models that we pre-train are a little bit better at this and so that's what we decided to embody thanks for the question so this is some like this some very early work that we've been doing and so what we've done is we use Gaia to add agents into real driving scene and then in open loop test that lingo can appropriately change its own action and also explain why it is doing that so at the moment this has been like a very I say manual process right because it's not trivial to Define given given this modification that I've done what is that I'm expecting yeah I think you're right and so yeah we started from here and you know we're scaling this and and figuring out but for now is like we're just really happy to see that a model can reason like this and understanding you know I use I I was stopping for a red light and now I'm stopping for traffic ahead of which really gives us that conviction that these models are paying attention to the right things oh yeah I mean uh yeah I can share some uh some funny stories about a Gaia generation so usually like the issue is around hallucination and I mean that's expected I think everyone is working in generative model no and so once you start if you like fully uh UNC like you try to do unconditional generation this Hallucination is a superpower of guia and it's what's allowing you to generate two minutes long videos because it's hallucinating things that don't exist but they're all plausible and they do make sense once you start time this with some other high level controls like you're forcing the car to turn right as soon as you turn you don't know what's there and so if you're foring you are forcing a very specific path Gaia could actually generate something on that path that makes the car collide with what's generated and so if you just go from the validation perspective like that's a little bit tricky and that's why we're kind of think thinking that generative models are super power but we're also coupling them with neural reconstruction to basically get the best of both words I'll say can you it as ask what the and also what happens say any no yeah yeah you can you can actually do that uh actually we could do that even with lingo one even before um I think last year at iccv we had um at our boot we had a demo everyone could just come and talk to lingo about the driving scene and that's actually what most of the people tried to do was just talk to lingo about something else and yeah it still you still maintain quite a lot of that information um not perfectly but yeah in most of cases we could see that lingo could answer like random Pria questions that people would ask uh and ignore the image input because it wasn't relevant answering any of these questions um and often is also was also pretty honest about not knowing information uh but yeah that was that was pretty interesting because it's something we kind of discovered by allowing other people to play with lingo because we didn't really think about that that much uh but it seems yeah people come and poke into lingo once to give them access to it so the second part I think was also about the guard rails how would you how would you actually integrate guard rails into into the system like you know telling it to drive now into the store front here to the left I mean I guess it should not do right so the gers are kind of emerging properties of the data you train on because it's just unplausible right you never seen when we train lingo model to drive has never seen that acttion so why would why would you choose it um so the gers are kind of implicit but at the same time again one of the things I've mentioned in the opportunities I had of lingo um is this multimodal training right so this is where GS will live in the text and uh you can explain a lot of things maybe saying you know it's uh it's customary to leave a couple of meters from the pedestrians that are crossing the road uh because that is nice to do don't get too close um and then you know you you can change that can say maybe now it's better to have 3 meters and actually that's something we started thinking about because uh in the United Kingdom they change some of the highway code like a few years ago where they decided that now you have to leave more space when you overtake cyclists and that's when we actually started thinking about all of this because like okay how do we do that because we have data collected where we don't do that and now we have to change this Behavior so if you explain and you do this multimodel training that's probably the way where you're going to be able to explain all these things to the model okay uh how does the mul training hand situations where like there might be two sort of harmful options but one of them is worse than the other like is there some kind of SC in that no I think uh metrics so scoring that is Will well there there isn't and I think it's also dangerous to try to come up with one because chances that you just get it wrong or youe but we believe that you know multimodal large scale training is going to be the foundations of the intelligence and then you can always find your driving model on some very small highly cated data um it's very similar to like how humans learn to drive right you don't need too many hours of driving before you learn how to drive and they can be highly curated by a driving instructor but that works only once you build on top of human like with human intelligence you can learn to drive in you know 10 20 hours and that's the same like multimodel training and Foundation models are going to build a foundation that allow us to safely train on highly quality data okay with this we unfortun you need to stop um yeah if you want to chat more you'll find me at the booth tomorrow uh and yeah I'll be I'll be around all week thank you very much thank you J fantastic if you do the in painting yeah if you do it like a um Image level and do you have any other control to in paint okay so with this we come to our second speaker uh it's my great pleasure to introduce cutan assistant professor at eth Zur very well known for her work on digital humans um super curious to hear know her opinion on on on this topic I think a little bit more on the also on the explicit inductive side on on generative modeling so we are going to see it like back to back really you know from the different spectrum of of generative modeling so really curious to hear what you're going to say thank you very much for the introduction can you hear me well people in the back yeah okay good so um yeah so we we indeed we we work a lot uh with generative models and today for this talk I will talk about generative models for human motion estimation so very specific topic yeah it work yeah okay so yeah people are talking about the age of autonomy right so we are people are building like for example automous autonomous driving cars autonomous robot and also embodied devices and um I think these are all very exciting uh like uh uh opportunities and for so in order to let autonomous agent or intelligent agent to interact uh with humans and work together with humans they should see and understand the 3D world around them as well as a humans right human motion human pose human intention and also uh like to predict future motion so I think that's really important so motivated by this it doesn't work again okay motivated by by this in my group we have been working on 3D human pose motion modeling uh and today I want to talk about four work uh and the first one is um so I our very first work on human motion uh estimation from uh from molecular videus and the key idea there is actually a learn motion prior and the second and the third third ones are on the E entric human motion estimation and here actually we leverage the dtive uh models diffusion models to help us to uh to model the uncertainty and also to model the humany interaction and the last one is the latest one which will be presented uh in two days so it's our very like recent uh work on human motion as estimation from monocular videos again it's uh we leverage uh uh actually diffusion models for this task okay and all this work are actually L by one p student um yeah so I will start with a human motion capture bit of motivation so I mean for people here probably you know like the best human motion you can get uh is probably using motion capture system right where you put markers on body on face on hand and you have lots of moap system and then you can really uh obtain or reconstruct a nice natural human motion for example this is amas data set and to me this motions are quite natural and these are actually reconstructed from moap system however what we want to do right if you want intelligent systems or agent to interact with humans we cannot um just uh capture human motion from Moab system right we we want to capture human motion just from a molecular camera and here is a molecular rgbd camera and this Pro I think was one of the first Works which can reconstruct human pose and motion from molecular rgbd camera that consider also 3D environment right to reconstruct the human environment interaction from this monocular camera but you can see the motion quality really drops right like the motion is uh very Jitter jittering and also uh like it's not natural and our question by then is how can we reconstruct natural and smooth human Motion in 3D things with a monocular camera and our key in that by then is to learn motion prior from high quality moap data for example amas data has hours and hours high quality motion data and our first work was very actually simple and intuitive so I'm going to just explain a little bit in details so when we try to reconstruct human motion right a 3D human body like a 4D human body right over time the first question question is what kind of representation we want to use how do we represent 3D body over time right and by then we use actually marker based representation so basically we predefine set of markers uh on body and how to represent human motion we first compute the velocity of uh of a marker in the nearby frame right for example this uh this marker on the foot and then we compute this velocity for all the uh markers on human body and then we compute this velocity for all the nearby frames and we get a image right so now we can represent human motion uh 3D human body uh over time with the image and with this image you can see this Dimension is a velocity of certain marker and this Dimension is a velocity of all the markers at uh attemp step and the next thing what we did uh is very simple so given this images we uh we actually get this training data this images from amas data set which contains our hours and hours motions uh natural human motion which means we have lots of such trainy images and what we did is basically we uh train a autoencoder right so the first thing is there's a reconstruction loss to reconstruct the input and the second thing is we want to learn a high dimensional latent space where the human motion is smooth right so we add a simple like a uh last term to penalize the first order right and this actually give us a very smooth uh like a latent space of human motion and next I will show you how uh how we use how did we use this very simple uh Auto encoder to uh reconstruct natural human motion okay so now this is a test uh sequence right we never seen that during the training and now we uh basically run this like give this uh test sequence to our Auto encoder encoder and you can see the motion is smooth the uh uh the representation is quite smooth as we as we expect and now if we give this Auto encoder the encoder part the previous like state of art result like this monocular rgbd reconstruction and you can see the latent space is quite gitter right so this is basically because our Auto encoder is really trained with high quality smooth human motion and what we did next is basically to uh penalize again the first order uh difference in the in the latent space and instead of back prop instead of update the model right here what we did is basically to to update the to optimize the result such that latent space become smooth again and you can see when we back propagate this gradient to our input and the human motion becomes much more smoother and natural natural right so this is the key idea of this work and this is the motion uh motion prior we were we were uh we were talking about and here I want to show you some more result and uh here is the input again molecular rgbd sequence and this was a previous state of Art and then uh this was our result the motion becomes much smoother uh and also like uh natural and the whole algorithm is actually a big objective we have an optimization algorithm for this so what I explained is basically the motion prior the the key component of that that uh optimization problem okay so that's the first thing we did the next thing we we we try to do we try to do in my group is we try to reconstruct human motion from embodied camera so I'm very interested in uh egocentric perception and for egocentric perception back then actually there was no really big data set where uh you have the 3D human ground Truth uh uh for like interacting people from the Ecentric view so that's what we did we uh collaborate with Microsoft and we capture this EOB body data set so basically we have one person two people in the environment like a social inter social interacting with each other we have one person where Hol lens and we also have a few KET camera and then basically we can reconstruct the 3D human motion from the few Kinnect three to four or five kect cameras and we synchronizes KET cameras with a uh with a Hol lens camera and now we have a 3D ground Truth uh from this Ecentric point of view right so we we captured uh I think more than 10 in uh like uh in more than 10 office uh like many data and in the end you can see this is um this is uh the the data we have so we have 125 sequence and 15 endurings and for all this data we have the uh the human motion ground truths as well as the Ecentric View and gase direction hand and high tracking as well as 3D scan of the environment okay so now with this data set the next thing we want to do is to reconstruct human body not motion yet but first the human body from this egocentric point of view and this is actually a very very challenging task so for example this these are two people interacting with each other right and our goal is really to reconstruct this human body from this egocentric View and then the problem is for this entric view especially when people interacting with each other they're always is truncations right all humany interaction we don't really see the Human full human body so it's a highly ambiguous task and one idea we had the time is because like we are talking about embod devices right and lots of them come with like a rough three rough understanding of the 3D environment and the Assumption we we made at that time is basically let's assume we have a rough understanding rough Point cloud of a 3D environment and given this point Cloud now let's develop model that can really reconstruct the human body that have a natural humany interaction for example when we consider this 3D SC rough uh understanding of the 3D environment and then we know like this is a right uh like estimation and this is a wrong estimation okay so that's a key idea so the key idea is given the molecular uh uh RGB image and give a rough 3D uh uh Point Cloud let's reconstruct human body and now how do we do that so this this is a highly ambiguous problem right so we don't really see like uh uh the full body we often just see the upper body and we need a problemistic modeling and on the other hand right one on one hand we need a probalistic modeling and on the other hand what we need is to really uh make the model condition on the environment so then what we were thinking is basically now given this two requirements let's design diffusion model right diffusion model we can basically uh consider the uh the environment 3D environment as well as the input uh like a monocular RGB image as well as the the body detections we can use this as a diffusion conditions to train our diffusion model right so our so the the a bit of detail so our diffusion like a denoiser is actually a a graph convolutional neon Network the reason we are using that because we we are interested in human body and we want each body part to be conditioned uh on the environment and to uh conditioned on the image observation and by leveraging this graph convolutional neur Network as a den diffusion denoiser we were able to to condition this prediction right this denoising process on the images and on the on the 3D environment and another thing we did here by leveraging diffusion model is to to basically design a a collision uh score it uh like uh classifier in the sense like when we do the denoising uh process we can um we can we can further like refine our output based on the humany interaction coration right so that's another thing we did in this work uh yeah here it is so as I mentioned so we have the denoiser right and uh for when for this dening process we can define a cation score guided sampling which basically comput the gradient with respect to the post parameters and kind of guide the post parameters uh in the way that there are less humy interaction and you can see with this Collision score guided sampling we can uh basically have a better humany interaction here okay and what in the end this ego HMR achieve is first it achieves like a relatively accurate uh body estimation for the visible part body part and plausible human see interaction for the invisible body part right so here you don't really see but actually this poles are plausible if you consider the invironment and as well as diverse sampling observed body part so here we only observe the upper body and also the environment and for the lower body you can see we have a diverse sampling and also here uh there are more results so this on this side where is my mouse yes on this side is the input and this is a 3 environment and uh like Ecentric View and here as a result and this are the different sample like plausible uh body pose and here you can see it's generate sitting POS but with different like poses okay so that's about entric post estimation and the last one the last the work I want to discuss today is ROM robust human motion reconstruction via diffusion so again it's a diffusion model and this will be a oral talk uh on Thursday and the poster session is uh also on Thursday yeah so so the first thing is let's think again about this molecular uh um uh human motion reconstruction so what are the key challenges here so if you look at here actually one key challenge is really noisy detections so here we we see really like very noisy 2D detections and the Second Challenge is occlusions so this was this is the state of art motion reconstruction from this molecular rgbd camera and you can see when there is occlusions the human you don't we don't really recover the human dynamics here right and for for Rome so in order to basically solve this tasks we developed Rome it's uh it has a few um key advantages so the first one is this motion recovery is in the global space so we really get a global 3D human motion the second is it's robust to noisy and occluded inputs so you can see this these detections are actually uh occluded but we can still get the human dynamics motion out of this like highly uded test cases and also it much more efficient than optimization based approach for example can compared to humor it is 30 uh 30 times faster compared to to to to that okay so uh what What's a model so I will try to explain to you it's a little bit uh complicated but I hope it it you can you can get the idea so the input is again a molecular rdpd we do right and we first have a initialization so this initialization can be flexible it could be some human MCH request or some optimization based approach and given this initialization normally what you get are noisy and imp incomplete poles and the root trajectories so you get something but it's noisy it's in incomplete and also you you also like this root trajectory is quite like a gitter right so given this noisy and in incomplete Poe and root trajectories the task now is how do we really reconstruct smooth and natural human motion so the first idea is the first key idea is let's try to model the root trajectory and the local poles separately So based on our experiment this is this is actually quite good choice so if so we design like a diffusion model just for the root trajectories and diffusion model just for the local poses and it actually makes the learning much easier so this uh trajectory kns and post NS were able to produce something quite reasonable right so a little bit in detail so this trajectory KN is a unit architecture and given the like the noisy and incomplete root trajectories it can estimate like a smooth uh root trajectory and for the Post night so basically uh the the the goal of the PostNet is to estimate the the body post and then the the input to this post net as a incomplete local poses as well as the estimated Global trajectory right so this post net actually take the the output of trajectory net uh into account uh and this already actually give us some reasonable result thought right if you combine this root trajectory and the post estimation it can give you some Global 3d motion but there is a problem for that so when we really estimate the root trajectory we only take the implicit and noisy trajectory into account but actually the local post how body moves also give us lots of information about the global translation uh rotation of the body right so what we did here is basically let's try to design another component which when we try to estimate the trajectory Nets let's also let it also take into the take take take the post night like the local poset into account and how we did that we basically designed this uh trajectory control uh like a model so just a little bit of uh in details as I mentioned right this trajectory nights is a unite architecture and in order to really take the uh the uh to find or to to to refine the trajectory based on the local poses we designed this trajectory control component inspired by control KN so this takes the local TR local poses into account and then this basically give us a better on new uh Global trajectory so um again the whole picture right so we give given this uh incomplete uh Global motion like Global trajectory and poses we have the trajectory net and post net to produce the global trajectory as well as the local post and then the next is we basically have this control trajectory control which takes the local post into account to refine the uh trajectory the global trajectory and then what we can do next is actually to run this in a iterative fashion and uh in our experiment we basically run twice this iterative inference and we can basically get a quite a nice uh like a global human motion and I want to show you some results so you can see here is the input and this is actually the ground truth so this is like a jittered human motion right and this uh is a result from humor uh I mean you you observe it loose like this Dynamics on the lower body part right and with our model you you can see it's actually can very nicely reconstruct uh like this motion for example walking motion right without really uh lots of artifacts like food skating we don't really see lot of Skating here and we also run this on real world data of course so this is the input again and this was a state of art result humor I'm not sure whether you can see it clearly from the screen so um when there's like oclusion it lose the dynamic lose this like a human movements this this type of things and then with a diffusion based this ROM model you can see the human motion can be reconstruct quite nicely I would say even when there is really like a high occlusion case when human body is really occluded by Furnitures or by other things and there's more result so uh yeah so this is another Baseline called face mp uh and it has some like a jittering problem not jittering but kind of liquoring problem um and we also no yeah okay that's all the video I have so just a summary uh with RW what we can do what is basically we propose this model we model human motion with diffusion model and this what we found is this is very effective for incomplete observations right noisy and uh incomplete uh uh detections body body body part detections and we propose this control like trajectory control to capture the traj po interdependencies to produce like a kind of a coherent 3D human motion and it's compared to optimization based approach for example humor it is it provide us like much faster inference time of of course there are also limitations so the first one is actually the generalization to out of distribution body shapes is uh is very limited at the moment and also we haven't really considered the syn constraint in our model and we don't really I mean we lack of data for for such of uh for for to model to really um model humany interaction in this model yeah so again like uh this I present this four works you can find the code data and model on our website so this work has been done by this wonderful collaborators and especially suway she she has been leading all this four project and um just if you are looking for someone working on human post estimation you should consider reaching out to her she's wonderful thank you very much okay thank you for this fantastic talk um we have plenty of time for questions and uh let me start maybe with one yeah I mean so there's so much data on this like such a big field you know digital humans do you see like a way towards like a foundation model for humans and how would this look like I mean also in the context of you know what we've heard earlier they like you know do do you see something like this coming so I think yeah I think depends on what uh what do we mean by Foundation models here so I I was attending the talk Yesterday by Michael Black and he was arguing like simple is already like a kind of a foundation model right in some sense so I I think really depends on the foundation models so if we are talking about like a human movement and the text semantic and now we have uh like a Aras data and also people label a lot of like a human movement so I think we have we have data that can kind of enough for us to start to do something yeah yeah but but I think ultimately we really want we really need to leverage the internet scale data right for YouTube we have so many human like Centric data right it's there are so many data about our daily activities uh and sometimes there are also texts right with that and also if you think about movie data and how to leverage those data I think that's that's that's a that's a really challenging question and the key question to really build kind of a multimodality generalizable foundation models you have an idea how to do this no no I I have some gu guess yeah so I one thing is I yeah this we can talk this for very long because I been thinking about yes so I think one thing is um how to say that so if so people always think like right if you want to we always so people say like let's try to reconstruct human motion from YouTube video because YouTube video they have so many different human movement right and then maybe we can train a motion model we can train a human behavior model based on our reconstructions right but that is really based on assumtion you can reconstruct human motion from those crazy videos yeah but I I now I have a question do we really need to do that it sounds like a chicken egg problem right but if you think about maybe we can we can kind of not really we we might not need to really go into the explicit 3D representation in the sense like explicit 3D body Reconstruction from this crazy in the world videos right maybe some text input is already good enough text annotations and maybe some foundational 2D video models right I think how to really kind of distill this 3D knowledge body movement body Behavior Knowledge from the foundational models right from 2D foundational models I think there is a kind of opportunity there yeah I don't know but that's my guess think that's also like an idea actually that uh you know like other video World model you know ideas that that we've heard already like in a sense you don't have like direct access to the 3D from right how to distill this something yeah I think yes yes like yeah I agree I think maybe we don't really need to go through the explicit 3D reconstruction although the whole work is that the whole talk about 3D reconstruction but maybe now it's time to think about not explicitly reconstruct those things interesting more questions uh anyone well I can continue uh so maybe like one thing that I'm thinking about when you talk about you were talking about occlusions a lot yeah so I mean that's interesting it's challenging everyone knows but how to evaluate you know whatever happens behind uder you know how do how can we actually Benchmark this since there is like there is no ground truth in some sense right we can eyeball it and say plausible but there are so many solutions so is there like a way of of Quant quantifying the performance behind the fluto yeah so that like um so we there are some data but very limited data right for example this EOB body data set uh from Ecentric view you included right like both the legs but actually we have kect yeah three four five connect it if the connect CEO we probably have a like a sudo ground TOS right to estimate that so there are data to do some evaluation but I think the the the it's very limited the the amount of data is very very small and um maybe I mean I I I I haven't checked the latest work but maybe on body device right IM use or other sensors on body sensors not really rely on explicit or external Vision sensors maybe those those way those are the ways to go so to capture large scale data yeah yeah interesting yeah that's my guess questions yeah please I just wonder it's possible to direct learn I think theel very very powerful relable I I agree that's also my dream but um so so do you like I I don't know how to do that right I think there are Works start to do that but to really reconstruct or to model human motion so let's say there are two type of tasks right so given a bunch of in the world videos one thing people try to do is to reconstruct motion from those videos another people another thing people try to do is to uh like or I'm at least I'm thinking is to not really to have the explicit 3D reconstruction but to try to learn some Behavior representations from those videos so I think for the to to give like to given like lots of in the world videos to reconstruct this 3D human motion yeah so so far people have been like maybe using the 2D annotations this type of things but I think language text descriptions is another way of annotating the the information right so that probably could be could be leverage as another kind of source of annotation not explicit Tod keepon annotation yeah yeah any questions any more questions good then we have uh actually good yeah we have a coffee break now we are continuing at 10:40 with dining C and then Alan you before we go into the L break and poose thanks thank you very much that's actually an interesting idea like learning these kind of you know video generation models of humans and then uh kind of that's so sorry let me close it yeah but do do response to me doesn't respond I I think I will come back very inspiring you know I'm actually so you might actually be interested in this don't access yes of course but I might maybe no it's okay it's just one yes okay so than talk there was one thing you like having aate like Collis free poses can you try to give an intuition how you get a good gradient to actually get like sounds like difficult thing to get like a that useful for that right yes so we actually we have a body model so the the we have a body model which has a body I think I remember so for every po we have a which you can basically give a cur point it can tells you how much you are in a body right with that information actually if you actually take the sign not sure the gradient or the occupancy but like but the thing is once you have such information volumetric information of your body and then you have the point CL and then you can push your body out of it so you can get so it's a I I would call it post let no no no but just like this this doesn't anyway e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e yeah so s is the professor in at NYU many foundational Works honorable mention with on the Mark Price Right so uh great pleasure to have him um I think also you know when we were talking about the the spectrum of 3D to to fully fully learning side I think we are here more on the learning side so great to have you around and uh very much looking forward to your talk thank you thank you Adam um oh okay this I hope on Zoom people can hear me right okay cool yeah um so uh in to this's talk I want to just like talk about like diffusion Transformers again like I think I talked about this last year at cvpr at a different Workshop uh but this is really about like diffusion Transformers uh and Beyond right and there will be some Reflections and uh and also like some of the followup work after the fusion Transformers um and I have like a you know a small title is like why you should stop worrying and love the it I'll EXP what I mean cool um so uh okay I think this okay good all right uh so if you are not familiar with defusion Transformers uh I was like before 2022 um you know like uh people start talking about like scaling uh but really just like all the language models in in NP folks are having all the fun right right like so we have this kind like you know scaling laws we are scaling the GP family uh for natural language understanding and Generation Um but in terms of like image generation right um you know people also tried scaling things up um but the scaling behavior is not ideal right so this is like plot from the Imagine paper from Google and you can see if we have this can like unite backbone and unite family for diffusion models and if we just keeps getting up the model size actually the performance does not really improve much so uh one of the problem um that we identified is like at that time diffusion models really uses this highly non-standard architectures this is like very different from the NLP domain right so in NLP uh Transformers started kind of dominating everything and you you know from GB3 and chinchila all the skining laws like people study the scaling Behavior using this shared standard architecture which is Transformer um and it's like very simple it's like stacking all the trans Transformer blocks right but in contrast this is what we have for diffusion models uh so this is like you know screenshots from the Imagine paper again uh it's a unit uh but it's also not just like the standard unit from the original paper it has been you know involved uh you know through the years and people are adding different kind of customized modules specific designs so it becomes really really kind of like you know a monstrous kind of architecture um that has like self attention inside uh this network um people still call it unit but it's really kind of a highly non-standard architecture so uh the key question we wanted to ask is like can diffusion model really also benefit from the simple and scalable architectures right so this is why we decided uh to work on this and we presented this new kind of architectural family which is uh called the which means like diffusion Transformers it really represents a new class of diffusion models and this is some samples from our model and uh this is just like another kind of a scaling plot uh where we can of compare different kind of like sizes of models in this di family from a a small model um all the way to extra large model and this is going comparing to U the stateof the art diffusion models with unite backbones at that time you can see like you know diffusion Transformer architectures is like uh you know much smaller uh and much more efficient butes like better performance so how do we do this uh I probably will skip this part quickly uh this is just like covering the diffusion Basics so for diffusion models if we really need to go uh it's really about just like sample real image X zero and we need to sample some noise Z from standard G distribution and we crop x0 with Z we can call it XT now right and now the goal is to train your network to predict the from XT okay and one of the semal work in the space is of of course like is really about like you know Fusion model actually have this is done by using um you know v v encoder and then diffusion process will happen by the V decoder encoder very important thing is about how we actually measure the model complexity right uh so basically there's a lot of discussions I think one of the fundamental difference between uh can division or um any models that involves can like you know veral modeling uh is we cannot fully can just to trust the number of parameters so uh the idea here is like if you just look at the parameter counts uh it's prob it's probably not a good me of compacity um and one of the things that we can of looking into in this work is flops so it's pretty good uh although it's also not perfect okay so now we can get into this kind of did design uh it's actually very very simple so again like if you have the the input to the whole system to the whole new network are basically a few things right you have the input image you have the forward process time step t uh and you have the also some of the conditional information for example labels text Etc and similar to ldm we also have like V encoder right this is a uh you know giving uh the diffusion model kind of later and space to work on and also kind of compress uh the raw input into something that's much much more uh smaller so we have this kind of spatial latent and then we we kind of have this forward process um and we we need to utilize this forward process time step and get like the noise latent and then like it will take uh the embeding of both the forward time um time step and also some other conditional information and put them in the it model and then like we need to um have our objectives which uh is noise prediction or it can have like coar prediction as well this is a very standard diffusion models and and here um this is how we can Define the training losses as well okay so now we want to really unpack this model and and disc uh discuss what exactly is so like the diffusion Transformer block design so um it's again like very very simple right um so is uh the dit design space is basically a Transformer block right so as you can see uh we have like some MLPs and edding layers um supplies uh not supplies conditional generation task c will be class labels and then we add them together like this to the to the did block okay and uh details for example the pacification is very important so like what uh when when working with this kind like you know noise latent is still a special can like you know um um kind of Cubes so we can actually pfy it and uh put this into the dit blocks so uh if you think about like the did design one of the key challenges is about about how we actually you know work with this additional conditional information right so there are actually a couple ways to do this uh there are like very simple and very basic ways of doing this following uh what happened in you know in NLP um so basically we can do this in context conditioning right this is something extremely simple we can just like concatenate uh the additional conditioning information with the uh the image tokens from the you know patch ification of noise latent and we just conc them together uh and then like we we we we we we can construct this can like sequence and then we um you know put this uh to the next multi sof tension L okay so we call this Inc contact conditioning uh you can see the training looks good uh we can get like reasonable performance uh but of course we can do something else for example we can do the cross detention uh which means like now we will use m attention this is one kind of additional uh kind of block between the S tension and point wise4 we can also integrate the addition addition to the dit block this gives us like a slightly better performance as well and finally uh we ALS right so this is again not something new uh this is has been used uh you know and also it has been used so here additional MLP layer um that will take the additional of conditioning information and then project this into um 60 Dimension you know um like kind of like kind of gating mechanism in some ways so here it's called like scale and shift right this is basic basically the iine transformation parameters uh on top of the layer n which is very standard thing but like because we we use this transform transform weights as like additional gaining mechanism uh and then like you know this parameters actually predicted from the conditioning information so this is also a very effective way uh for us to really integrate this conditioning information to the dip block okay so if we do this we can see yeah like the the model uh this add LM variant actually converges a little bit faster uh but the performance not really kind of better um it's slightly better here but it's not much compared to other options right so here um we started looking deeper into this kind of design and we found something right so uh if you are familiar with um you know rest net and you know like all this kind of like classic architecture design um and from this paper this is like very fam famous paper uh I create large minib batch SGD training image 9 one hour uh from fair so there actually they proposed this uh kind of initialization term uh they call it B normalization um you know GMA initialization so basically what it says is basically you want to initialize the sca to zero in the last B layer of each residual block this actually improves the results for both small and large mini batches uh and this has been a pretty useful trick uh in many cases so it leads to better optimization Behavior Uh and and results can also get better right and if you look at original unet code um from the original um the dvpn paper you can see um there's also like one special kind of like you know design or trick not mentioned in the paper at all but actually this is super important in practice this actually makes us to rethink you know what's the difference between our Transformer of dit design versus the common practice people have been trying in the past so here it's the same thing you can see you know we are learning this residual path right um and then here you know it's going residual block so you have like X Plus H But Here If You initialize the scale to be zero at the final convolutional block here right it really means like you are you know kind of at the initialization you only have kind of like this highway this shortcut from the input direct to the output you are not learning anything you're not really um you know kind of like mapping anything on the residual path okay so in terms like this is something even though it's not measured in paper but empirical is very important so um I I guess like one of the hand we intuition here uh is like you know when you have like really large diffusion time step uh the input is almost kind of Pure Noise right so if you think about diffusion models like what should the ddpm model do um probably you should just do the identity mapping right just to do the skape connection and this can provide an easy path to just like copy paste the DPMS input so just like some intuitions um but if we can incorporate this can like new design we can see um you know it's very easy to do this because we can just add additional kind of scale uh you know weights uh after the muler self tension block and also the point Y is going fit forward block right and we will actually initialize this term to be zero so that means like you know at initialization you have the shortcut from directly from image tokens to the output through the whole network in terms of this is actually really important um you know if you you run the whole thing with this kind of design um and with this of better initialization we can see the add l in design actually Ys like much much better performance okay so this is like the dit block design uh and I really like this type of like visualization in terms like implementation um so originally like for ADM ldm you right because it's kind of a highly non-standard architecture you just need to write so so many different modules and code to do this but now with this like did block just like a few LS of code and you can achieve even better performance okay so um one of the fundamental reasons uh that we want to study kind of diffusion Transformers is really about like scalability so um one kind of design space that we haven't really talked about is this how we get this image tokens from the noise lat right so this is also very similar if you are familiar with v transformers this is basically the same idea uh although like now we work on like this kind like noise latent which is also kind of a 2d structure so um nothing fancy here we just need to define the patch size that we have so if you have like you know a large patch size you'll have fewer tokens right if you have a smaller patch size after doing the petrification you have like a longer sequence with more and more uh input tokens so this means like we can have this one additional Dimension to scale up our model because we can very flexibly control how many V tokens that we need so here in our design space we have like P equals 2 four and8 okay and uh again like for other model conver the variance Design This is largely follows the viit model we have the small model base model large model ex large model where we vary a number layers the hidden size dimension number heads and and and by doing this we can have like different G flops right like the model is getting bigger and bigger okay so this is like overall the design space of this this architecture so we have like different block designs we have different patch size uh and will also have different model sizes so for experiments just like quickly go over it um I think the one sentence summary of this is basically uh more compute will lead to a better generate models so we can do this by adding more tokens so this is basically showing within each of the model variant right like from small to x large um you know like if we in increase number of tokens by decreasing the patch size we'll have like better and better performance um of of course we can also scale up the model sizes so this shows like um under the similar patch size sequence length if we scale up the model parameters we can also observe better performance um what's even more interesting is like in diffusion models we can actually show the veral scaling Behavior so like again like we have this two dimensions for scaling up we can have like bigger models with more you know parameters or we can have like smaller patch size which means like longer sequences so you can see like from the left top corner of this like uh visualization to the bottom right uh the performance is getting uh increasingly better and better okay uh there are like more visualizations um you know across different kind samples we observe something very very similar okay so I think like you know um I wouldn't call it scaling law but like we do have this observation like somewh can compute is all that matters right so there's a like actually a very high correlation between uh the the flops of the model versus Bic see the image generation quality like as mared by the FID score right it doesn't matter if you just like scale up the model size or scale um you know scale up the the sequence St um you can see this kind of scaling Behavior um and um again like this is another plot uh and we have like more plots showing different like matrics uh Beyond just FID so we can see the Precision recall Inception score the all all similar kind of trend okay and another thing is like you know uh basically for diffus diffusion model uh it's a very interesting family model right because if you talk like compute you you you always have this two things like the training compute U versus like the sampling compute because you can uh flexibly increase the sampling compute by adding more steps uh but here we show like sampling compute cannot really compensate for the model compute right uh basically you know with a small model um like however you want to kind Like You Know sample M steps U it won't really out outperform a bigger model with bigger flops so there are like some more examples uh this is all uh done on image net data set trend on 1.2 million you know images um this is you know from almost two years ago so there's like we haven't we at that time we didn't do any text image uh General models on this convenient skill data set but can already see really really good quality here uh we can do something even more interesting uh because we have this kind classing balance we can actually have kind of random walk between uh you know this can classing datings um and this is how we can show this kind like cool realization of interpolating between different classes in image net and finally this is like interesting image I really like um build did this so basically like this is cool example from original big Gan paper and they are trying to generate this like dog ball example uh but this is after like maybe of five years this is uh the sample that we can get okay uh in terms like quantitative results uh we were the state-of-the art method um at the time and you know all performing other ldm variants so okay so this concludes uh the dit presentation uh this is like not new content U but then I want to really talk about like the timeline here right so uh we released the paper um 2012 uh 2022 uh December and was released on archive and in early 2023 uh it was rejected by cvpr 2023 due to lack of novelty uh at that time I also like think oh this is a really good model and we shouldn't use un anymore we should just like you know push people to try to use theit uh so I actually you know at that time I just like went to NYU become an assistant professor so I was like in contact with many companies and say oh like we have this new model do you want to try it U people are actually kind of excited about but at the end of the day I think nobody really wants to take the RAC and say oh the is great but we'll continue using you this kind of a huge momentum there and this is now like I want to talk about like you know why you should really stop worrying and love the it so the first reason is really just like about scalability right so like after a few months um I think this is like September 2023 this is like maybe kind of a N9 month up the original it paper uh so there's one uh kind of group in China uh they released a pretty serious effort in like scaling fur the scaling up dit model to uh text to image generation uh so they call this model pixart Alpha it's called Fast training of diffus Transformers for photo realistic text image synthesis right um and the basically what they did is like they they scaled up the original D model and tested on the text to image generation so what they observed uh which is you know which makes a lot of sense is this great training efficiency right they can show like only using the faction of the original training cost of a unit based uh for example stable diffusion model uh they can get like much better performance for the for this typ like text to image generation tasks so I want to talk about this model a little bit more because I think this actually a very good paper if you want to you know use diffusion Transformers for your own task um so because now like we don't have the class uh condition um you know we we don't have this class labels as our conditioning um because we're not working on image net anymore so you need to kind like um tax encoder right and then remember like you know in one of our dit block design uh we have this kind of mulab cross attention between the MLPs and the self attention uh so basically they also adopt similar design uh with they learn the taxing Bings using T5 encoder and then use a multide cross detention to integrate this information the tax conditioning into the did block okay and then uh everything El is pretty much similar to the original did design but one thing that's critical and this is something people I don't think a lot of people knows are aware of is like original add Ln design has kind of a serious parameter um issue which which means like because we need to take this MLP layer which is like 60 or six times the hidden size over the entire NE Network and we need to repeat this for each individual d block okay and then at that time if you have only have like smaller models let's see a few hundred uh medium model this might be okay but if you want to scale up your Transformers and now this actually becomes an issue because this part alone will take like oneir of the parameters so imagine like if you are training a 3 billion model right one billion of the parameters are kind of Wasted by this MLP layer doing the addn um like you know conditioning and what's worse is like before you can justify this because now because in dit we combine the class labels with the for time step T together and have this MLP but in this context to image WS okay so but in this kind of attack to image generation we don't have the class label anymore so this MLP is really just about conditioning of the time step T so literally wasting all the parameters right so what they did here is pretty smart it's just like you do this only once in the beginning of the uh Network and then you share this um you know like uh embedding or this projection layer across all the unblocks so this actually saves all the parameters okay but of course there are like other details for some how do you do conditioning and so on but I really like this solution uh another kind of important factor here is they show um the network can be per Trend with original di weights that we released from image net but and it's actually very very helpful and they have like more follow-ups uh showing like we can do faster sampling better control uh like very high resolution generation like 4K generation as well okay so this is September 2023 um although like people start noticing diffusion Transformers and start like serious effort in scaling things up U but still like the uh like the majority of the diffusion models are still un at back ball um and things finally change uh earlier this year in in February 2024 op I released this s model stories like Bill is the first author of the DI paper it was like my intern at Fair we did the diffusion Transformer together um and bu join open ey and become a covid of Sara uh if you read like The Sara technical report um this is basically very similar to the Reginal D design and they also can acknowledge that uh so because now you work with videos so instead like just have a 2d uh noise latent you have the SpaceTime patches and now you can have like proper video diffusion model with kind like a Transformer backbone so uh in terms like real scaling right this is also something I'm pretty sure like Bill did this uh because in our original paper if you remember we have this real scaling in a 2d space uh but in SAR actually they also do this video scaling for for videos right but you can see um you know kind of similar of scanning Behavior Okay so this is about scal talk about is like you knowbility so want to introduce another work this is called sit uh this is going to upgrade to the original di work um and TI of this work is exploring flow and diffusion based General models with scalable interp Transformers um and um yeah so so there's like some kind of like interesting um you know concept here for example interance uh and this is actually from the paper um in 2023 so this is paper called stock casting interance right really just a theoretical framework to bridge uh any two arbitr probability density functions uh exactly in in finite time and also the theoretical contribution um also includes unification of flow based and diffusion based methods so one like example uh under this channel framework is you can actually connect thetion noise to data distribution which is basically uh for example the image generation model that we have right now so uh like more specifically for the interp framework um I think this is kind like General thing um because for flow and diffusion models they both utilize this kind of stochastic processes to gradually turn the noise into data right and the time dependent processes can be summarized at this term this is call like interplant so XT equals Alpha T * xar plus Sigma ttimes epil right and here Alpha T and sigma T these are like two functions Alpha T is decreasing function of T and sigma T is an increasing function of T um so you might have heard about like you know rectified flow flow matching and so on right and basically they are all the same thing it's kind of look crazy uh around the same times like like September October 2022 um you know there's like complete independent work um from the community and people are study something really really similar I'll skip some of the details but I think one of the most important thing of having this kind like more General framework is okay is basically we can actually relax the rigid definition of that spdm so basically spdm actually introduces this noising process right and if you think about this we can reust this as an interp Point uh so basically now we have remember the alpha T and sigma T here but now like instead of choosing this Alpha Sigma T fre um actually you know like this coefficients determine this going to close form but they are determined by the noising process above uh and another things like this only maps to Gan and doel in infinite time right so you can see sbdm is actually one but possible interp points I'll quickly skip over this but like we can basically compare diffusion models and flow matching objectives because we have the diffusion Transformers we can study all the design decisions and variance in a unified framework with the same backbone so we can actually compare their results so again like you know there's some Theory contributions but like things you know are still kind of remain on Theory level um early 2023 but sat actually further investigate the connections between diffusion and flow models and the performances in this relative large scale image generation tasks uh we use same dit design and start our starting point is actually an common from like conventional ddpm dit and we study in four key components time discretization model prediction interance and sampler because of this like Interpol framework everything can be decoupled and everything can be studied like orthogonal right like we don't need to tie everything together so this actually enables a whole new search space that we can actually play with this image generation tasks so our final sit model like for further details please take a look at our paper our final sat model is a model with velocity prediction linear interpon and SD sampling with specific diffusion coefficients so we can see with this kind like new model design sit can consistently op performs dit uh it also converges much faster and uh sometimes almost twice as fast comparing dit across all the Mel sizes and this is samples we can get from sit again kind like calization if you care about implementation so in diffusion models if you really want to implement this um you you you will be annoyed by this kind of like Massy varant schedules in diffusion right but if you only have this linear inter in this flow matching framework this is like three or four lines of cod and the performance is better so why not there are like some future work but one thing I want to say is like basically flexibility uh really means more opportunities and this is only like you know this is like we we made this happen because we are having this kind like unified architecture of diffusion Transformers unfortunately s stor uh was released archive like uh this year and we just got rejected by c24 and we s like you know full matching is not new diffusion Transformer is not new your results are great but lack of novelty um we got like really pretty good reviews from eccv so fingers crossed and then you know history repeats right like just after a month stable diffusion 3 was announced and what is diffusion 3 actually combines a diffusion Transformer architecture and flow matching objective so I'll skip this details is a very interesting kind of like dit variant for multimodel kind of like information uh and one of the major contribution for them is also they have this even larger kind of like U exploration uh with flow matching objectives similar to the our ours work so I would say like given this large scale results based on the it uh I think using in interance or flow matching appears to be the future and to be honest I think there's like already a lot of contraction now uh with flow matching I think you know it's simpler it's better uh so if you don't know about this like feel free to check it out there's like a lot of tutorials online uh I know we are running out of time but like maybe if you can give me two minutes I'll just like quickly uh go over this final thing so um and I I hope this will connect with Allan's talk after this as well so I think this kind of architectural unification is also something why you should love the it because this really enables exp exploration across different domains so um people are talking about like G models and representation learning recently uh you know analysis bythis um and we have this recent paper ldae uh deconstructing the noising diffusion model sof suppli learning um and um so basically this is in collaboration with SH in climing Affair um but this is basically saying like starting from original diffusion Transformer Baseline we can actually gradually more of this uh Network into uh like uh kind like the very traditional Den noising olding coder right and then through the process we actually identify the key factor for diffusion model to have good con representation learning capability is you need to uh can of model uh do the modeling uh in of lower dimensional l space in which noise is added and another thing is like generation quality is right now still uncorrelated with the SSL representation uh quality this is something I'm not sure uh if it's going to hold for the for the future but at least this is what we observed so to summary uh this is a couple reasons why you should stop worrying love the the Simplicity because it really Le the scalability also like because you have the Transformer architecture you can really explore other things like objectives because you have this type of like flexibility and the third thing is like because you have the Aral unification this really enables uh exploration across different buildings uh thank you okay we have three minutes for for questions um sorry about that let me let me start maybe with one so you said like one of the final sentences was that actually the repres don't match yet the SL quality um um in in in these generative models so so yeah if I want to elaborate a little bit more so it's basically me like we have to kind of give up on the generation quality to get like good SSL performance like in terms of like linear probing so for the best like model here that we studied right like you can achieve really really good representation inal quality achieving like maybe around 65 linear probing results on image now but FID is horrible it's like 200 FID or something why do you think is this I don't know like I feel like B I think maybe related to the specific design of the diffusion models um or maybe fundamentally these are two different directions that shouldn't be connected together I would love to hear more from Alan maybe uh but I think this is kind like really kind of open research question I know a lot of people are discussing the unification generation and you know understanding but I think there's a lot of things has to be done nice more questions no one oh yeah you try without without it because in all the that you show always have two four or eight yes you try without it uh yeah it's a great question U so I guess like there two things from this question one is like can you do this directly on the pixel space without doing any any like you know be encoding so this is something uh that's very challenging for the dit model uh I haven't really seen a lot of kind of good successes successes in this space uh so it seems like the diffusion model uh will struggle if you directly work on like the patches in the input raw pixel space um but then like for the for the other question like without doing any kind like you know petrification do you mean like maybe a p size of one is is something which yeah so this can work but the only down size is like you need to spend more comput because because you will have like much longer um you know sequen l so there's actually a paper from chin uh I think two days ago and they actually studied this they call a pit I think it's called pixel um Transformer yeah so you can find like L empirical results from that paper as well cool oh yeah for video for vision U yeah that's a that's a great question uh and and the answer is like I don't know right like of course I think um Theus Transformer is great because it's really really simple and um again like I I talk about some of the benefits why you should try to use the ID but you never know uh you know if there will be yet another thing in the future I'm I'm pretty open to it uh yeah so hopefully we'll have something new okay then let's uh thanks signing again thank you so this SSL result is actually my for for okay okay good so uh it's a great pleasure to introduce our next speaker Alan Yu who's a Bloomberg professor at Johns Hopkins University not sure how many of you know that he was actually a PhD PhD student of Stephen Hawking and then since then transitioning into Ai and really shaping the field in many ways over the last decades winning all prices that that our community actually Awards so uh yeah very much uh looking forward to your talk Alan okay well Adam thanks very much and thank you for coming here um so analysis by synthesis saning mentioned words in the last few minutes um so this is an old idea and I think um somebody said recently that the uh cvpr papers typical cvpr papers only publish only site papers about two years ago so I'm proud in this talk to site papers that uh even one of which dates back to the 1970s so the idea of analysis by synthesis is perhaps that you should think of vision as being inverse computer Graphics there is a 3D or 4D world out there the ask of vision or image understanding is to uh estimate the 3D properties of the world and you know perform whole series of tasks after that so there is a type of Duality between models that can generate images or syntheses as I called here and models that can understand images in this case analysis so I think the first person to articulate this was Al grenander in 19 76 I think he wrote about it earlier and it can be expressed as B and inference it's out of fashion I'm not sure it ever really was in fashion but in my long experience of computer vision things that are out of fashion usually come back into fashion sooner or later and there a lot of very good ideas in the past that uh are forgotten some deservedly but um a lot of them I think are can lead uh to Big improvements so from pan singer over there on the you know you see a cube uh you know you see an image 2D image of a cube and everyone sees it's a cube why not because we've trained deep networks to estimate it a cube but we have constraints generative constraints censor this constraints of how a cube could be projected to form the image now that doesn't mean that the image should be understood as being a cube because there many things that could have generated exactly that same image but then we have prior knowledge about the world that cubes are more likely than anything else so we interpret uh you know so we use a combination of the generative process the likelihood and the prior to interpret it there are really nice illustrations of this my colleague friend Dan Kirsten had illustrations of a ball in a box this is visual perception and in fact the motion of the ball um depends entirely on the motion the perceiv motion of the ball depends on the motion of the Shadow the ball does the same thing in various sequences the shadow does something different uh and from uh analyzing the shadow just from from wearing the shadow you can get the ball to travel along the surface or to go upwards this is just to suggest that humans have sort of do some form of analysis by synthesis you have far knowledge about it if the shadow moves the ball you know there's got to be some reason for it one reason is the ball moves upwards so there's lots of experiments about this so lots of visual phenomena that you can use uh you you can sort of describe admittedly sometimes in a hand wavy manner by assuming that vision is basic in inference with generative models uh and trying to interpret the world inversely I should say when all this was being done basic ideas came out computed Graphics was very primitive there was a conspiracy theory that the Apollo Landings in the 1960s never happened that they were all simulated by computer Graphics but anyone who knows the state-of-the-art of computer Graphics even two even 20 years later know it would be a lot harder to have simulated the moon landings than to actually send people to the moon so computer Graphics barely existed here um diffusion models were never thought of autoencoders did exist as a very simple newal networking manner of doing principal component analysis but that's about all so analysis by synthesis and actually this does get I think possibly to an answer to one of sing's questions at the end so they're dual to each other um so if you have a world and this is for a paper for myself and Dan Kirsten 2006 just the thought experiment world you have a world that's where the generative models are really simple like a um in that case generative models very simple means the inverse process of analyzing the image can be simple too so you make the images more complex be okay more complex generative model more complex generative word and so the analysis becomes harder and then you move over to the right which was essentially a capture type of um image in number c I think captures weren't even popular at the time you generate a very complex image and so the process of going from the image to interpreting it the analysis becomes uh becomes harder so in principle analysis by synthesis as conjectured for the brain and as a promising way to do computer vision says that the two processes are dual to each other but then do we actually need good synthesis in order to do good analysis people who spend a lot of time making really good synthesis models of uh you know diffusion models or computer Graphics models they are evaluated by FID scores and things and how well they generate the image so you should think if you believe in analysis by synthesis the more realistic the sense is the better the analysis but if our goal is analysis we only need syntheses algorithms that are good enough to be able to form the visual task that we want to do so maybe if we just want to classify image net then hey it doesn't you know we don't need a really strong diffusion ble to generate those images because frankly classifying image net is not hard human Vision uh your visual systems do much harder tasks uh every few minutes so a few visual tasks may require very good synthesis but many do not and the ones that may require very good synthes this are not ones that the computer vision Community work works on because we don't have annotated data sets to evaluate our performance and also if you are like myself influenced by human perception you know people have said okay you you carry pra of of objects in your mind and you uh have a perhaps a virtual simulator which means you can imagine everything in the future Josh ten bound but m is very eloquent at saying this but um think about it for a moment um if humans have the ability to do analysis by synthesis with really accurate analys synthesist models then if you simply close your eyes you could imagine this whole scene and generate it and I bet none of you can you know we can imagine things we can dream about the world we have that sort of capability but it's probably not terribly accurate possibly a few people like Architects my father was an architect you know film designers might have that ability but most people don't so the take home message here is that you know how good do we really need to be able to generate images in order to analyze it and so here we can take a pragmatic approach and say well we don't need the generative models to be really accurate we just need them to be good enough for the tasks and that's partly an excuse because of the work I'm going to present here a lot of it done or actually all of it done I think in collaboration with uh Adam cusi um uses a rather simplified generative model so I'm giving the excuse for not using a more realistic one okay going back um again I I it may be unfashionable Envision to believe that the world is 3D or 4D and that to uh address it to model it you need to have threedimensional models of objects and 4D to take into account the spatial properties of course in computer vision we do this for humans you know you can hear Talks by Michael Black and so on and we do it for humans because we got annotated data to do it so we are you know we're sensible enough to to do it we don't do it for other objects mostly simply because we don't have the annotator to do it and we are somewhat slaves of the annotated data sent that we have so going back to the 1980s which was a wonderful time because people could come up with lots of conjectures and nobody had to validate them on any data set at all so there was a lot of nice imagination there was a theory by Earth medaman called geons that you represented objects in terms of Elementary components called geons and the objects were just combined together uh out of these Elementary components so you had a 3D model and then the important as aspect of the geons was it was compositional the model the object was made out of these components combined together so this was 1987 and to get my earliest reference you can say this sort of idea was also in David marah's book in 1982 but the first person to do it was somebody called Tom Binford which I'm not sure anyone has heard of no uh oh one or two uh well perhaps best known as these days as jend Malik PhD advisor but uh Tom Benford was one of the true Pioneers uh of computer vision it's a Pity his contributions aren't quite so well known so okay so if you want to do analysis by synthesis and you really focus on the analysis you'd like to have three-dimensional models of the object um and you might make them compositional um and so we have papers here uh which I'll generally call 3D composition or networks 3D cgms sometimes called neural Nemo neural mesh models um the titles changed a little bit in the papers and frankly uh a precursor why that happens is one of sunning other presentations about papers being rejected for lack of novelty and so this isit frankly a bit ridiculous to my mind because if people rejected papers for lack of novelty Alex net should have been rejected because uh the basic architecture who was there somebody had even won a prize for it uh you know a few years earlier uh but and it was just a small tweak on other things that were there not to disparage imag net be obviously getting it to work not to disparage Alex net getting it to work on image net was incredibly impressive and nobody in their right mind would imagine that you should reject that paper for of novelty but if you follow many of the uh the experience of sanning and others this happens POS perhaps never to Jeff Hinton but to less Mortals it happens quite frequently anyway sorry that's a little bit of a devotion but we've got three-dimensional compositional generative networks so they're three-dimensional the factorization the generation makes two approximations first you're not actually generating the real image at all you're generating features and the features are trained in certain ways so that it's fairly straightforward to generate them and secondly the Genera B is factorizable so what you generate here is independent of what you generate here except this conditioned on being a human so this is not a very good generative model you couldn't generate anything really realistic from this you couldn't get good FID scores but um we don't care because our goal is analysis and as we say you may not need really good generative models in order to do good analysis at least not if you're valuating your algorithms by on the current Benchmark data sets that we have so there's papers here there's more papers and this earlier stuff we did on 2D comp Nets which had a lot of the similar ideas in them so how well can the 3D cgns perform analysis and I'll show a figure about them a little bit later later on and we're going to test them in C two types of ways one way is how well they perform on data that's very similar to data that they're trained on so this is classic machine learning you train uh you have some training data you have some test data um and you know what the last 10 years have shown is deep networks do incredibly well on that um but um essentially from my perspective they're behaving somewhat like intelligent pallets intelligent parents are very good at recognizing stimula they could do certain tasks better than doctors because of those abilities but they're not the perhaps the important point of well I don't want to be too critical of parrots and who really knows how their veins are working is that they are not good at generalizing to new novel data and the same thing applies to the previous set you know the standard set of deep networks and until possibly you have these enormous train data sets which have some ability to do it so you test these algorith conventionally where the training and testing data IID samples from the same underlying distribution but you can also test them in out of distribution settings where you ask them to do tasks that they have not been particularly trained for and on data that they are not particularly train to do so these models to train them requires 3D annotation um at least there may be clever ways to do it that we haven't thought of but essentially you need 3D annotation for this and you need 3D annotation for testing them as well now this goes against certain of the uh bigger current popular themes at the moment as you try and see what you can do with lots of data that is hardly annotated at all uh and there's a lot of progress you can make with that but I believe uh you know the cycle will turn and we'll realize as well as that data we need to have data with 3D annotation for The annotation if only for evaluating whether our algorithms are any good and probably for training them this however put restrictions on what we can do so we worked on the Pascal 3D Plus data set because that existed people at Stanford had made it and we could use it for training and testing although more recently I mention at the end we've scaled this up a lot so for out of distribution what we do is we can test to see if the algorithms are robust to occluders by photoshopping occluders and putting them in there in them by the O CV data set that we created and I'll show in a minute and on realistic synthetic data that I'll say later on how we generate it realistic synthetic data meaning okay diffusion data but diffusion data prompted with 3D okay so here is sort of one figure of what these 3gn cgns can do you have an input image in this case of a car you have a 3D model of cars of uh of buses of airplanes whatever else the object is for that for for each object you have a generative model a 3D cgn of how it can generate feature vectors uh at different parts of the object and then you can do what was called um vendin compare which essentially is analysis by synthesis the sort of dumbest way of doing this is that you consider every possible object that could have generated the data you estimate you know you have an algorithm that tries to estimate it 3D pose that generates it best and then you consider uh the uh you know you you find you interpret the image as being the object that is best of generating the data with the pose that generates the data for um data best as well okay so generative models and feature vectors um and the feature vectors would be vertices of uh CAD models type you know for that type of representation these 3D pose the object class and known during testing during training now making another Point here we are not training the algorithms to do classification or 3D POS estimation at all we actually train the algorithms to distinguish between different vertices vertices of the same object and vertices of other objects so it's trained to do one task but then uh it's able to do other tasks object classification and 3D post estimation as well the inference rendering compare you can do it we came up with other methods that are faster at e for classification about the speed as the standard deep Network okay so to show people tables which is what you expect at a cvpr conference here here are some of them this is the performance of the 3G cgn compared to the others uh the First Column is okay Reg regular data regular testing IED setting L1 occlusion 20 to 40% L2 40 to 60% L3 60 to 80% um and you can see how much more robust and reliable these algorithms are when the occlusion becomes M medium to uh to extreme this example here with the car uded by gigantic cat is a bit extreme uh but um I was told by my students we actually do get the three pose and reestimate the cap from that so this is so these models because of their knowledge of the 3D geometry of objects have the ability to transfer as well you know and generalize another example of Rod you take images here from Pascal you now go into shape 3D pose texture Etc unusual cases this was the data set that we have and you see there's a huge performance drop okay now if you do the 3D cgnns again because of the knowledge of the 3D structure of the objects they and other properties they their performance decreases but not by nearly as much all these figures are really a bit old out of date I think our algorithms and actually the uh the alternative algorithms would probably do better but the basic it stays the same so the 3D cgn summary this works for 12 object classes on Pascal 3D plus there is effective state of the art Alternatives when tested IID setting they're much more robust uh when tested on new domains and the presence of occlusion on synthetic data and this can be improved by um extra things you can do which again exploit compositionality by P Kik at how Okay so this is okay but maybe in computer vision you don't really care about 12 objects you know right you may uh you care about more can you scale it up so the issue then is this reliz on data where do we get the data okay so how do you get data sets so there are two sources that we've developed um one of which does rely on diffusion model models which of course everybody knows um what we did though is diffusion models are wonderful generating data but they don't come with annotations at least not yet people would like to get them out but they're not there the diffusion models have some knowledge of 3D but it's sort of implicit and not easy to get to so I'll present a little bit of work that we had an iclr which where by combining the diffusion models with uh 3D CAD models you can generate data that's rather nice fairly realistic and has 3D annotations on it as well that's one thing the second one is a recent data set with 3D annotations of 2200 object classes uh submitted somewhere okay so diffusion models are impressive uh many of them particularly sunning work to Bal him but uh you know they are incredible they're very nice very realistic as I've said earlier maybe you don't need them to be realistic if your purpose is to use them for analysis but what you do need them is you need them to have 3D annotations in order for training so you want synthetic data for 3D task you can take there certain computer Graphics models that exist already but diffusion models um have the nice property that's you know they incredibly Rich you know enormous data sets and they can look more realistic than many of the synthetic data so here is the basic idea very simply you have a computer Graphics model you have a diffusion bottle with PRP generation you can put them in there sorry we're using units within the diffusion model uh but you know that could be change you produce large amounts of Fairly realistic data with 3D annotations uh which you can use for training uh training algorithms as well so this is an attempt a way of generating the uh you know of getting missing the data Gap you know as I say I believe The World Is You Know believe the objects in the world are 3D I think Compu Vision needs annotations of that uh we don't have it they have to be created uh all the evidence about human perception that I'm very much influenced by really implies that humans have 3D knowledge of objects and we learn that during development by interacting with the world touching objects playing with them things like that so we learn it from multi modal sources but we have that type of knowledge computer vision tends not to have it except for humans because we've got data for that and anyone who's sensible working on uh any you know working with humans is going to use three-dimensional models for humans okay so we should be doing it for everything else okay so this means though that if we believe in this type of approach apprach which I do because of the many properties the robustness the ability to get information out that you could use for Robotics and other things you have to create this data and then you have to show that we can scale up the algorithms so given this data we could scale up the 3D cgnns um there's some technical issues about how what we did in them they were trained if you remember to uh distinguish between different ver veres the number of vertices uh to distinguish between is going to scale roughly quadratically with the total number of object classes that is a lot you don't want to you want to avoid doing that and so we developed a uh a method called composition or contrast of learning which essentially reduces that um by uh starting to remove vertices between object classes which are not confused so then you scale this up to 116 object classes uh that's work in in program you know in review somewhere why 116 well because it depended on the data sets that we were constructing at the same time and they weren't although there 200 classes they weren't ready enough at the time we had to submit the paper so the findings still remain that these 3D cgns are more effective um against standard types of deep networks by about 5 AP um you can do more complex things with deep networks it would increase that um but you could have done that with our algorithms too um now this may be a little bit surprising but the point of view here would be that the our algorithms know about 3D data they're trained to use 3D data where a standard deep networks whever clever they are don't know about 3D data even if they giv it to train on uh and the classification performance uh increases as you increase the amount of occlusion on these objects uh and so you're all performing them by 10 AP uh Etc okay and if you do it to synthetic data again out performing the Alternatives no them is actually a earlier version of our our work um Etc so there the conclusion so I'm arguing for an old idea that's out of fashion but maybe coming back into fashion again because there's so many people working on nice uh generative models of images um but I'm making the point that good analysis may not necessarily require good uh realistic synthesis you know it depends on the task that you want to do and computer vision is basically doing pretty qu tasks at the moment maybe you need accurate very accurate generative models to distinguish between my car when it is dirty on a you know and my car when it isn't dirty but we're not doing that yet so then the 3D cgns perform well for object classification and 3D pose estimation when tested IID um you know about five you know and particularly when tested oad they do require the data for 3D annotations um so we were originally restricted to doing it for 12 object classes but now we can do it for 100 and probably 200 uh by now as well um so the series of papers with this I'd say yeah Adam has been a collaborator on all this work uh and I guess this is the you know the purposes here are generative an analysis two sides of the same coin uh having annotated data sets rather than relying on huge unannotated or weekly annotated data sets uh frankly I I think you need to have it uh not necessarily for learning but certainly for evaluation and that's a huge gap I think in conveni abion community so uh thank you for listening here is here is the yeah so this is the summary slide I'm down to one second or so so uh I won't say anym uh but I welcome any questions and feedback okay thank you any questions I mean I could start with one I mean we're discussing this very frequently so there are not many questions that we haven't discussed yet but what do we do with with things that are not objects like when you go into the jungle and there is like so much stuff that is not like like this kind of close form object um I think we'll cross that bridge when we come to it I mean I think there's the issue of how you how much data you need to learn these objects from so certainly here we required synthetic you know you required annoted enough annotated data learn the models I think with insights perhaps from computer Graphics ging splattering methods it may well be possible to learn these with far fewer images if you're walking through the jungle you've also got uh you know you you've got image sequences you got the ability to estimate the 3D structure of the scene anyway 3D structure is sort of you know you know comput a structure for motion binocular stere you've got all that ability going back to David Mar quote Old references you got the ability to get 3D out from a single you know from a very small number of images it's obviously not used for that purpose much in computer vision but you've got that ability so I'm not you know I can imagine that you could construct a model of an object on the Fly just from a few images without much PR knowledge about it okay question yeah yeah I just want to hear your thoughts about you know the RO language you think it's necessary think we should get uh okay so yeah thanks for that question um I think that language is important uh because language gives us knowledge about objects and about the world so Vision you know again partly from a CO cognitive science human perspective on it we've got images there's a vision process of estimating objects but then there's a lot of common sense knowledge we have about images about scenes about object properties and large language models are incredibly good way of using of taking advantage of that type of knowledge before that we had nothing you know nothing equivalent so we certainly need to combine Vision with uh you know with with language in my opinion the question is how do you do it if you take existing very popular you know Vision language models and they're throwing all that stuff together is that the right way to do it it's obviously very popular uh and people some people I've seen a comment saying okay vision is solved because of uh some version of this from Deep Mind a few years ago um I think that's I think it's good but I I don't feel it's adequate to deal with the the really hard vision problems I think you're hallucinating things we have some examples on this which have published in this conference which we uh we didn't put in the critical side but uh I got the student to you know to do it afterwards the model will work it will give you incredibly uh nice answers to questions about images but then you find you can generate images quite easily where the they give you the same answers and the answers ought to be very different so I I think language is really important but I I for myself I think it ought to be I you know I'd like it to be combined with vision in a rather model modular method to use another term that's completely that's almost completely out of fashion do it at the object level maybe at the scene level otherwise the vision is using the language as a type of crutch I mean I did like your work about you know okay you're trying to find a coffee cup so you search for a table right that's that's that's a good use of it um but anyway that's a view point that the models have some and some also some use data to train your model the bring some bad effects on the and how to avoid that right no it's good good points there um you want to you have to make sure that the generative models could do with everything you know are really good generative models and they could generate anything you any real image that you that could occur in reality and some of them you know maybe some of them do we found with the diffusion models okay they trained on who knows gazillions of images they're not very good as far as we can tell of generating a guitar SE from a side view because of all the data sets they've got they're still inet images these are somewhat biased they don't have it so um and that would apply to even you know to image that data sets these are bias too and so there is and this is a good thing for the community I hope not just just us we have to go outside we have to get all types of image as we can uh you know with cell phones attached to our heads or to other people's heads or to babies as some people do uh and you have to make sure that we can generate anything that exists in the world I think yeah otherwise yeah the things will be Bast they will not attend but that's actually a bigger picture of the generative models you really want generative you know you you want to say okay we' got an analysis process we've got a generative process the gener process that we have in our model should be able to generate anything and if it can't we need to change it that means you have to be able to evaluate whether we can generate a particular image or not which Auto encoders as one potential strategy for okay and with this we actually ending on time our um presentation so thanks again to all speakers and so post the session starts at 12:30 make sure to be there and then nice talk hope yeah we lost a few people but well yeah also L time yeah yeah I'm about 3 like for like some like scalability like compared to the signning Dr sign work uh let's get started with the second part of our Workshop super happy to have Jun Vu with us assistant professor from Stanford very well known for his work uh during his PhD with with Josh and uh of course work that happened afterwards numerous Awards career Awards young investigator Awards and he's going to tell us about uh generating objects and see and World very excited to have you here thanks for coming thank you thank you thanks for having me here see a lot of people okay disclaimer uh I have a few talks but then this one has quite significant overlap with one talk I gave in the morning it's not that many people though so but other than that it shouldn't be any overlap with other talks so if we were there in the morning this compositional 3D stuff that you may want to leave right now um okay um okay okay so yeah um I'm going to talk about generating you know generation it's exciting right so you know how can we generate 3D objects and scenes and how can we go further into generator world and a large part of it is I'll say um you know how can we leveraging uh the composition compositional structures or the caal structures of the world and the reason we want to do that is as I put the second line here you know what it means for computer vision because you know if we can just generate a lot of scenes and videos and that's it sometimes happens it might get okay I just try to speak louder I heard this is a common issue so you know if you can generate a lot of beautiful videos and scenes and um does it work okay yeah yeah if you can generate beautiful images and videos and and shapes and scenes right so but how does it have anything to do with computer vision right because what what no the definition of computer vision has becomes really Broad and everything is geni is computer vision well that's probably not traditional computer vision that's more like Graphics right but now I guess computer vision is everything but if we if we look at this Workshop you know I think we still thinking caring about what is real computer vision in the sense that how you can have theal pliers and how these pliers may be useful if you want to solve actual computer vision problems so I would say you know to better solve these problems you probably do want to think about the composition of all those structures you know generation process as well so you're generating things not just because they're beautiful not just because they're realistic not just because they're diverse but also because they captured the call structures and that's why how they can be used when you want to solve computer version problems okay so what does that mean I have 30 minutes right yes okay um so you know you have a picture you want to generate that picture but then let's think about why the picture is the way it is right if you look at this image formation process then the question is you know how does nature generate this picture right so we want computer Str picture let's look at how nature is gener this picture and you know the picture is because know they're just underlying geometry objects and textures right what is made objects and every object this line had this particular shape of certain materials it may move in a particular way right so all these an physical properties is sort of decide okay know nature is doing the rending process for you so of course you get this picture for free and people have been trying to replicate that right you replicate in computer Graphics when you build a graphics engine what you want to do is exact so you can take all these physical object properties and produce the picture and in the context of this Workshop right and also the more traditional definition of computer vision is you want to invert a process right that's why people say Vision as inverse Graphics because what we want to do in computer vision is sort of try to dender this process and take a picture and you want to get you know what's the underlying geometry or material motion of the objects and ideally because there's ambiguity right you know Vision always kind of under constraint process the inversion process um so you want to not only model a single output we only also have a distribution of these things right how can I go from image and try to model the distribution possible anine geometry materials and motion of these things from the image so how can we do that we have learned that data is extremely powerful you probably want to scale up who can have data right and but unfortunately unlike imag and videos and language which you actually do have a lot of data for free you know for the things a lot of things that we care about targets computer Vis even the intermediate representations you just don't have data right how can how much data can I have for the 3D Shape Of The Line I'll probably have like very very few right even for General arst any s object you may have like 10 million as ope but for a specific object you just have very little material almost zero so maybe we can say why don't we just collect label right so we try to scale up and collect data and so that's what people have been doing adting 2D and trying to Ting 3D as well and then do more caping get geometry and motions of animals and humans as well you know apparently you have a lot of issues one is is hard expensive TDS second is it's really hard to skill in real world in the wild data right MO is hard to build allom this giant sphere it's expensive how can capture all these possible things that happens in the wild data and as I said earlier you know we care about the C structure what will happen if I move the camera what happen if I change the light that's not possible to do if you just collect data so natur I want to say is that possible how should learn in a wild right so if you look at all these videos that's great have all these diversities and if we can learn just from these videos in in saliz oriz way that's that's better that's great but as we said these videos are I see beautiful videos of cats moving but I don't know what's the underly geometry and materials I can have an I can have a gas I don't have qu annotations so what should we do you know we can have this gener model right gener model configuration now we want to solve a computer problems and want incorporate gen model inside what we can do is we can dender uh the world into things we care about actually for materials and then we can rerender them we can have a g model you can take this intrinsics reender reconstruct and of course I can have a reconstruction loss ideally you know all these things can be done in a selfic way so I can just use the Reconstruction law which has requires little or zero annotations like this 30 for free on top of that right the things we choose are not random it's like okay I choose like five different random properties because I care about it but I choose these properties because these are like these are also this examples but they like the fundamental fics or F op that's basically deciding okay why the world is behaving the way it is that's how people design Graphics in because I know if there's a surface and if it's light then I can write a reflection function right so there's you know all these vdf functions for example they're kind of approximate how real life f is works so in that these things are not chosen randomly these things are chosen because they actually bring in you know inductive biases themselves and also you know sometimes you may have even additional knowledge domain knowledge whatever on specific parts of them oh maybe this thing can have you know a little bit of a fire that I can try to incorporate okay so that's kind of of overview of one approach that we're trying to do and there's a second approach which I actually just later but before that you know this is like the high level typ style one approach ideal one approach if you were I I talk yesterday I this is category one that you try to put structures inside the system and how gen models can be used in in verion problems and what you can do is you know you TR do self learning or mostly self learning on collection dat data during training and during testing just take a single image or a single video you dender to get things you care about okay so now I'm going to give us some specific examples about how we're doing that first look at single object case single object and also I assume it's just rid okay so there's no parts and object you know the things I care about here is geometry texture material light and Camera parameters right okay so as I said you know object intrs they may be in different indu biases right so for example you know if you have an image of a base then you know the way behave the way it is is because it has underline geometry surface normals the textures which is the Bel material which is how it reflects light and you know some lighting components if f c specular Maps as well as the environment liting conditions okay so these are very simple the opting trees care right of course they're different and we have some prior about okay you know how these things should be composed in graphics to produce pictures and how these things may be used in new render as well use you know again the renderer can be graphic can be new renderer right can be differential graphic so but new now having no using them in a graphics or F based way to compose these as well on top of that we have additional knowledge for example in this particular case we know that Bas is like rotational symmetric right so you have this additional prior about okay there are certain assumption you can make about a geometry or Surface normal of the objects we also know that the material of the object know we can make a I think a safe assumption that this thing is homogeneous right so every point on object is reflecting the line the same way right so what do mean is okay you have you can use the same Prime transation put a material opes of objects everywhere right because you assume it's it's you know homogeneous every point is reflecting the same way you use the same VD function and all these things so you can make pretty explicit assumptions about some of these properties like normals and geometry and materials foro is a little tricky right if you look at the texture of this object you're like okay you know it's not like every point on the on on the base is the same color it's not like a Pure Color vase like this place this point is wide and that point is wide or stuff like that right but it does look kind of you know perceptually similar right even if there are spectular regions this place it's really highly spectular you know I know that underline color the actual color of the base you know it should be like similar with even these dark reges so if you look at AO Maps you know all the patches look kind of similar to humans and hopefully also to machines as well right so you can make like an implicit assumption about okay what you know about aido although it's hard to re write it down as a equation like reinforce so these are like different types of inducted biases or structures that you do have you know when you look at specific problems and how can we lever these spes on top of the inherent structures among these properties themselves for better object understanding and DEC competition so because I think um this you know the work here is a bit old so I now try to sort of I think the idea is still really interesting and directly connect to a lot of more recent work that we have been doing but I'll just talk about it at relative high level because many of you may have seen this paper or have me talk about this paper you know so this work done by who did PhD at vgg with anality here um and do postop with me right now and so um he did his beautiful work if you don't know I have a size for it but I'm don't show it because I have time but he did this beautiful work on how you can use reflector symmetry to do swee phase reconstruction without supervision which one best paper award at cvpr 2020 so you look forward to you're welcome to take a look U but then after that you know he went to Google to do internship while I was at Google uh so with Noah on and a bunch of other people we like is it possible for us to extend the work right so back then he was like using reflection Sim to do 3D phase reconstruction without supervision but what we can do here is you know okay is that possible for us to enforce these additional constraints that we may have about the surface Noels about materials of the objects and maybe some implicit assumptions about a of the object so that we can use them as constraints into this still end to neur network right the renderer part once we can do that what we can do is we can reender the scene and we can produce the pictures in now different views and we can rely the objects because Auto model factors but the here key here is you know the re rendering is again new network and new renderer inspired by Graphics s but you want to incorporate all these different properties but then how can you incorporate these properties right so you can look at the paper to see more details but the high level I would say you know if it's surface noral so you can know that okay the object is R stric then you can use a specific parameterizations you can parameterize it in a certain way so that you can say okay the object maybe I just have you know a vector of different radiuses at different heights as well as a scaler for the height of objects and you can easily parameter object in much lower dimensional space and the same for materials right and to enforce implicit constraints like AOS you will say okay if the object different parts of Theo should look similar to humans uh no matter whether it's from a highly specular region or un specular region then it should look similar to machines in the sense that you can have a neuron hour discriminator which will force the constraints between these different patches okay so basically you can look at the intrinsics of the properties and you can look at the properties they have you can look at how they're connected you can also look at the features they have so that you can enforce these constraint into the systems which allows you to again train without supervision you know you just pay a pi in a collection of pictures of the basis train on them putting some structure inside and then during testing you can take single image you can dender into 3D and you can so here some results I can take an image you can dender into 3D you get surface toos and diff spectum components material estimations and environment Ms as well as you can turn this picture now in 3D just doing testing from a single image including like paintings and stuff in your 3D you can see it from different views you can rely it so I thought this is really cool and um but um you know I also thought you know here we have this rotational symmetric assumption right so how can we enforce that if the face is reflection symmetric then you can flip the face and you'll say okay the face should look the same if I flip the face but you only you can only flip for example the geometry you flip the geometry of the face it should still look the same but assuming the Ling is constant and in the case of rotational symmetry case know we can say we can rotate out a little bit and it should still look the same because we have this assumption is rotational symmetric which is great U but yesterday I was saying oh when you putting these constraints into system and you present this work to humans and act question people like oh wait but you're only working on this particular case of basis right or how can you be more General know it's not like everything is rot symmetric which is totally true so how can you go beyond things rotational symmetric right you know have you heard of a bit lesson that you make this more inductive device into your system and it just first work and then you probably want to just go with more data you know okay I have some cont arguments to that yesterday so I'm not going to repeat them here but I think it's totally valid question that you you don't want to just work on basis right so or you don't want to just work on objects that are rotationally symmetric so natural objects are much more diverse and so we went on and we asked the question that is what is really shared among natural objects what is really shared am at least you know objects from a specific object category right so in the case of vasis yeah maybe they share this particular privatization you can privatize the shape in a certain way make learning problem much easier but that's not the case for any other object categor on most other object categories so what is really shared for any object categories if I look at particular category objects I know okay they're sharing this so here's example right so I have this you know a bouquet of lot L this here the Roses we know their R is very you know different you know I know they they have the same name there's reason we give them the same name we say this is rose that is row they must mean something right and so if we look at just examples of these objects we like okay you know what are these row is okay these are just like you know multiple instances of the same object category right so there's one r one RS two so what is really shared among these roses and the entire picture is like you know you have this you know 20 50 different instances of roses so I argu that you know the reason we give them same name and the reason that they're sharing something what they're really sharing is not a specific par trans that shape is rotation symmetric but instead it is the fact that they do share some the same distribution of the intrinsics themselves right so what I really shared along these roads is from the instances is you know they actually have their name Ros is because they share analy distribution of the intrinsics the things you care about you know they must have underly distribution of possible geometry and textures and materials so you can sample from it and once you can sample from it you know you can put them in different lighting conditions and poses you get all these examples right so here you're making an important assumption but sometimes is so weak that is you know you're not like oh object has to be symmetric object has to be rotational symmetric it is not anything that is you know it's easy to parameterize it or anything that you can write as equations or you can still write some equations but it's kind of very high level guidance which is sort of instruct you how you should design a newc how this different you intermediate repr should be connected of course it's closely tied to computer Graphics but you really want to learn this a prior that is okay if you have this up category so say there's a roles and just give you a single image which is very minimal observations they already give you so many instances about okay what is could be about so how can we go from this you know very minimal information to learn the prior that is really shared by all these instances which is the distribution theine distribution of their intrinsics right that's the gener model we're using and once you can do that naturally you can solve this inverse problem computer vision and can solve a lot of computer vision problem because Sol reconstruction we relighting and stuff like that okay so again I have don't have that much time so how I would say you can learn the distribution inic you can sample after what we want to learn is you can sample go from the noise right you can sample instances of objects of different shapes in The Beetles you're also trying to approximate or this can be learned as well but I think in this work we have some a little bit about heris stics that you can have some distribution of the X six which is the lighting parameters and object poles right po of object once you have that you you can put object in right pulse and you have the shading you can put a on it you have the appearance and you put Cera or something and you get a fake picture generate picture of roses you can compare that with realations real instance of roses and back then because the SP was like almost one and a half year ago one and a half year ago we're like okay diffusions not as popular yet we're like okay we can just use again right to classify this input image whether it's a real image or it's a fake image right with very Min you know again nothing is pre- right I don't have a pre model I'm not using the diffusion part which is pre-ra everything is just stand on this single image from a single image when you have this kind of a little bit very minimal guidance on the inducted biases you have how the word how the object should be generated I can construct this system I can learn from a single image and what I can do is I think we can do a lot from very minimal input that is you know from that single image we can actually learn and this entangle a distribution of object geometry and textures and and materials and you can sample from it so that you can get roses of different viewpoints and as in the bottom row you can see them in different lighting conditions as a second to the bottom row and because you learning A J model you know you can sample different instance rows as a different geometry or different textures that's in the middle row right so this is done by Andrew one of my PHS okay so of course you Lear model you can use that to solve computer vision problems you sol a lot of computer vision problems now relying seeis here some more results all this from a single image without pre trining you can get a lot from a single image okay so results here I think are great if you just look at a single image case but they're not as you know perfect as you can imagine for a number of reasons right so one of them is we really want to do everything from a single image there's no prior right so of course we can do much better now if you have you incorporate some diffusion product which has learned they have learned very good prodcts about what natur image is look like right so incorporate diffusion prod just work much better and stuff like that you know so there there's still also a lot of advaned new rendering in the sense okay what would be presentations because I I didn't wasn't able to talk about the details right oh how do you specific how do you really parameterize the shape how would you parameterize these uh the materials and stuff you know I think there are a lot of advances in reasons in new rendering that will help us to do better as well but I think the overall concept the same right you build this gener model of objects and think about you know this you know but the G model itself already takes as input or intermediate reprs that's exactly the target of computer vision an intermediate rep you want to get or the final output that computer vision system would like to get so the really can help each other okay so I'm going to talk relative quickly about the other parts about how we can go move beond from a single object case to cap form you know um so OT has been you know the person who did this facee paper before that face paper and then as I said he he graduated from BG and came here to do a post off to me and then we did discovered to work with Anda um how we can go beyond you know just learning from single object category right we said okay we have been doing work account roses or raises or these are great and you know you can make assumption other categories you can relax the Assumption you can say okay what is really shared am object category is intrinsics themselves but you're still reting yourself to case there's a single object category so you want to move around saying okay can we not doing a single up category can we learn something that's beyond one object categories right so AA had this earlier work called Magic Pony which uh both beautifully trying to reconstruct everything I talked about before you know shape and stuff but also the articulation articulation of horses Magic Pony we reconstruct horses by putting strong inductive deves you have a t model you can reconstruct the geometry but also articulation of the horses so we thought okay we can move out and saying we want we don't only do horses we want to go be on a single object category what if we do 100 different object categories right so that's what we're trying to do you know we can extend our system to work on only a single object C forces but this is thing called learning 3D fora of the web I we try to make this system one single system one model to take any picture of a quadrad still limitations we're not doing fish we're not doing like birds and stuff yet but any picture of a copath that you can trying to learn the s b of the web doing testing again from the single image you'll be do that so I say the biggest change we make you know still conceptually the orator of course if had to in you know that's different change we have to make is now okay you need to have like a semantic base of shape bank so you have a learn Memory Bank of possible different base stapes they're all quadrats but you can see that they more or less assemble reassemble different slightly different quadrats and on top of that you know you have to do now you have specific instance level predictions right so we can do the same in Aro geometry Ling view point as we need before we have this more articulation prediction Fork which we just to directly from mag Pony which is another you I said earlier what ear did if you haven't read it I think that's great um and but you also want to predict instance typ of you know deformations about how these faes can be integrated merged as well as instance specific deformations so that you can use that put all these things together to explain the final object shape articulation and deformation given the test image okay so during training you have a bunch of reconstruction laws as we said before uh but reconstruction law is not just like we have L2 right know that's part of it but you also need to have image Lev similarity loss which can be learned as well like per lo as well as these you know feature matching L like now DQ and stuff like that is really really powerful so we probably want to also have the mat feature as well so use a combination of these laws but they're all like you know reconstruction laws in the sense that they require very minimal supervision know the only sitution if you need about if you need if at all is maybe you want to have a se you want to have a mask but that is you know and of the Shelf method can do that so it basically else it's basically learn without Supervision in self bypl on a collection of data which is images of you know quad from 100 different object categories and this category Master training and you're having a pile of these imid of Cs you're not assuming have C labels I'm not saying okay train model on horses and now train model on sheeps or whatever I just or just combine a model one model two car but they all label that's not the case right we have a collection of images not I don't have any category labels I'm not seeing any template shapes and stuff like that and you know you have this TR two models inside the system you can use it for computer vision so going from a single image turning to 3D you put textures on it shape on it you can see it from different views because you have the articulation you can also you know manually trying to animate rig these parts so you can see that how these parts can be moved mot here they not learn okay they think is static so we just you know animate them to show that you you can move them because we have learn articulation okay um so you know are conference and the work is C 2024 so you welcome to see our poster and also Beyond animals Beyond animate objects there also inim objects which probably have more form that you care about right if you look at manade objects you know all these chairs and tables and Furnitures or buildings and scenes realize you know there's actually much more regularity in these objects this is important because you know humans have this preference about making things to be regular right look at all these like wooden blocks and whatever lights you just like arrogantly repeating themselves because human has some conference about Bey and regularity and it's much more you know obvious in M objects than animate objects so naturally we also talk about how our system you know the fact that you have the Str models can be used to explain these things we had this eario work about okay you want to go from an object you can TR to info a program that's sort of trying to explain the shape just like you don't have annotation stud object and physics how can you have annotations on programs you don't have any annotations so you have to do in a self way in you have new that had to generate a program you have you have another new network you had to explain and execute a program right so this allows you to do the program learning a moreel way as well and when we did it in 2019 you know we are making the same set of assumption that is okay the previous of the programs are the Cinders in qway right just like we assuming things are rotational symmetric or reflection symmetric that's not the case in real world of course we only relax it so in a followup in newf 2022 we relax assumption then now we're seeing ENT of perers are no longer assumed to be per to be Cubs and cylinders but they are learn implicit functions so you have learned local implicit function to capture specific details of object Parts but you have this high level program that sort of tells you okay okay these are the things that should be repeated right so basically if you look at airpl when you're saying okay you know I want this loal to capture specific geometry of the airplane wing I I have this program that tells you okay they have to be the same air same Wing they have to have exactly the same shape because I'm reusing the same learning functions and the only thing that has been changed is the action s it's the PO of the part right and that's the case for scenes as well right so if you look at this building and say how can I make it taller right so IDE thing I want to do and the simplest thing I can imagine is I have you know a volume box and I do interactive segmentation I get objects and I say is it possible for us to do see one drag one step of interaction how this make the building poer and make the building wider right so this is something we did a demo we did a few years ago where we put our algorithm inside this adob Photoshop G so algorithm here this one is actually our algorithm but this is not easy problem because if you want to do it with a single step interaction you have to have understanding of objects or scenes and multiple levels of abstraction right at the lowest level you have to understand what is the texture and the middle level you have to understand there's prospective geometry there surfaces and there surface Nels and you know if you want to make building taller they have to face the same direction in the highest level you have to understand there's repetition right wings and wos and four they're repeating themselves so you know of course we have some combination of the program structures also was learn you know JN and stuff like that you put in the textures so some kind of G models we put them together into you can think about at high level still this encoder or dender and render framework so this is in 20 I can't remember okay 2020 right I thought it's pretty cool that you can make the building powder you stand in a corridor you can turn around right it's actually results are very good but the issue is again you know the water is not made of you know vases or it's not everything is you know a p and cylinders right it's also the case that not everything is a building or a cal right nothing is perfectly repeating themselves and there are much more in a wild pictures so you want deal with so you don't want to rest yourself to these specific object carries so how can relax that so now it's 2094 and we made this Improvement at this conference that is we can take a single picture arguably any picture in practice it doesn't work that well so still have some assumptions but much more diverse pictures but still a single picture during testing and you can do now 660 s this is zero Ms by K Sergeant who is another my co with f okay it's much more diverse and you know quality wise I'm not sure if it's actually better but it's much more diverse you know you can work out any arguably more or less any type of input images but here we're also making a change of the Pam shift or we're exploring that is you know in most cases we're thinking about G model for confu vision you take IND devices into the system you can render it right and then you can use it construct also in different ways so that is you know I was say the structure in approach ideal one approach I presented yesterday and here we're exploring a different Paradigm which is I think still very interesting and they have the same Target but it just used gen models in different way that is okay now what if I don't put structures inside neur n inside G models but I want to distill the structures outside out of the G models right so that's a second approach idea two of using models for computer vision that is you take this ver very powerful pre models because now you can train models much more powerfully and on much louder data set they work much better so if you can take these models here let's say powerful diffusion model and how much can still the structures outside this diffus model right there a reason that you may want to explore this change because as we said we wrap up very soon one two minutes yeah is when you go back to par one then realize here is render rer which is think about it as a vendor or it can be a new Network graphic s it can be you know some kind of whatever it can be all the decod of all incode anything right but they're making different assumptions about the SC especially if you use a more traditional graphic or even a differential graphic or a neur neur render right for example you can think about as if in the middle is Radiance and space then this is like Nerf and in in case of Nerf then you have this reer pipeline which use volum rendering volum render is great right it's pretty General and that's why nerve can work on a wide range of scenes but Vol rer is not but any renderer is making approximations real world that's why they're working better on some domains and they work less well on some other domains right and so if the domain say what you care about that's great or the renderer can be very very powerful and they can differential they can do everything but to let them do that they may be very slow you know they take an hour to make a single step but if you want to put them into the training pipeline you want every three4 pass to be less than 100 millisecs and that's is not possible so you have a lot of constraints on these renders that either you only work well on specific domains or just some to slow stuff like that right so if you relax on some you can think about these powerful diffusion model as a very general in sense that they can do pretty well and they're pretty fast on a bunch of other things or even if they're not fast enough you don't care about it as much because now you're just doing it during testing doing fun new distillation right so you can just try to dis as much structure as possible from this F models that's the second part this structure out approach and um so this is Zs more results single image more diverse and you can still you know doing testing single image you can does and finally I'm going to wrap it up by saying you know uh Co and you who was a m of my PhD student thought about how is that possible for us to not only just disal from a diffusion model but there are other type of foundation models right can we also disal from Vision language models or large language models right so he did his work on wonder Journey which allows to go from a single image or some language to only get a 6 View one scene but much longer Journey right so a few steps you first take a large language model to generate a long description of scene descriptions and you have text Point generation p to syze these 3D scenes you know using diffusion model for text based outting and stuff like that and finally you also use a vision language model to not only do in generation but to check its generation to see okay if the generation is consistent with what I want am might just putting everything to your frame and stuff like that right so you know we use multiple different F models in different way try to Ste as much structure as them as possible from them and here are some results right so again similar you can go from single image and this is really like more relaxed right because now you're no longer function on real estate here it's much more relaxed you can take any picture AR I think it's now it's really almost any pictures and you can go that we say from anywhere you know you go from anywhere you can these pictures and for single picture you take you know because disting structures out of it you have to capture a possible distribution public distribution of what a thing will be like you can have multiple different samples from you can go from same place to different places right by to everywhere okay I have to SP on slide and uh this is what we talk about forums but have to talk more about them offl and for motions long to talk about them because I just don't have good enough results so wait and save for next time until we have a better result okay um so to summarize um you know I think there's always a fundamental question about what you learn what you model there a problem in G model that's problem in computer vision how much you want entry and if you want to have something inside what is inside and stuff like that if also what would be the target for Generation what would be the target for computer vision right so you know I think questions we're exploring asking here is what are the minimal assumptions right for S perception for S generation that's well I should put perception and generation right so what's the main assumption you need for perception generation what ofes you need that you want to build a g model and you know how they can have Synergy with the perception model with the Reconstruction model because they may just be the same thing the same set of assumptions right so how can they serve at the same time both generation and perception what the relations between them and a lot of things we have been doing I can I can summarize them as do inverse ring or Vision but based on this caal fysical Universal object intrinsics or C intrin in the sense that because they're they physical naturally you have this model so kind of you get a gen model for free or that's you know um but naturally have it and allows you to really uh because you're using this call process in the wild in Wild data you have very effective use of this in Wild you have very effective use of in Wild data allows you to potentially leverage synthetic data as well and an important part of it is the uh inductor biases or assumptions often they naturally align with uh interpr interpretability and also the controlability right because you know why is it so hard to control a video generation model how can we control the camera parameters if you can disentangle the camera parameters you get video control of you can control of these video generation models for free because now you can just directly change the camera parameters and finally allows you to hopefully better generalize as well because that gives you the compositionality and composition generalization is a very important problem that people care about yeah thank you okay we have two minutes for questions so maybe one or two y please uh I think it may I think there are two challenges one is sometimes these equations are too complex and they're not differentiable so if it's not differential I think I actually quite like oh why do you care about differential ability because you can do entation gring design has a lot of problem but still Bas tools we have and also to use really in Wild data so if the equations are not differentiable or they can be made differentiable but they're really slow so that's kind of a problem right and I think the Second Challenge could be um you know I think a lot of these as said I sort of suggested is you know if things are in domain it's all good you know the reason that there's Paradigm two is oh sometimes you just work on things that out of domain right if these are things are not explainable by models which that's a disaster so you know defion models and just kill things out of it has this benefit of their arguably you know more in the wild the quality I'm not exactly sure but they're more in the wild because you're you're you're longer with the assumptions that come with the generators so the issue we' been doing with all these F methods or P there's all these entire community on like fing 4 hours right but the issue is you know because we actually unfortunately we don't understand physics that well right so the fys equations especially in flu simulation they all just approximations so when try to apply them to World data you feel like the real world data is actually out of domain because the equation you have us so bad that I can't explain them that well so that's another challenge okay then uh we are on time thanks again thank you e e yeah that works great okay so we are going to uh continue on time actually that's fantastic with Karina fraki um associate professor ATU very well known for her work on 3D understanding from 3 video streams awarded with all young invest investigator and early career Awards and of career award um we are very happy to have you here thanks a lot yes thank you very much for for organizing thank you for inviting me um yeah so so the topic of this Workshop is very close to my heart as I'm going to explain in a little bit so we have discriminative perception that learns the bottom mapping from image to labels that we care about uh directly Maps models the probability of Y given x uh unfortunately it also learn shortcuts to get the Y from from image pixels then we have generative perception that this this top down mapping probability of x given y a much harder task that has seen an explosion in recent years which is extremely exciting and of course the natural question that the workshop asks is can we combine B the two uh so some uh questions that were raised this morning from signing and Alan um was about um you know can is important you know the visual quality of the generated image how much it affects the quality of the analysis when we do analysis by synthesis so so here are some slides from 2019 actually that that remind of some some talks I had given back then uh that again relates to to the questions about what should be the output of 3D perception so back then in 2019 everybody that was doing 3D Vision was trying to go from images and pixels to 3D shapes and then illuminance and then uh this type of uh uh you know quantities that are both impossible to get impossible to supervise impossible to generalize across categories and so on you have a piece of paper you have the book Etc it's just impossible to to go beyond vases and so on as as as as also janun mentioned and I'm extremely extremely you know impressed by the fauna work that was able to fold in that framework such a wide variety of let's say animals which is which is really great and and I very much believe on on the progress in that direction but back then we really gave up and we said what if the output of 3D perception is not this explicit quantities but rather some implicit uh representation there are just some featured vectors arranged in a 3D grid okay and the only thing that uh you know this the inductive B we're going to put is is about the 3D nature of that of that g so we're going to go from 2D perspective images to this 3D feature grids we'll be updating the grid as frames goes by okay so you update your 3D feature volume and you train that Network to imagine other viewpoints much like no view synths essentially all right and there's no diffusion here and everything is deterministic and you know there not even a Transformer it's just 3D convolutions which is a terrible idea when you go from partial 3D Volume to complete volumes but uh even that we were very excited to show that look comparison to architectures that don't go through explicit 3D bottlenecks this can generalize from when you train from two objects to four objects while other architectures were stuck to two objects for example okay and again when instead of generating pixel values you generate contrasty features much like what Alan was discussing okay so you render feature Maps 2D feature maps from that 3D feature volume as the camera moves around um the the quality of perception improves so what you have right now is 3D feature volume and you can do with it whatever you do with the 2D feature maps you can put object detection and object segmentation and so and indeed if you train for contrastive prediction the curve is going to be higher than if you train for Pixel generation back then but I would say this has uh has been adopted to autonomous vehicles you know they get perspective images fuzee them and geometrically compose them into the you know bird eye view map or a 3D feature map from where objects are detected and so on so you don't use 3D to directly through analysis by synthesis toate the pose of objects and Sh which is the right thing to do rather you just generate a feature map that is EO stabilized and then you put your uh perception uh routines on top of that 3D feature map okay so and we are very excited that you know what Tesla causes the vector spaces is very much something closer to this um so this talk I mean also trying to to think of how can we use generative modes for better perception which which which makes a lot of sense I want to discuss two things first some autoencoding models okay that encode and decode the encoding is essentially perception and the decoding is essentially generation and I will give two instantiations of those models and the benefit of those model that is a test time because they generate the input you can do test time training by through that unsupervised reconstruction loss and the second part is again how using this type of uh Auto encoding how can we go from monocular videos to to 4D essentially tracking in 3D across occlusions and across viewpoints uh so so here is how fit forward difference what fit forward differ does and I'm going to talk about Point clouds right now and I'm going to go back to images in a in a second so the 3D Point Cloud comes in and you have your favorite detection Transformer that spits out you know 3D segments all right we have query vectors attend to the input pixel features and they spit out the shapes and you know you can train it on your training set and then you go out of distribution and you ask your detection Transformer to segment those shapes that for all of us you know we see the familiar primitive shapes of horizontal and vertical uh you know lines and and cylinders and so on and and guess what it it fails miserably to segment those okay and indeed I mean I trained on chairs why would it work here it's extremely out of distribution now if you have kids then everybody knows that more willam's author and and you are extremely surprised with difficulty of the pictures on toddler's book so the the toddler do handle compositionality quite well many mouths many eyes Etc and one of course hypothesis is guess what our detectors that fit forward in nature uh you know cortex has a lot of top down feedback what if this compositionality has to do with our ability to render and complete things so here is one architecture that we're trying to explore to to make this compositionality emerge in uh 3D uh Vision so we said that first we're going to learn the distribution of individual primitive shapes in terms of latent vectors through Auto encoding and then a test time after I do that I'm going to instantiate an architecture with multiple such slot vectors and then I will do test time uh let's say Auto encoding so I will optimize for reconstruction and this optimization this GR in the sex will be essentially a search over those primitive shapes so so that you can assemble the new see okay uh so so this is the stage one essentially you have your primitive shapes and you encode them into the slope vectors just embedding vectors which you then use to decode the scene so you just optimize from these reconstruction laws on each individual shape so so these vectors now here essentially learn the distribution of the latent that I can generate different primitive shapes and what are the Primitive shapes we take any object category that is labeled and strip the different parts and these are your Primitives and then fit them into our architecture so you train your neural with one slot vector and then what you do is sh multiple of those slot vectors okay and they all use the same slot decoder and then you do a Max pool which doesn't have any parameters and then you optimize through Rec construction loss and that's it so you've never trained on chairs or complete objects you have only trained on individual parts and then you start doing this um essentially gradient descent and what happens is that this operates such a search so the network is searching over those shape Primitives to assemble the complete shape so it's searching over things that it has seen that are familiar to it to generate the complete thing okay and and it searches because you both need to obate the prior because those FL Vector the sample from that prior as well as you need to satisfy the rendering loss uh so so these are the two steps we talk we call it SLO Centric TTA because it has slots and it has the sign adaptation um so so here is how this works as time goes by during the search the Reconstruction L goes that so you are able to reconstruct better and better but at the same time you are able to segment better and better than you see so here are different surgeons essentially and and the hypothesis here is that maybe this is what let's say we do when we see some unfamiliar composite concept we try to search and figure out what are the Primitive familiar things that we can assemble it with and of course the Details Matter here and these are a specific instantiation of this um now you can do exactly the same thing so this is a toy example simple to to explain what happening you see those shapes there they're extremely heavily uded but our slots have learned the individual shapes and as we do this search you know every slot recognizes and can render the complete object okay so the network sees a very heavily occluded scene but it can still decompos it into the complete uh shapes um so we compared our method to know fit forward the detection Transformers as well as the shape program that that J you know presented earlier and apparently this type of test time search okay through gra inent is able to do very well on decomposing complete shapes into individual primitive shapes and it is very you know you cannot just say fit forward difference you need to be able to do a test time uh you know great sent this is where the red bar is higher than yellow bar and it's higher than other models that don't do such a on coding and and and also it was able to to to to you know capture this extremely out of distribution shapes the key here is that you do need to train into a set of primitive shapes in terms of cuboids so these are like geons from long time ago you know spheres and elongated ET if you just train on shape chair parts then you're not going to be able to explain you really need General Primitives that can assemble a lot of different uh things so the same thing you can do not only from P clouds but also for images and specifically multi images you attend on the image you render different objects and then at this time again the image comes in and you are able to render it and as you maximize you know the Fidelity of image you're constructing the segmentation accuracy also goes up and you are able now to render a lot of different uh uh you are able to render a SC from different View view points so your decoder here because it's an image you also takes as information the camera Viewpoint uh and again in these cases you're able to do better than two des segmentors and of course the most natural question is why didn't you show this in Coco and you show it in this toy scenes uh and the big problem was the decoder so that decoder we used to go from slot back to the part or back to the object in two the images was a deterministic decoder and this was the major limitation of this work as well as many other works that were using this type of Auto encoding but but now we know there are high capacity renders diffusion models that you can BR them with any representation and generate back the image so what if we use switch to lows high capacity renderers to do such uh you know Auto encoding uh so this is our work diffusion test time adaptation uh where we're test time adapting discriminative model classifiers and segmentors using generative feedback from diffusion models so diffusion TTA works as follows you have an image and you pass it through your discriminative model and you take the output of that model which can be a classification labor or segmentation map and then the output of that model enters as conditioning to a generative model that knows how to use this condition for example a text to image condition model in case you have a classifier on the disca model or um you know segmentation Mass condition image generation in case you have a segmentor there and then at this time you can put here a diffusion Lo how well you can explain that image so basically you ask your discriminative model to infer the description of the image so if you condition your generative model you'll be able to get back the correct pixel okay and then this diffusion L is back propagated both to the generating model as well as to the discriminative model and are adapted at test time their weights are adapted at test time uh and then you're able to see this this nice you know as you see as we the generative model is able to predict the IM that looks like the the right duck here okay so it was or good whatever so it was between uh Drake and c and as we adapted was able to get the right uh answer back and here again from the tiger and S so now that we have these very you know powerful renderers we can go and test this in image and with a really really high capacity discriminative models because the more high capacity discrimin model the harder is to improve upon it with the generative losses and you see that still we get boosts across multiple different clip back bones trained on lots a lot of uh data uh using this uh combination um and then you can go in in image and different here U uh variations from the IM Benchmark and train your discriminative model on imag it and see how well it can explain the different variations and we always get a big boost when over the basic let's far using this diffusion test time adaptation and I'll perform all preview test time adaptation methods uh that are being discussed in a uh you know neighboring Workshop um now the biggest boost came in the online test adaptation case in this case data arrives and the gener model and discrimin model are continually adapted okay so you see a tremendous boosting performance over you know the Baseline classifier and again much higher as opposed to previous uh test time adaptation uh methods um yeah so this is all for the case of image classification uh we also did the same thing for segmentation and for that case we had to use you know uh diffusion Lo that condition on the mask to generate the pixels so basically you improve on your inference of the Mask by generate back P the pixels and I I find this method U interesting now the details of what is the right way to do this Auto encoding to really be able to use it you know throw away every discrimin model use this type of approach is still up for a debate um I do not see this type of models to have replaced discriminative models yet but I think we are very close and I I think the the place where this is going to take over is indeed in video understanding where you have all these occlusions that you need to understand detail motions of where things go so my last part of the talk is exactly about uh this so we have seen in 2013 a tremendous progress on lifting to the images to 3D models Nerfs and and then meshes and so on um how do we do that well following uh dream Fusion you use the image instantiate a differentiable 3D representation and you train that differentiable 3D representation to maximize reprojection error from The View that you have as well as uh diffusion losses from the views that you do not have Okay using a diffusion Model A View condition diffusion model or uh variance of that you know there are fantastic things in the literature that generate a whole video from a single view so this has even better and more High Fidelity image likelihoods for different viewpoints so definitely this is extremely powerful and is used everywhere we specifically have used it on scaling up simulation environment building for for robot learning and so on um but the the natural question is how much can these foundational models tell us about video fine grain perception tracking ferin tracking um extension of objects behind occlusions and so on so the question we asked is can we use existing discriminative and generative Foundation models like segment anything and this um view condition image diffusion uh to uh do video perception in the sense of lifting 2D videos into 4D 3D plus time okay by tracking through every pixel in 3D um and and and this is our our method dream dream 4 which came up a month ago on archive and and we can see things like this you have a monocular video which then we can essentially decompose into background and individual objects reender from any Viewpoint okay complete it through occlusions but also uh you know link the point tracks in 3D into trajectories so you can see the trajectories from the Viewpoint of the video as well from any viewpoint so how is this done and why this is a problem well because diffusion prior image likelihood right now we don't have good models that can tell us for a scene what is the same scene from a different Viewpoint or rather these diffusion models work much worse than object centri ones that can tell us for a particular object what is the likelihood of seeing from different views so what we need to do is we to be able to lift whole SC is to segment this into the individual objects as what we show here um and then track and complete the object throughout time and across viewpoints using score distillation losses and reprojection error and the reprojection here is both in terms of pixel value reprojection as well as motion reprojection error and then reassemble those into uh the whole scene by adjusting their scales so that you know reprojection error in the whole frame is actually respected so we're taking advantage not only of the tremendous advances of generative Foundation models but also tremendous advances of discriminative models like Sam that can segment objects in the wild when those objects are not heavily occluded we do rely on our model to be able to understand that object under very heavy occlusions so the deformation of Which object object is represented in terms of XYZ motions of the set of 3s which are predicted from these triplane features that are all adapted at test time for a particular video so this is a test time optimization method there is no training set and and there's no explicit uh priors beyond the priors of the diffusion model so the the object motion is essentially decomposed into that slight deformation object translation as well as the camera and object uh you know relation and then we put all those together to do the projection error at the full frame once that the formation of every object has been optimized um so the results of these methods look like this you see different uh videos of multiobject scenes that are being tracked and then they're being rendered from different viewpoints uh we also did this from our own you know capture videos and videos from Davis and so on uh and of course for us the biggest application is human understanding human object interaction for activity fine grain activity recognition and retargeting to robots Okay so we want our final goal is to be able to see what an human is doing and not focus that much of what the hand is doing and so on which there is tremendous research effort on hand object on hand essentially 3D construction but rather understand what happens in terms of of the object okay uh because we want the robot to generate the same change in the scene as the human did and and here you see our initial attempts to be able to both you know parse what the human is doing as well as what the robot is doing because you do need to have very accurate robot perception to be able to do fine grain progress monitoring of the robot's activity and you see that in comparison to methods that actually do not keep track of objects and and so on I mean it is harder to deal with occlusions right I mean occlusion is a major Point uh in fact I mean I'm very excited about methods like C tracker and Pips Etc because many people say nowadays that oh Transformers can never fail you know anything you can throw enough data you can solve well guess what the problem of tracking any pixel is not really solved you put a little bit more occlusions to the method and things tremendously fail it's very hard to learn all possible uh you know uh modes of cross object interaction in a fit forward difference way so I'm wondering in for this case of Highly deforming Dynamic since and Analysis by synthesis method will be able to do better and improve over a fit forward approach not canceling the fit forward approach if we know when the fit forward approach F we can even use it as a loss right by rendering our 3D trajectories into Tod you can totally put a loss if you know when contractor fails Etc or Optical flow fails but at the same time we know how objects are and you can put local rigidity losses or other better data driven motion uh priors over of you know the Motions that you are extracting and we also have a quantitative comparisons on the paper both in terms of the quality of the video that we're entering which to tell you the truth is a little bit irrelevant uh so you ask humans to tell you do you like my video better than this other method I mean clearly they like our video better because it is not easy to do this lifting unless you put this factorization of the scene in terms of objects but most importantly in terms of the tracking quality do your 3D trajectories when projected to 2D are they highly accurate or not uh so these are the things I wanted to discuss uh I I like this Workshop because it poses most more questions than than answers okay using generative models for accurate F gra perception I think is the most exciting uh thing to be working on um very very key for for accurate robot learning as well uh and and yes and I'm very excited to see the progress that we're going to make as a community in the next year or next semester so so thank you very much thank you thank you Karina for this great talk so any questions from the audience I mean I could start maybe with one or two you started with like feature level representations but then in your talk you kind of transitioned into like Image level rendering so and this this probably because the fusion models are good at at Image level prediction so do you think like this feature level approach is is is gone or is there a way of combining it yeah I think you're asking whether we should be rendering features rendering pixel values right yeah very good question so even in the case where we're rendering pixels this were're not rendering pixels we're ring the Deep features of the uh you know staple diffusion that already does this Auto encoding now the question is do you do do you compute those features based on uh Image Auto encoding which is irrelevant or you instead put some beautiful foundational dino2 or other features uh uh and and I think yes of course the second is probably the best or some other features of that VMS will come up with so you can talk in the same languages as vlms and so on so so so yes I I don't believe that we need to be rendering of course intensities but even in this approach we don't render intensities we render features that you know the autoencoder of stable diffusion will translate into pixels I think rendering features is the way to go absolutely but what features then I mean very good yes excellent question uh uh very very good question um I haven't we haven't experimented with this uh I do believe that some you know video features like the things infer from jepa Etc would be pretty pretty great where you can or other features of discriminative approach so when you render you can also put your perception head so you can interpret what you're rendering right so you can what you're rering can be still fed up to the next encoder perception module let's say yes very very very good question yeah because like we had actually yesterday in another Workshop a couple of discussions on you know like some people believing that that video diffusion models will actually have the ultimate features because they can generate really long videos but then we had signing I'm not sure if you still in the room saying today that actually gener features do not seem to be the best features yeah so yes so I haven't read signings paper I put on my D his talk um yeah so video I mean one way to see generative models which is not what I present in my talk is that by you know generating videos you end up learning very good uh image features which you can use for something else okay okay that so that's one way and people do this already everybody uses stable diffusion features for their pipeline or they combine this with tyo features there is this beautiful papers that they show their complimentarity and so on so that's one way to go another way is to you know use those fantastic generative models to squeeze out structure explicit for the structure that you can use for your Downstream direct Downstream tasks like uh you know uh sub goals for the robots uh policies and so on the mostly what this was about now I think what you're saying is what is this Majestic perception architecture that will process videos render back upd is lat representation and repeat I I think this is what we're going to discuss in the next Workshop the third I think gener model for vision yeah more questions yeah please so mentioned the decomposing Point levels and objects and mention G as well what are the properties that you def to define those basic parts the basic Primitives you mean yes yeah so they can be anything you want so we experimented with two sets of Primitives the first set of Primitives were directly taken from xun's paper on shape programs where he divined a set of generalized elong how called rods and spheres and different cuboids Etc um so this is one we did and and this worked very well on decomposing these weird shapes that I told you now your primitive shapes can also totally be labeled parts of any object cut go that you have available from your annotations and then these are going to be your essentially visual recognition Primitives at at test time yeah than for your amazing I have a little question about drink Wonder yeah it has tremendous FES of course um yeah so the most challenging part is that objects ol clude one another okay so what we need to do Beyond you know tracking the object which you know we do get to some extent from thism is we also need to imp paint the object so to roughly have an idea of where the object goes so there are two things you can do either put very strong in painting diffusion models and we use the the best one an object centering diffusion in painter is called from the paper gal from um uh you know Carl V Rings group that was reasonable or we adapted some video models for imp painting another thing you can do I mean the problem is that when the object moves the visible part you know you can guide it from motion rendering but the invisible part you don't know how to guide it so there the question is you want to put data driven priors of how this rest part of the object moves while you don't have actually rendering losses uh this precise motion priors we don't use Beyond some local rigidity losses which are completely insufficient Beyond rigid objects and this is exactly what we're working on we do believe that it's very easy to bre in some data driven 3d motion priors uh as well in this approach okay and with this we close slightly late two minutes late but that's fine um let's uh one more round of applause please so we going to meet in half an hour um e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e oh yeah you can turn yeah full Program full program yeah they don't see oh it'sit yes is gone it's wrong what's What's Happening Here I lost I lost control say whatever he wants what a fundamental title yeah no but there is a subtitle oh yeah okay which I did that because it was too fundamental yes exactly right we can even there's a clicker here I don't know what it's useful but couple of minutes right yeah yeah it's four five minutes yes you can actually hear that that's a fresh such a one for organization what do think aboutus bring to the table oh one question want to ask you uh yes yeah yeah yeah yeah so hopefully yeah yeah yeah should you know David me yes okay so that's what he said yeah yeah so he's the one doing it so you should ask him when he's going to come okay it's going to be same you which one I had give two talks I had talk on 3D generation on Monday that would be that would be the same talk but with today it was some over not 100% I hope so there are PL least yeah I mean it's the DAT is been collected there they have tons of videos I think it's just matter of packaging it okay hopefully maybe I really hope so no I mean but ask and if it thinks that there is demand maybe it's going to be more incentivized so yeah yeah make sure that to to ask him okay I think it's going to appreciate that a lot nice Yeah Yeah we actually waiting burning burning yeah do that you do that um yes thank you but yeah I think the other issu is how you going to publish that it's just an extension of what came before and so you need to find an excuse to publish it meaning like how to write a paper about speaker yeah okay yes same but bigger yeah I quality I mean sometime that sometimes that counts it's not exactly the same either R say the the criteria for collections I think they've changed a little bit it's same nature of the data but not just just the same I guess I don't definition of categories it's much more open ended I think like cameras and people like this oh that data set no is a different that's we have at least one such data set yes ah okay yeah that's already available so yeah yeah um um yeah no me just just look it up yeah find yeah yeah yeah so no that that one is with with people acting yeah yeah okay there are 40 videos or so which are pretty long so more data that you can possibly use even download oh no yeah a lot of cameras that you barely can downlo yeah switch into anyway thinkk um I guess so okay then uh we'll come to the final part of of today kicking off with Andrea vadaldi who the professor in Oxford co-leading the VG group as I guess all of us know CP our best paper Awards and we are like super happy to hear uh his work on 3D generative AI new advances from his group thanks a lot Andrea for coming thank you not this year word though that I think think nobody knows except who actually got it and it's not me all right right okay excellent so thank you for inviting me is is a pleasure to be here so I'd like to talk to you about 3D ter of AI as the title says here um some recent progress and uh advances that we have obtained both in Oxford and and in meta so let's uh start by defining the the problem a little bit so what I been talking to you about mostly is text to 3D so that means the task is given a text like that so Bing Ving ax uh what you want to do is to get a 3D model of that object okay so it's like reconstruction but what you have to work with is just just a bit of text uh there is a related problem that we're going to use which is image to 3D if you like so here instead of giving a having a text as input we have an image and body go is the the same so what you want to get out is a 3D model which means shape and appearance okay now you look at this and you immediately realize that this is a very ambiguous so it's not just a standard construction task in fact is much better cast as a sampling task so what we have here is a posterior distribution P of M given I not so m is again our 3D model I not is the image or text prompt maybe that we have and what we want to really do is to draw draw a sample from that conditional distribution so it's a stochastic problem it's not a deterministic problem so if you want to solve a task like that uh probably your standard solution these days would be to use a den noise and diffusion model so just as a very quick recap so how do you train a model like that for images what you have is an image which is a say part of your training set you're going to add noise to that so you're going to get an image which has less information or information has been somehow cancel I deleted or corrupted by the noise and that what you do is you train a network a generator Network that tries to come up with a missing information back all right and of course that is also usually condition on the well additional information you have for example your texal prompt or your image okay now what is good about this is that these models can train from very large collection of Fairly instructure uh data right so you can go there in the on the internet download tons of images maybe a billion of them capture them semi-automatically automatically using another Transformer Network and then off you go and you can train a model like that do that does generation very very well so say for example the or emu if you're in meta all right now this is great and in theory you could just do the same for 3D except that well you do not really have equivalent quantities of 3D data to learn from so now you're stuck so you have models that in theory could learn from that but you don't have 3D enough 3D to learn them properly so what are we going to do so what we do normally is to try to co-opt a pre-rain image generator model okay so we take a model like emu or Del or imen one of those that has been trained to generate images starting from text and what we do is we engineer the architecture and we do some f tuning using a small amount of 3D data to solve the different tasks for example New View synthesis so here we are the conditioning is not a Tex prompt anymore but is an image I not as well as a camera C and condition on that you want to generate a new view of your object still ambigous task so still the problem is still to sample distribution but we are going from conditioning on text to condition on an image and and camera right now why is this useful for our task because once you have that what you can do is you can take your image you can use your new view syn synthesizer to generate new views of your object and when you have enough of them assuming that these are good and consistent then you can simply reconstruct the object and you solve the problem now you have sample one version of that object in 3D this is great and we're going to look at this two stages uh approach quite a bit today so again the two stages are first we take our prompt and we generate several views of the object and then given those we have our reconstruction process or maybe a network that gives us the 3D object okay so that's what we're going to do now the first thing you might wonder is how difficult is to do this um so the trade off here is how much effort you're going to put on building your multiv generator and how easy it is it is to get the 3D object out to the views you get right so ideally what you really want to do is to sample several views of your object I want condition on the image you have why not maybe text pront but what you actually can do is to sample some somewhere from a distribution which is sumar between this one which is the target one The Joint one to just the product of marginals okay so you would like to sample from the top one but because of limitations in your generator actually you end up samping with distribution which is just some sort of approximation to that and if you think about it the less the worst the distribution you're sampling from the Le least consistent uh are going to be those views in 3D and so the hard it is to be uh to get a 3D Reconstruction from them right so now we have these views which are not perfect and the least perfect they are the more difficult it's going to be to do the 3 construction and to try to reconcile this in consistency you can do different things you can for example use scoriation sampling to try to achieve multiview consistency through iteration or you can give that up and just have a very robust reconstruction Network that can compensate for the defects in the images you have you're given in you give it as input and we can put this into the sort conceptual graph in which on one axis okay this is gone again on one axis we have multi consistency so the more to the right the more consistent views and then the y- axis here is the Reconstruction effort and really what you you would like to be here but you know if you have a model that is not view condition at all like just distribut image generator model then uh with score addition sampling you can still get good 3D objects out but it's actually quite expensive and quite tricky uh you can condition on one view so that's real fusion which is just a direct successor of dream Fusion in which you at least you condition on one view your object and you get another that's a bit better you can condition also on a camera so you have 3D 0123 um MV dream which you sample several maybe four such views together and the better the more you do that uh the more you go to the right here so the more view consisten is are your views and the easier it is to reconstruct the object and in the past year the has developed some best practices on how to do this so you start from a dening diffusion model here which is usually unet uh you have several such streaming parallels in parallel because you want to sample several these together jointly right and so what you're going to do is you're going to condition your uh generation on the input image here and you're going to inject that as additional channel to your inputs as well as through attension the different laser unit so that's one best practice another one is to have cross detention layers that allow these two streams to communicate so you can can get views that are more consistent to one another that makes sense 33d so this conference is a poster session a pooser Friday am so if you're interested we have found few more so if you want to condition on viewpoints here the best representation you can use for the camera is actually to use pler R modulation you can see uh more details of the poster but basically that means that you encode a camera Viewpoint as a bundle of rays if you like and this is a theme that uh if you look at uh several recent papers is more and more common so that's a very good camera presentation and the other thing is if you want to get views which are more consistent one simple trick is to just share the same diffusion noise between all the views and because these networks they tend to be continuous they are continuous functions then uh you going to get views which are more similar more consistent automatically so here you can see some results from free 3D so these are 3D gener 3D like Generations where there's no 3D model underneath it's just simple this networks that is able to generate this different and okay looks fine uh but you know uh there's more to come all right so this is free3d so it's a maybe a bit more consistent so a bit easier to do the construction from them from it but if you want to go even further one thing you can do is to switch to using for as a starting point not an image generator but a video generator so in meta in particular we have emu emu video and the feature even better models but the point is that because these models they Shain on videos they already have some level of understanding of 3D geometry and I think this is also very clear in the models like sort right so once you have those this intuitive that you could take those as starting point and then find tune them to generate new views and you should get more and better more detailed better consistent views and this is actually what happens so you see here the input image on the right you see the little video has been generated by putting that into this video generator which was fine tuned for the purpose of generating new views okay so that's what we have here and because these videos that you generate are fairly consistent then you don't need to struggle too much to reconstruct a 3D object from them you don't need to use ction sampling you can use something as simple as 3D G splatting if you want to call that simple everybody knows about that just as a quick refresher 3D G splatting is just a a representation of the word that is you know a collection of 3D color gions with an opacity you can think of those as threedimensional pixels of course this is very large because you want to see them even from back there but really it would be a very large collection maybe a million such gions really tiny are kind of conceptual equivalent of 3D pixels really once you have a model like this that run really fast and differentiable manner so it's actually quite easy to fit it to data so you have an image like that and in fact you're going to have several DS VI seene you have a rendering function rent that takes as input the gash mixture the cameras gives you the pixels back very very quickly and also very efficiently in a differentiable manner so now you can try to optimize minimize the Reconstruction loss with respect to the gash model you have and the good things should happen in particular you should be able to reconstruct you're seeing in term of this three dimensional GS so this if you apply that to the output of our femo model that's what you get so the idea is that these two should be very very similar one is a video the other one is the actual rendering of threedimensional model you've got from the that video and if you carefully you see that this is a little bit more blurry because the input views are still not completely perfectly consistent but it's still pretty good and the key Advantage here is that this is way faster than using um the SS loss so this takes just a couple of minutes for each object SS loss could take say 30 minutes maybe even more maybe an hour okay so and this is because we have put more effort in generating more consistent VI to start with so it pays off to do that now the other thing you can do is to get better to Constructors you can use G splatting which is fine but uh there is no intelligence in that in a certain sense because it's just pure optimization but on the other hand you can use machine learning to try to do reconstruction faster and better in the sense of being more robust so here a lot of progress happen in the community a typical example would be larger construction model by the folks in Doby excellent progress there so as very this was also very inspiring for our work in in meta and in Oxford so what you do there you have a big Transformer Pi has input either a single image or a small number of views and spits out a representation of the object that would be volumetric um using basically basically a Nerf all right great works very well the problem with that is that it takes more than 380 GPU days to train the model okay so it's a large model he's in the name right takes a long time to train it works well but if you're in University you look at this and you cry okay because I have a job at meta but also University half of me was actually crying when I saw that so we wanted to develop something much much more efficient and uh this is what one our student in Ox for came up with again an approach that I really struck me out very strange initially so I didn't believe in it the student fortunately did and didn't made it made it work which was really amazing to me okay so what is the idea here this is called the splatter image um it's like another theme I guess but really what this means is we do g splatting at the level of indidual pixels so that's why there's an image there and we convert each pixel to a TR Gan so the way we do that is by simply designing unit that again operates on image as input and produces as output another image which has the same resolution but now the pixels are not pixels anymore these are the parameters of 3D GS okay so what this means is again we have an image bunch of pixels a camera Center and for each pixel there we come up with the networ comes up with a bunch of parameters Sigma mu capital Sigma RGB plus and Delta these are theity of the gashi the location of the gashi along the ray the shape of the gash the color of the Gan it could be just RGB or maybe sperical harmonics if you want directional color and then Delta which is an Offray displacement okay and that's really really crucial so what does that mean so okay here we have the threedimensional center of the gion which is along the ray that goes through that pixel fine everybody understands that just uh you can do that using dep estimation but in addition to that we have a Delta Vector that moves the Gan off the ray okay potentially doesn't have to but it can and of course we have one of those reconstruction or gash for each pixel it's also possible for the model to set the opacity to zero so if the model decides the gash is not useful it can shut it off right and so how well does this work quite well so this is applied on one category 3D which is a data set of um multi view images of different kind of objects in this case steady Bas and to the left here you see the input image and to the right you see the spinning reconstruction where the camera is moving around the object okay and something should strike you as surprising this definitely surprised me so you get a reconstruction of the background it is blurry because this is regression is not sampling right so you're going to get slightly better results but you get a back so that's possible again you know this is pixel oriented so shouldn't shouldn't work also works for something which goes beyond single category so this is a train on observers and then test it on Google kind object and it still works quite well now going back to this problem of why this does this work but here you can see why or how anyway so you have an input image like either a car or a chair and then you see here these are the opacity channel so everything which is black has been switched up everything which is not black is a gash which actually exists in the conr instuction here and you can see that you get the something which matches the pixels in the car but then you also get a bit of information beyond the car boundaries and this is where the gashes actually model the back of the the object actually live all right so you see here for example that we see this we have this little circle this is this wheel at the back all right so this network is smart enough to be able to say okay this is a pixel which is off the object so I can use it to try to model the back and it does that automatically which is really nice because the advantage is that you still have to work only with the image to image networks which is very simple and very efficient and efficiency here really is the key so here you see performance against the GPU days in training this is a new V synthesis quality and the the message of this T this graph is it works well for reconstruction comparable to state-ofthe-art but in term of training is much faster so this takes seven GPU days it doesn't take 300 GPU days it means that if you're in University now you're happy because you can actually play this game still gets sely competitive results so in fact very competitive results much faster though okay and it's also much faster test time and of course once you render it it's so super fast because this is just Cas splatting recently we have moved Beyond uh scenes sorry object into scenes so you can apply the same Concepts to the construct enti scene in this case you wouldn't have you know Reservoir pixels to use because everything is occupied here but you can simply sample two G per pixel and there are tricks you can well look up in the paper to make sens of dots and you can get a nice construction as well the good thing here is that thisiz also very well so we train it on real estate 10K because it starts from a very powerful monocular dep Network that we F tune to add this capability as well producing this gussion on the Fly then it turns out that train on real estate generalizes to outo domain in K and you still going to get high effects in this case the art results at least for this kind of Benchmark okay and again this is great if you're University because this takes only a couple of GPU days to train you don't need gigantic gpus either okay so that's part one part two I would like to move on to another thing I want to discuss with you today which is meta 3D gen okay so this is about uh there is about a year work that we have our team in metas done on the task of Tex 3D and uh this conference we're presenting the first results the group has obtained so when we set out to solve this problem we are goal was really to ask ourselves what does do we need in term of quality to satisfy professional users so them would be people that say develop video games for a living or maybe you want to use that in special effects or maybe in the metaverse right so we want quality which is sufficient enough so that you can actually use it in real applications of 3D computer graphics and so there are many many requirements that you need to satisfy so that's why spe generation is in my opinion a lot harder than generating just images or movies all right not to say that that easy is easy but when it comes to the generation you need to think about quality of the shape quality of the texture that's similar to image Generation video generation you want to have also high quality control or High Fidelity or high control fine that's also common but then you also want to have materials you want to have Parts nice UV topology and quality nice mesh topology breaking animation blah blah blah all these things that go beyond what you do normally when you generate images and videos so it's hard today we're going to focus on these three these four uh things um and we I'm going to start discussing materials so materials means that it's not enough to estimate RGB colors for your object what you want is to estimate are material properties you want to estimate a b directional reflectance distribution function or brdf for short because this is what you need in computer Graphics to be able to relight your object put it in the new environment to make sense with the light source you get there right so what you want to estimate is the alido inic or base color these are different way to same the same thing although all of them with the same level of accuracy so this should really be called base colonar model and then you also have roughness and metalness which are the scriptors of the material you actually do have so this say a a pug made of metal so you get dog but the materal is really high because it's supposed to be made of metal and so it's all shiny and now once you have that you can change the lumination here you can see different envirment maps and once they reflect off your objects it makes sense because now the materials have been modeled not just the color right now you have control over materials because we text pump can say something like a cat made of shiny plastic and now you get that roughness is really low because this is shiny so it's not rough and the metalness is also very low because well this is plastic it's not a metal right but I can say shiny silver so roughness is low but now metalness is high and again you see that it makes sense when you render it with anation map uh thank you and then now now you see you can see this much better actually rusted iron sorry Rock so you get a roughness Time and Time slow and then you get rusted item so you get um High roughness as well a high Met metalness because this is rough metal yeah it was better before but okay thank you all right so how do we do this we need to generate this PBR channel so metalness roughness and albo so there are two options you can uh you can pick here so one is to charge the multi generator network with generating the information you need right so you generate several views object in RGB space for the foro but it also generates traditional Channel roughness metalness this is fine you can try to do that but remember that we started by trying to we learned this multiv generator by starting by from a model that was trained to generate natural images so the Gap the statistical gap between generating albo and generating roughness metal metalness is actually quite large so if you try to do that you sort of destroy the good quality of these models here so the Gap is too big right so you don't want that on the other hand this is nice because you have all the information you require to come up with the Reconstruction of those quantities so the reconstructor network here or reconstruction block here would have an easy life the alternative is well you don't try to force this model to generate roughness and metalness you just generate a a shitted appearance which is very close to Natural images so that's easy for this model to do relatively easy but then the construction network has a hard time because mapping this to roughness metalness is actually very ambigous non-deterministic and if you remember construction network is deterministic so that's not very good for it okay so the solution we came up with I'm not saying this is the only way of doing it but that's one good way we found is to task this network here generating the albo so RGB colors for uh the albo as well as the Shaded version of your object and these two quantities they are fairly close to Natural images that are easy for the model to actually generate and then by comparing the Alida to the Shaded version of um it object then this network the construct Network which is deterministic enough has think easy time coming up with the roughness and mentalness by itself infering them again so what does it mean so this is a what you get if you reconstruct four views in the Shaded space and this is what you get if you do the same in theb space again shaded albo shaded albo and you can see here for example that in the Shad alido version you don't have highlights here but in the alido in the Shaded version you do and then the constructive Network looks and Compares these two quantities and he knows that it has to be shiny metal essentially okay I can infer it so again this is what you get if you try to reconstruct your materials using secure use which are shaded and you can see here that this is not quite correct because the metalness is everywhere but it should only be on the bit of the bear where is weding his armor right but if you do provide these two different inputs then you actually do get that so you get that the bit of the the clothing which are metal light actually they light up in the metalness map say get much better okay that's so that's one one idea here's another so uh we also wanted to get better quality the constructing our shapes and corresponding meshes so the question is which kind of representation are you going to use to do that in your reconstructor so we don't do meshes directly although probably that's the holy graay so as some people are starting to do that now we still do the Reconstruction in in the space using an implicit or explicit representation like spatting so but here you have a choice it's not a mesh but you can use a volumetric representation like Nerf or you can use G Spa instead so which one would you pick it's a question for the audience okay here's the answer well it's not it's not obvious right so Fields they are nice in that they are they give you good surfaces when you go there with say marching cubes to extract the mesh and give you very nice realization good Global convergence but this bit slower than splatting to rendering but most importantly when it comes to back propagation they are disaster in term of memory consumption right and the reason was going to be clear next slide but basically that means that you can only render a few pixels at a time before exhausting M of your GPU which also means you cannot really use pixel I mean Image level losses so you're limited maybe to train with psnr you cannot use LPS now it's important because remember that we want to reconstruct from images which are not completely consistent so you want a loss for the construction for training these models which are actually a bit forgiving so much better to use lpip here G splatting is better faster uses a much less memory which means you can render entire image very effectively and then you can use an image level loss to train your model OKAY the downside is that you get surfaces which are not quite as nice I know that there are works now that are tackling this problem specifically but at the time you couldn't really get very nice surfaces of G splatting which is a problem because we want to have three dimensions for our artist to use right and then um they are not quite as regular in term of training so when it comes to training robust things well maybe Nerf still has an edge okay so now the problem with Nerf though is the memory bottle L and why is that because we are shooting these rays and you're taking these samples and each one of those things should be differentiable and it is differentiable but if you implement that with autograd then all these samples get instantiated in memory VI gpus in term of the gradients so the space complexity is R by P where R is the number of rays you're shooting basically one per pixel and P is the number of samples you you have for each aray and that's stable so you can easily use 10 gbes for sing for rendering a single image all right and you want to render several images want have a large mini batch size so yeah bad idea so what we did was to develop light PL kernels so these are just an implementation of uh uh some popular versions of Nerf that uh use basically fuse kernels and the nice thing about that is that when you do back propagation the space complexity is not R by P but it's simply R so it has the same space complexity as Gan splatting okay so we are making Nerf as efficient as gashi splatting maybe not in term of speed still a bit slower but certainly in term of memory consumption when you want to do training and that's enough to actually be able to actually use it in practice you see here a little graph showing the image size from 16 by 16 pixels to full HD and that's the amount amount of memory which it takes uh to autog um uh back propagation and you can see that there is an order difference of I me three order of magnitude difference so it's a huge huge huge difference and this is why this is feasible and these ones are not really when it comes to training this kind of reconstruction models a you decent resolution anyway the other thing we did was to change a little bit representation so we don't use just opacity we use a sent function that are known to give you better shapes you can see that here this is reconstruction using opacity field and the same using S functions much neater in term of the details we didn't invent that use the Vol SF formulation for this but what we did was to implement that efficiently in the Light plane kernels and that Al together with some um lws which are based on the ground shape of the object so check this out if you're interested but this is actually PR is quite useful all right now with this now we have this Constructor you get your 3D shape you can use some standard marching Cube and UV mapping method like xus to get a mesh with a corresponding UV map and from that you can go back to your threedimensional volume and Sample the corresponding texture great and this again lives in PBI space so we have albo roughness and metalness for out good but this is uh what you actually get in term of quality of the texture which is fine for this particular example of a popcorn box is a bit messy because we use x Atlas but most importantly you're going to see that the details of the texture are a bit blurred out and the reason is that we're sampling them from this volumetric space which is good but not it's not perfect it do different thing though so you can go back to your views these are the output from your multiv view um image generator and you can try to resample the textur based on the map you get and the three dimensional shape you your object now you're going to get sharper results but they are sparse and the reason is that not all the pixels in your text are visible in all the views and so these things also are slightly inconsistent because the views in the cells are slightly inconsistent so what we did was to develop a yet one more Network that takes uh this initial texture map this additional back back projections and process them choosing for each pixel in the UV space which one is the best version of it and also fixing some mistakes interpola when needed extrapolating where there are occlusions etc etc and you can get out a much better texture so let me show this zoom in version of that so this is before text announcement and then after and you can see that the details here of the writing for example much clearer and you can see this even better here where we have uh reconstruction without section answer reconstruction with you can almost read what's written here in fact I think you actually can and this is the ground truth another example you see the ey the be is much sharper the details of complex geometries they get reconstructed better as well compared I mean text are better as well and here you can see some examples of of the object we can get okay and because this is a physically based rendering uh you get your llamas with materials and you can change illumination everything responds as it should final concept this is good still not good enough so what we wanted to do was really to get very very very good textures and the idea here is okay so we got to some points so far but now we have a 3D object and given a three object we can use that to go back to the problem of generating a texture for it and given the the knowledge about the geometry we can actually increase even further the sharpness the quality of the texture so now given the prompt and the three geometry we go back to this problem gener texture and we get much better results as as a as a consequence okay and because this Tex generator now um can work even for a different prompt we also get texture editing capabilities the way this works is actually quite simple so it's the same story as before we want to generate four views of our object but now in addition of conditioning with text we condition with a representation of the threedimensional shape in term of position map as well as normal Maps normal captures the details of the geometry and position map gives you some sort of ABS absolute reference for where the 3D points actually are so you see that the colors they match in the different views which make it much much easier for this multiv retro Network to actually give you text views which are very consistent to one another and this is really crucial the rest is going to just be a few results so bear with me so here's a this is not generated this is a 3D artist of a butterfly but now you can use detect net to apply a majestic monar butterfly render to it and it's not a render sorry it's a text and it looks quite nice and you can shange CH to make it also very complicated a magical butterfly AR can SES on its wings sparking glitter on its body I have no idea how we came out with this but yeah what it looks very nice and then time to put everything together and then to close so this is meta 3D gen these are the sort of results we can get Now by combining this text regenerator with the text 3D method that we just discussed and let make this spin so you can see how the object look like from different viewpoints I think the answer is my view quite nice um because we have a texture generator we can apply different styles but without changing the shape the object so we have here sty Christmas style and we let Spin and soon enough you should change say into a horror style it's possibly quite the opposite okay it looks fun and again you can this is all quite fast more that in a second we run some user studies comparing to at least some dominant um industrial competitors like Rodan MH and Luma and we win out especially when it comes to quity for complex texal prompts and you can do a breakdown and you can see here that the higher the better means that our win rate is yes higher and this increases as the complexity of prompt increases these are the results of lumag with the same sort of prompts these are results mesh with three gets tripo 3D they're all very quite nice really um so now we can compare all of them so these are all kind of St methods and then we can zoom in and I let you to be the judge but I think it's quite convincing at least in these cases uh we have an edge in term of either quality correctness of the texture okay if you want to more know if you want to know more about this we have three papers that coming out hopefully by the end of this week uh they're done okay um one of them is text 3D the other one is text generation then is a meta paper from meta that combines the two into this very nice high quality reconstructions Generations okay so time to thanks our team in Oxford our sponsors as well uh team in metagen uh geni so these are the folks that are behind this effort and yeah just to conclude what I presented to you today was uh some progress in of the AI it started with some best practices 3D for example in how to design these models uh control so going to up to material control here where you want to specify details of your object um that go beyond just the shape and the appearance quality was also very important for practical real life applications so you know you need to pump this up and you can do it especially when you combine with very high quality image generators in the first place but then you near have to work your uh work hard in order also to get very high quality text and shape generation as well and for folks that are not in Industry efficiency I think is also very important I think also for tell the truth but especially for Academia so we have this class of models that can do at least three reconstruction very efficiently and because those are really a core component of three generators then they can have an impact there as well in fact they did have so there are already papers that use this sort of technology for three generation text 3d2 okay that's that apologies for being slightly over time great thank you we have five minutes for questions and let me maybe start with one so when are we going to get the parts r and animation ah uh I cannot deny we're working on parts right now okay um for animation I mean there are plenty of people that are working this is a very cled area so there there are already plenty of work that look into that so we already have it uh in term of our work yeah upcoming I suppose that's what you could say yes also with like skeletons and rigging um eventually yes now we're thinking about parts so this is another thing which is very important for 3D artist they don't really like to have this sort of Monolithic constructions in which you know the turkey here is glued to the plate they want to have in separate separated components so they can animate them maybe edit them and do stuff with them all right so yeah that's uh not something we uh we have done but we look into it do you also care about the mesh topology like you know artists I guess care about you know mesh being you know regular around the mouth and yeah you're guessing all of our some intership here but yes pretty much yeah that's that's very important as well so we are thinking how to do that and uh yeah we just started basically to work on that again there are other methods that might get currently better mesh topology because they actually cared about that before we could get to that but yes that certainly is also very Port problem and I'm G to spin this again just because it's fun okay yep yeah modelc toate and it's okay so this is very interesting question so we don't do anything special for symmetry but if you look at the back the dinosur here you'll see that even the texture tends to be quite symmetric so this is being picked up completely automatically by by the model we don't do anything special we're not forcing it okay so when you don't have to have symmetry like here for example you don't get it but when it makes sense for the model you do get it so this is purely but the behavior is entirely learned we don't we don't do any anything speci for it y mod like do models diversity of it certainly does so it's a tricky business so we do have an internal so we don't use a for vision that you can imagine but we have an internal data set which is conceptually very similar and we use that so pretend we using obers we're not but pretend we are um so we are not using all the data we could because if you use too much then we destroy the ability of the model to generate nice things so that is certainly is a problem okay um I don't know that we have a good solution yet but that would be one area in which if you're interested that would be very helpful to us if you could solve that problem thank you I saw the work are in the but it seems also you see um well it's just more complicated I suppose so when you have we're dealing with a single object it's easy because having a single mesh makes sense you can look at it from all possible viewpoints very easily just looking at the scene from all possible viewpoints is harder etc etc I don't think conceptual is hugely different but certainly makes your technical life easier so that's why people are focusing on that first I think but you are hitting on the nail um in the sense that yeah doing is not enough we want to create universes or metaverses okay I guess a very nice final word then thanks again thank you e e so I'm muted and need to present yeah exactly that looks good yeah are you are you muted I yeah that's great as well good so we are actually on time that's fantastic uh so we're going to yeah it's a big pleasure to have our final speaker here Federico tombari who is a research scientist and manager at Google uh in the leading the research to product team in in computer vision and machine learning I think across North America Europe and uh he's going to tell us about creating 3D assets from the fusion models I think highly related to what we've heard earlier very much looking forward to your talko thank you for being here thanks you're gonna get another 40 minutes of uh 3D asset generation but it's the last talk for today so bear with me I also want to thank the organizers for the nice invitation for the introduction I'm very happy to be to be here today and to talk a little bit about our work on uh generating 3DS specifically with diffusion um and so we we've seen Also let's say with the previous talk from Andrea um about some differences between generating 2D images and generating asset in 3D and there clearly like a gap in the sense and it's really hard to fill this Gap and so with with images we can now get um gener we have generative models that are par particularly realistic in the way that they can uh generate uh result but they can also be uh now used for example for editing so we can specify in a pretty detailed way which area of the image we want to uh modify um and they tend to be very accurate in that sense and these type of things are really hard to bring towards 3D and so one of the things that we started looking into and asking ourselves is what is the best representation that we want to use for actually generating things in 3D and in this case this is a bit of a summary that I made where we can see on one one hand uh gener representations that are particularly let's say detailed they actually capture a lot of um information about the geometry of the scene the semantics we can think about Point clouds 3D meses box and maps and so on and on the other hand we try to let's say abstract a way and sometimes actually using this type of abstractions of the of the scenes um is something that also help the generation in particular we're going to see that this is a nice way to condition the generation process and so um what I'm going to present you today is actually how to do a generation of 3D Assets in particular scenes and object across different types of these uh representations and and powered by by diffusion so I'm going to talk in particular about three works that we recently uh published one is text to 3D mesh uh the other one is text with nerve to another nerve and in particular is how to actually insert through the objects in a given scene rather than generating them from scratch from a prompt and finally a a word that looks into actually a series of work that looks more into conditioning um 3D mesh and 3D layout gener generation condition by a Sy graph so starting with the first one um so in this case we are particularly interested in in something that we've seen also before this task of generating um 3D assets from from a text uh prompt and we are envisioned let say this typ of applications we part of Google or we are also interested in applications that let's say touch particularly um closely this type of AR application so we can take something out of the box that we can generate and place it in a in a in a in a real world scene and so for us it was particularly important to actually have a easy to ous method that could generate an actually implicit representation as an output by the 3D mesh that we could directly use for this type of applications the other um the other aspect that was very important for us and we want to to to improve on with respect to certain um Methods at state-ofthe-art was actually to make the results more realistic so we've seen for example dream Fusion that is a is a very effective uh way to generate 3D assets from from prompts but tend to have this this type of let's say oversaturated and not very realistic a little bit cartoonish kind of effect and that's something that we wanted to improve specifically with our work uh one of the let's say uh pillars of of uh Dre Fusion is actually this very bright idea of SC distillation sampling where the idea is that we can actually generate and optimize basically a new radi field and then we can render image iteratively and these images then we can add noise and then the idea is that we do some sort of noise prediction by predicting the noise and making the difference with the ground chose the noise that we actually add we can let's say find a way to uh keep fine tuning so uh the weights that are inside our MLP within the Nerf model and so we can practically have a an iteration where the diffusion through IM and is actually done on images but this tends to go back into the Nerf model and provide very consistent results in 3D so it's a it's a very powerful way that leads to Fantastic results with with this type of characteristic that I mentioned before and so uh our method text mesh is based on four main steps and the first step is actually to pick actually a better what we think is a better uh let's say 3D representation for the data and so we started from this uh let's say analysis of the difference differences between neur Regence fields and S distance functions and neur Rance fields we know that they are particularly suited for n synthesis so um let's say they clearly excel at this type of task but uh they tend to neglect the underlying geometry so it's hard for example to extract three the meeses out of that they're not really designed for that while s distance function it's kind of the way around so these are 3D representation that have been used for 20 probably more years and they are used and and developed specifically for having a good powerful representation from their line geometry but they're not designed uh on the other hand of course to render and optimize using a set of RGB images and for no VI synthesis and so there's now been a set of methods uh recently proposed in literature that try to kind of brid the Gap right and so they're called neural distance fields or neural occupancy fields and they try to get the both of the best of both world so yet they have a TOD representation that can be suited for no your synthesis and for rendering while having an underlined geometry that can be very quickly and very accurately extracted and so that was the starting point so we develop an SDS kind of framework where we intrinsically replace the Nerf with a vol SDF Vol Vol SDF for those of you that are not familiar with this approach is one of these uh let's say methods that tend to use together a a let's say volumetric representation uh within within a neur Radiance field and U the rest of the approach is actually the same for this first step so the main difference as you can see here is that we can already get a much better mesh out of this we can get much better details in terms of for example the edges and we get a very bounded 3D mesh that we can extract uh directly from the um zero level set using marging cubes and so once we have a nice 3D mesh The Next Step that we want to apply is an improvement of the realism right as we mentioned before um the um let's say problem of this cartoonish effect of the infusion I think it connects to what Andrea explained before that there's this tradeoff between let's say the realism that we're going to get to the images that we want to impress we want impress our 3D uh representations on the other hand the fact that we actually want to have something that's geometrically consistent and geometrically accurate and so if you want to let's say have this kind of high guidance on the geometry you're going to lose on the other hand in terms of realism and vice versa and so in this case what what we're doing is practically uh the idea of extracting views from canonical viewpoints and then um use that um on a diffusion model to generate some pseudo ground truth images that are based on the current mesh so what we do is actually we take uh this rendering from the canonical View use we apply conditional stable diffusion in this case the conditional stable diffusion is very important because we condition on the depth the reason that we condition on the depth is that we don't want to change the underlying geometry right because we're going to just bring this back on our mesh and so this it's really important that the geometry is unchanged um and at the same time this improve the the quality and the realism of the image now there's one problem is that that's if we render from canonical views and then you individually and independently fit this to your diffusion model then you're going to get again inconsistency right for example this is these are the four conditional the four canonical views of this mesh and then you can see that the diffusion process applied independently tends to let's say create large inconsistencies uh on the object and to to try to uh fix that what we're doing is actually to tile images together and by telling we let's say notice that the stable diffusion process tends to increase quite allot the consistency of the results and so uh the last step is that we bring back this texture that we have now improved in terms of quality back to our mesh of course the E canonical views might still miss certain parts of the mesh that they're not well capture because they are for example occluded and so we have a final fine tuning step where we actually fine-tune directly the RGB texture of the mes so what we're practically doing is we apply a loss function actually it's two terms of the loss function and uh uh in this case what we're going to optimize in terms of let's say the way that we're going to apply our gradient is directly on the values of the RGB texture of the mesh and this again tends to uh clean up and remove the few inconsistencies that are left by um let's say uh propagating these two uh losses and so these are some of the results uh that we get from different prompts and the associated 3D mesh again this is a method that give you uh a 3D mesh out of the box we think it's particularly useful for many applications this is a comparison that we made quantitatively although the metrics are let's say we can provide a limited signal in terms of improvements we can look more at the qualitative examples and we can see how the method Compares in terms of realism and reduce cartoonish effect with respect to for example dream fusion and Magic 3D and this is some some additional results that we that we got good of course uh with respect to the time when we publish this work the there's a lot new more new uh methods out there the field is literally exploding these are some of the more recent results and this is also let's say they provide a very some of them provide a very interesting way also to improve as future work the text text mesh uh work that they just presented um okay moving on with the second uh second Topic in this case the idea was to focus specifically not on the general purpose let's say generation task but actually to do insertion so we only insert objects and we start from inserting objects in a in a neuro Regions Field representation of a scene uh our starting point in this case is instruct ner to Nerf which is a method that was proposed last year and it's a text driven 3D editing approach with Tod diffusion models it's extremely um uh good in providing flexibility to take a text prompt and then edit the 3D scene represented as a Nerf in in different ways for example changing the Styles and changing also other characteristics of the scene um the idea of of the instruct Nerf to Nerf method is that it does this kind of like iterative Fusion of 2D edits so the edits are are done on the images and then they are gradually fused back into 3D representation in order to avoid uh inconsistencies uh although we still notice that the final results are prone to 3D consistency inconsistency of the edit so for example there some um you can see that depending on the Viewpoint the results tend to change quite significantly in the type of edit that's being made that's kind of normal because the edits are done I think this is gone no can you still hear me it's okay so you can see that depending on the Viewpoint the edit changes quite significantly right and so once you start to throw this back into your uh 3D representation it tends to create inconsistencies this is one of the main issues and the other one is the So-Cal localized editing issues so whenever you for example specify a specific location when you want to apply the edit say for example you want to create a parot specifically on the left shoulder then sometimes this is not not very well let's say interpreted by the by the diffusion model and tends to create these type of inconsistencies and so we wanted to improve on this by starting from a simplified assumption that we want to insert uh objects in the scene and the idea is that we expect the user to provide an annotated reference view so we have a render view from our Nerf scene and then we have the user that has simply to put a 2d bounding box as a starting point and in addition to the two binding box of course we expect a prompt that is the definition of the edit that we want to make and this is how we we we're going to start so the first uh this is the entire pipeline I'm going to explain it briefly step by step the first step is to create the 2D object edit on the on the image and then we're going to warp the 2D object edit into the 3D model and finally we're going to merge it through the object we see and so um the first step is actually to start from the annotated reference View and then we use a simple model like imag and repaint to apply the to apply the edit on this 2D view uh the idea of this image and repaint is just as I was briefly mentioning at the beginning it's a way to edit a an image based on a text prompt right and so we can really do like something that's very localized and to the to the area that specified bying box and this kind of tends to already in terms of the localizing in painting problem that I just described before once we have this edited reference view then we have to throw basically this object into 3D or leave this into 3D and for this we use a method called syn dreamer it's practically a method that generates a set of multi view images from a single image uh and then from this we can create our nerve for the for the object uh in this case we have a our own let's say fine tune version of syn Trier which is train and object verse it data set which is around 800k through the objects um the next step is actually once we have the nerf of the object now we need to put it back into the scene and for this we have a develop a specific 3D object insertion stage uh the first thing that we need to do is to actually estimate the depth and so for this we use a monocular de prediction model in this case is uh Midas and so we we start from the image uh the input image we apply the rendering of the object and then we estimate the depth and so once we estimate the depth then we have to do an alignment between the estimated depth with the object and the Nerf depth that we get from our nerve model of the scene uh once we made this alignment this generates a 3D frone which is practically the location in 3D space where we know that our um object needs to lie uh there's a few tricks that needs to be taken into account during this process one is the scale so typically because the method that we use to generate our 3D model is trained to generate multi multiv images from a fixed camera uh parameters then we can get typically scale issues and so we have to let's say uh estimate the scale of the object with respect to the scene and we do this to by optimizing the scale via main Square arrow with respect to the original image with with a with a cap or the object that we want to insert the other thing that we need to take into account this is a little bit more more tricky I think is actually the density so the density of the two Nerf models so the object that we generated and the original scene they actually different and they can just be merged via simple averaging if you do simple averaging then this leads to very degraded results in terms of the rendering as we can see in this images so what we need to do is actually rescale the density to retrain the appropriate color and this is actually a procedure that needs to be done for every single Ray and so we have for every single Ray to Define and determine uh through these formulas uh the right the right density and that gives basically the appropriate rendering uh properties uh to to our added object um I just jump to the results uh so this is practically uh one of the examples so we we take the prompt and then we generate uh the object inserted in the scene and then we can practically generate the sort of like trajectories that on the scene with the added object I'll show you some more some more examples here you can see on the left hand side the original image uh the original rendering of the Nerf scene then in the middle the edited one and also the depth that we get uh in terms of the edited one so on top we have added this uh paneton to the scene and on the bottom we have instead a Mac uh next to the carterpillar these are another set of examples qualitative in terms of outdoor scenes so on top we can see uh one of the standard um Nerf uh scenes where we also had a MAG on the table and here in the middle in in the bottom instead we see an example where we had a butterfly in the center of the scene and then we create a trajectory around it seal of one of these Nerf SCS uh this is also like a qualitative comparison with instruct Nerf to Nerf where we see for example the difference that we can get in terms of specialized U say localized editing and in terms also of the um consistency of the results in terms of multiv view um there's of course a lot of let's say uh limitations in the in this work and so uh one of them is that it's still hard to handle very complex lighting effects and it still suffer of biases that are included in the diffusion prior and so these are aspects uh that we're trying to improve with with future work great and so uh the last third and last uh part that I wanted to to to briefly introduce to you is actually some work that goes on top of uh let's say uh actually a direction that we've been following for quite a few years which is the idea of using S graphs as a very powerful way to to represent a 3D scene um um in this case let's say um looking at how ways that we can generate SC 3D scenes uh there's actually many methods that do it for example example iteratively by going into a database of 3D objects and then using a network to in this case Auto regressively go and pick the next best 3D object that needs to be placed given a layout and a set of predetermined object and so in this case we start from the layout and the network will for example pick the best 3D object and then based on the this 3D objects just fit the next one and so um these type of methods are quite uh interesting in terms of the results that the can get they're very realistic but of course the leverage objects are already predetermined in a database as well as they can lack a little bit the um relationship that needs to be uh taken into account between objects in the scene and so um in order to increase the controllability we believe that uh sing graphs are a very good tool to to generate also 3D SC so that's something that we've been now exploring for quite for quite uh some time uh so in order to uh let's say describe how we can generate scenes with sing graphs I want to briefly Define what a sing graph is and particular 3D Sy graph so we we started with this definition with a work we did in 2020 where we take we took actually the sing graph approach and definitions that were typically used for for images and brought them to the 3D field um and in this case we defined scene graphs for 3D scenes as a set of nodes that typically represent entity in the scenes typically objects but also structural elements of the scenes and then relationship that would be generally encoding different types of properties between two objects or two entities in the scene such as for example their geometric relation their similarity in terms of texture material attributes and so on so forth and so um s graphs are actually powerful tools for both doing inference and Generation Um of data and for inference we've been using them to for example at given a set of 3D scans of scenes or given a set of sequences rgbd sequences or RGB sequence sequences of scene to actually extract the 3D Sy graphs out of this but in this case we're actually interested in kind of the opposite process which is I give you a s graph and I want to generate a scene and the 3D scene is to respect the entities and the nodes and the relationships of the graph that I provide you as input uh this is one of the works that be presented in 2021 and so the idea is that um if you provide s graph that us as a condition then in addition to generating a scene then you can also let's say play around with with a scene graph in order to apply manipulation so you can for example start replacing the nodes or replacing the relationship and view will generate different types of scenes where you tend to let's say apply this type of editing directly on the generated scene in this case the way to uh uh develop this approach was actually an O encoder like uh Network where we start from a a set of let's say shapes as input that we get they get encoded into a shape encoder and then on the bottom you can see a set of layouts that represent the 3D bounding box in the scene of that object with a class Associated as a label they both get encoder encoded and then we have a shared encoder which eventually is going to be a gcn a graph convolutional Network which is what creates a shared embedding So eventually you can really get a a scene graph as the embedding and I think this is a very powerful property because then you have an embedding that you can really play with so you can start manipulating uh your Laten variables because they're going to be entities and relationship edges of this graph and so you can also do this type of manipulation in the latent space and then you have to decode everything right so we have again two decoders one decoder that decodes the 3D layout in terms of the bounding boxes 3D bounding boxes in the scene and then for each of these bounding boxes we need to fill them in with a shape and so we have a shape decoder that uh decodes the particular shapes uh that needs to be filled needs to fill in each of the 3D bounding boxes and so in this case we really generate shapes that are not taken from a 3D database but can be generated based on also the relationship that are imposed by our by your graph um the next work which is actually what I wanted to present uh a little bit more in detail today is a work that we published last year at NPS and it's called common scenes and kind of improve and builds on top of this idea that I just uh explained by using or trying to exploit the power of the fusion models and so uh the idea is very similar in terms of the over overarching let's say structure and so we have a we have a set of we have a um a SC graph that is used as an input to condition the process we have an encoder uh that's eventually also model with a scene graph as part of the latent um embedding let's say of the of the scene and then we have this type of decoder that is split into two branches a shape branch and a layout branch and so the main difference is that H the shape Branch we actually want to use a diffusion model to generate better shapes um conditioned also by the relationships of the graph um the first step that we do is to pre-process each of the uh nodes and relationship that we have from the S graph to go from a standard s graph which is what I defined before to a so-call contextual graph a contextual graph you have to think about it as an augmented scene graph where in addition to the embedding that we have uh for each of the nodes and entities we actually add a clip text embedding so that practically takes the label of the um entities of each of the nodes and uses like a clip embedding for to to leverage more information about the object properties and also encode the 3D scene layout as a set of param parameterized 3D bounding boxes so imagine augmented let's say the the nodes and relationship with this type of um small uh feature vectors and so this is this uh let's say contextual graph is now ready to be encoded in our encoder and so for the encoder we eventually again use a gcn a graph convolutional Network um and we use it in a way that the uh features that are learned in this latent space are actually to be interpreted as a as a VA and so because it's a it's a variational encoder then you can think about it as a distribution represented by each of these uh features and then you can sample from from the nodes and from the relationship in order to generate based on on the same starting scene graph different types of uh 3D scenes now the layout branch is very similar to what I explained before so we typically decode from the uh notes of of the graph a a layout so a set of 3D bounding boxes that are in a relative position to each other and this is simply done with a with an MLP so with a simple MLP uh we we decode this and use this type of loss um for the layout which is uh nothing by but something that penalizes the bounding box size the bounding box location and the angular rotation of the bounding box so very very simple the more interesting uh part is on on the shape Branch as I said before here we wanted to explore the idea of using diffusion and in particular use a 3D unit based diffusion so it's really like a diffusion model in 3D space uh where the underlying representation for our data is a sign distance function from the sign distance function we practically encode it into a latent space and through the diffusion latent diffusion model we practically tend to denoise basically this latent representation of the SDF uh one interesting thing to not here is that um we are now conditioning see here we wanted to condition the um diffusion process the denoising process with an embedding of the relationships from the graph and so this is where the graph let's say magically comes in in the process uh by taking the edges between the nodes taking their embeddings from the gcn like we've seen before and using that as condition we want to make sure that the shape that we're generating gets influenced by the nearby uh nodes in the graph so those nodes in the graph there some sort of relationship like as I said before they have the same shape or they have the same label or they have the same semantics they will condition so that for example if you need to generate a set of chairs around the table you have more chances that these chairs are going to be all the same rather than individually and independently developed um decoded um I think that's that's mostly it about about this about this slide um and so jumping to some of the results you can see some some examples starting from this type of of graphs graphs how the 3D scene is generated again these are the individual shapes decoded for each of the node in the graph and placed into the 3D bounding boxes generated by the 3D layout these are some other examples um of scenes we Al have in the paper several comparisons that we made one is a very important aspect which is the realism how realistics are realistic have the generated scene so in this case we have two metrics FID and kid in terms of real that measures let's say some sort of realism of the scene with respect to the State ofth art and we got particularly improved results also through the um 3D sln and graph to box methods that I just mentioned before uh diversity is instead a study that uh wants to show how different 3D shapes can be generated by seeding so by sampling differently the variational O encoder that we have at the level of the embedding um based on a gcn so you can see here how the same SC graph can actually lead to different type of 3D shapes in the scene that are all semantically consistent with each other and finally this is also something that can be uh used in terms of manipulation of the scene graph so we can actually control the scene by manipulating for example uh changing the nodes the labels of the nodes you can for example change the attributes make for example the night stand uh taller or make the TV stand bigger uh we have a let's say more recent method has been just published in archive um very recently that tends to further improve this work if you're interested you can have you can have a look at that and in this case we can also generate texture for the two scenes actually it is improve a lot of uh let's say the aspect related to the um realism of the scene because we can now also use the prom to generate um let's say uh stylistic uh attributes that can be applied on the scene this is actually all I want to thank you for the attention I think we have still a couple of minutes in case of any questions and uh thanks thanks for listening until here thank you federo for this great talk um I mean we have a couple of minutes for questions and uh let me maybe start with one how far are we away from having this like very efficiently you know like generating 3DS like for AR applications like you know I'm wearing AR glasses my son is wearing some and I would like to you know generate a unicorn running on our on our table how far are we away from from something that is like um fast yeah so in in the methods that I presented today in particular text to mesh uh we didn't really focus on the efficiency aspect so this is actually a method that's definitely not designed to be for example real time or to generate things efficiently but there are now several let's say approaches that uh have made like a lot of progress in that in that sense and for example recently I think Andrea also mentioned some of these works that tend to for example rely on um the idea of generating um consider that once you generate for example a a frame sequence or a video sequence the consistency underlying geometry consistency might be a stake uh it really relies on how how well the uh foundational model that use for for for generating video sequences is able actually to reason intrinsically on 3D properties but that's definitely one of the way a very promising way to generate things much more uh much faster let's say much more efficiently to say our met 3 one minute just just as a reference that's that's what you get 30 seconds for a preview and another 30ex and how far are we away from getting this on device like um yeah okay yes but it's a uh I think I think there's there's already methods that that can be let's say used on device um but they would definitely provide a let's say you you you'll notice a significant deterioration of the of the results um for example um there's this nice tool that Luma AI put let's say so also on the web that can really provide very very quickly uh asset is not on mobile but it's very fast you can also see that if you really request results in the order of seconds then you have a strong degradation of the results so you get for example 3D matches that are incomplete or the missing parts or they inconsistence and I would expect that once you start putting this on on mobile right now you get also similar results so it eventually also the question is what what you want to get you want something fast so mobile and extremely accurate right and so if that's if that's your goal yet then then yes you're going to need to wait still a few more years have a question I was very interested in in the paper where you add the objects to the scene so how can you make the say the the lighting consistent for example so you have shadow cast Shadow for instance or in general you want illumination to be compatible with what you get in your scene is something you can do yet yeah so that that's one of the aspect that I mentioned uh in terms of limitation SL future work so in this case we don't really take that uh specifically or explicitly into account and so um we let's say in case for example you're going to place an object on on a table and on this table there's a array of light that just Shades some part of the table that's where you're going to put the object that's something that would be generated say inconsistencies I think there are ways that you can actually deal with that and so that's that's some interesting let's say next step for for the work yeah hi I to ask for examp combine aspects the first and the third so like how do you think how far are we from generating entire game uh scenes or Maps um again I think I think we are still uh scratching a bit the surface of this type of of methods uh one of the main limitation is also that it's really hard to talk for example about 3D uh um Foundation models right that can that can be let's say for example a nice a nice backbone for for generating these type of assets and that you can also then try to optimize in order to generate quickly quickly SC 3D scenes or 3D 3D assets together um then of course it depends also on the type of application that you have in mind in the sense that many many type of also gaming uh use cases can be simplified by generating uh most most of your data in 2D manner right you don't really need for example 3D meshes for certain for certain GIC applications or not necessarily right and so uh these are also tricks that then you can use to to speed up a little bit your your work yeah good we are perfectly on time so thanks again federo e e e

