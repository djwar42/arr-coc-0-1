---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=_vtae4Sc9EU"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:02.609Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=_vtae4Sc9EU

4aef2e8d-c82e-4dcc-8dad-7985ab5b90ae

2025-10-28 https://www.youtube.com/watch?v=_vtae4Sc9EU

e82a5820-5a92-48a3-be82-fd6b6360e31e

https://www.youtube.com/watch?v=_vtae4Sc9EU

_vtae4Sc9EU

## ComputerVisionFoundation Videos

okay so we start the afternoon session with the first invited uh of the afternoon uh K snoke from the University of Amsterdam and uh he's full Professor there at the University and also leads U three uh independent uh uh AI Labs I would say okay like companies and um he has been uh a speaker in the past in our Workshop already and uh uh his work on multimod learning is I guess well known in the community so uh let's thank the speaker for coming and enjoy the talk all right uh thank you so much for the introduction and thanks uh thanks again for for having me um today I want to talk about uh the phsd thesis of Yun Wang who uh who I had the pleasure to to supervise and uh the title of her thesis is multimodal learning under visually challenging uh conditions but before we get there let's first give you uh an historic overview of uh of multimodal learning in in the video uh context um this is one of the first uh uh data sets in activity recognition um used for machine learning based uh video recognition um if is done work done by Ian LV when he was a PhD student at kch together with Tony linderberg and and they created uh this this this this this data set where uh other PSD students were instructed to stand on a on a soccer field and do very simple activities like clapping and boxing like this so very uh artificial and articulated but this was the start of sort of like trying to recognize activities in in video you may even recognize some of the students in those uh videos who are now professors uh themselves I'll leave that as an exercise uh for the audience um they were using uh stip features at the time and and support vacum Machines of course now nobody knows what a support vacum machine is but this was sort of like uh early days and uh boy did we make progress so this is a a video from people at uh at Facebook with the the c3d uh features if you still uh remember so convolutional based uh backbones and this was really classifying activities in short trimmed uh Clips uh with uh yeah excellent excellent uh results so for short trimmed Clips this this is essentially nail bit um uh work for my team got everything I need right here with f a gift I don't intend on wasting got um group activities in sports like uh volleyball uh match is also a topic heavily uh studied in this uh in this community uh combination of uh long range relations between actors and and their objects and and the relationships between them so also a topic done in uh in in video recognition uh this is early prompting uh work where given a sentence uh we could recognize and segment an object in in a video segmentation results is not particularly uh good I would say but it is interesting that it it does trigger on the on the right dog so not does not trigger on the dogs in the in the backgrounds this is work from uh from icv the last Edition in Paris unsupervised object uh segmentation um it is actually quite impressive how much how much progress we have been uh making when I started as a PhD student it was sort like unimaginable that that that we would would get there at least in in in at this moment in time um last paper I want to show this is work done with with hazel Dy working not only on what uh can be recognized in in video but also uh how an action is being performed so looking into the the adverbs uh of activity recogition so to say so an action is done slowly or quickly or sneezing is done instantly or messily what have we um then there is one common assumption that all all these Works have in in common and also at this conference when you look at at at multimodal learning papers everybody has the same uh assumption and as you may guess this is uh known as empirical risk uh minimization and the IID assumption so all these Works assume that we have a training and condition that are very similar right so what we have learned during training we are bound to see again also uh during testing and this is a very nice uh assumption if you're training in test sets uh are similar because then deep learning and our favorite approaches can do really really uh really well um Yuna questioned this assumption in her thesis and uh she considers multimodal machine learning under under visually challenging conditions uh typically where the the visual conditions at test time are very different uh or slightly different from the ones experienced during training and what is the influence on on on the multimodal learning scheme so to say so here you see some typical examples of Real World visual conditions that you typically do not train uh for um so you WR a very nice uh thesis with uh five chapters I'm going to highlight three of them uh in this talk where we will consider robustness to domain shift uh modality shift and also resource shift and I will highlight uh these one uh one by one so uh the first uh chapter is about robustness to domain shift this is work that was published in in cfpr 2022 so it's already a while back work together with hazel and and Ling Ling Shia and uh here we looked at activity recognition under domain shift so here are some uh examples so this is what is called scenery shift there should be audio but I don't hear uh much um this is another domain shift camera Viewpoint shift so we have the same uh activity going on I've recorded from two different uh camera views this is opening drawer right so the activity is the same but uh the pixel content is completely uh different and here's another one that we defined ourselves we call this actor ship so the actors are eating so the activity is the same the actor is uh totally now the proposed uh solution uh that we explored with Yuna is to really approach this problem from a multimodal uh learning perspective so while the pixels uh change a lot for each of these three domain shifts the audio is pretty constant right so the audio is actually a pretty good uh invariant for this domain shift uh problem and uh the audio can be used into in two different ways so the first one is to identify characteristic uh sound uh signals so for example the eating uh makes the same characteristic sound although the audio looks very different so that is one cue for the audio and the other cue how you can use the audio is sometimes you don't know what the audio indicates in relation ship to the activity of interest but you do know that certain activities can be excluded because of the characteristics of the audio so you can use it sort like as as negative uh information especially for activities that are silent by nature and don't have a characteristic audio right it's the now she proposed uh an approach that we call Audio balanced uh learning so what typically experience when you train on a on a training set and you move on to a uh independent test set is that the label distributions between these source and Target sets are typically different so if you over optimize on a certain label distribution during training you will fail at testing because simply the distributions uh change so uh and here you see an example Source domain where you have rare interactions and and and frequent interactions and on the target domain the frequent interaction is actually much more uh similar to the to the rare one but it will simply not be recognized because it is under underbalanced in your training set now the solution here is quite simple at training time make sure that each clause and type of interaction is accounted for equally then the other uh thing that we did is what we call absent uh activity Learning so here uh the activity is now this this sound of pouring may not be very characteristic to sort of like if you recognize the sound you would immediately say oh this is pouring right but what you can say based on this sound is that certain certain activities are very unlikely like wash closing and and opening so this sound is not indicative for these activities so you can scratch those so you have uh less room for mistakes so to say um these two are nicely combined in nice architecture with different losses to to optimize it you can find all the all the nitty-gritty details in the paper I I I want to demonstrate here how how it works so if you would do the the senior shift uh experiment on Epic kitchens where you train on the one scenes and apply it on the different scen so the the kitchen scenery changes for different activities if you would train your classifiers in a a visual only uh scenario you would typically score in the top one accuracy of 48 now if you use our uh audio balanced learning and the uh uh compensate for the for the negative uh Parts it would improve improved results to 50. 7 top one accuracy and this is only during training time so you only use audio during training at test time you would only rely on Vision information so even if you use Audio Only in training your visual classifier will also improve so that was a surprising finding here and if you also have audio information at at test time results are actually even uh uh even even better second column shows a Viewpoint SI so you go from third person view to Ego entric uh view so here again uh this audio adaptive uh learning is is is beneficial now let's look at some uh examples uh so this is actor uh shift and this is sleeping and uh the algorithm Su successfully predicts it as such here are other examples so listen to the sound opening door so here you have three three training examples in the source domain and this is the uh this is the test so the actor has uh changed but the activity sound has remained uh the same nice to uh look into so here the source domain is is drinking Al those don't don't have video but I have have for this one so the the duck is is drinking but the algorithm thinks it's resembles more the sound of of of eating in in this case here's another one um these are people uh running on on a treadmill and here we have uh dogs uh running but it is confused uh so the the dogs jump into the water and the the sound of the water sort like confuses it for the activity uh swimming so it is not uh perfect uh yet but it shows that uh sound has actually a very strong invariant property that you can use from multimodal learning across uh domains um and has a much better adaptation ability than visual Only Solution so it is really advantageous uh to be multimodal from the start um our approach benefits from audio more than Alternative Audio Visual Fusion methods so we also tried in in in the paper to see what what do other audiovisual Fusion methods bring to the table uh but the fact that they could not sort of uh weigh uh the effect of the audio uh was hurting these approaches where we have a benefit um and we demonstrated that with this approach we can generalize not only two new uh new environments but also two viewpoints and even can replace uh the human actor uh with an animal and still recognize the activity well all right so then uh the second shift uh we consider is the robustness to uh modality uh shift and this is work that appeared at uh last nurbs in a paper called learning unseen uh modality interaction and uh here we question uh what we call the modality complete assumption so when you have a multimodal learning problem and you have a training set and a test set here you simply assume that the modalities that are available at training time are also similarly available at at test time and of course in in in a real world multimodal practice with multiple sensors uh this this assumption is is pretty strict because sensors can fail or become can become unavailable uh so typically these algorithms would would would break when these assumptions are are hurt uh others have also looked at this problem uh so they have looked looked at uh robustness for modality incomplete uh data where uh one or more modalities can uh can be absent uh during inference but it still assumes that all the modalities that you see at test time have also been seen at uh training time uh so our goal is to recognize uh unseen modalities sorry the text is a little bit hard to uh hard to read uh we we are interested in recognizing unseen modality interactions so here you can see that at trading time we have seen RGB in audio in one device and audio and Optical flow in another other device we still see those modalities at test time but we now also see the new combination of RGB and Optical flow that was not considered uh during trading time um simple concatenation of these unimodal features cannot learn this cross model uh correspondence effectively uh when the modality complete data is uh is unavailable uh then we want that the accumulation is agnostic to the order of the modality so we want to be set in variant so to say right so the the fact that flow always comes after RGB should not should not should not play a role um and since all modalities come with their own uh feature space and dimensionalities it is not so easy to combine them right it is it is very easy to uh conceive that one modality will dominate and that is what you need to need to avoid so our approach uh consists of two stages so each of the modalities has their own modality encoder we don't uh change this but we have this uh feature uh projection stage where the goal is that the dimensionality of each uh embedding is uh reduced where we strive to have all modalities to have the embeddings reduced to the same size and while we do this projection we also make sure that the modalities are aligned so that they are reduced to their sort of like semantic uh uh uh summary and these are then coinciding so this is the feature projection uh stage um then the second stage is what we call a dual Branch uh prediction um so we have a supervised uh loss often for the the task of of interest but it is very uh likely that some modalities are more informative than others or more reliable uh than others so to take this into account we have this dual Branch prediction approach where in addition to the to the task supervision we also Al have a pseudo label that indicates how reliable this particular modality should be we performed evaluation on on three types of uh problems so we considered uh video classification on on Epic kitchen we considered uh video retrieval with lots of modalities like objects OCR faces speech audio what have we and also uh robot State regression is in the paper but nothing in the talk we made separate splits for these uh problems because we explicitly want to have modalities at test time that were unseen uh training time right so the numbers I present are not comparable with with previous numbers reported on these uh data sets um and for video classification uh we use a swin t uh Transformer um a res net for the opical flow encoder and as for the audio recorder and for the uh multi for the video retrieval work uh we use the provided uh unimodal features that come with the paper original paper um here you see some uh results so these are results for our modality incomplete setting so we have at test Time new modality combinations that we didn't see at training time and if you do traditional uh late Fusion you would score 18.1 top one accuracy on Epic kitchen if you would apply stateof the- art modality complete uh Fusion method like the one from Nani at all it would actually hurt your performance because all of a sudden it doesn't know how to handle this unseen uh combination if you follow a modality incomplete uh tactic it is slightly better than the late Fusion Baseline but also not uh super effective and if you would go to our unseen modality interaction approach uh you clearly have an edge with much better top one uh accuracy and much lower uh uh mean uh uh recall for video retrieval we also did an ablation where we Vari the amount of uh modalities at at test time and what we noticed is that uh our model can handle any input modality so it doesn't matter how many modalities are there uh it improves robustness for all combinations we did notice that the more modalities are available uh the bigger the game because there's more opportunity for for cross fertilization uh so to say um then the key takeaways from from this uh part is that we can effectively make predictions for unseen modality interaction by feature projection and pseudo supervision so to take this reliability of the modalities into account uh and our approach is suitable for classification uh retrieval and regression uh problems and can handle a wide variety of modality combinations then the third uh chapter uh that I would like to discuss is shift in uh resources and this is a work that's presented at this uh conference uh again with Yun and uh and Hazel and here we look into low resource computer vision uh uh challenges for foundation models and um the inspiration for this work comes from the field of NP like so many uh Innovations in in Vision uh these days uh because they were first with these large uh large language models and now large Foundation models and a topic that has been studied for a while in the NLP Community is low resource uh languages right so large language models are great for for Mandarin for English for Spanish uh but for poor resource countries like the Netherlands uh these models are not so not so great so is there we wondered is there a low resource uh equivalent also in in in computer vision um and this problem uh has basically been ignored so far in in our community um so we Define uh the problem of low resource uh uh Vision challenges for foundation models and to give you an overview of of this problem we are very familiar with high resource uh problems like like think think the image net uh data set right so this is typically big data that you can easily find uh on the internet rather coarse grain uh uh categories that Define uh these data sets and it's typically a very general uh domain consumer photos uh what have we low resource is uh is the opp Opposite end of the of the spectrum so there's typically very limited data uh available for low resource uh data uh it tends to be uh very fine grained so very specialized uh topic with small differences between uh categories and a very specialized domain that you cannot easily uh get your hands on and uh we defined a benchmark with three types of uh data and challenges defined on low resource uh uh Vision challenges so the first one is uh circuit diagram classification so if you had electrical engineering uh during your uh education ational studies you may recognize this problem if not then this is really hard so uh the diagram on the left is a pictorial representation of the circuit and um this is a circuit diagram of an audio amplifier as it happens so our goal then becomes to given a circuit diagram classify uh uh the circuit function here's an another task that we defined the task of historical map uh retrieval so given an given an old uh map from let's say the 17th century of a city in Europe can you find the equivalent uh map based on Modern uh cartography and the third one is mechanical drawing uh retrieval so again a very specialized uh domain where the goal is to retrieve a 3D three view drawing from a 3D rendered image or or vice or vice versa now I challenge you to find lots of images for these uh problems uh we we tried really hard to find uh many uh on the internet and um this is what we what we found after a serious effort so this is in the order of hundreds of images per train set validation set and uh and and and test set so this is really uh this is really low low resource much much much smaller uh than uh we are used to it is also nice to write the paper actually on on such a small data set because your experience experiments go really fast now okay so this is all nice but then we tested how well do present day Vision Foundation models perform on these uh on these benchmarks right and we took four off the shelf model so we took clip Sam uh blip and image bind so image bind is a from a paper cfpr last uh last year and turns out that all these models perform kind of poorly on these tasks so if you look at xaxis the the numbers are really close to well typically 10% or or less and uh overall image bind uh does the best and some of these models are actually uh uh very poor so uh blip blip and and S perform very poorly on these on these tasks so that's good because that means there is a room there is a lot of room for improvement so um we consider three challenges for low resource uh Vision data so the first one is uh data scarcity so you don't have a lot of data so it is really hard to learn something from uh from such few samples the second one is that the data is very fine grain so the categories uh are very close so it is really hard to to get to the essence and the nuances in your representation again given so few examples and last but not least is a very specialized domain so you should need highly semantic cues to make the recognition now our goal in the paper is to provide three simple baselines for this Benchmark to demonstrate that exist Foundation models can be adapted to this low resource scenario and also to inspire the community to to take up the challenge and come with much better uh alternatives for these baselines so um let me briefly highlight these three uh baselines to you so the first one focus on uh generating extra training data the second one focuses on on tokenization in the Transformer architecture uh to have more room for find details and the last one is to update the attention mechanism in Transformers for uh for specialized domains so let me first talk about generating extra data um so the first thing we did we we we took stable diffusion and we generated more of the same data right that is what probably everybody uh would do and turns out that is not a very good idea because what will happen is that your small amount of data will be expounded a little bit but it will be so similar that you start to overfit so that is the big uh problem um this is what we call label preserving uh uh generation of extra data so what we did is we also have what we call uh label breaking uh examples so if you play around a little bit with the parameters of stable diffusion you can generate images that look so still sort of similar to the to the data you you have but you can no longer recognize the categories um so here you see that for historic Maps it still looks like a map but you cannot say what was the city that the map is uh is from um but it doesn't matter that we lose the label because we can generate images that are similar in this in this negative sense and they become valuable examples uh for for for learning um so we combine both these label preserving images and the label breaking images uh for fine here you see some examples so on the left you see uh label preserving circuits for FM transmitter and on the right I I wouldn't say still an FM transmitter but it is sort of still a circuit diagram Baseline two is tokenization for to allow for this fine grained distinction um we have limited data uh so we cannot train a tokenization layer from scratch that is sort of like out of the question so what what can we do so what we did is we have the linear projection uh a kernel and we divide this into sub kernels and each of these sub kernels we assign a weight so then we have we add a few extra parameters for these sub kernels and those we can optimize with a limited amount of data uh that we have so this allows to have some fine-tuned adaptation uh so to say um so that's Baseline number two and then we have Baseline three is uh exploiting the attention uh mechanism so what we do here for each token we crop its region in the in the global attention map and we combine this with the overall multi-ad self attention so to really sort of like overemphasize on characteristic details in in in the image that are indicative for the category or the map of uh interest now in the paper we show that each of these adapters uh improves low resource challenges a little bit uh you can go over the details in the paper not so important here this is what I want to highlight so um here you see when you combine all these three baselines together uh and you fine-tune then existing Foundation models so we did this for clip for blip for Sam and for image bind so the the top uh the top bar is sort of like the default uh when you run these these models out of the out of the box the blue bar is a uh another uh adaptation method that is more like for general purpose like high resource data so not optimized for challenges of low resource computer vision and the last bar is is our combination of of baselines optimized for low resource uh setting and that really has a very uh uh good effect so this demonstrates that low resource challenges uh exist and they require another type of solution uh than the common approaches that we we take for high resource data um here are some hard samples where we where we still uh fail so uh what we notice is that we are still overfitting so our our predictions are are still overconfident because we have so limited examples so this is a problem and um when there is a key regions like uh the presence of a battery in an LED circuit we tend to over overemphasize on on these key regions so how to balance what information uh locally in the image that is a quite important for this Challenge and uh that is I think interesting for future future work the other thing that we noticed is that we are not only low resource uh but within low resource we are also very longtailed so when we looked at the Historical Maps we found a few samples of very distinct styles that were also very rare in our small training set like the historic map of inbrook uh in in in Austria the style of this map was very particular we didn't have many instances of those and then recognition uh fails so that is also uh a challenge left to address um so the three takeaways of this of this work are that uh low resource vision is is a real problem that deserves more attention in our community it has uh three clear challenges so extremely limited data uh fine grain differences between images and highly specialized domains um we have provided a benchmark uh that that the community can use and and and improve upon uh we have demonstrated that existing Vision model struggle and with three relatively simple interventions uh adapters um we can sort of like uh make make these models better for the low resource setting um and that already concludes uh my talk and I would like to conclude with an encouragement so uh learning to generalize for multimodal learning in space time and across modalities is really an open research challenges open research Challenge and first ideas are appearing but there's really a lot of room left for further Improvement so I would like to encourage you uh to uh keep on working on that thanks a lot for your attention Okay niik please right I want to to ask for the last Str because these images in general so these low resources the one you show they have a functionality to them right in the center of the circuit just does something the map it's also has a little bit of extra information not only that so are you keeping this uh this information in mind so just in the circuit if I don't put the resistor there then it's not an amplifier anymore and so just just generating something very very similar it doesn't mean that it's still an amplifier or something else so it's not only just replicating the pixels but it's also to keep the functionality I think that will be interesting to yeah yeah we we haven't considered that we we we we do notice that many uh many low resource scenarios are very very specific and also often tied to an industrial area you need the main know that in order to solve it uh yes yeah because uh I I I was not trained as an electrical engineer I had a huge difficulty interpreting these diagrams myself but Yuna was very uh very fluent them yeah yeah first task uh I'm I'm pretty sure that we we tried this too but that is uh much worse than Vision only uh so well vision is very strong and uh it's it is really the the multimodal combination that helps so by itself audio cannot uh generalize strong enough yeah too much confusion from other classes but for if you consider slightly longer video where maybe more than one event is happening can you hypothesize yeah that's a good that's a good question uh in indeed I think all the data sets that we considered are trimed so relatively short uh so we're Focus focusing on single single action um if we would do longer videos I think you also need to have like temporal segmentation to identify what belongs uh belongs where but we we didn't consider that uh in this work it's much harder of the hypo might down right because the audio that is like uh consist yeah we saw that in the negative example of the dogs because they were running but then at some point they jumped in into the water and they were swimming and then got confused yeah yeah in that study what is the weightage certain actions definitely definitely um and I I'm pretty sure we have some analysis and ablation indicating the effect of uh of the fusion per per category but I I didn't include that in in in in the slid I also don't remember it to be honest but I'm pretty sure it's in the paper or in the supplemental matters that's what I I recall yeah treat the mul modality to similar or same in Transformer or the or we treat it different modality Case by case different modalities each with their own encoder um but we make them similar with the projection state so they keep their own modality and their own specifics but we make it uh similar in the projection state so that it becomes a common representation where it can be combined you're welcome yeah one like thank you for and and about this audio right and vision before with with Dima we discussed about the complementarity and the redundancy right of the Dual modalities do you have any takeaways to tell us you know about this right so you know audio may be complet the information or be ADV of the information that you are getting so in many of the of the problems that we work on in in video or in in Vision um the data sets is is constructed in such a way that the best role audio can play is uh complimentary uh so by design of the data set you can only demonstrate yeah Vision will do best and audio will add a few percent but as soon as you break this Assumption of having perfect vision condition at training and test which we always assume then the conclusions change and then uh there is a reason we have multiple sensory information as humans right else we we would be we would all be de and Only Have Eyes if if vision would be best so all modalities play play play their role and I think we can benefit from having more uh real world realistic uh data sets uh that capture multimodal data also presumably other other modalities well the The Click Paradigm is not really good for audio and video You Know audio is more complimentary as you said right instead of like language maybe with description yeah I don't know I haven't looked into that uh aspect yet any more questions okay have yeah very last question yeah so for the uh the incom modality during testing in contrast to that what is a new modality that mod so come a scenario like that um I think that is a yet another uh variant but then I would maybe opt for test time adaptation type of approach where you try to sort of like uh take the knowledge that you have and adapt it to the best possible knowledge for this unseen totally unseen scenario okay uh I guess we have to move to the next session and can thank the speaker again thank you uh okay so the first presenter is online who yeah so hello everyone uh first let me just confirm like is my screen visible and am I audible yeah uh audio is not uh great but uh I guess this is uh what we have in the room and uh you try change try again no the point is explaining on from that one yeah we can try this just just a sec MH yeah okay can you try to speak again yeah hello is it clear now try again hello hello everyone M check uh is it clear yeah I guess the audience cannot hear so we go that settings yeah but it's just okay you can start okay so uh hello everyone my name is beiman so today I will be presenting our work but before that I would like to apologize on behalf of both of us for not attending the event in person so that being said I present multimodal understanding of memes with Fair explanations so as evident from the title our goal is to examine meme explanations generated by Vision language models through the lens of fairness so let's understand with an example here so given a meme when a visual language model is asked what the meme conveys it generates an explanation based on its understanding of the meme poster you can observe here that the group of people are identified as participating in a race instead of people running from fear of death from the rocket Additionally the word democracy in the meme has swayed the model thinking that rocket symbolizes Unity instead of death so this example clearly demonstrates bias in the generated explanation and our goal is to ident ify the reason behind it but why memes memes are unique combination of image and text plus they are widely used metaphorically all these characteristics and Associated metadata make memes challenging to understand than regular images so we aim to examine the fairness of generated explanations when explicitly presented this metadata to achieve our goal we collected various publicly available meme data sets however they lacked certain inputs required for experimentation as shown with gray gaps to fill these gaps we used OCR to extract the meme text and visual language models to generate their captions later used to generate the explanations as well this new endr and unified data set of memes is our first major contribution we used lava 1.5 and Min gp4 Vision language models with following categories of prompts where raw contains only meme poster and as we proceed we explicitly add more information like OCR caption and metadata so moving to evaluation we begin with automatic evaluation using offthe shelf models like toxicity profinity and apis like Google's perspective API the table shows the result for perspective API it can be observed that very few explanations are classified as toxic and when OCR and captions are added which is like when we move from raw to P2 the toxicity is increased we also see an increase in toxicity when moving from lava 1.5 to mini gp4 but this was not our main goal our main goal was to tell what the story all these metrics are telling interestingly we wanted to find whether they are telling the same story or not so interestingly they are not and when We examined the correlation between this Matrix we observed very less correlation among them evident from the darker shade of the first two rows or column Columns of the Matrix shown in the right so this finding is our second major contribution and raises concerns about their reliability so now to find the reason we performed manual evaluation looking into the previous example we found out that because of common visual feature of people participating in a race has led to such misattribution additionally since the democracy is generally used in positive sentiment here the deadly rocket is seen in a positive sense so with this uh evaluation we found out four broad reasons of bias that is towards common visual feature towards group entity or gendered towards image then text and towards sentiment associated with words the table shows they spread across the data sets and prompts these findings become our third major contribution during our manual evaluation we observed that Vision language model May struggle to understand the text in the memes and generate biased explanations however by providing the correct text through OCR we can mitigate the bias as shown is the example where earlier the explanations says US democracy is flawed which is not what the meme post is trying to convey however later with proper text provided it got the explanation correct another Finding is that if the caption generated is biased like here the model thinks that the human is holding a hammer this biased caption later influences the explanation to be biased so these findings add to our contribution and should be utilized in mitigating bias finally I would like to conclude that with an aim to examine fairness in generated meme explanations we found out that certain Vision language models carry biases through our proposed unified data set we perform both automatic and manual evaluations and found different origins of bias with this work we want to encourage efforts to have development of fair and safe AI thank you everyone feel free to contact us for any discussions thanks again I'm now open for questions any question from the audience no okay so I wonder um I have a quick question um do you do you evaluate some qualitativ in this in this work so can you can you tell us maybe some takeaway that you that you that you noce by looking at the actual results and just comparing it like manually uh between the automatic and the manual ones mhm yeah yeah actually uh we did uh so if you see this table so for the first three data sets actually when we are finding this like these explanations are biased we also checked whether automatic evaluations are seeing them biased or not and surprisingly the automatic evaluations among the first three none of them were uh classified as biased or toxic so that was another thing which we found out and uh with the below two in the below two we only took so here we have told uh like 10 10 samples and those 10 samples are actually all the 10 samples which the automatic evaluation metrics told that they are actually biased they were classified as biased but in our manual evaluation we found like six and three where only actually like these are the numbers like to be biased so so yeah so they were not telling the same story like in our manual and they don't coincide with each other yeah okay thank you yeah welcome any other questions okay so let's thank speaker again yeah thank you e right nice uh good afternoon everyone uh welcome to my presentation I'm jangu and you can come me Kevin uh today I'm going to talk about our recent work the denoise language fustion Guided by visual cues for e-commerce product search uh this is a collaborative work with is Shasha main AR and Doc when I was interning at Amazon the multimod fusion is a powerful tool to integrate information from different sources and the successful training of such models heavily relies on the alignment quality of the of the data sets in the e-commerce domain the such data set are usually collected in automatic Fashions and hence without additional human annotation the alignment quality is far from idea the sellers used to provide a very long title that optimized for tax base search that contains a lot of attributes cannot be revealed by the image look at this example there are 23 uh words in the title but only three of it you can directly see it from the image in such cases if we train data TR model with such kind of data set it will be very challenging to let the model align the image with the entire data set the words like the BPA of Battis are noisy to the model and to positively get a sense of how severe the the discrepancy it is we Leverage The Flip two model to compare the embedding similarity of the each phrase to the image and we can see that for those words that has can be directly seen from the image has a significantly higher similarity than those cannot be seen and this goes with our expectation and also indicates that the current e-commerce data set contains a lot of noisy nonvisual descriptive phrases on the other hand it's not very feasible to manually claim a very large uh e-commerce data setup so we're wondering given such noisy image text PS how can we effective align the vision and language and inspired by the previous example we are further wondering how we just let the model to pay more attention or reduce its attention to this online part and in this work we propos to Leverage The Token pruning which is the me commonly used for efficient de planning to mitigate the negative impact from those noisy tokens since we want to let the model adjust its attention we work with attention directly here we are looking at a heat map of the Cross attention in one of the our models layer uh between the text tokens and image tokens a brighter color indicates a higher similarity and we can see that for some words it's consistently have a higher similarity than the others so if we take an average across all hats we can get an average uh of language tokens important scope and assuming that if we can have a threshold we call it gamma and we compare it with this threshold we can have a pruning Mass those tokens that has lier lower importance will be prune and question is how can we get this G and our ANS is we want to model it as a learnable parameter through a pruning loss and more technical details can be find in our paper and we evaluate our mode proposal method with a data set of over 700,000 products and also we investigate the efficacy of our model in two stages first is we want to see whether it can benefit fine-tuning or off the sh model for the specific e-commerce visual retrieval task and also we are wondering whether for the model already in the production that is not trained by our method can our method also help it and all the training I are based on the same data and let's look at the the results we can see that in both stages our proposed method can benefit the model with some improvements in the recall especially when we want to F tuning of off the shelf general purpose model for the e-commerce uh purpose we can averagely have a 5% points uh gain in the retrieval and in this indicates that the the the data set is really noisy and our proposed method can serve the purpose and we not not only evaluated with the like cross attention based model we also look into the L Fus model like the clip and in conclusion in this work we form the noisy image texpar issue as a token importance problem and we propos the multimodel learn token pruning method to effectively reduce a negative impact from nonvisual noisy text tokens and we conduct extensive uh experiment with multiple kinds of models and in different stages and our Master is proven to show to to benefit the model with additional performance games that's all thank you thank you for the nice presentation any question from the audience okay sorry thank you foration using of Comm product uh when you're looking at it what percentage of the terms that were in your descriptions were like marketing or nons terms as opposed to actually distri terms uh we that's a really good question we don't have very quantitive analyze to those but my my experience is the ratio is rough 3 to seven so only 30% of the say in the title is about the product and rest of it just for the marketing of for optimize the tax base search yeah so that means we have roughly 7% of nonalignment part yeah have you noticed some some difference in maybe different categories even know some categories are more noisy than others uh yeah we we we do have some investigation in that party and we found that is especially in the uh in the daily life product that it tends to have more like noisy words and for the very specializ like Moto Auto like say for the cars for the phones those things are more specialized because people just search by the like TX yeah yeah yeah yeah yeah that that that's that's also something we observed and uh the and to be honest we don't have a very good solution to it and we compare like by looking at different ways like by only looking at CS token or by looking at the average and we found each of them has have have their proc count so there it's open research question Yeah question oh yes so uh we we we that's a good question and we we don't actually don't have the label for it we we just basically uh trying to our our task is for the visual retrieval so in this way we just the the the the image of different angle as our retrieval label and here we are just to trying to show uh uh in the in the in in in the title basically we can tell it's uh what what's what's re relevant to the image was not but we don't have a very quantive say how how correct the printing it is yeah in principle yes yeah okay uh we can speak again all right thank you so the last presentation of his session is how word translating window Panorama yeah hi am I audible to the zoom people speak okay perfect all right hi everyone uh this is Thon Mo I am a thiry year PhD student at Northwestern University in computer science so this work uh this work titled zind towards translating IND panoramas into descriptions that was done when I was a research intern last summer in Z so fortunately my managers are here as well so yeah uh so the high level overview of the problem is that we uh like we we are looking into a new kind of situation where uh the input to a model like we expect a model that will take a set of Panorama images as input uh let's say for each floor it will take unordered set of Panorama images and we expect the model to generate a coherent description of the whole property or home right so to this end uh yeah so to this end uh there's like multiple kind of Novel situations that we faced first all first of all that there's kind of no similar research that dealt uh with set of indoor panoramas to description related research so to this uh to solve this thing we came up with uh so there's like no data set available so we came up with a uh data set which is uh which facilitat this uh this research and this problem is highly practical because let's say like the real estate agents that those those people don't have to write anything if we have a model that actually does the job for them right and they can just just do like slight modifications on top of the existing gener description of the home so in short our contribution first is zind tail data set so from Z there's already a Zillo IND data set if you know about that so we extended the Z window data set with language modality and we call it zil and uh the the the language that it that that that is present in the data set is semi-automatically generated and it is then human evaluated validated and then based on the data set we propose two benchmarks so so we propos Benchmark on two objectives first objective is basically evaluating the description generation and the second one is based on the description language can we or that can model retrieve the home or you know the floor class so uh to like get started with the research we proposed a very kind of simple uh zero shot Baseline model named zindy agent and after that what we did is that we took some nice we made some niif baselines um and then compared the zindy agents Performance Based on those KN baselines on top of the zind TA data set to establish like a benchmark for further research so there are couple like related data sets uh available so the most Clos one was is this one I'm not sure of sorry yeah so the New York sentences newor version two data set but this one uh so in this data set they have one Panorama image with descriptions that is there is no multiple Panorama image per floor or per home that describes the property like in in overall so this is just like a room description that's it so there like other work I'm not going to the detail due to the time uh so let me just briefly tell you the the overall pipeline the approach that we followed to generate the data set uh you can see here so what we do is that first we have all the raw Panorama images that is available in the zindi data set also the floor plan image based on the raw Panorama image there are like annotations by the real humans of the room layout window door uh opening vertices and also like other information like room labels transform and so on So based on this and also the floor plan image data we have the factual information okay and also as we have the room exact locations of the Panorama images we can extract the connectivity graph between the floor plans and combining this uh like factual information and the and the connectivity information we built like a structured schema that is detailed in the paper and that schema is being used in an llm for Fring and then generating a description which is kind of corresponds to the ground tooth data that was annotated by the Z people uh the people that annotated the z z data set right so on top you can see like a reconstructed Flor plan where we have reconstructed the floor plan with the room to room connectivity and also opening connectivity between the rooms and those are being used as a ground truth information for for for generating the description so here is a sample description that you can see and later this description is evaluated by people real people and then we calculate we we like extract the relevancy score and see if the description is relevant to the kind know main property okay so just just to uh put a note during the uh during the uh the the ground to generation ground to data generation uh we use both raw Panorama images and the floor plan however during so we we don't we expect the model to generate the description only based on the Panorama images not the floor plan because that the ground truth right so yeah so this is kind of like uh the UI that was used to check the relevance by the humans so here are some insights of the data set on the language endpoint but this is the most interesting part that I want to share so what we did is is that we extracted the the ril data set language descriptions and maap them into uh like map the Mings of those description into like 3D space like using TNS tsna algorithm and what we saw is that when two uh floo plans that are kind of similar they share the same eding space whereas like the different kind of properties or homes or floor plans they they are projected into different eming space so that means we kind of found some interesting phenomena of like can we like retrieve the home based on the description itself because we clearly seeing that this kind of thing we can do because similar kind of homes correspond to similar kind of descriptions and so the embedding space right so this is the second objective that I was talking about initially so to initiate the Benchmark uh we have uh we propos Zend agent which basically takes the raw Panorama images and it has four uh kind of four modules one module uh does the room classification another one does the like the that does the layout layout estimation and the third one does the uh the depth estimation and the fourth one extracts the room to- room connectivity geometry based on only on the Panorama images and based on all the all the modules combination we uh generate the predicted description and then uh like do the evaluations based on the uh predicted description and the or ground to description from Z detail data set right and on the bottom table you can see like uh the results where we have two objectives the first one is language based home retrieval uh where uh we kind of compute the recall of the model to to to find out which floor plan or the which home is the closest one based on the language and we can see that the ZD agent does better performance compared to the clip uh nice clip and same for the description evaluation we use like Foundation model bleep and that our vindi agent does better compared to the uh the bleep 2 uh model in the sentence evaluation benchmarks so there is like uh this is like the qualitative uh sorry this is like the qualitative example where you can see that uh so for for a for a whole property or home there is like multiple floors three floors and for each floor there's like multiple Panorama images and here is a ground truth description here's the description that is generated by the knife Baseline bad to in our scenario and this one is the generated by the zind agent so in terms of several kind of matrices you can see that zind agent does better even in the qualitative scenario as well like the text she if you can but feel free to reading the papers yeah thank you okay there's a question from the audience maybe you can answer it yeah that's a great question so basically we yeah for those who done you can see can read it yeah yeah so the question is how to how to quantify how to quantify the similarity the similarity between two floor PL so basically what we did here that's a great question first of all so what we did here is that um let me just go there so for what's that yeah for each home okay so this this belongs to one home like the property here and this one is from property B so we have the description from the properties right so then we map like like we extract the maing space of the description and then map them into the 3D space and that's how we kind of say that these two properties are semantically similar because the embedding space projected embedding space is pretty close like the points are pretty close to each other yeah follow question about that enough any other questions from here so just a quick one uh how this model generalized in general right so if you you can potentially apply it also for outdoor right or is there any for like small pieces of the no that's a great question that's a great question for example here for the zindi agent Baseline sorry uh sorry but start yeah for the zind agent Baseline so there is like a module named Salve this is actually a paper from Zillow like it was also done by an intern in 2021 so what that that guy did is that so so given some indoor Panorama images it tries to do like cois it tries to create like a connection between multiple panoram images based on the co visibility so that one is probably difficult for the outdoor but if this kind of model can be made for outdoor definitely it's possible to generalize yeah yeah thank you welcome so if we don't have any discuss we can than speaker thank you we have a short break now we are coming back the 34 see link just fore just s of I mean I no one yeah presentation so we can leave LA's face so just passing by this h okay e speak B we e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e okay um sum no now I'm Shing am I sharing the right thing Zoom so you mean you just arriv two days ago and tomorrow you fly back right on Friday um then we did some hiking or yeah and then and there was the AC Workshop but everyone but plan was to attend right and then yeah then the workshops and tomorrow I mean all the papers are on Thursday and Friday so I will meet them and we also have a demo which I will threee papers right two okay two okay good yeah yeah and yeah so how big is your team I mean I still have students at Tu I would have three new PCS at Nvidia and then I have like four full thers two more are joining and then a million inters okay so yeah and um do you have say a research lab at Fan so oh no no it's remote it's all remote right yeah yeah and you you're used to this so it's better than I expected okay so yeah I mean my whole most of my team is been so I can travel there and then we meet with everyone okay and often I can hear now you still official un officially let's say yeah okay yeah but yeah maybe like three four times a year okay that's not to no it's okay it's also on so it's fine yeah that's true Munich is really very well connected yeah yeah I even fly here too via Munich so really I M okay because people flow through Frankfurt or like I don't get it neither I got interes I met lots of people visiting CPR was full yeah maybe yeah yeah I think I I book once work shop accepted because a small head start so I arranged and others of stuff okay was and then I had no no issue with that so maybe this is the reason that's true and um I know that in in to son she's also are you somehow connected to Connected we yeah working a lot but this is all one big family so to say not really like we are officially in different organizations but it doesn't matter in practice yes this I was think that this is splited a little bit within Nvidia like having companies oh no no no no it's just a different organization inside media butes so how much computer sources do you have you know a bit it's not it's not crazy it's not like I think meta is much more okay but it still like allows us to do a lot of stuff so you're also diving at generative models a little bit dive yeah and this whole stuff this is more like for you or do yeah do you give it up a little bit now or do you think this it's enough no it's more like expanding it a little bit like what I'm going to talk about today like more expanding open world segmentation for the different modalities so I think yeah okay so something let's say it's like a nice playground yeah interesting yeah you know that Roberto is now at pixart yeah yeah yeah I think let say former student now at he defended I think two weeks ago okay and now he's at M London cool so um yeah so um we we quite have a few people now having finished and moving to very interesting companies and it's very nice to see how how you all developed over the last so such an amazing experience for me yeah yeah yeah I can imagine yeah yeah and uh yeah I'm still very proud of your development all the last few years just been see your papers and some of your YouTube calls whatever just sometimes popping up for whatever reason it's it's really nice to see cool yeah should we get started or yeah made my homework not really oh there all right so um welcome back to our workshop and it's my pleasure to introduce to you uh laa um she basically studied at the Technical University of Catalonia in Barcelona then she went to the LI University of Hanover as a PhD student she was basically one of my very first PhD students helping me building up my lab in Hanover um and afterwards um she went to eth to zich to Conrad chinda in the domain of photogrammetry and uh followed by a t track position at T Munich where she's still Affiliated to too uh but in parallel she's also working in fena in Florence for NVIDIA and she's well known for works on multi-op tracking autonomous driving uh also more towards 3D tracking generative models and um she received several Awards over the last years and I think the one of the most recent and important one uh was thec starting Grant in 2022 one of the most competitive Grant themes we have in Europe and yeah it's a pleasure to have you here we're looking forward to your presentation yeah thank you yeah thanks for the very kind introduction uh so I'm really super excited to be here uh and to talk about a project that is deep in the heart of our research team at Nvidia um so I'm going to talk about open world segmentation and tracking and um for this Workshop I decided to focus a lot on lighter representations and Distilling information from 2D to the lighter sensor um so let's start a little bit with the motivation and the research that we're doing in the team um so we're working towards embodied autonomous agents right we're interested in autonomous driving Robotics and for this like there are several um several places where perception could be very helpful uh for example providing the ability to the autonomous agent to understand what is around them to understand the the surroundings to understand the Dynamics so how do uh agents around the autonomous agents actually move can I predict their trajectories uh and also essentially to predict where the autonomous agent is right so for localization so for this talk I'm going to focus more uh on understanding what is around um the the autonomous agent and how do these agents actually move so this is essentially a task that we can maybe call globally Dynamics in understanding and I'm going to start with a little bit the reasoning on you know different task that Dynamics in understanding and Compasses and also different levels of granularity in terms of how many classes um can we actually handle for each of the tasks so let's start with the basic semantic segmentation right so this is kind of one of the first tasks in which you want to assign a semantic class to each pixel in your image and this now is something that we know how to do definitely for a handful of classes but even for thousands of classes right so we do have data sets that contain this information we can train our Network in a supervised way and this works really really well that we have seen um in the past years um a little bit harder task is panoptic segmentation in which we not only want to find out the semantic class of the pixel but we actually want to differentiate between different instances within a semantic class and this is also a task that has been relatively well solved for you know a handful of classes or even thousands of classes then we start getting into uh the realm of temporal uh the temporal domain so adding uh this component the temporal component in which we have to now do multiple object tracking multiple object segmentation um and again here for a handful of classes we do have data sets that are well annotated um but you know I I very well know that it's quite painful to annotate videos uh for multiobject tracking so this is not something that really scales for thousands of classes um but these are not really the task that I want to talk to you about today right I want to go one step further and I want to add another dimension and so here I'm not going to talk about 2D or even about 3D I'm going to talk about 4D panoptic segmentation right so so I'm really interested into going into this 3D domain and adding the temporal component on top and being being able to analyze this and to find out handful of classes thousands of classes but like honestly this talk is about going more into the direction of open world right so so we should not be limited um by the number of classes we should really go ahead and say can I actually label and segment anything um that I find with my sensor so um in terms of how how would we go about solving these problems right if we find find ourselves dealing with semanti panoptic segmentation for let's say up to thousand classes we'd be like sure you know let's get some data let's annotate it let's train it in a supervised way um if we want to do multiobject tracking um we would say sure let's get some more data but it starts to get annoying because now we need to annotate you know 100 very very similar frames and you do need that um annotation in a really fine grain way also temporally um so it starts to get really really tedious right but if we really want to solve 40 panoptic segmentation in the open world like this this is the realm where we actually want our methods to be working this is the real world this is not something where we can rely on data annotation alone right if we have to annotate um lighter scans in a temporally consistent way for you know open world like this something that is not scalable so in this talk I'm going to focus on a couple of works from the group on actually leveraging the data sets that we do have in the community leveraging all those labels and trying to generate a model that can sudo Lael my data for for the panoptic segmentation right so so this is um the goal let's say is you know how to obtain these Pudo labels uh in a meaningful way and in particular I'm going to focus on 3D lighter pseudo labels and I'm going to focus on a couple methodologies one is I want obtain pseudo labels but looking at the appearance of the objects and the other is by looking at the motion of the objects so the first work that I'm going to talk about is going to be uh the segment anything in lighter uh that essentially um aims at leveraging Vision Foundation models to do the task of panoptic segmentation in 3D and the second work is going to be based on um let's say the old motion clustering but in the lighter domain where we actually want to detect new objects even new object types by clustering uh point trajectories in the lighter space so let's start with the first work so what what do I mean to segment anything in 3D so we actually want to have a method um that you can prompt via text and you can say for example hey s can you segment trash can and fire hydrant so in this case the trash cans are going to be shown in blue and the one fire hydrant is here in Orange there's just one fire hydrant in this scene but we're um I'm going to run a demo later and we can we can have a little bit more fun with the model but essentially this is what we want to get to right so how do we make this happen so we uh proposed a method which is called the better col Sal uh which is basically a first step towards a lighter Foundation model and so here my goal is to have as input a lighter Point cloud and a text prom and my model should give me a set of instances plus semantics so this is the task of panoptic segmentation in the lighter domain and I will be I want to be able to segment and classify potentially any object right so so really targeting this this open worldness of the problem so of course like the question of you know can I actually manually annotate all the data that I need to perform this task is not really something that we want to pursue we rather say look you know there's such great models that are already working in 2D why not leveraging all of those how can I transfer all the knowledge from two division Foundation models into the lighter domain and the important thing is that I want to really transfer this knowledge right I don't want to depend on having an image at test time and having to run these Vision Foundation models on the image at test time so at test time I just want to run everything on my lighter Point Cloud so this is an overview of of the uh two of the three ingredients of cell one is the sudo labeling and the other is the actual model so we have um as input and unlabel set of camera and lighter Point clouds and these are nicely calibrated and this is going to become important later on then we have what we call the pseudo label engine so this is where we actually transfer all the information from the 2D Foundation models to 3D labels labels directly in the lighter domain and then we're going to distill this information by training a model on the sud labels and this model is going to be able to do zero shot segmentation via text prompting okay so how how does this model actually obtain instances and semantics so just a brief overview of uh the architecture of the model we have you know your favorite backbone for for L Point clouds and then we have this you know by now kind of standard architecture um your object queries are going to give masks as output with some objecs value and this is going to directly give you the instances and in order to obtain the semantics we're going to rely on another Foundation model which is going to be clip so you're going to basically have clip features that are going to analyze text Proms and then the clip tokens coming from the lighter Point clouds and this is what is going to give you semantics to your lighter Point clouds so with this architecture we can do what we call zero shot classification in the sense that we can take you know any free form text and and get the semantics out now the question is okay how do we actually train such a segment anything model because here it seems like we do need to have a lot of BR truth in the lighter domain and this is something that I specifically said we wanted to avoid and this is where I think the the interesting part of the method comes in this is the pseudo label engine uh so here I'm going to show is kind of a video depicting how the Pudo labeling works so we have our camera rig at the top and we have the lighter Point Cloud this two need to be calibrated because what we're going to do is we're going to unpro the Sam M so again leveraging a vision Foundation model we're going to unpro them to 3D and this is going to create our instances and then we're going to take clip we're going to extract the embeddings that are coming from clip from the images and these are the ones that we will attach basically to the uh to the instance mask in the lighter space so um one of the things that that happen on some data set is that we don't really have 360 uh coverage of the cameras for the entire Point Cloud so as you can see here like part of the point cloud is really not covered so we cannot obtain any pseudo labels for that part um and we do have a way of creating some sort of Frankenstein training Point Cloud um so I don't want to give too much details here um but this is something that we actually uh found a problem like you cannot really train on this partially uh pseudo labeled Point cloud and we have like ways of dealing um of dealing with this in the paper now um i' I've just talked or let's say I've just uh told you that we're going to un project 2D mask into 3D and this seems like an easy task but we actually put quite a lot of effort into making this really work right so if you just take the S mask and unpro them to 3D you're going to have all kinds of bleeding effects you're going to have all kinds of errors uh because the 2D and the 3D don't really completely match right um so what we actually proposed was to kind of combine um the object notion of the 2D domain of the samas with 3D clusters that we can obtain with DB scan directly on the lighter domain right so so there's an overlap between these two clusters the Sam clusters and the dbcan Clusters and this overlap is what we're really targeting right um so again there's there's t of analysis in the paper uh but this is just to say that this is not a simple un projection like there needs to be somewh there uh to make it to make it happen so okay once we obtain these pseudo labels um you know we could just stop here we could say here you know this is my output the Pudo labels are my output but this would have two problems right one is that you would rely on the images a test time right and the second one is that your results are still going to contain noise even if we like put a lot of effort on DB scan and and whatnot your results are still going to contain noise so what we propose is to actually train a model on these pseudo labels and here I'm going to show why this is important right so um here we have like some baselines like some 2D baselines of Sam Sam plus DB scan several strategies on how to combine these two sources of information um but here I want you to focus on two lines so in the orange row we're going to show the results on the pseudo labels that we actually use for training our model cell um so this is going to give us this PQ of 48.7 which is not really great because as soon as we actually train our model on the pseudo labels this is the green row we obtain much better results right like 70.7 um so this is the difference between simply running the Pudo labels and actually training a model on top that is able to filter out the noise and really learn much more from these sud labels uh and the other result that I want to point to is um the fact that we have at the top the the first three um or let's say one two three four five rows are with a semantic Oracle which means that we focus only on the segmentation problem and we take semantics from ground truth um while the bottom four rows are with some semantics that com in from clip features so the real system that we would actually deploy and you can see here again comparing the orange line with the green one that we obtain like 70.7 with the semantic Oracle which is very very close as the result that we get when we actually train on ground truth semantics um basically panoptic segmentation which gives us a PQ of 78.1 U sorry 71.8 so results are really really close right while if we actually use the semantics that are coming from clip the result goes down to 27.6 so basically what this means is that we're doing well with the segmentation problem and where we're having the real issue here is in semantics right so Pudo labels work really well for the segmentation problem for semantics not quite you know we we're not quite there yet and this is essentially mimicking exactly what is happening also in the image domain right so so we're having exactly the same problems here okay so here I'm just showing um a video of results this is not at all temporally consistent this is just you know taking frame by frame and showing all the semantic classes um that we can find here so you can see a variety of classes that we can work with like their scars the traffic signs people um so on and so forth uh but I think the most fun is if I actually show you the demo and we can you know play around a little bit with it so let's see if this is running okay so um here's a demo that we built um the images are only for visualization purposes so remember that at input we have only the lighter Point clot and our method is only working on the lighter Point CL um so let's see I'm going to try to find our favorite object which is a fire hydrant and you can see there a little bit tiny but you can see one up here and you can see one down here and maybe even more in the images you can see like the fire hydrant being correctly segmented and also the second one here um we can then do you know your most like common objects like cars Road um we can try also like storefront and you know more like weird type of objects trees so on and so forth um we can also go for like maybe more like different types of vehicles like for example we have this kind of um vehicle that is not really seen often but that we can uh still segment and we can still obtain the the semantics from the clip features we have also for example like small objects like post box so this you can actually see here or I don't know trash can which you can see here um and the cool thing is that of course like we're relying on lighter right so if we go into for example night sequences where visually it's super hard to see what is happening and it and it's super hard to really detect these objects we can still do like a really good job in the lighter domain right so cars are not really a problem um I don't know what else we can try like curb I don't know if anyone has prompts for me to just try traffic signs traffic signs so there is one here one on the back they're not really visible in the image so that's that's a problem because I mean again like the coverage is not 360 but I think other sequences maybe have better traffic signs or more visible traffic signs let's see yeah here you start getting more of these see on the back there's a stop sign sign and all these Bows Oh yeah here you can see it at the bottom this stop sign here any more ideas Cy I don't know if we have any cyclist here do you see any cyclist okay okay we don't have any cyes here estan oh yeah that happens yeah the The Prompt engineering on this thing is crazy so yeah for example person Works a little bit better so we have for example here on the left like two persons being detected sorry mural with it since that's only mural oh but I mean yeah I don't think the lighter is so smart I don't know if it even understands this word so I was actually surprised when you type like storefront that you can actually get this um yeah so so it works really well it's true that the the whole prompt engineering on this thing is a little bit crazy but not surprisingly um the cool thing is that this is just going to e e e e e J e e e e e okay do you think they can hear me yeah they should okay okay sorry about that so um yeah essentially we we have uh or I presented now a method um in that basically get data in an easy very scalable way right so we don't any annotations uh on the lier domain so we can really scale this up nicely just get more data uh Soo label and retrain the more right so so this is a very nice property of this model that it skills really well um but there's of course like some instances that you're not going to recognize right and the question that we have next is if there is anything else we could do and so we thought okay you know from all the objects that we might be interested in the moving ones are the most critical ones right for for Aon driving so why not actually find a solution for those specific objects um so this is where we um I move to the second work which is essentially obtaining sud the labels by looking at the motion right what we call uh what moves together belongs together um so the idea is that we're going to have a very small set of label Byer streams so here we're not going for zero labels um and we're not relying on Vision Foundation models but we do rely on a small set of labeled data and lots and lots and lots of unlabeled uh lighter stream and what we want to do is we want to train an object detector right we want to train a detector for example for cars and the question is how can I train this detector and still leverage the unlabored liary streams um so we actually propose a method um to generate pseudo labels based on motion based on motion cues right so we want to be able to analyze lighter trajectories and we want to say you know what the trajectories that are moving in this in the same coherent motion are going to represent an object and then I'm going to extract that object I'm going to place a bounding box on it and then I can train my model with these twoo labels so let's go step by step into the three parts so in the first part we have the pre-processing of the lighter Point cloud and the trajectory generation so we don't want to work with static Point clouds but we actually need Point cloud trajectories in order to make sense of the motion of those objects right so this is the first step then we're going to have the clustering this is um you know our our main um technical contribution of the paper um which is going to Cluster these trajectories and once you have these clusters you can then say well I'm going to put a bonding box on top of each cluster which is going to be the s that I'm going to train my detector on um now this is this is just the pseudo label generation and then we have the Second Step Right which is the object detector training so here we just you know apply our method to all the unlabeled U data stream set that I talked about before we take those sudo labels and we train our favorite of the Sha detector so in our experiments we use Point pillars for of course like this is you can take your favorite detector and just train it on the Pudo labels um so let me focus a little bit on the on the technical part that is interesting here I'm not going to go too much in detail but Jen is going to present this paper at CPR on Thursday yes so go bother her and you know don't ask me the questions um but yeah like overall what we want to do is we want to propose to segment moving objects with graph networks and I'm going to explain later why this is the way to go it's it's of course based on it's a learning based method right um so that's why we need this tiny set of later liary streams but the advantage is going to be that this is going to generalize much better than if you use you know any other clustering method like for example DB scan so what we actually propose to do is we say um you know we're going to create the graph where the nodes are going to represent the point trajectories that I was was talking about before right these lighter point trajectories so you can imagine for example you have a car you have about 20 lighter point trajectories and each of them is represented by a node and the edges are going to connect the trajectories that might belong to the same object right so you're going to have edges connected all these nodes and essentially what you want to figure out is the connectivity of this RW so for this we're going to solve it uh with a message passing Network in which essentially what what is happening under the hood is that you have this message St uh message passing steps in which nodes and edges communicate they basically share features so that you know each note is aware of what is happening in the surroundings and you can imagine that this is basically what gives the note a really good notion of how nearby points are actually moving and can then make a decision to be connected to those nodes and form an object or not so the third step is essentially going to be our loss is essentially going to be the goal of classifying these edges into active or not so an active Edge means that the two point trajectories belong to the same object an inactive Ed means that they belong to different objects then we're going to have some you know correlation FL string to make sure that we have all the constraints respected and we're going to extract bounding boxes from all these cluster of point trajectories and these are going to be done our sud labels so okay let's see some of the main results just to get you know an idea of what is happening here um so first of all I want you to focus on the Gap that we have between the purple line This is essentially a method train on ground through detections and the green line which is our method so if we look at the column the the one before the last one the AP uh we see that we still have quite a large G right around 57 compared to 89 when trained with ground truth but it's already much much better than prior state-ofthe-art which was essentially a similar method uh but using DB scan as a clustering method um and the main bad thing about that method is that I mean if you have played around with Tv scan you know it's super super sensitive to the parameters that you actually use and so it really doesn't generalize um to different sequences so for each sequence you really have to find the optimal set of parameters for clustering well we wanted to do a bit more of an extreme generalization we wanted to say can we actually train our GNN for example on wayo and then test it on let's say AR tool completely different sensor what happens right like does our GNN really generalize and you have to think that the genn doesn't really care about detecting objects just cares about putting Point trajectories together or separate and so this is a task that actually generalizes really well and we found out that it doesn't not only work on Argo 2 but it also is able to detect objects that it has never seen before like for example here we have a couple of um of examples on trucks and buses and these can still be detected even though you never saw them at train time so here are some of the results that we have for cars on wayo um and again like these results don't really need to be perfect right I mean these are Pudo labels that you then train your model on so again a similar reasoning as what we did for Sal if we have some noise on the pseudo labels the model is going to take care of it and here we have results on ARG veru again the model has never seen this data at training time and it actually generalizes quite well okay so um I just have like you know two or three take home messages from uh from the talk one is that sudo labeling is really really a powerful tool and especially when you start to leverage strong 2D Foundation models and so what we tried to do there is take these strong capabilities and bring them directly to 3D without having to collect um the same amount of data but in 3D um there's geometric and 3d motion F that still need to be very much explored so this was kind of a first exploration that we did but there's so much more to do there and so and finally uh maybe kind of a um hopefully an inspiration uh for you to actually work on um 3D task which are actually relying on 3D label data um and to really work in this direction I think it's going to open up tons of possibilities so with this sorry for the technical problems but if you have any questions I'll be happy to answer them yes thanks for the SP we have time for questions or remarks thank you I have a question about the first project um you presented so um I I might miss something here so you said uh with the semantic Oracle you are able to get 70 something and if it's just clip uh it drops you 20 not then for the demo like I'm wondering which which model that oh no the demo is with clip right the demo is is the real yeah then uh so that one is the one with a 20 uh something people yeah exactly so so that that works pretty pretty nice to me so so I guess uh that should be like I guess there's no no need for additional training I me I mean I think there it depends right so it depends there's still quite a lot of noise in the output right there's still quite a lot of dependency on the text prompt so it's not so clearcut when you actually you know compare with an annotated data set with a close set vocabulary right um we also have an analysis I didn't show it here with Su let's say what we call Super classes and there you see that results are much better right so so this really means that it depends on the actual prompting that you use right well I I guess depends on you know this constraint domain like for example if you or clip if you car versus cars versus Vehicles like sometime you'll get very different EXA output so so I guess it makes sense to you know go fix it to Super exactly yeah thank you yeah I would have also a question for the first work so you've shown that it works really well on night and evening very score ones but there's certain things where I as human would have strong bias to textual information for example zbar Crossings on the road yeah do you think your model is able to handle this at all or do you need multi model information no I mean of course like the like this was a first goal of saying okay how far can we get with ler the final goal is to have a multimodel multimodel model yeah no that's clear right I mean I mean zebra Crossings or or or like mural for example this example like like there is no way the lighter just doesn't have that information right so this is where an image would come in and provide much more information there is also the problem of the range in which the lighter sees right it doesn't see too far away in depth so there you can only rely on the image right so there's multiple um motivations for using multimodality for sure yeah other questions I have a question um in my experience uh when you train with pseudo labels the model is actually overfitting a lot that noise but this doesn't seem to be the case here yeah do have an Insight I think it's because the noise is not always the same right so so when there's really noise that has a pattern then you're going to overit to it um here our experience was that the noise was kind of random and then you don't have this problem right the model is able to filter it out but this of course we only found out after we did the experiments right so it's not so easy to so there's no like trick to avoid noise um noise overfitting here it's just a plain uh yeah plain training yeah standard losses okay because there are tricks you can use and I was wondering if you put into place anything but just just okay yeah yeah yeah I guess we're not as familiar with like the situations that self-driving cars could be um put themselves into liar Etc you think this is something that will be uh applied out of the box in the future or do you think it's a foundation you said you're trying to get as far with only lar like what should we use from this going forward can it be used out of the box or is a foundation for further work on some um so so I mean you can use it in two ways right like let's say Foundation model for lighter can mean many things but one of the things that it could mean is that even if you don't use lighter at test time in your car right and you only use images um you still want to train the models on the images with very very accurate information right and sometimes you want to have this information in 3D right um so so for example we're using a lot this model for auto labeling um which we then don't use to train any more on lighter but we used to train it on image right and then we do kind of the opposite like transferring the labels from 3D to 2D um so that's that's one way in which you could use this model even if you don't have lighter at P time I guess there's a question online and then we can move to the next session and the question is what frame is used as a reference to calculate movements and trajectory oh it's just like the first one and then we take like 25 frames into the future okay so you take the frame of reference of the first frame correct and then move on okay okay all right fin question maybe question from an old man so when I seeing your talk I was thinking about tesic words on motion segmentation Mark of random Fields so what is why is your model more powerful or what are the similarities and dissimilarities I think I think it's the classic machine learning versus the old old stuff um the model is is much more General right so the fact that you can actually train on wayo and just apply it on algo and it works um this is this is pretty powerful and I think like all for all the other method you would just need to Fe the hyper parameters and this is not just feasible in practice so so that's the main strength of I would say most machine learning this all right and that's thank the speaker again take okay good yeah okay so uh we move to the last U paper session of this workshop and is visional language to so yeah if the next speaker paper five can come here first is online yeah oh he's online right okay so first Speaker can um and share his screen and yeah okay hello can you see my screen yes okay uh so uh hello everyone uh I am Ali D from schula carneg melon University and today I'm presenting our work uh titled uh leveraging generative language models for weekly supervised uh sentence comp component analysis in vo language joint learning so here we will give an overview of uh the targeted tasks in this uh paper in our work we have tried to tackle some common difficulties of traditional voo language models um in general we observe that when we extract representations from text uh information the model doesn't attend enough to each component in the sentence for example you can assume that we have like majority of the sentences they have a subject they have a verb they have an adverb adjective things like that we notice that these during the training of the vision language models the uh representations extracted from the text doesn't convey of information uh for each uh component of this sentence so the task that we are focused here are vo and text jointly given as inputs for the Target task here we focused on vdo moment retrieval uh so voo moment retrieval the task is to locating a specific segments in vdo that match uh with the text input we also consider vdo text retrieval uh which involves finding relevant text based on the input video and so on uh so these are some some of the common task that we uh basically evaluated in this work however the general idea can be applied to almost any uh type of video text uh tasks so the challenges here is that uh first of all conventional models fail to attend to every part of the sentence properly in the example shown here on the right for the video moment retrieval task it fails to emphasize the word shot and ultimately fails to retrieve the correct moment uh correct moments from the vo similarly in text to vo retrieval the conventional approach fails because it cannot attend again to a specific part of the sentence which are tied to the video the reason behind these the reason behind these cases can be rooted back to the presence of noisy text labels in web craw data sets which are used to train these uh feature extractors so to alleviate this problem uh a few papers suggested using week labels alongside the original ones while training for a downstream task and V labels here basically shows the role of each part of the sentence for example given a sentence a cat gets a shot we can label the sentence by by explicitly specifying different components of the uh sentence and then use those information to train a model which better attends to the adjective the the subject the verb in the uh visual task however collecting these uh weak labels is very time consuming and could be uh very costly and conversome so here in this work uh we will talk about the uh contributions of uh here will talk about the contributions of our work which is to mitigate the challenges we have already talked about so our goal is to develop a model uh a mechanism that can be applied to any type of visual text type of learning and improve the attention of the model to different uh components of the sentence to address this to achieve these we first uh get a help of an llm as a generative model so we generate weak text labels by changing a specific part parts of the sentence for example here the input sentence is as a a cat gets a shot we consider that as our uh anchor text and by prompting llms we generate negative sentences which have some like negative and positive sentences which have modified uh components for example one negative case could be a dog gets a shot another U negative sample could be a cat gets a chocolate so all these sentence they are very similar to the original sentence or the anchor sentence however one the major one of the major components of the sentence is changed using the llm one important thing is that this sentence modification and generating the negative samples is completely a automated so by prompt engineering we achieve we we basically developed a couple of prompts that given input sentence can modif y uh some part of that sentence uh and generate additional or negative samples for training the model so then we propose a mechanism to utilize these generated samples into the training to improve the uh attendance of the model to different and specific parts of the sentence so at the end we also like as a contribution we we show qualitative and quantitative results that that this type of cheap uh sample augmentation for text Data can improve the performance of uh visual text based tasks such as moment retrieval and uh uh text retrieval so here is an overview of the solution that we proposed in the general uh video text uh models we usually have a text which is considered as uh which is paired with the input sample which could be a video or image or anything else these two things are passed to a model which is a Transformer based on recent uh uh developments and techniques and the model tries to combine these two uh samples together using a multimodal attention so that it can come up with a representation that connects the two modalities together in our case we propose to add additional pairs to this training so using llm we generate different sentences as I mentioned one sentence is a positive sample such as like the original sentence was a man is looking at a paper so the we can alter The Voice make it passive a paper is being looked at by a man to generate another um positive sample and simultaneously we generate negative samples we change the adjective we change the subject and we come up with sentences like this is looking at a dog a man is tearing a paper and then during the training of the model we use these uh generated samples to perform contrastive learning on the output of the multimodal attention so we use contrastive learning we maximize the mutual information between the positive pairs using the contrastive learning criteria so the model forces the positive pairs to be close to each other in the embedding and far from each other in the same embedding so one important Point here is that during this contrastive learning the model learns that when the subject of the sentence is changed the output of the multimodal attention should change as well so if we have a video of a man working with uh like looking at a paper it should be different than a man looking at a dog or at a specific object or a different animal so this is the main idea behind this word um another contribution of the work is that instead of using simple contrastive loss in our approach we have proposed a Noel adaptive contrastive loss to achieve this we have decomposed the simple contrastive loss for a sample into K contrastive losses each corresponding to a specific negative so during our evaluation we notice that not all uh changing uh not all parts of the sentence results in the same uh basically negative samples for training So based on the importance of each component in the sentence we come up with a weight that wees rates the contrastive uh uh training objective in order to obtain a better performance in training additionally we have introduced an important uh estimation module which I discussed uh for each negative sample a nice thing about this importance estimation which is like used in the training is that during the test Time by generating the negative uh text uh queries we can get some insight about insight about the importance of each sentence component in the input video for example by changing the subject we can compare that with the original sentence and by getting the attention weights we will know which component of the sentence is more important with respect to the video here is the attention mechanism for for the work um here is the solution overview for the model um the L function that we developed to basically uh have two different types of contrastive learning here are some qualitative results of the model so the the uh task here is moment retrieval and we observe that in the ground through like after training the model the model misses some important information regarding the object in the sentence but after training the model our model like after using the augmented sample negative text samples our model uh uh is enable to basically ATT attend better to the specific components of the sentence in the uh input data uh here are also some quantitative results uh across different baselines we have uh we shown that uh our model achieved significant Improvement and uh even simple contrastive uh uh loss applied on the negative samples augmented in our approach can result in Improvement of the performance in moment retrieval and video retrieval tasks thank you very much okay so we have time for a quick question from the audience any um right okay so I have a very quick question so maybe you can uh comment a little bit on on the uh okay looks like you you kind of rank the importance of the losses right based on the negative samples and this um and uh how yeah exactly so you're basically waiting right based on the importance estimation module the the the different losses coming from the different negatives so uh how how actually this incorporates the the idea that uh uh you you want hard negatives to be to be like uh sorry um yeah um actually um easy negatives to be uh more important in the training because usually when you you when you do contrastive training you would like uh to um encourage easy negatives to to come into place because those are those negatives which are more similar to the right right that's a very good question so all the weights in this uh method here for rting the negative samples they are being generated by an an additional attention mechanism and the objective for training that additional uh attention mechanism is to minimize the whole contrastive loss one important thing is that all the attention values they are normalized so they act as Pro values so the uh basically the the total probability of or total importance of the sentences or sentence components uh is fixed however the model learns during the training to assign higher importance to the sentent components components that uh eventually can reduce the contrastive loss and can result in uh like harder negative or easier negative samples for the training so all the these things are being done adaptively during the training having the main objective as the goal of the optimization okay thank you very much thank you and uh next speaker um we have paper number 18 uh for which I have a pre-recorded video to play but if the speaker is online I kindly ask you to step in yes I'm already Okay cool so you can share the screen and and start the presentation anytime yep I'm am I and visible myself yes okay so I am Uncle let's start our invers which stand for intent visual inut for briding the modities in report generation and so there are some key challenges mainly we Face three key challenges in this project uh first in medical imaging uh in pixel level details such as color for xray images basically is unnecessary but reg uh region intensity and other details are important so inverse employ this by using S man and Joint Ting architecture one minute sorry uh soive architecture to efficiently train the encoder in self Sur manner to obtain high level stic representation and second one is hard to align two different modalities in a common space uh in complex medical Visual and textual datab biases uh makes the STS also more challenging so Inver incorporate a lightweight Transformer which known as CM qfl layer which stands for cross model qu Fusion layer which utilize the output from a frozen encoder and find the most text grounded image Ting from this uh from this and Bri this modity Gap and third one is High compar resource to end to end training of a large scale models so by Sol to solve this we employ Two Step training approach in first stage we uh train the CM qfl layer and to enhance the uh visual representation and in this second stage we find our decod so coming to them inverse framework so mainly we have uh one target encoder which gives the image embedding and this is projected to the input of the uh required input of the CM qfl layer and from this cmfl lay we get the text ground image embedding and this embedding is given to the our decoder to generate the report so basically work of this CM qfl layer is reduce the burden of the uh decoder and it will also enhance some key features to uh generate better report so we'll talk each part of this model in further slide first uh this is our training of the our encoder which our train encoder is basically the architecture of IA which and after the training of that uh model we employ Target encoder uh for the further process of the model uh so in that model we for the training of this igp model we use uh ni data 14 class data set and this to train the cmq layer we employ three uh objective first one is multimodal contast learning so basically main objective of this objective function is align image Tex level using contrastive similarity of positive and negative pairs uh to calculate this objective function we calculate mainly image to text similarity and text to image similarity and compare the uh calate the loss function using the below equation so basically we evaluate similarity between each query output and the text class token and take the highest pair similarity as the image Tex similarity and calculate the whole LW or objective function using the P equation second object is multimodality matching basically it enhance the quality of pairs with hard negative mining it uses uh B self attention mask for queries and text this means uh all the query tokens attend both uh Tex token uh and the all the text token also both attend the query tokens and text token also and uh this querium meing captures the multimodal information uh and third objective function is mass language modeling this is the main uh one of the most important loss function objective function so basically this is the language modeling loss Mass language modeling loss for this we use cusual S poal M so it will in this case qu tokens ATT both qu tokens and tex token but all the text token data only the previous squ tokens and the previous text tokens only so after you using this three objective function we get uh some increasing the performance of the result and this is the qualitative result so model generated two model generated output and the uh some visualization of that models intermediate layer and the each word word plotting the atten and map of each word um this is the some quantitive result we for this model we use three data set one is mimic cxr second one is Indian h x image and third one is base data set and for seeing for the effect of cmq f we see that after selecting the correct encoder and the cmq we in we our performance of this model is continuously increasing and the objective same for the objective function uh when we have employ three loss function or objective function we significantly improve the result there are some conclusion firstly we present a Noel High pering report generation model to significantly Advance the alignment of text and corresponding visual features by employing two stage training approach uh mainly Focus the initial trend on the CM qfl layer and then finding our decoder and second one is models ability to generate reports without recording any additional annotation like uh some uh some uh bounding box or some external age requirement of the age and some criteria of the patient patient Health uh some smoking smoking or not this kind of annotation this is externally uh and third one is cmq Lay which is three objective function contributes create but High informative image embedding so this is the main uh one of the main thing and we have one limitation also so our models decoder is pretend on natural language uh which lacks of medical specific knowledge so we have to integrate a uhm which which diversely tend on the medical image data medical image and caption data set so it will help the model to generate better contextually relevant reports and these are some reference and we can we also in future work we also include some uh some Medical in include so that we could generate better uh reports and thank you is there any questions okay thank you um any question from the audience okay so I'm sorry we have to go quicker on the last presentation of the session um and thank you very much again for uh the presentation so good day everyone and thanks for every me uh so the title of this paper is um I an adversar approach for instruction generation in vision and language navigation uh my name is Loreno baral and this is a joint work with my Coos Niti raal Roberto Basi andita so yep so uh we are talking about Vision language navigation which um for those of you that might not be uh aware about this task uh it's basically the task of uh teaching an agent to navigate an environment which is usually an indoor environment um by following natural language instructions so basically what the agent is given is a 360 degree view of uh the current position that it has in the environment uh plus uh an instruction in natural language which basically uh asks the agent to uh to navigate the space in a given Manner and then the task that he has to perform is basically to follow the instructions and and navigate the uh the environment accordingly and the instructions might be something like take a right going to pass to the kitchen into the alleway rather than anything else that mentions objects that we have on the scene directions landmarks colors and and Etc uh and what's peculiar is that the agent doesn't know the environment in advance so it's basically spaed in the environment for the first time he hasn't seen it and still it has to tackle with it and try to follow the the instruction so what we do in this paper is uh well instead of developing another architecture for VN and trying to make it working better uh we take a slightly different approach and uh we don't um develop a new architecture so we take an existing one and instead we focus on the training data so of course uh annotating training data for such a t is kind of um expensive in the sense that you have to have path in your environment plus humans to annotate uh with with the instruction to provide the instruction and uh what we try to do is to annotate uh trajectories manually so obtaining trajectories given a 3D environment is kind of easy because you just have to sample a reasonably good trajectory in the environment and then what we do is to develop an algorithm which can label those trajectories automatically so without the need of a human being annotating the trajectory so that's the the model that we have uh it's basically an a progressive decoder Model A GPT like model basically it's actually a fine-tuning of gpt2 so it's a model which has already uh been trained on language it reasonably knows the how the word is uh is done and how it works H and what we do is to extend it to take visual inputs and to uh predict uh instructions for for trajectories so basically the input of the model is going to be a sequence of images uh encoding the path that the agent is going to take uh which are encoded via visual encoder and then inserted as the input to this uh to this to this language model in addition we give it an awareness of the object that on the scene so we are an object detect on each of those scenes and then uh place them as an additional input to the model and then the model is trained to be conditioned on this and to generate the corresponding instructor uh accordingly what we also do is to add position embedding segment embeddings taking somehow inspiration from bir uh to distinguish whether the information is visual or textual and give the mod better awareness of this and in addition to this actually instead of training the modeling the classical outgive way so with a centropy loss uh we do something a little bit more sophisticated because we basically train this model in a gun like manner in which you have a generator which is the model that I just described and you also have a discriminator which is in this case again a language model which uh takes the scene takes the generated instruction and tries to uh tell whether it's fake or real so whether it has been generated or whether it is actually an instruction from the um from annotated by a human so from from the training side so you train this again in a gun-like manner with the two with the with yeah with the two classical losses for for for any gun uh and then at the end you obtain a generator which has seen uh how to uh to annotate those sequences and in principle can give us uh new training data to train uh our models so of course the uh the question then is uh will it work and will it answer the uh the navigation capabilities of the model if we if we add data like this and to answer we basically emplo three data sets uh so this is uh room to room Ry which are both already annotated so we we use them basically for training the generator and then we have this hmtd data set which is very large uh and made of trajectories which however are unlabeled so basically what we do is to take those trajectories run our model on them uh generate the instructions and then train a navigator on the generated instructions uh and if this works it means that the approach makes sense so uh a few uh few numbers on the quality of the instructions that we can generate with such a model uh on the top part you see uh basically we evaluated the generated instructions with the classical image description metrix so for those of you that are familiar with captioning blow metal Rouge C and spy so all the metcs are basically going to tell you how much my generated instruction resembles the instructions that you have in the uh in the training set so which are annotated by a human and uh we compare basically without the gun without the detections and we see that both of those things can actually increase the quality of the instructions and also we compare and measure the diversity of the generated instruction so okay we are capable of generating instruction which look reasonable but how diverse they are are we always generating the same thing or not and uh with the other side training actually we observe that this diversity increases which is actually a good thing because it means that uh given different trajectories I can actually tell different things which is which is a good thing and these are some of the of the samples so you can generate instructions like walk into the bathroom with the sink and to turn to know on the fa or go to the office on level one and two the T So given this SE of images basically are capable of generating those instruction which more or less look reasonable and potentially they might be employed again as training data uh the last thing that we uh that we try to do is actually the final the final proof so train a navigator on this data and see what happens um so what we do is to take doet which is kind of a very famous approach for for VN and instead of training on the regular data set we also add our generated date to the training set and and see what happens and what happens is basically that most of the metrics actually improve with respect to the original duet and especially the success rates this s column which basically tells you how many times have I been successful in reaching Target destination improves and especially with a lower um length of the path that the agent has taken which which which again is positive because it means that the agent uh can can go to the object in a more straightforward way uh this was on the every data set we also did the same on the roomto room data set which are some somehow different descriptions and more or less the considerations are the same in the sense that again you improve the success rate and and also you reduce the length of the of the path that you have taken so that's it more or less so in conclusion uh yeah we we developed a model to generate synthetic instructions for bln uh and demonstrated that yes if you train on those generated instructions actually you can increase your success rates and decrease the length of the of the path that the model takes which is actually good things because it means that generating synthetic instruction is a reasonable way of of increasing the qu um so that's it of course if there are any questions happy to take it and thank you the problem of you know model they have hard time to understand that Val and stuff but if you if you ask for example go left work uh so yeah well this is um I mean it's not really a foundation model in the sense that uh we start from a gpt2 which is I would say language model more than a foundation model um but yeah I mean in this case you have the the sequence of the images so actually distinguishing whether it's turning left or right is I would say easier uh even though uh I must mention that the uh encoding of each of those images is actually probably an open problem still uh what we have done here is to use clip features uh yes they are not super good in matching different things from different views so that's that's an open issue and yeah probably one of the future possible works you have better of those images which actually improve the thing that you yeah thanks again thank see you so that concludes the last oral session and we have now our last you want to connect I had one I need to share screen you have some you can simply share the screen and speak through my microphone and without connecting the HDMI just con just share the screen all right so e so Paris and uh she um she served as a another chair to all visual conference now she is a program chair for ECC 20 Spanish 4 in Milan so participate and um what else I mean uh apart from what we are going to see today she was she's working in many many interesting Fields like uh text to 3D and also um sign language to to text so many many interesting stuff so um give you the floor and thank you thank you well uh so happy to be here um the last I guess session and also with the jet lag so bear with me um so I'll be talking about a Trilogy so like three series of three Works uh we have have uh recently worked on getting audio description in movies so I'll just Define what that is in a minute but before that a little advertisement uh of our group in Paris uh where we like to take group pictures um so if you happen to be visiting uh the city like just drop by give a talk um yeah so it's um active a lot of people in at cvpr now uh in computer vision with that so yeah thanks already introducing some of my research here are a few examples of different things I'm working on so they seem a little different but to me they are kind of the same tools uh similar architectures these days vision and language uh problems uh but more uh to itemize the tasks I gather them into like text to video retrieval for generic videos but specifically for uh sign language videos to text this is like one source of video where time actually matters compared to some other tasks which is not necessarily the case and what I'm going to talk about today is like specific type of videos as movies but that's kind of a nice test bet for testing uh long-term understanding high level understanding um so that's why um yeah so it's it's important not just as an application but also as a intellectually as for research and yeah the final one is text 3D human motion generation uh which is also getting a lot of uh momentum in the field like there were I think four workshops at cbpr and I couldn't get in because there was a que but still uh it's good to see more and more people working on this so let's see if um my not here can hide this I don't okay let's see if audio works but I want you to First listen to a movie um without seeing the pixels so audio description is um narration in movies that help visually impaired follow uh and enjoy the movies so let's see just the movie alone and hope for the best H where is this connected to Jonah hey Jonah hey Jonah he Dad this is Jessica well it's nice to meet you Jessica Dad this is amazing if you play this backwards says Paul's Dead uh yeah yeah I I know how do you know goes on and on um but yeah so you don't get to to understand what's happening in the movie if you just listen to this uh audio right so it's normal I mean I don't know if you hear it from the back but uh let's hear it once more with audio descriptions in in between so just also will help uh Define what it is Jonah he goes upstairs and into Jonah's room hey Jonah hey Jonah an egg-shaped chair turns around Jonah's inside with a girl he Dad this is Jessica he has a portable record player well it's nice to meet you Jessica Dad this is amazing if you play this backwards says Paul's Dead all right so let's now see the pixels that correspond to this hopefully uh you can hear it's it's a movie from Sleepless in Seattle that I Wasing the SLS last night he goes upstairs and into Jonah's room hey Jonah hey Jonah an egg-shaped chair turns around Jonah's inside with a girl hey Dad this is Jessica he has a portable record player well it's nice to meet you Jessica Dad this is amazing if you play this backwards say Paul's Dead all right that was the last time uh so I guess you get the idea I mean unless you were familiar already with what audio description is um it's a little different than captioning or dance captioning in videos uh because um there there is a goal like we we want to describe things that are important for the story uh there are guidelines on how to record audio generation so it should describe changes uh things that matter for the story and um emotions of the characters so it's it's very important to uh yeah so ground it to the characters um so it's not like the like the number of chairs or the colors of some jacket so it's not like in captioning where you want to describe every single thing in the in the scene but uh things that are important in in the story uh um yeah so our objective is giving a video of a movie ideally as a function and output this in the form of text so for now even if I show some audio we are just outputting the text and then rely on text to speech that's why it it sounds like U the airplane announcements but there is some room for research on how to generate the audio the sound uh in a in an engaging way also um yeah so that's what I said said already and obviously there is a societal impact societal impact on uh making more uh more movies accessible um to visually impaired all right in terms of kind of the input and output formulation we we have video frames as input and I'm showing like couple frames but in practice it's it can go to one hour video so it's not like a typical uh fewc uh input uh task and then we have a visual and language model that can also have multimodel input it can have the subtitle the speech um as input the context so ad is a description the ad from the previous frames uh so all this information we want to integrate into a single model that generates the caption so before I go to this Trilogy of Auto ad1 123 I want to give a little background on like Vision language models I'm sure uh if you're here you know quite a bit of it already um but basically these days what we are mostly doing is um playing like Lego bridging pre-trained Visual encoders and pre-trained language models to find a mapping between them um to to to tune to our task and the language model it can be GPT it can be fine-tune frozen visual encoder can be anything convolutional viit pre-train frozen so these are like kind of decisions we make uh um and in terms of the bridge it can be learning prompting so the visual encoder is trying to generate prompt tokens in the space of language uh tokens or it could also have more interaction as I'll will show a bit in a second uh can have cross attention architecture so the first one prompt tuning is yeah so having this mapping Network on the left uh to convert image into uh text space so some popular example of examples of this is clip cap or blip 2 for those who are familiar and then it's kind of gpt2 as if the previous text was this image and then learn to generate a caption of it and then there's the number two uh version which is the cross attention a bit like Flamingo style where you have more interaction between uh the the vision encoder and the the text decoder um where there's this cross uh an gated cross attention so that's a kind of in ize um by by one and then over training it it decides what to pass um to fine tune and yeah I'll show some results that uh if if you have access to a pre-train model of type two then it gets uh better performance than uh just flat promp tuning but it's it's a matter of what what kind of pre-train model you have access to because we we don't have the resources to to train these models typically so in terms of training choices you can have like discrete or soft tokens for the llm um yeah so it can be this mapping Network can be a simple MLP linear layer it can be a Transformer on its own um and you can train end to endend everything or free some part of it and in terms of uh the models that we pre-train on typically they are image uh encoders and we need to work with video inputs and what we do typically so like just between this and this is means there are instead of just a few patches per image there is a number of patches per frame and then you just stack them all together so you have the frame features but um what is typically done is to have some learnable vectors some queries at the beginning that will kind of resample whatever the size of the video is it will become a fixed size Vector at the end of the the training which will the visual tokens will will go into as input to to GPT Okay so that was kind of the background um and let me let me describe what we did initially as a first step um to solve this task at the time there was yeah a gpt2 I think uh for this work and a clip for visual features and we were looking at how to combine different M multimodel data into a single model and the first one is yeah a series of clip features and just to say that uh the data we work with for mov movies it's typically copyrighted right so you cannot just have access to pixels what is done um for some data sets is to release clip features or pre-extracted features and then you build on top of that uh so that's one of the reasons uh for this work we've we didn't even AC have access to pixels I I'll show in the later work um that we have also some efforts to get pixel data um the second one is subtitle context so just like what we've uh heard at the beginning there is a speech in between uh audio description so basically you need to hear um you can only hear one thing at the time so there's either speech or audio description uh in in between dialogues there's the narration that comes in but whatever is outside the audio description is the subtitle which is in the form of text uh so it's nice for us because we don't have to have a mapping Network um the same for the ad context by ad context I mean the previous uh let's say I have like a series of ads and uh the previous generations are give some context about what's happening ideally it should also learn how to not repeat the same thing over and over again if it says like har looks it should it the next time it should have a core reference like he looks unless the word a subject changes so in general like giving some of the context um should should help and yeah the last bit is PPT which is uh Frozen so the mapping Network that Maps the series of clip features into um visual tokens is as I said a series of frame features together with like fixed size learnable vectors and then the ad context is a number of previous captions not just the last one but we take like the last five we do some appliation that five is a good number um and to differentiate different modalities we have separation tokens that are learnable so most of this uh yeah so it's a pretty straightforward architecture but the challenge here is how to get data for this so there's um to train such thing you need complete movie data set where you have the video features subtitles and AD and there's this mad uh movie audio description data set that came out in CPR 22 um where this is uh available but it has 500 movies uh that corresponds to 300,000 ad captions which is already a lot but we've seen that this is not enough um to fine-tune uh such large models so just to give you an idea of what is available and what how we can use them um there are lots of web videos out there that that are typically very short um they are from a different domain than uh movies but they have lots of like cap captions the pair data of videos and captions but they don't have ad they don't have uh subtitles necessarily then you have the movies um which are I mean in in theory a lot of them uh three hours of movies apparently produced every day but of course we don't have access to all of that but you don't have ad uh the narrations for each of these what you have uh for ad is like relatively small so let let me show you the actual picture numbers for this so MTH is the one that has complete it has the pixels not the pixels but the features so visual features uh caption like ad and the subtitle and we have a 300,000 captions for that corresponds to this so what we did in this work is to uh collect a text only data so just captions and subtitles from by scraping a web page called audio Vol uh that gives us 3 million captions corresponding to much more diverse set of movies 8,000 movies but we don't have pixels corresponding to those so that's partial data web vid is one of those um internet uh domain video text pairs uh so just Visual and caption but they don't have subtitles conceptual captions is image and text but no subtitles doesn't have the video so how can we combine all these data sets uh to train uh these large models so yeah the solution was to pre-train with partial data and then find do a final fine-tuning pass with with the complete data and uh yeah so just before I I I show some results let me give a bit more detail on this audio world data set the text only one because that's important source of uh label that we will reuse also in u the last work um that's basically volunteers who upload uh narrations for many movies online uh so it's not really necessarily professional uh audio but there's no and then people download the mp3s the audio and then plug it into their movies so there's no movie associated to them um so what we do is we download these uh audio files and process it to have like a text speech to text ASR um with whisper X which is a extension to whisper um that includes diarization so diarization is who speaks when which is quite important for us because the narrator is kind of the one of the speakers we want to pick out what what is the original movie dialogue and what is the narrator and we have a few theistic like the narrator should have a should never have um first person sentences like we kind of have a cluster of speakers and the biggest cluster and the narrator speaks a lot throughout the movie so that's a big cluster we know that and that's a cluster that uses third person a lot we know that so we kind of uh find it speaker ID that corresponds to narrator and then we have the time steps that correspond to the the narration so we get kind of AD automatically for 12,000 hours so that's pretty substantial amount of data except that we need to figure out which ones of these correspond to our movies um right so we train as I said partial with partial data video and text part using um internet capture uh data the ad context part um with text only data from this 12,000 hours of data and then we do a final fine tuning pass um with the Mad data which um which is much smaller so in terms of results here is kind of the trend the Baseline is a single image uh per ad so we assume that the ad segment the timing is known and if you only have one frame from this and generate the text cider score it's like pretty low if you add visual context so some frames around that uh you get much better and then ad context meaning the last five uh text Generations you get extremely a big boost and then um the the last two are kind of the effect of pre-training web with pre-training is not as effective as audio World pre-training perhaps because of the domain Gap one negative result that came out from this work but I'm sure that what it could be pushed uh maybe um REI Revisited again is the subtitles that did not help maybe there were too much information in the in the speech that was yeah the signal to noise ratio was too high um one thing we could do is to detect which of the subtitle elements are can be relevant and then just give those so kind of a filtering here are some qualitative results of Auto a one and I don't know if you know the Great Gatsby movie um so the prediction is a man and a woman dance in a circle so it's very caption video captioning like it's not really like mo movie audio description like um as opposed to the ground TRS which is that g it has the character names Gatsby reclines on cushions as Nick and Daisy dance in the ballroom no no no another example is from Harry Potter one of um and yeah his eyes widen it's it's not incorrect but it's a bit U missing let's say compared to the ground truth which is his mind fills with terrifying memories all right so from the first part uh We've developed a prom tuning like multimodel approach and we have kind of our first results for this new task uh but one big limitation is that it keeps it's very Anonymous someone or a man and a woman which uh yeah we know that is bit useless for uh following a movie and uh even yeah the text Generations tend to be really short and incomplete not very expressive so in the next part in Auto ad2 we have uh looked at this character uh referencing a bit more in detail and um yeah the question is whether we can give to GPT potential characters in the scene as an extra information so that it it generates uh the names as well and the way we do that is we have uh we want to um yeah provide their names not just the we don't want to just provide the character names in the movie but also the actor or actors names uh from real life in case clip features hopefully know something about some of the famous people already um so how to achieve this well we go to IMDb so the movie database on internet where it comes with uh for each movie a cast of uh pictures and their names and as a character name and an actor name um it's kind of an external knowledge base that we can assume is known when you like produce a movie um and then we basically do a similarity visual similarity between the IMDb image so this portrait on the left of George clly and um cosign similarity across all of the the frames of the movie and find the top five uh highest similar ones to say these are the exampl for this character the reason we do that is that if you use the original image where that the actor maybe ages over time or maybe has a different makeup for the movie there's a domain gap between how they look in these photos on imtp and and the movie so this is to reduce that domain Gap uh so we do that for all the characters to find a set of examplars for uh each character so we have let's say five characters for this movie uh what we do is we have a character recognition module that is flexible that can take in um any number of characters as as queries and um takes the video uh features as input and has a binary classification to say whether each of the characters is present or not in that particular scene so at test time you can plug in uh so so you can get the output of this to say okay the first two characters present and give that to GPT to say potentially present characters are and then it it solves uh the association with the with the with the caption um since we have this data to train with also yeah so in practice we have almost the same architecture as before so clip features in um and then in instead we have an extra prompt which says possible characters Jack play by George cloney the image feature Karen blah blah blah and um yeah so we we expect the output to find uh to to also use these words another change we've done as opposed to A1 was to switch from this promp tuning architecture to cross attention uh version as I uh showed in the beginning like Flamingo style and in terms of results um in a comparable setting we have seen a big Jump by using this architect picture so the cross attention GPT and in terms of qualitative results it's much more so for this little clip uh the ad segment uh it says snap points at Harry Harry's eyes close in horror as opposed to I think at the beginning I showed it was his eyes widen or something so it starts to make much more sense um another example is yeah one of my favorites again from Harry Potter so how are we going to get to London we fly of course it says like har rides on the yeah so it's not really a a horse for those who are familiar with Harry Potter but still it's a it's yeah much more expressive and surprising that it has all these character names right so it's not it's a bit cherry-picked it's not always always is getting the character names Association right with the with the actual verbs so it can mix up things um but yeah it's it's better and um in terms of the the limitations of Auto ad2 well we' we've got the character names uh in there but still the the what the what is happening in the scene the descriptions are not uh great so especially because we are using pre-trained clip features which might maybe for like um already lose some of the information um as opposed to using higher spatial resolution from each frame and I haven't talked much about this but I guess for people who we've talked about this captioning metrics for people who work on generating text and having a single ground proof that compares against these metrics like cider and blue they are not necessarily indicative of the performance there are like recent Trends to have llms uh as a judge so in this last auto3 part I will show some of the efforts of getting coming up with new metrics to to measure the progress so yeah improving the what so what is happening in the scene and U the main like one of the main differences here is the change of like going to a stronger vision and language model um so it's a like instead of clip and the the gpt2 we now switch to video Lama or um the the variance which use uh Lama as a language model in the first place um and um we have now A Q forer so for those familiar with like blip um it's it's um it's a visual encoder that has a higher uh res higher spatial resolution so instead of a single Vector for the entire frame it has multiple uh vectors so like to be precise 32 uh per frame and to apply such model which is pre-trained so it has already been trained on a video captioning data set so it has even the bridge Network so this Q forer is itself is a bridge Network between the encoder and the decoder is pre-trained already so you can use it off the shelf as a captioning which is our Baseline uh but except that it requires the pixels as inputs you cannot just have some clip features on top so we need the pixels for this one um and once we have the pixels let's assume we have what we do is we train only this little tiny projection to fine tunit um with the data we have and have substantial improvements so how to get this pixels data uh so in accv 22 um Max and like some of the colleagues they have released the condensed movies data set CMD where it's basically from YouTube where U the most important scenes from a movie is made public so there's no copyright issue but it's on the order of 10 scenes per movie like each about two minutes so like 20 minutes per movie instead of one and a half hours but at least they are like representative moments that are important for the story um and we have the pixels more importantly but we don't have ad for these we have just the video and the sound of it so remember we have uh the audio WT that is only sound and the CMD now has the pixels so Jah I'm home here's I'm home okay he puts his keys and briefcase down and it's a bit difficult to hear but they are actually not the exact same audio one is like from the the web page we craw the other one is from YouTube we have to basically uh solve the problem of audio alignment uh between the one that comes from audio Vault that has the ad in it and between the one that comes from CMD that has only the movie sound in it they are not perfectly aligned for number of reasons but um more importantly one has a one has doesn't so we do kind of an alignment um to get to yeah so this plot is a representative one that shows um the full movie on the x-axis that has maybe like two hours a audio and then the y- axis is the tiny um two-minute clip for which we're trying to find where it comes from from the entire movie then we can transfer the ad so uh they both have the audio in common and uh we can create another data set CMD ad we call it for those that are intersecting by transferring the ad back to the pixels so this way we've collected uh quite like drastically larger in terms of the diversity than the M that I showed earlier so 1,300 movies for training 500 hours of uh data even if they are like shorter Clips because they come from a larger number of movies uh they total to quite a lot of hours of data and a lot of uh 80 so this is made public I believe this is a released for at this conference going to be presented on Thursday um yeah so the materials are being released and the code also for you to play uh so that was the first source of pixels the second source of pixel is a little bit funny one um how to 100 million for those are familiar with comes from YouTube instructional videos where um there's speech so people are like showing how to perform some tasks and this speech has been used traditionally for uh self-supervised learning so learning a common space between Med and text and more recently in this paper how to caption they've shown that this speech which is first person uh can be turned into a caption like text using some llm so um it becomes I am poor ping becomes he he is pouring and then um we kind of assign fake names uh in this case it become John is pouring um and then we like pick a frame um that is kind of the character um profile picture and we create a fake uh how to ad um data set as a way of pre-training okay so once we have all this data so how to ad and CMD ad we can train this video uh Lama or um video blip to to get same as before getting the charact POS possible characters and this kind of stuff are the same except the the backbones are are uh more stronger and in terms of result on this previous mad eval test set which is comparable for auto ad1 and two with movie Lama which is uh fine-tuned on CMD and how to ad but has never seen mad training set we have already a substantial boost so that's that's uh one good outcome but this was on the good old cider metric and we started looking at things more qualitatively one advantage of having access to these pixels we can finally make qualitative results uh analysis um there you can you can see like the first row where you have um ground truth says the donkeys make a smoke and then a several predictions in this case dunkeys use smoke Dunkey children right uh where the cider score even if they are similar is uh decreasing over the the different examples that have maybe not the exact same wording but quite similar in meaning and the same thing for blue metor and rou that are um based on yeah te text matching and what we do in this work is to design a prompt that asks n llm um can you evaluate um the following movie audio description pair this is the correct one this is the predict one out of a score of five and then it can be GPT 3.5 we used or Lama so we found that they're correlated um and then it gives a score to say they're like out of five this one is correct as four and for this same examples I shown earlier we get 5 five44 that are also like correlated with the other metrics but much higher uh and more interpretable at least for me than this values of cider okay so that was metric number one metric number two is I I haven't told you but in the second work where we have character names we didn't really evaluate how well we are um referencing to the characters so here we are introducing critic core referencing in text for identifying characters it's basically just a kind of an intersection of Union of how many of the characters and the ground truth we we get right so for that there's a bit of trick to Cluster each character into different ways of uh referencing to them so it can be Jack Mr dson Jack daon maybe even like um the name of their profession so the doctor or the relation the father of so uh all these things refer to the same person so we just um have a pre-processing to to Cluster them into characters and then we just count how many times we get them right so this is results on the new test set we've introduced from CMD ad CMD ad eval on the on the left and mival on the right which is using the old metric as before so first thing to look at I don't know if my cursor is working is the first row video blip two and video Lama 2 these are two kind of pre-trained large models they don't perform well off the shelf so without fine tuning on movies so we need a ad data to F tune these things and then to ad one and two where we can compare against M eal I've shown already the plot before they perform worse even if they are trained on Mad training split so the the last two rows are movie blip 2 and movie Lama 2 basically the fine-tuning of the first two rows with our data uh CMD ad data and how to weet trining and we get a big boost um and the the two architectures are quite similar we just tried both uh to see if one is better than the other but yeah it seems with movile Lama to is slightly better and here are some final quality T results Greg closes his eyes and folds his hands oh dear God Greg's eyes widen Max looks at the candles dad couldn't tell the candles on the cake flickers and go out the car is covered in Smoke the car is lowered into a pit she runs down the steps and out onto the street he lands on his back and rolls over he turns to the crowd protesters run bunnies dance okay uh I don't know if you could hear them by the way from the back um it's all right yeah so I mean it's um it's getting better so this was one two three series of works that occupied us for a year or two um but there are lots of future work so it's a bit difficult to prioritize which one to do so we always like uh having a long brainstorming sessions and here are a few like a future itemized list of things one can look at so there's it's it's a good thing that there's plenty of uh open topics and and this research space where people have bit depressing modes that everything is sold it's not sold at all there is like little cerence between one ad to the next even if you have this context um so yeah it's it's also computationally difficult to scale things to one hour so we process things one at a time so one can look at as a postprocessing how to like clean this whole thing up uh to make it engaging coherent uh to listen to and to avoid hallucinations so honestly like when I look at some of these it's it's full of hallucinations partially because we're using some language model that only picks up at a few visual features and then fills up the the gaps um but there we can do like another round of grounding to check maybe question answering to check is this really the case do you do you see the horse here and um yeah one way to do this is having a memory across the movie because we have to save important places important uh objects and um maybe movie specific uh entities and one way to get that is from external knowledge such as plot summaries so a lot of these movies they come with like a one paragraph of summary for the key events um and yeah even if we call it audio description we don't really uh either use the audio stream so much so the the musical the sound effects or we don't generate the audio itself um yeah so the last thing which is I think a little bit bit uh more futuristic is to have um conversation so for someone who who wants to know more about the movie can have iteration of uh chat so Q&A um and with that I'd like to thank uh the cers who've done most of the work and the the data as I said for both ad only data mad V2 I didn't talk about it but we've kind of clean up V1 Like official version which came from other colleagues uh as V2 and the cmd8 and how to ad are just released also can have any questions thank you any questions yeah please you mentioned the issue of copyright with movies right but if you're doing a service for people who can't see the movies they're blind yeah is that not possible to have a relationship with the movie company to say hey we're doing this for blind and therefore give us access yeah it's a good idea I think they are doing it themselves like they Netflix we know have some research and Amazon as well uh but yeah I think I one yeah one potential solution is to have partnership with them but we know that it comes with a lot of overhead uh so I think once we get to a point where we can start deploying things we can get contact but for now yeah I think uh we need a bit like we we need more steps to convince them that things are uh Deployable in the near future I guess andon Prim are doing a great job today with 14 languages and so on well they're trying to hire our first authors uh but I yeah yeah to be seen I don't have a good answer yeah thank you uh great work than and uh yeah how how do you control the Verity of your produced ad description how do I control yeah that's like the data is very veros has lot of details yeah we like the predictions are relatively short in the kind of subject object verb uh format and that's one of the limitations I think one has to find the right amount of detail also considering the Gap so one other thing I haven't mentioned is that we don't consider how long the ad should be and how much time we have to narrate it so if you have only two seconds between two dialogues you should just say very short things but if you have like 10 seconds you should keep talking so I think that's another uh interesting area to look at but we don't we just to Max cross cross entropy law on the next uh like highest possible word and it tends to Output things that are pretty short Yeah question so oh by the way before I forget I think we recently tried a more recent Vision language model which was better at captioning and there we started having more expressive output so maybe it's a matter of using better models very hard to Define it for how much for we want yeah so that's I think constrained by the time for us uh and the level of like the context so if you've already said that we are in the Monica's living room uh in France you don't have to say it again and again so there's a bit of redundancy to be considered maybe you say it again in five minutes because the uh the audience might have forgotten it but yeah so it's it's really there are guidelines another thing we could do is to prompt the llm with these guidelines so if describe if a new character came in in the room describe if they are like looking at each other so the relationships emotions changes and yeah so scene changes complet they move to a completely new location you should mention it so there are a few guidelines but um it should not be extra verall either yeah so do you actually filter those like the descriptions with some post processing no I think for the cinema this for this movie you don't have to be real time to can produce many yeah that's a good idea yeah another question because I'm also working on the character identification do you think the current MBD is enough you know to identify have a good accuracy I didn't see accuracy we have some plots in the paper but um I mean it's not perfect especially it mixes up rare characters like uh by picking up always on the main uh main characters of the movie which are appearing more most often but it's still pretty good I think I mean recognizing faces these days is I think much better like I can call it solved rather compared to captioning so it's not too bad I think the question is what what if you don't have the IMDb how can you cluster and identify the characters um f yeah with f you can maybe ask are these the characters and then just ask the user to name them or you can even propose names based on speech because they talk to each other they mention mention each other's names so that's also another interesting area I there are plenty of areas thank you very much thanks yes you already par answered my question first of all thanks for this very nice BL I really enjoyed it um so I was wondering image capturing a very often very object sand and human sand and it's hard to capture the atmosphere the context of the see in the time of the day and things so um how how was this reflected in your approach and how much does it affect the let's say the experience of following the movie yeah I think that's partially why this fine-tuning helps so much between off-the-shelf application and this I think for us this more the ACT actions that matter um as opposed to yeah if if if there's a chair we don't care but we care if they're sitting down um but yeah we don't have anything explicit rather than relying on the ad ground truth that has this kind of domain um yeah so what's what's the question how do we we evaluate evaluate yeah I think there was one of the feedback we got for either at 82 or 83 which was to have a user study but we thought this is a bit at an embarrassing stage to present it to people uh that's why we presented to nlm but yeah uh I think at some point we need to someone to rate how good these things are uh especially po hallucinations so to check how well grounded they are but yeah I think we're a bit far from having um metrics for engagement U we're we're still like this should be story relevant because as you said we start from captioning models that are used to image captioning and short video captioning and whenever they see a policeman which is very common in movies action movies there is a police and then they start hallucinating the rest like they're chasing someone or they see a hospital they say they're in the waiting room but there's actually like a action scene happening there's like blood everywhere um but they just pick up on one word and then the rest they fill up and that's partially because the captioning models refer to these a lot I think yep a lot to do yeah maybe just uh I mean first of all thank you and and it's really nice to see how um you know a project very hard project comes together time right you know and keep pushing and see you know how it enhance you know the results and so on so one thing is like you mentioned at the beginning that um you are um having an arbitrary long temporal window and you instruct the features like fixed size right how this impact I mean have you have you I mean how the long the length of this sequence how this impact in somehow in the yeah I mean we haven't done much work on yeah we have a fixed size I think even Auto 81 we did some like number of frames of experiment but I think we're still doing the dummest thing possible which is equally spaced frames over a given segment and I don't think we've experimented with different architectures beyond the ones I've I've shown so yeah and especially when when you're looking at some of these failure cases now um it can be easily you you can tell why the model makes makes mistake and if you gave it another frame from like two seconds ago it would resolve it yeah but how does it how do you know you should give that frame like you have this shot between two people like uh they're shooting one is a gunman one is someone like raising arms and then because the camera keeps moving between the two it thinks that the arms belong to the gunman but I mean it doesn't make sense but if you give it the previous frame where you can see both of them at the same time that's obvious yeah there are some for yeah so I think how to sample frames is itself another question and about the um the Magics right you said okay caption Magics are good to certain extent eventually you you have to use some like like you use like GPT and so on but GPT is really let's say good with marks right so so I mean it's it's very generous very generous right so I mean I would like to be evaluated when I was a student but G uh so how I mean have you thought about the know giving some examples of for example grade one or grade two or just to yeah let him use more grades I don't know if that's a problem I tried it but didn't make any difference as far as I remember but yeah that was of the ideas because for some of the movies we have two versions even um English and American English sorry British and American English and then we have pair data of things that should correspond and then we have like an oracle to say where like to calibrate what the score means uh but yeah giving examples from those it didn't change but Lama is more stingy with the grades we have to tell it to be more generous to get it to the same scale no not Lama two but Lama 3 is more stingy so I think it's not like off-the-shelf uh calibrated score across models either uh but at least one thing is just nice with these is is repeatable it's reproducible you don't need to rely on some users um yeah so pros and cons that's interesting to see that different models yeah that's why this was kind of two models I think we got asked in the rebottle uh but then after the paper was published we tried another model and it was not it was still correlated they all uh go up at the same time but the values are not calibrated yeah yeah for comparison between models it's okay I don't know any other question on the I think my slides were stuck from the first uh slide on Zoom but apologies for that yeah I see some chat messages uh I didn't detect it from the beginning that's another new tech problem yeah sorry that e wanted this one take this one all right so we are now reaching the end of our Workshop thanks for staying here to the very very end um it was a long day and uh yes we are struggling with some technical issues but um still we manag so would like to to thank all the peers who have contributed to the workshop here so the participants which are you would also like to thank the authors and the presenters the reviewers who supported us in the review and the selection process we had five uh great keynote speakers here um from the beginning to the end and um so also big thanks to to all the people who contributed here and I would like to thank the sponsors not just the computer vision Foundation also our home universities and and other projects which supported us here with the workshop so um we hope we motivated you a little bit to continue with your research and PhD studies or whatever you're burning for and um yeah we are looking forward to a great cdpr this here please enjoy it have lots of discussion with other people interact do you networking and then when you go home I hope you're inspired for the next uh research steps you plan to do and then who knows maybe we meet next year for the next cvpr maybe the eth version of a Moola Workshop maybe with a few slide modifications but anyway so enjoy the time here thanks you stayed here all the time and then yeah this Workshop is now finished thanks brother

