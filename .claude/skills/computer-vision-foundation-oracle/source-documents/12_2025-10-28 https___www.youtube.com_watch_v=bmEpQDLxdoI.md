---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=bmEpQDLxdoI"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:03.037Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=bmEpQDLxdoI

c9af220d-d78f-479f-a68a-04451a7b8523

2025-10-28 https://www.youtube.com/watch?v=bmEpQDLxdoI

83c9288e-688e-4a51-a58a-c739080db0ca

https://www.youtube.com/watch?v=bmEpQDLxdoI

bmEpQDLxdoI

## ComputerVisionFoundation Videos

this is the this is the first time that uh we organize an equarian Workshop explicitly for a vision audience and uh we are really super excited about uh this opportunity to bring you uh both foundations of AAR and sent applications uh in an one day event uh the workshop has been uh uh organized I the the leading organizers were Chu and yuang and uh I see also among others vtor here and I don't know who else is already here but yeah thank you very much and I will thank you again at the end of the workshop so we feature uh this is not a workshop with just invited talks uh we have a uh a uh we have five wonderful invited talks uh starting with hagy and Leo gibas Carlos Nina and Eric Beckers uh we have a tutorial which is really for getting started with equarian networks even if you haven't uh uh heard anything ever about equarian networks and uh we have four spotlights and a quite long series of posters um the history of equarian is quite long obviously the definition comes from the mathematics from equarian maths from from equarian Maps but uh the first time that it appears quite explicitly in the P recognition Community is by ich Amari in icpr 78 this is the same Amari that also is has been publishing in information geometry and uh a lot there is also a whole like conference uh that uh he organized on information geometry and then even in the 90s uh uh there were several uh like efforts among them from the disertation of uh Patrick to at Stanford uh where uh they were uh trying to build the equivariant uh and steerable actually uh functions by handcrafted uh networks not neural networks coming back uh uh to today uh today we have equivariance has been applied uh widely in The Sciences in computational chemistry in biology and uh even in a vision uh where uh we have a in the main conference we have a uh several papers spanning from uh uh descriptors uh and uh in the 3D Point clouds uh and even like image reconstruction uh it is uh uh a quite exciting field uh even if uh what we realized lately and I think also Nina uh will talk about it is that uh even if uh uh we discover that uh uh filters by themselves become equivalent during the learning process uh we weren be able to recognize that for Transformations beyond the translation if we hadn't studied all the mathematics behind uh both equarian kernels and equarian nonlinearities uh and uh in general uh uh while dat augmentation uh has been the way to go it is quite expensive and does not provide that inside when we build a system so I'm going to uh just finish with uh a very old meme this is from The Matrix pingpong when the the movie The Matrix came out there were several efforts to mimic it and this is a way to uh make dat Commendation work in 3D and you're going to see how expensive it is uh if you see the makeup actually there are all these people in Black actually turning these people in order to really mimic uh uh novel viewpoints of the scene which uh uh you cannot always get by just having a d of cameras around it and uh there were several actually memes and there was also a whole like a uh a line of theater plays in Japan using these techniques uh while you are still watching this uh I think we are going to start setting up ha guys talk and uh Chong will run also the uh we introduce all right thank you for joining us uh today and uh I urge you to stay for the whole day up to the tutorial uh which will be between 4 and 5:00 p.m. you e e e oh yeah hello haay can you hear me yeah I can hear you well can you hear me oh wait sorry um sorry can you speak again yeah can you hear me now it seems that oh I see yes thank you so much is this a uh sorry ha would you mind speaking again yeah can you hear me now oh oh yeah yeah great great yeah yeah we can hear you now yeah you so much and sorry for the confusion sure can you see my screen uh yeah yeah we can see it here great yeah uh we we we just had an introduction to you and sorry because like because of the audio issue probably you didn't hear that oh no no worries so I can start yeah um feel free to get started we are ready and thank you so much for delivering this talk at this Workshop sure and thank you so much for inviting me uh this is a real pleasure to to uh talk here in this workshop and I'm so sorry cannot be there uh in person hopefully next time um so my name is hagay I'm assistant professor the techon and research scientist at Nvidia and I'm going to talk about about sries for learning in deep weight spaces uh which is a recent topic uh my group and colleagues are working on recently which I find very interesting and I'll try to convince you as well so this is the outline of today's talk uh we will start with a brief intro to symmetries and equivariant networks um hopefully this will take only a few minutes and then we will talk about learning in weight spaces what exactly this means what is the setup we will see two methods for learning in weight spaces the first one using linear equivariant layers the second one is using graph meta networks and then we will uh conclude with a few uh uh very recent papers uh we are going to uh uh present at icml next month uh with new directions in this topic okay so we'll start with the Deep learning with uh symmetries and we are going to focus specifically on permutation symmetries and this will become clear here in SEC um so in general many structured objects have symmetries and when I say symmetries I mean Transformations that keep the properties of an object unchanged so the usual example is to think about images and then to think about translation uh and then translations do not really change the essence of the object right the image and in many many cases learning tasks that we are interested in are invariant to those kind of Transformations and again the uh prototypical example is image classification and the main idea the way I see it behind this field is to leverage those symmetries that we have in the input domain in order to construct efficient learning models and the principle maybe uh a bit more directly is to restrict the hypothesis class uh that we are learning on to models that respect the symmetries that we have so for example use invariant models for invariant tests and we know that there are several benefits to that first of all uh uh on a broader sense it injects a very good prior knowledge to the model about the problem that we are trying to solve but we can also show that you get better generalization and that the networks themselves have less parameters and are more efficient so we turn to permutation symmetries uh permutation symmetries are symmetries that can be described by subgroups of the symmetric group SN of course not all symmetries fall into this category but this is quite General so for example uh translations in images one can think about them as uh certain very specific subset of uh transl of uh of permutations to the pixels right so This falls into the category of permutation symmetries also Point Cloud symmetries assume that you have a point Cloud which is an N by3 Matrix then changing the order of the points in the point Cloud doesn't change the point cloud so in this case the Symmetry group is the group of row permutations in this case uh and this is another example of a subset of uh of the full permutation group that we can use as a symmetry group invariance means that for any transformation in my symmetry group applying the transformation to the input doesn't change the output uh and again image classification is a good example for that if we apply to the transformation and then apply F we get exactly the same thing equivariance uh is is generalization of that of that uh uh idea essentially equivariance means that the function and the transformation commute and this can be seen nicely in this commutative diagram here we have a cat here we uh move it then we apply uh F and get some some other function but it doesn't really matter if I first apply F and translate or first translate and then apply F and essentially we all know equivariant functions from convolutions right convolutions are uh exactly the set of all translation equivariant linear functions so um very uh central question in this field is given a group G acting on our data how can we design an invariant or equivariant new network for this data type and this Symmetry and perhaps the most popular approach is uh to compose several simple equivalant layers and this just this approach just follows the idea that that used uh in in CNN for all these years essentially you use a linear equivalant layer and on top of that you use a pointwise uh nonlinearity in the case of images those linear equivalant players are just convolutions in case that the uh problem we want to solve is invariant and not equivariant we can have a pulling layer on top of that and then uh run any fully connected Network and now the uh question is H how can we come up with those simple equivalant layers so uh there is a nice recipe for that uh if we restrict ourselves to only uh aine layers WX plus b uh and and a pointwise nonlinearity on top of them then a nice result shows that if uh a layer the by W essentially is equivariant if and only if it has a very specific parameter sharing scheme defined by the group so in other words if uh I can move IJ to KL by a group element then those two uh entries in The Matrix should have the same parameter and this has been known for for quite some time let me give you a examples for that for Tod translations for example um we will get just the convolution Matrix uh which we see on the left side here for the very uh nice case of of set permutations where we don't really care about the order of the elements in the set we have uh Point net and uh dip sets uh to uh uh very exciting papers from seven years ago that showed how those layers should be uh should look like and when we have no symmetries we have um no parameter sharing scheme right so each one of the entries has a different parameter and this is a fully connected net and we used this approach in the past few years in order to to come up with equivariant networks for uh all sorts of data types that we encounter in practice so for example on the left side we can see graph permutations so it's again the symmetric group but acting differently on graphs by conjugation on the right side we have sets of elements with their own symmetry in this case we have sets of elements uh where each element has cyclic symmetry um okay and key challenge when constructing those kind of newal networks is understanding their expressive power essentially we are restricting our hypothesis uh space to a specific subset of functions and we are interested in understanding whether there is a gap between the invariant networks that we can approximate and all the G invariant continuous functions okay so to recap if we are in interested in in invariant task it is beneficial to use invariant models uh in the case of uh uh in any case those can be constructed from equivariant layers and in the case of permutation symmetries we have a very nice recipe for constructing those equivariant layers by parameter sharing governed by the group action essentially so this was my uh uh kind of a uh starting point for equivariant and now I would like to move on to our recent work on learning on deep weight spaces so this work was led by uh three amazing students a and other people um so essentially we know that new networks are uh a very good model for learning from data uh for example CNN and llms and any other neural network that we train in order to predict something on an input data point um and recently neural networks also became a primary model for representing data samples themselves uh so what I mean here is implicit neural representations essentially neural networks can be used in order to represent images 3D shapes and uh even scenes as we will see shortly the idea is that the neural network takes as input a pair XY of coordinates and outputs the RGB value corresponding to this position in the image if we train the neural network to predict the correct RGB value for each XY uh then we essentially encode the image inside this neural network this is uh in many cases an MLP uh a fully connected Network and then essentially the weights of this MLP represent the image another example is nerves uh neural Radiance fields which allow us to do something similar but on the whole scene and not only on an image or a 3D object so the situation now is that there are very large collections of trained networks online uh so for example on hugging face alone there are over 600,000 models uh there and we start having Nerf and INR data sets online and we are going to ask two questions here so the first is does it make sense to apply machine learning to these newal Network collections does does it is it useful for for us or uh or uh it's not and second question is that in case the answer is positive what is a good way to do that so how to apply machine learning to neural networks as inputs right this is those are the two questions so in order to give you some motivation what we can do with those kind of networks um I will try to give you two motivations the first one is for processing inrs so uh in this part of the figure we see INR training procedure in which we train the INR to represent the figure five from mnist here and Below we can see us taking the weights of the INR and pushing them into a newal network trying to predict the class okay so we might want to perform classification tasks on inrs right and iron RS are naturally represented as weight vectors so we need neural networks that can work on weight vectors this is not only restricted to a classification we can think about editing irrs generating inrs and so on essentially those weight vectors are becoming a new data modality and we need to understand how to apply machine learning to it the second motivation is about processing more General networks so think about uh a case in which we would like to take as input some network and to get as output some altered version of this network think about pruning a neural network uh binarizing it predicting uh um making it more robust or stuff like that it will be it would be extremely uh nice if we could train a newal network to do all these stuff so essentially after training such a neural network we can take an input model and and for ex and push it through this special neural network that we are going to discuss and then get another neural network that has the uh properties that we want in this particular case we perform domain adaptation so we take some Network trained on C 10 and then we uh um adapt it by mapping it through a neural network to a different data set and now the challenge is what is a good architecture for learning on neural networks and this will be the main topic of today's talk so let's try to formulate it a bit first we focus in this talk on MLPs all things here can be extended to other types of models so MLPs are just sequential models that apply a linear and pointwise nonlinearities as we all know uh and for us an MLP is represented by its weights and biases so think about this Vector concatenation of all the weights and all the biases and when we say Pro we process an MLP what we actually mean is that we apply a function to its parameters so essentially this is the idea we take uh a neural network represented by W1 W2 W3 and so on we push it into a network or function f then we output new parameters and this is a different neural network now okay so what would be the uh most uh naive uh or uh first solution we we can we can try to solve this problem we can try to vector all the weights in a single very long vector and then apply uh the most simple networks that you can think about essentially a fully connected Network um so this was done and was successful to some extent but it has uh several limitations and those are the uh you know natural limitations that we always see when we don't use the uh correct neural net NW work for our data type or for the Symmetry type so we get huge parameter space for example for a neural network that has like 100k parameters every linear equiv every linear layer would be 100K squared so this is a huge number of parameters second thing is that we totally ignore the structure of the weight space and we know that structure can can be used in order to to uh come up with better models and essentially a better approach which we suggested uh in an icml paper last year is to take into account the weight space symmetries so again just to make sure we are on the same page for images we know that we have translation symmetries and we use CNN for Point clouds we know that we have permutation symmetries and we use dip sets or Point Nets or now uh uh um Transformers for example and the question in our case is if the input is a deep Network represented by its weights what what should be our model and what are the symmetries okay so specifically in our case in in the case of of uh uh images we have arrays of pixels as input the Symmetry group is 2D cyclic translations the basic layers are 2D convolutions and the resulting architectures are the cnns we all know and uh love in our case we know that the input is the weights and the biases of the MLPs but it remains unclear what are the symmetries what are the basic layers and what do we get as the architecture and also I would add what can this architecture do in terms of expressive power so we should start from the symmetries and the symmetries in the weight Space Case are extremely uh interesting and cool I think so the idea is that we can switch any two neurons in in uh in an intermediate layer and we will get exactly the same function right the order of those neurons doesn't really matter for us only the order in the input layer and in the output layer algebraically what we see is that we can apply any permutation Matrix and then compensate in the next layer by the inverse permutation Matrix so those two permutations when we apply the uh linear layers they cancel each other and then we get exactly the same function and this of course thally extends to even deeper Networks so those are the symmetries we are interested in I should say that there are other symmetries as well like scaling symmetries for example we can apply some scaling before uh a nonlinearity and after a nonlinearity and then compensate on that this is something that we are not going to tackle in this talk but there are some works that try to tackle that now so we know what are the symmetries and according to the scheme that that we uh discussed we need to construct simple equivalant layers and the idea is to characterize all the equivalant uh linear equivalant layers for those kind of symmetries this is what we are going to do now uh essentially what we're are going to show is that the layers follow a natural block structure and each block Maps between specific weights and biased spaces and those blocks are essentially very simple they can all be implemented by broadcasting pooling or very small fully connected layers those architectures uh are called uh DWS networks for deep weight space networks so the main observation driving our uh characterization is the following the weight space is a concatenation of all the weights and the biases uh and the Symmetry group acts on each one of those independently this means that if I have W1 here and I apply a group element all the entries of W1 will end up here the same is true for all the biases and all the other weight weight matrices we can see that pretty clearly from this equation right W1 moves to a permutation of W1 W2 moves to a permutation of W2 and so on in more formal terms this is called the direct sum of representations of the group essentially a concatenation of several Vector spaces on which the group acts uh independently and this gives us a nice structure to work with so a very simple uh result in group representation theory states the following if we have an equivariant map between a concatenation of two Vector spaces V1 and V2 uh and to another concatenation on which the group acts independently then we can actually break this uh uh equivariant linear equivariant map into four different blocks and each one of them is going to be an equivalent layer between the uh respective representations so as we see here we have a map between V1 + V2 to V1 prime plus V2 Prime and if we look on this Square we can see that this is a map between V1 to V1 Prime uh and this helps us a lot essentially this is a decomposition theorem that allows us to take this huge problem and to break it into several very small problems and let's see how we exactly use it so the first use use case is to take the whole weight space and to break it into four different uh uh linear Maps the weight weight map mapping between all the weights to all the other weights the bias weight uh uh map that Maps biases to weights and so on so we took our uh problem and broke it into four smaller problems and now we are going to use this decomposition again to show that each one of those uh blocks decomposes again into a block structure where each one of the blocks Maps between a specific weight Matrix and a specific weight or bias metrix so for example this block Maps between WM the last uh last weight space last weight Matrix and W1 and so on different colors here uh represent different types of mappings but as we said all of them are more or less variations of dip sets and point net uh so those are very simple things uh maybe in a bit more detail if we look closer and zoom into a specific uh like a row of blocks here we can see that we have a self update rule which essentially updates the M weight matrix by itself uh this uses an equivalent layer characterized by howal from icml 2018 and essentially this is a tenso product of De sets or Point net with itself here we have dip sets this is just simple summation and this is another variation of dip sets so essentially uh breaking down all these block Matrix structure gives us a list of very simple uh equivalent layers we need to to uh Implement um as we discussed before equivalant CS uh architectures might not be expressive enough this is for example a common problem in GNN so we uh we we uh have two theoretical results uh on this matter um so I will state them very informally but I'm happy to talk about it later if someone wants I'm available uh online or on email um so essentially DWS networks are capable of approximating Feit forward operations all the on their inputs what that means is that if I give the weight SP Bas Network set of weights and an input to the input Network there are weights for the DWS networks that will give me the application of this input MLP on this input X can think about it as some something that is similar to a universal during machine or something like that the Ws networks can can learn to think about the inputs not as weight vectors but as functions and using this result we show that DWS networks can approximate any nice function which I will not exactly Define here but it's a preliminary uh result for expressive power of DWS DWS Nets on the space of functions on MLPs uh I believe there is a lot of work to be done in this direction of of understand the exessive power of those uh of those methods uh we turn to some uh experiments so the first experiment is very simple it's an INR classification experiment so again as we saw before we are training 70,000 inrs on amist each one representing uh representing uh uh some some digit and then and we uh train a DWS Network to take those weights of the inrs as input and to predict the correct class so let's see what are the results first of all applying an MLP doesn't work well and this is something that we could have uh could have imagined before we know that an MLP wouldn't work well in this case because it doesn't take the structure into account the second second uh row here uh third sorry uh shows MLP plus alignment so essentially we run a a procedure before training to align all the networks together so kind of find the optimal permutations between the neurons so they all live in the same space um this is an NP hard problem uh so we we had to use some uh approximation algorithm in order to do that which runs a long time and we see that MLP plus alignment works pretty well much better than MLP alone or MLP plus augmentations um our networks uh perform even better than that and by a wide margin and we do that without solving any NP hard problem without uh uh you know a pre-processing and anything like that all we need to do is to use the correct DWS uh the correct Network and layer architecture for the data type that we have so so we consider this a pretty good result I'm sure that all of you now think that 85% on on amist is not so good this was the first result we had on that and uh the results now are are much better and we will talk a bit later in this talk um on how to improve the results here uh but the main thing to take to take here as a as a message is the very wide margin we have from other approaches uh this is another experiment that I like to do when when designing equivariant networks and essentially what we try to do here is to understand how the sample complexity uh of those models behaves so on the uh y- axis here we have the error of a specific task that we trained on and on the xaxis we have the number of samples we use for training and so what we see here is how well a a model generalizes according to the amount of data that it uses and you can see the very uh uh large difference between DWS Network here and all the other approaches you can see that it might take forever for us essentially to uh uh get uh uh to to to a good place with MLP plus alignment uh but we already solved the problem pretty well with uh 800 samples in this case if we use the correct uh equivariance structure so um we are not the only ones that that uh worked on this direction and there three very nice works by uh Ellen Zoo from from uh Stanford and I encourage everyone to look at these works as well uh they came out in parallel to our works and um do stuff in a similar way although there are some some differences um so what are the limitations of of our approach first of all the architecture is tailord for a specific MLP architecture so an MLP with K layers and I don't know 128 neurons uh if I want to apply these networks that I trained on on on specific architecture to another Arch architecture I cannot do that because the architecture is tailored to a specific weight space second thing is that well while I try to convince you guys that that uh uh everything is very uh nice and uh uh elegant mathematically it involves very long derivations and the implementation can be challenging a bit and also the it is quite difficult to modify and apply design choices so for example if I want to add an attention mechanism for example and if I want to add a normalization there everything needs to be derived uh for the specific architecture that we are using so in the next work uh from I clear this year uh we are going to talk how to alleviate those problems and this work is called graph new networks for uh learning uh in deep weight spaces it was led by by Derek Lee from MIT and James Lucas from Nvidia and essentially the main idea is very natural uh it it says the following a neural network is essentially a graph how can we construct this graph well the inputs are the input nodes in this graph each one of the neurons is uh uh specific node the biases are going to be another node and uh uh we are going to have edges between uh neurons that are connected to each other and on those edges we are going to encode the weights right so this is just the weight between the first input and the first neuron here essentially all the uh weights are encoded in this graph on the edges so a good question now would be what are the symmetries in this case uh and the idea is that we can design the graph such that any automorphism of the graph such as the ones that we see here preserve the neural network function so in that case we have a bunch of permutations on the nodes of the graph that will be our symmetries the good thing about it is that we know how to be equivariant to permutations on graphs we can just use a graph neural network um and this is exactly what we did so we use a GNN to process the graph generated by the weights uh and the MLP architecture and those dnns will be naturally equivariant to those automorphisms so why is that a good idea I'm going to to say three things uh first um one GNN can process different architectures different sizes anything because our GNN can process any graph so as long as you can encode or describe an an input architecture as a graph you can apply the GNN to it and this is great this is something that we couldn't uh do before and in the paper we show how to build graphs for many types of gadgets and architectures that are frequently used in deep learning so for example attention layers convolutions and normalization layers and so on the second Advantage is that by moving to this uh model we don't lose expressive power so we can show that on MLPs uh those graph metan Nets can simulate uh other weight space networks and can also simulate forward passes like we discussed in the previous model the last Advantage which I think is very significant is the fact that we inherit a very large body of work on gns what I mean by that is that essentially I can take any off the shelf GNN it can be a gra Transformer it can be a message passing neural network it can be an exessive model anything you want and you can apply that to this task and by doing that you can use all the knowledge that the graph Learning Community has gathered uh until now all the different attention where normalization ways to positional encodings all the different stuff that people did on graphs are applicable to weight spaces now so uh in parallel to this work uh another work by cofin acetal from the University of Amsterdam did something very similar to that um so yeah I encourage you to take a look on this work as well and now we turn to the last part of our talk I'm going to discuss uh two um recent papers that that got accepted to icml in which we used weight space Networks so the first paper tries to learn to align weight spaces and the motivation is for merging models so merging models is a very interesting problem the idea is that we want to take two models uh let's assume now that they have the same architecture and we would like to create a new model that will have the benefits of those two models for example and one generic way to to align to to um merge models is by first performing weight alignment so weight alignment is a uh is the process of finding the group element that makes the weight vectors as similar as possible this G here that we see it's a concatenation of permutations for all the layers and it is applied to this W in the way that we saw by the permutation matrices before so our idea is to First the idea is to First align the uh weight space the weight vectors and then to perform some convex combination on top of them in order to merge the models the problem is that this weight alignment problem is NP hard as we already mentioned so in line with with a large body of work that tries to learn how to solve NP hard problems on specific data distributions we try to learn how to solve this weight alignment problem and again key to the solution here is understanding the Symmetry structure that we have in the problem so let's try to understand what we have here here we have two neural networks represented by two vectors weight vectors V and V Prime each one with its own matrices and and our task is to find the optimal permutations for uh the alignment so this is sound function maybe it's hard to compute it but there is some function that takes me to the optimal permutation what are the symmetries of this problem so let's say that I apply permutations to both the ve weight vectors so I can apply some plementation P1 and P2 to uh the first Network and some permutations P1 P1 Prime and P2 Prime to the second one what we can show and show in the in the paper is that the output permutations will undergo a similar transformation so essentially we have a very similar but different symmetry structure to the one that we described before in the uh previous papers in order to solve the problem we devise a new architecture that uses DWS networks and predicts those uh uh optimal matrices hopefully optimal matrices and we show this works pretty well uh we have multiple experiments showing uh the benefits of using that in in many cases for example in U uh Federated averaging for example so if if you're interested in Federated learning take a look the second problem or paper that I want to talk about is uh about data augmentation in weight spaces so what we saw in the year or so that we are working on that year and a half that weight space architectures suffer from severe overfeeding um the problem is that generating data for weight space learning is hard right we have to train each example thing is that we tried to do that to come up with very much larger uh data sets and we thought saw that the overfeeding problem is solved but still the data generation problem is is is is a problem right so what we suggest in this uh paper is to use uh data augmentation in order to come up with new weight space data and the main idea is to use weight space mixup um as you can uh you can realize by now what we do is we use some alignment algorithm in order to align those two networks and then use a convex combination of those two um and the results are pretty amazing we show that here uh the Blue Line uh shows the loss without augmentation and there is a severe overfitting when using the augmentation we see that the problem is more or less solved and the same thing can be seen here uh where we uh show the test accuracy and you can see uh an improvement of uh about 10 points here uh when using the um augmentation so I I believe augmentation for weight is extremely interesting both uh I would say theoretically to understand how to create uh weights for uh uh that are meaningful from different from from uh different sets of weights and also practically it's going to play a a significant role in we space uh models um okay so let me uh conclude first um I guess I don't need to tell you because you came to this Workshop but equivariance is a very effective way uh and very effective design principle for uh neur networks for uh structured data types I would add that the math is also very fun at least I like it a lot and for weight spaces those are weight vectors are becoming an important data modality uh and what we we saw in this uh in this works is that equivariance is key to success in this case there are multiple Works uh this is really trying starting to to gain uh to gain uh uh the attention of the community and I think there are lots of new exciting directions both on Theory and applications of that and if you're interested then uh yeah I'm happy to talk about it and uh yeah that's it those all the papers that I discussed today um thank you so much hagay for the super exciting talk so uh probably we have a couple minutes to take a few questions from the audience if anyone has question okay c um I repeating the questions or um Dad can you hear kasas talking right now so ha this is really great talk and great Direction uh for research I'm wondering uh uh whether you have made any thoughts about applying this not to weight spaces but other functional representations like obviously like G 3D gaussians or any other like basis to describe uh uh like a function yeah interesting I I think this is a natural uh natural extension of what we did here obviously we need to think about the symmetries of those uh presentations and then understand how to work on them but I think it's very exciting yeah thank you so much hagay actually I also have a question so um as you mentioned in the talk so like you're um you're focusing on the symmetry of the way spaces of the new networks um which are super cool so I was wondering like um because the neur networks they themselves as function they're ALS Al representing certain Addies or certain objects in the Primal space so and also like for the Primal space objects you also have symmetries there so probably one analogy is that for Point clouds you have the permutation symmetry for the points themselves and then for the objects represented by the point Cloud for example a chair or a car you also have like rotation or translation symmetries which like further like on top of the point Cloud networks you can further incorporate like other symmetries so I was wondering like for like w space NE networks is it also possible to further incorporate the symmetries of the objects represented by them for example you have an implicit representation for the Mist digits you have the Symmetry for the w space of the representation but you can also have like permutation sorry um translation symmetry for the digits images themselves like would you also consider certain expansions yeah this is a great question uh I don't have a concrete answer to that I think something in this direction can be done we are thinking about it now but I'm not sure if I have a good answer now it it can definitely be interesting to to be able to uh encode those those object symmetries in the INR itself as well yeah thanks for your discussion and in the end this is really an exciting talk and thank you so much for presenting your cool works here thank you again for for inviting me was really a pleasure cool thank you um e e e no that's going to be right I see it's going to be a little bit different from even don't know what's going on now with my why disappear this a problematic close let me try something well that's no idea what probably can make the window then this will cover the screen now okay again okay except Advance the slides now if I click um um I think there's a here um sorry okay sorry for the technical difficulties um let me start by uh thanking uh Costas and kongu and ji and all the organizers of this ni workshop for the meditation um equivariance is a truly exciting topic that cuts across many different fields not just in computer vision but in many other areas in computer Graphics in robotics in physics um I'll talk about um enforcing equivariance in two somewhat different settings the first is the more classical one where we have a known Action Group like the rotations or the permutations and then we're going to have neural networks that uh uh support that the second is one where the symmetries or the maybe I have to speak uh closer to the mic is this better okay sorry um so I was saying I'm going to speak about equivariance being enforced in two different settings the first being one where we know in advance the particular Action Group we would like to support maybe rotations maybe permutations like hay was mentioning the other is where the equivalant entity is learned from data perhaps with the help of foundation models and then by enforcing it we have tools that allow us to manipulate our data in ways that reflect and support the structure um my setup is not exactly what I expected so I may have some difficulty here but let me start with the classical setting of enforcing classic invariance and equivariance for 3D objects um I guess ever since the card invented analytic geometry we have the issue that when we represent geometric entities algebraically we are given them in a particular coordinate system and of course the coordinate system is sort of a nuisance factor most of the time because if we have a different coordinate system we get different representation of the geometry yet the underlying the real geometry has not changed so we would like to be able to guarantee invariance or equivariance to these kinds of changes uh and so how to handle this kind of geometric Transformations has been a classical topic of interest for a long time um now when it comes to machine learning in the early days of 3D we found it was a lot easier to do standard type of discriminative learning like classification and segmentation if we allign the shapes but we apply this to because then the network has an easier time since the information is given in more compatible ways so in the early days of the shap net we spent considerable effort trying to align objects that belong to the same class so that all the chairs are aligned the same way all the cars are aligned the same way and so on and that led to quite good results in things like classification and segmentation but if you are given an object in a different pose then this network may fail so what we would like to do is to support natural invariances and equivalences for example the class of an object does should not depend on the PO you if the airoplane is POS this way or that way to an aeroplane we would like segmentation of an object to be equivariant if we rotate the object the segmentation should rotate with it then there can be mixed situations for example for reconstruction we may want to have an an equivariant encoder followed by invariant decoder formally and I guess has covered this also the notion of Varian is we have an operator that acts on the data and we would like to have in the computed output have another operator that's a function only of the original operator that represents the output and the two classical cases is if the actual of the operator is the same on the input and the output this is the equivalent case or when the operator of the output is the identity we have the the invariant case how to make that happen is of course a topic of many works and and of this Workshop the the most trivial way oh yeah sorry uh so of course we want all that because that's how we think about the world we don't always think think of the coordinates we think of the object sort ofon frame and if we have this ability to take object in any pose then we can be more General and the real world of course you know that's how the information is given to us in arbitrary posits and we avoid the costs of doing the obvious Solutions of which the most obvious one is to data data augmentation namely to take the original data and then generate Variations by Say by rotating the object and then training the network with a large data set but of course this can be quite expensive and it's also not perfect because of course this is an approximate equivariance or invariance it's not really built into the network this is the setting of the first uh uh enforc I will talk about in the second one I'll talk about learning and leveraging certain covariation patterns in 3dcs and these days we are seeing quite a lot of efforts on Nerfs or gasia Splats that give us representations of 3D scenes that can be computed from from Po's images very efficiently and give us very good results for a new your generation but at the same time these are really lowlevel representations yeah uh they don't really understand the scene structure they don't give us any way to edit the scene in semantically meaningful the ways and one way to to to approach this is to try to think about the the relation between sinen structure and scene variations so let's imagine we have this scene and let's have to imagine not only what this scene is but how it could be different well you can imagine that maybe this table here no maybe this chair here you can move a little bit or maybe this table can become a little bit longer or this table can become a little bit darker this is an actual semantic variation space of the scene and what this means in terms of the visual element of the scene whether you thinking 2D pixels or 3D points is that there are certain natural dependencies or Co variations that is these pixels that belong to this Share A and B are much more likely to change together than say A and Z similarly uh X and Y are much more likely to change color change position together than say x and C there another kind of equivariance certain things that vary together understanding that is really a way for us to understand the structure of the scene to parameterize essentially the tangent space of the scene variations using it using using some lower dimensional parameterization that reflects yeah our understanding of the scene one way to approach this is by bringing in Foundation models because they they understand the world and perhaps for semantics the most classic one now is is Dino you know Dino gives us colorings fixures that reflect semantic similarity semantic consistency and there've been many Works trying to extend this Tod semantic information to to 3D by essentially distilling it from multiple views like in a Nerf to the MLP by by adding one more head to the MLP that now computes a semantics so now you have essentially another channel that you can use to structure your data there's also an interesting connection between or difference between you know first order supervision and second order because normally when we train this representations we ask that they match attributes of a particular pixel we say this pixel should be red or this pixel should be car but it's also interesting to to think of supervising with second order information we don't say what something is we say what something should be the same as we supervise so with relationship so this a bit more like the difference between semantic segmentation and instant segmentation and the second order information is again this covariation pattern that I'm talking about certain things will change together and so now we have the foundation models like some but that give us nice instant segmentation they give us kind groupings that again we can try to lift into 3D and get some information about what parts of the scene are somehow the same and therefore will change together it change color or change position so on and that's the second kind of equivariance I will try to cover so again this is summary of The Talk The First will be about the traditional prior equivalences and the second will be about this learned equiv variances let me start with the first one of course this a classic topic L survey all that has been done there's a lot there be many more uh uh discussion of this later on today uh I'll just present one yeah one word the vector neurons which is a very lowlevel uh yeah way to realize equivariance and incident that was the first research project of p in my group she started um as as compared to other methods Vector neurons have the the the attractive fixure that they're very simple they don't have complex ma in them there's no um you know vard matrices no you know sperical harmonics and so on and they can and because they are so basic they can be applied many different architectures and the idea is extremely simple our normal neural NS the basic neural n that we all know and love operate on scalars well let's extend them to operate on vectors in this case I will discuss the case of three vectors but but but it could be a any Dimension so it's it's not just Vector input it's vector all the way at every stage all the intermediate layers are sequences not of scalers but of vectors and the output is also vectors and this is of course attractive from the point of view of say learning in 3D equivariance because now operators like rotations can be applied at all layers of the network and that's the the you know the basic idea we you equivariant should be kind rotating the input and then running the N work is the same as running the net work and rotating the output that makes sense now because you know because our lat space is not just a vector of scalers it's a vector of three vectors and those three vectors can be rotated by the same operator yeah so this is really just this very simple idea going from scalar networks to Vector networks and so classically if we had a 3D shape give us a point Cloud where we have end points with six channels of fixures each of them of Dimension One now we will have the same thing only each channel will have three the three coordinates it's kind of interesting that say for the point Cloud case we map an unordered Point cloud in real 3D to an order Point cloud of fixures in this latent 3D okay to implement this scheme we have to realize all the operations that go into a neuron the nicest and simplest one is is is a linear layer and say multiplying the weights of the network by a matrix can be extended to repres to multiplying this you know n by three matrix by some some other Ms W on the left and the nice thing now is because of associativity of B multiplication if we post multiply the input data by rotation Matrix on the on the right because of associativity that will be naturally the same as as first multiplying by the weight operator and then post multiplying by R so you can you first multiply by R and then by W or first multiply by W and then by R and you will get the same result because of associativity and so this is this linear layer setting this is very forward and notice we don't have a bias term here to make this work the interesting uh layer is in is in a linear layer where for example yeah in standard uh uh the neuron we may have say relu no linearity that's clearly not equivalent so we can try to um lift this relu from 1D 3D essentially instead of using say the X direction we will learn a direction and then Define essentially the clipping that that that the does with respect to this learn Direction so if our new Vector is going to be projected into the positive half space defined by the secondary direction that we compute and that is shown in this diagram here and mathematically it corresponds essentially if we point in the positive space nothing changes if we point in the ne if we have a component in the negative half space that gets clipped and by the way this also works not just for rotations but also for uniform scaling something that that I will use later on so this is uh essentially how we can deal with a certain type of linearity by using this High dimensional rlu and one can do similar things for other types of nonlinearities and this idea of Lear directions can be very useful in other settings also for example let's think about the pulling well average pulling of course is linear and there's no problem with it but if we going to do Max pulling again you you Max now you don't have scalers you have vectors so again you learn certain directions and you Max pull along these directions so the new thing that the neural ABS is this learn directions on which you project your 3D vectors to do the classical Rel use and pulley right and I just mentioned this so I will skip it uh there's also of course various kind of normalizations that have to be fitted into this Vector form and layer norm and instance norm and dropouts Naturally Fit the one that's a bit problematic is back Norm because there you are mixing different data that make come in different opposes and it's not quite okay to you know to aage those so for borm we apply the B normalization process not to to the vectors themselves but to their Norms because these are invariant so basically these are the building components that go into equivariance and for invariance it's a very small variation namely we simply use the fact that if we start from some input and then compute two equivariant outputs and multiply one by the transpose of the other we will get something that that's invariant because oops I don't know why yeah because if we apply a rotation Matrix to the first one we'll apply the transpose rotation Matrix to the other one and because R is a rotation R * R transpose is identity so we get uh the same result we get we get invariance and in particular in our setting what we do is we use identity as the first Vector neuron and then a second Vector neuron that ESS computes a coordinate system the 3X3 Matrix in which the result will be expressed and the product of the original times the transposed coordinates gives us something which is VAR e canonicalized set of fixures and we found in practice it helps to do this not on a per Point basis only but also append the mean from all the points because somehow this gives the network information about the entire shape and not just this one point so this is how you can build an invariant layer into this network so that's really a summary of the vector neuron this all there is and now we can take you know s of classical 3 networks and vectorize them in this form for example we can take Dynamic graph CNN which essentially is a graph neural network operating on a point cloud in 3D at every step we do an ex convolution to our nearest neighbors learn some new fixure recompute on the nearest neighbors using this new feature and then do it again so the basic architecture is this sequence of hcon and nearest neighbor computations which what it does basically is to transition as you go through the layers from a oce of distance that is more geometric to an oce of distance that is more semantic and in terms of transforming this from a standard neuron to a vector neuron is really extremely simple we simply just replace all the components by their Vector neuron counterparts and basically that's it and we can do the same thing for other classic Network that was mentioned by Guy also point at where essentially we lift the points of a point Cloud into a high dimensional Space by learned mapping and then we average them or or Max to compute to compute a fixture um here to we can just essentially vectorize the operations there a little bit of a tricky part in the first layer that they will not have time to mention and so we can get essentially a vector neuron version of the point net and so we played around using this this Vector neuron networks for uh many classical problems like classification and segmentation and I show here a number of different setups I have a slash to the left of the slash is what we train on to the right of the slash is what we test on Z means kind of vertically aligned shape this is the classical s SO3 means arbitrary poses so here is test train on vertically align test on vertically align train on vertically aligned test on arbitrary train on arbitrary test on arbitrary and the results are shown here the first four rows show the standard Point net and d gcnn and their Vector neuron counterparts below that there's a group that does not try to op to optimize for rotations this is for a fixed pose and the second last group is are methods that try to optimize for all poses and you can see that the vectorized network here do do the best uh and it's interesting because uh they do better even than the versions of the same networks are trained with augmentation so it says that somehow building the equivariance directly into the network is better than um than trying to do uh dat augmentation and here you can see something very similar happening for segmentation and we also played a little bit with reconstruction where you can see like a classic Network like occupancy networks has a real hard time trying to reconstruct shapes when it's trained with shapes aligned and given something that's in a different pose un aligned in many ways okay um short on time so let me move quickly to the or I have some some applications of vector neurons and because I'm short on time I just mention them very quickly uh so one is we introduce equivariant neural field I'm Sor however two main challenges arise when we try to generalize the knowledge of single object shapes to the scene I had I had cut this off but it came on um right in this work the idea was to train the network in a equivalent way way on shapen objects but then be able to detect these objects in scenes where there's multiple objects in the scene all close to each other but uh without any supervision at the seene level all the supervision is at the at the um at the object level and as you can see here the basic building block is a vcoder that takes a point cloud and codes it into a representation that then is decoded into an SDF and you can see that because of the VN encoder as you scale and translate and rotate the point Cloud the output also scales and translates accordingly and then to solve the SE problem again I don't have time to go into any detail but the idea is you simply take your scene and take chunks of the point cloud and try to reconstruct them using this network that knows about shape net objects and this gives you some reconstruction and then you reassign points to the nearest object and iterate it's a class em type of type of approach and you can see that you can handle yeah quite a complex scene that involve multiple objects in close proximity and this a new data set that we created that have shars and mugs the second application that I very quickly mentioned is one where we try to learn equivariant the visual motor policies for manipulating articulated or flexible objects and so again the setting here is that we would like generalize from a demonstration that involves some object of a particular size to an object of of a very different size and since I'm short on time I will skip all the details and then go straight to some to some videos this is some demonstrations uh of uh grth folding object covering and Bo cling in simulation but we actually have tests of this in in a real lab setting where we have two ciling cameras that that observe and give us uh you images and point clouds and then these two mobile robots with skin of arms and here you can see what they were up here is what um they were trained on or or the demonstration and down here is the Target and and you can see some of the demonstrations here and then the application to objects doing the same type of motion but in a very different size context so this these are applications of vector neurons because the fixures that we that are used here are computed using a vector neuron encoder SI is Vector neuron encoder okay okay um I have 15 minutes so I'll go very quickly over the second part uh where we have learned equivariance and um you know if we build a Nerf B of A scene we have some noobs for varying the scene these are the parameters of the MLP that defines the network but verying the parameter of the MLP will not naturally vary the seing ways that are sensible for example if you randomly very the MLP parameters I mean colors will change and so on but you will not see something that is Meaningful so the goal here was to try to it could be like guys approach of working on the w space to work on the W of the network so that in this new w space then varying the weights produces meaningful changes to the scene and this a little bit like a classic um hypothesis from Neuroscience that neurons that fire together wire together which is the soal heier hypothesis so we would like essentially to build resonances between things in the scene that should change together and in the MLP case what we can do is this like we can take our Nerf and take say two pixels one in this View and and one in that view and if we think of the network parameters as being random variables if we add some noise to them then one can talk about the mutual information between those two pixels right and and it turns out that this Mutual information is really captured by the alignment of the gradients of these two pixels with respect to the netork parameters so so so what we would like to do here is to use information about what things coary to align the gradients of the pixels that give rise to things that should coary so effectively we're trying to project the network into a low dimensional space where this these equiv variances are enforced and that's shown a little bit here here we take this scene we have a whole bunch of of points some belong to the same class and some not shown here by colum and we would like after the shaping to guarantee that the gradients of this um of these pixels with respect to network parameters align for the pixels that belong to the same class and we do this bya contrastive learning of course and in this small MLP setting what's going on is that we just train the last color layer of the MLP uh according to this uh to this contrastive learning and it's a very very lightweight training I mean we just have 64 boxes of 64 a pixels across all views and that somehow suffices because somehow contrastive learning is like transitivity of equality if a resonates with B and B resonates with C then a a a will resonate with c one can also do this in 3D because in this network it's possible to compute depth and and push the information back to 3D but I don't have time to cover this but what this gives us is say if we have say this original view from a earth and if we say vary the the gradient of this pixel in the unshaped Nerf everything changes but once we have shaped the Nerf only the resonating pixels change essentially we have learned how to select sematic entities in the nerve and this shows this example again where in this So-Cal Jacobin because we deal with jacobians we are able to essentially learn the semantic components out of this uh yeah contrastive learning and this works even if we very slightly perturb a pixel again we can see the ones that change are the ones that belong to the same class and so now that we have this we can do information propagation in a very lightweight way namely you know so we can take one of view annotate just a few points with a semantic class and then propagate this not only to all the pixels of this view but to all the pixels of all the views because we can see who resonates with the ones we we selected and so this is an example where we annotate just the shown pixels here in this view of this replica scene and then somehow we have annotated the whole Nerf all the views of the scene and this is an example with uh scannet one can do better if you annotate all the pictures in one View and because I'm short on time I will not dwell on that and here you can see some numbers for example even by b a single pixel per class in one view you assign correctly about 50% of all the pixels which is not bad and if you do all the pixels in one view you get up to 85% and even more what you can do is start modifying the SC for example I can take the ceiling and change the color of one pixel and the whole ceiling will change color because now my network has learned that the ceiling points are equivariant they change together notice that this preserves the the you know the lighter Part near the windows and so on so this example show that we have essentially a way to manipulate the sin our Nerf has learn some structure that allows us to manipulate the sin in structured ways and the very last thing I want to talk about is doing something similar but now your nean splatting setting but for motion again we all know that gasan splatting gives us very nice reconstructions of static scenes and they been work also to extend this to Dynamic gaussians many of them work by essentially taking a gaussian and having a small MLP that predicts changes to the parameters of the gaussian position and pose a change in the excord if the exposition of the center The Quon defining a the rotation the taale and so on and now we can use some supervision to tell us that perhaps certain Gans belong to the same object again some is our is our learned equivariance and so now what we can do is in this Mo MLP we can do contrastive learning so that gaussians that that somehow belong to the same object that resonate move together and we can do this without having any motion we can take a static scene and do this so we can essentially using the sub masks take pixels that fall the same mask see which gaans they come from and do this gradient alignment that I talked about um between the Gans that shine versus sayians that give rise to pictures in different objects St Al orthogonal and again so you're doing this shaping in the white space like like like before um and I will not discuss the mathematics behind I don't have time let me just show you some results this is a scene from from from 360 this is a static scene just just the camera moves if we look at this gaussians and if we take a particular Gan on the tractor say this small giian sh here and try to see which other Gans in this whole scene it resonates with we see that after we've done the shaping it has picked out the tractor and notice we have not looked even at all the Gans essentially the this about 8,000 Gan in the bulldozer but but but but in the contrast of learning only about 4 60 of them were touched so somehow the structure of the MLP the structure of the moving agent gives us the extension of the supervision to all the gaussians and having done that now you can see the trctor is moving and nothing else moves here another view of the same motion to show you this is really happening in 3D it is not it is not 2D and let me end with a few more examples of this here's another scene from lur and again this is a static scene the camera is moving I'm going to move the pumpkin here is a pumpkin here is a gaussian on the pumpkin and I show in color the pump the gasan that resonate with with with this this white gassian and it's kind of select the pumpkin so now we can move the pumpkin in the scene even though the SC was origin static and again we supervised very few of the gaussians and again we have learned essentially this equivariance of this group of gaussians through this contrastive learning here another view of the same motion again this is happening in 3D it's not 2D and as elastic here's some more examples of this resonance uh kind visualization and you can see it is not perfect I mean there is leakage here that you can see this is this chair here and you can see that you know it is leaking onto the table but overall it is remarkably good and here the last scene our show this again a l scene again a static scene uh here the camera moves and then I'm going to here move the Apple move the teabag and the Quicky jar and as you can see the Apple move the uh coffee B move and and the the cookie bag move and this the same motion from a different angle again this is fully 3D okay so I can finish on time let me summarize what I talked about um equiv variances can be known to us in advance and we want to enforce them and the vector neurons was a way to try to do that by essentially using these building blocks at a very low level of linear layer no linearity Max ping normalizations and invariance and then we saw how for example this can be used in the case of dcnn and point net to give us very good results and in the case of learned equivariance I try to show how we can build this resonances between the building blocks the low level building blocks of the scene that can be learned using Foundation models that sort of tend to understand the structure and then once we have built this kind of resonances essentially we have bonded the molecules that form the scene whether it's the Nerf dust or the gaussians so now the SE has a structure so if we try to change something other things that coary or equival with it also will change and I think that's the end and thank you and I'm happy to take questions thank you thank you so for the very exciting part so we can take a few from the audience although we don't have much time can you hear me sorry what is and is it a gr Sim 3 what is Sim three Sim three ah the similarity groups Sim three is is is rigid motions and uniform scalings similarities three yeah sorry yes hear I have a high level question so we talk about uh building equivalance with part guarantees like we know the groups Ser we also talk about how we learn that from beta right uh the question is do you have any sense of is there anything between them that we can apply some kind of know structures but also make it arise from the data like for example we can have some kind of constructive way to use the network that oby some some rules that high the absolutely I think I think that's a very exciting area I think in some sense I presented two extremes right the one where you know everything in advance and the other where you learn something really local and the example I was showing is kind of learning G say groupings that give rise to rid emotions but since you have articulation now the motion of my elbow I mean of of of my lower arm and the upper arm I mean they they're different parts but their motions are coupled because of the joint and how can we learn and we know in advance certain kinds of dependencies that arise in the real world so so I think be very exciting to be able to couple priors um coming from the foundation models and priors coming from the dependencies of motions that we know exist in the real world yes like chance that will break scenar thank you yeah that's a very good question the question was um what happens if the equ variant breaks especially in this learn settings because nothing is is for sure and I mean actually I think je and I had this discussion just last night how to do something where essentially there's a symmetry or equivariance that holds almost all the time but you want to also allow uh it to break sometimes and I don't really have a good approach for this right now but but a very nice I think to to work on more questions yes all the mind like how how to in real applications with lied yeah I have to think about that I mean in in the network shape I was showing I talked about your projecting the parameters of the network to a lower dimensional space that enforces the equivariance but maybe it's not really fully projecting but bringing things near a lower dimensional space so you see allow some deviation but how to properly handle that I don't know right now I think it's a very nice question to address yes abstract question can you comment on the relationship between equivariance and disentangling and also um you know in disentangling you kind of think of it in terms of Independence assumptions or you can think of it as know statistical Independence versus functional Independence iar it's more functional sense but I don't know love to know your thought it's a very good question also um and you know when we were doing the vector neurons work we found that U when it came to testing on align shapes we were doing a little bit worse than the network that is trained on align shapes and we never fully understood why that was but my my so my H was that is because there's some fundamental ambiguity between shape and pose that is here is my hand if I twist my hand very slightly is that maybe a different shape in this pose or this shape in that pose and so there are this situations of imperfect this entanglement that I think somehow we have to allow um and uh so you know these perfect factorizations are nice mathematically but in real life they don't always work and how to handle them again in the spirit of the several questions we had it seem there some consensus that this is something important to address for the community okay thank you e e e e e e e e e e e e e e e e e e e e e um Okay cool so well welcome everyone back to the second half of our morning session um so now we're gonna have our third invited speaker for the day um Eric backers um unfortunately Eric is not able to travel here in person but he will deliver this super exciting talk um virtually on zoom and let's welcome Eric um Eric can you hear us I think you're are muted okay yeah I mean I can hear you oh um can you hear me too sorry we cannot hear you let me sorry can you can you speak it again yeah can you hear me yeah yeah we can hear you now okay that's great yeah thank you so much all right so um yeah um first of all thank thanks thanks a lot for inviting me here um be it virtually I'm still very actually sorry I forgot to introduce you that is fine yeah yeah let me read the introduction first so I'm so sorry okay so um Eric beers is an assistant professor in geometric deep learning in in the machine learning lab of University of Amsterdam and in his PhD thesis uh back in the days he developed a medical image analysis algorithm based on sub ranan geometry in the leag group s uh se3 using the same mathematical principles that underly mathematical models of human visual perception and such mathematical uh mathematics find its application in machine learning where through symmetries and geometric structure robust and efficient representation learning methods are obtained and his current work is on generalizations of group convolution of neur networks uh improvements of their computational and representation efficiency through spars grafts and adaptive learning mechanisms and now let's welcome Eric for his super exciting talk all right um yeah thank you very much and um I mean it's a p i can can be there in person but I'm still very honored nevertheless to be part of this amazing program um I mean the the organizing committee has like an amazing track record themselves when it comes to acent deep learning it's just amazing to be part of a workshop purely focused on E so it's it's exciting and thanks for having me um yeah so let me just start with my talk so it's going to be about um a concept which I'm trying to pitch today like neural idiograms and the idea of geometry grounded representation learning and this geometry grounded representation learning that's a concept which where hosting a workshop on at icml uh in a couple of weeks um and this IDE of neural ideogram is basically the vision that what we want what I want and I think many Among Us want is to have reliable representations of con of real world Concepts and when we talk about the real world a real world is geometrical it's physical in in nature at least it behaves for you know um through certain geometric mathematical laws and we want that sort of theory and this tools to be applicable to learned representation so this is what I'm aiming for if we have an image we want to convert it to a representation a symbolic representation like a pictogram or an ID idiogram if you will um which allows us to reason about their parts much like capsules or what's happening in point clouds or molecules and this is something and the idea which we' have been pursuing now for a while and it started off with this idea of a shape space Fe so Auto encoded that maps to a latent space which captures geometry or shape um so I want to start with that and then move towards more recent work in this line of of thought uh about acent neural field so this is very recent work and we just put it on on archive as a preprint and basically it allows us to to relate visual representations to to fields or images if you will and um in conjunction to it we appli this to a really nice application where we do forecasting of PD so evolving Fields Dynamic systems appli to Fields but we do this in latent space so in this symbolic geometric space and lately Central to to all of these methods is uh are some results obtained in our recent I clear paper and this paper presents uh well a general purpose eent architecture for for Point clouds in 2D 3D or homogeneous spaces of the group in general um so I hope I can cover all of them but it might me that I I'm a bit quick uh on that particular paper so this figure illustrates conceptually what I'm aiming for so this is beautiful uh illustration by Picasso you know um making an abstract representation of a Bo it started with a lot of detail and we have several versions of abstractions up to this point which is really a bunch of lines but it's this they relative configuration and this basically it's compressed to a bunch of lights that still capture the essence of the thing the shape itself and this I find super intriguing and this is a concept which we see throughout our society right if we we work with pictograms with idiograms um throughout society as an efficient way of conveying information and here just to get some terminology straight I when I talk about a pictogram I mean an almost literal shape representation of of the concept that you're looking at right this is clearly outlining the shape of a bird but such a geometric representation does not have to be literal it can also be more abstract for example representing the abstract concept of Happiness of being happy right and what I find intriguing that these are geometric symbols but they do not necessarily represent something geometrical and well to me at least I think many concepts are very applicable to getic reason even if it's not concretely geometric um now let's think about some other types of representations that we encounter in our daily lives uh well this is a very much compressed representation of the concept bird it's just a word but it's not really applicable to Computing uh which we as deep Learners uh are aiming for right so um yeah this is then another class of representations it could still represent a lot of things with all the all these numbers but it's less interpretable and it's not explicit at all about how geometic information is sort of hidden in these these numbers so what I would aim for like this is super practical and Computing in neural networks um this less so well we could tokenize it but still you lose a lot of information and this would be ideal right if we were able to um you know reason about geometic relations they had above the the body Etc so that's what we're aiming for and just to illustrate the concept this is just one of many well the typical um representation learning method the variational auto encoder to illustrate the IDE that even if we're able to compress an image to a vector and decode from it we could say okay well this Vector contains all information there is in this image but it doesn't mean that this Vector um actually carries semantic information it's semantically meaningful right um and that's something what we want so we want actually something like this um where we are able to compress an image to a geometric symbol from which we could still decode the data because then then at least some geometric information is preserved and we could start to reason about the relative positioning of geometic features okay so naively this is what we could do right we have an image we can encode it to a point Cloud so we just built a neuron nric that's spit out a bunch of coordinates but obviously this is not necessarily leading to meaningful representations right because if I were to rotate the image the neural network essentially receives a different input and there will'll predict a different output and there's no reason to believe that it will predict rotated coordinates right it just these coordinates that the ne neural network predict is just a bunch of numbers now of course this is unless and now I'm preaching to the choir unless we make this right so with equivalent architectures we know that we can design the networks in such a way that if the input rotates the output rotates accordingly we can actually spit out quantities factors that you know transform in a meaningful way now I would argue that these equivalent architectures they lead to what you could call grounded representations and more importantly in the context of shape like uh I think in many ways a a concept um like a bird is characterized by by a shape right it's really like what it looks like and shape is well invariant to Roto translations or essentially distance and orientation preserving uh Transformations um okay so that's one part why equivalence is important we want to preserve the essence of the data um but this grounding actually with with that I mean like both is image this image is a field or a function over an edian domain and the obtained representation is a collection of points in the same domain okay so in that sense I already share some Comm commonality but most importantly both can be subjectable to uh group actions the same transformation laws that I could apply to the image I could also apply to the to the data points this is really just the principle of accurance but I would like to make a slight conceptual change here in that uh I I like to think of this this type of representation as being grounded in the same input space uh or the space over which these functions live and with grounded I really mean that these representation are geometrically meaningful that really uh points I can really talk about relational structures between objects and parts so a short conclusion up this point what we get with eent architectures is that the representations are physics and geometry grounded meaning while we can talk about transformation laws and relational structures in this representation space um these are some results I'm not going to cover in the stock but uh there's plenty of papers in the equan Deep learning literature that indeed show that these equ methods they are parameter efficient and expressive well because of of a weight sharing principle and they actually provide geometric guarantees such as equivariance and invariance so that's B basically by construction okay I just want to quickly drop this paper because uh relatively reent work by AR MF was about investigating to what extent data augmentation can be used to uh obtain equence and the conclusion was simple data augmentations do not make a network equivalent it might may seem equivalent but it is not and it will not uh generalize to out of distribution date or this equence property and that's it's a point which I'm trying to tell for many years now if you really want equ guarantees there's no way around in using a form of weight tying in your neural network or weight sharing okay uh conclusion to this point acant deep learning leads to grounded representations now I want to give like our first work on actually learning this type of geometic symbolic representations and this was worked on by sh VK about kendle shapes based variational Auto encoders just want to give a quick break breakdown of what we did there um so we know that with equant architectures we can predict all sorts of quantities like vectors rotation matrices tors that transform bya a group actions and so we use this property to have a neural network that predicts a bunch of points or vectors we plotted in here them as coordinates and a pose like a rotation Matrix and since it's equivalent we know that if the input rotates both the pose or the rotation Matrix and the shape rotates and we use this property to canonicalize uh the representation meaning that if we apply the inverse rotation Matrix to the predicted latent we get like a stable a view of the thing because if you if the input were to rotate this post rotates the shape rotates and sort of here in between they cancel out and then you get like a stable representative of the orbit of representations uh uh if you will so right so the the essential shape is invariant but an important remark has to be made here like this um object still consists of objects like a points that are configured relative to each other so invariant doesn't mean that we just get rid of all geometric information it really means information is preserve it's just the poses the object is canonicalized just a brief mention of the vae framework um a candle shape space is essentially a shape consisting of a bunch of landmarks maybe let let's just go over here like a bunch of landmarks and since we want a shape to be translation uh invariant well we could say we could always subtract the mean of it that removes one degree of Freedom we since we use this canonicalization this shape representation is also invariant to rotations as the shape should be and in order to make it invar to scale uh we divide by the the norm of the vector and that essentially Maps it to the hypersphere so if you then use a VA it should be hyperspherical VA and we know hypersphere hyperspherical rep St have some nice properties and so that was an additional bonus of the this framework that it's both equivariant and it effectively Maps representations to the hypersphere okay maybe I'll leave this as a brief remark like in in followup work um we propos a generalization of the classical kandle shape space framework as stating it as not just a sequence of landmarks or coordinates but as a sequence of objects that can transform via group actions uh of the so uh well of the rotation group special orthogonal group um and that means that now we could also predict a spherical signal which is rotatable or a sequence of spherical signals um Etc I know it's a bit suggestive this blot it's not exactly like this but it does generalize to a lot of um um interesting cases some quick results so we apply this this is an unsupervised framework right so no supervision at all so we tried this on on mnist and we thought this was an interesting use case because M are really handwritten digits like symbols themselves and essentially what's happening the neur learns to associate a geometric symbol so we just plot it as such by drawing lines between them and it predicts a pose and interestingly all these sixes or nines I don't know maybe they're upside down sixes or nines they all receive the same um representative shape with some variation just like we would expect variations in your your handwriting um but it still allows for distinguishing sixes or nines based on the poses and and I find this an intriguing concept because in a way it it assigns symbolic representations to the digit six just like you would have across World different types of ways of representing six then well let this be sort of the neuron netk way of thinking about or thinking I should not entrop moriz uh these Concepts um this is what a six look like in the the eyes of a neural network um in the same way such NE networks learn well U representations for other digits which which are consistent in their shape and I just find this an intriguing approach um but then we figured out well this there's one essential drawback from this um and it's like maybe I should phrase it like this like there's plenty or not plenty there's actually some quite some eant Auto encoded type methods but we realize that actually none of them are translation equivalent and this is Maybe surprising given that translation is such a simple transformation group and what I mean with that if I have such an encoder we I stated before well we have a group action on the input space a group action on the latent space but in practice this group action on the Laten space is typically the rotation group and I must mention there are indeed interesting methods that also try to induce translation equivalence but it's not quite do doing the thing uh in an exact Manner and why is this happening it's happening because if you want to compress data we have to do down sampling right so a Max pooling layer to compress the image to less pixels fewer pixels fewer pixels until you're left with one pixel let's say and so these points that you see here are actually Direction vectors coming from the origin and it still represents shape in a nice way but it's not uh translation equivalent so this this bothered us um a bit and we didn't clearly see a solution to this until recently another limitation but this is more of the eent deep learning field in general is basically that we want to explicitly apply geometric reasoning but how do you do this when the geome geometry is only implicit and let me Illustrated as follows like what what I mean with explicit geometry is like Point clouds if I have an atomic Point cloud for example I can clearly reason about relative distance between atoms their angles that they make so because the geometry is really the input um and this is really where equivalent debling shines but if you have image data the geometry is implicit right it's hidden behind the pixels it's maybe clearer in this type of image right it's a 2d projection of something 3D and um like a just put simple only sees a bunch of pixels but it doesn't see a bunch of coordinates relative to each other right so if we want to reason about these things we need a way of making this geometric structure explicit move it from behind the pixels to really the foreground and so that's what we attempted to achieve with acal Fields um so this is recent work by well two Davids uh in our lab I think group has a nice history of producing nice papers by people called David and it's um and sorry so maybe there's a paper coming up with five of them but let's let's keep that uh for future work so this is about grounding continuous representations in Geometry e neural fields and so before I explain that I want to explain a bit about neural Fields um you heard a great talk by ha mares this morning so I think you're somewhat primed to to understand this um these things are ubiquitous nowaday in deep learning and the concept is as follows we consider for example images as Fields as fields that take as input 2D coordinates and spit out in RGB color vector and this field is parameterized by a neural network which your BNS of uh neural network uh weights Theta in this case right so we approximate an image with a neural network and we tune their parameters Theta such that the field looks like the image and the nice thing is we can do this for all these images and in this way essentially we construct a database of neural networks um represented by their weights so these things could be considered representations so in principle the image of a cat or the concept of a cat is somewhat hidden in this representation now this is a somewhat com cumbersome representation to to work with in ding test I must admit there been brilliant work work in this direction you saw a talk this morning also from our lab and and by m us on basically using these neuron netork parameters as representations in themselves and process them in invariant uh manner but um basically this ID also spoke new follow work right and what the next thing was conditional neuro fields and there IDE is if you have an image you could still represent a bit of neural field but now this neural field is conditional on a latent factor and what does it mean U it's conditioned in a specific way typically like you have your neural network layers but these layers are now modulated with uh like a scaling factor and a bias that is dependent on this latent factor so this Laten fact is injected in every layer and that modulates the output of the layer now what does it bring it brings us that now we can share the same backbone neural network so the the neural network weights are shared over your entire data set okay weight sharing is always good um but maybe more interestingly now the conditioning Factor determine what the image look like so if I have a pre-trained neural field with its weights I can change the latence and a different image comes out and this is convenient because now we have a vector representation Associated to each image and obviously we can just pass this through a newal network to make a classifications or predictions and I think this is really nice paper byan D and and Kim at all about well they call it basically funa like you have a data set of functions and well you should read them as suchar and they use this representation in all sorts of Downstream tests and that's also what we're going to do but the difference is that we're not now going to vectorize the image but we're going to associate a latent Point Cloud to this image and this point cloud is going to be grounded in the same geometry of over which the field is is defined so let's zoom in a bit so conceptually what we achieve is that with each image we Associated a latent Point cloud and this is again a bit stylistic idealized but we'll see some real examples later on um yeah where we really represent an image with uh let's let's call them just Parts just like in capsuls and they appearance vectors and we have a bunch of them they configured relative to each other um and we do this also in the same type of framework we fit a base architecture and I'll say in a minute what this look like um and obtain these points like Point Cloud representations for for all these images and we do this in such a way that these neural fields are equivalent meaning that if I transform my lat and point Cloud the the decoded image the decoded field is transformed in the same way so we have this commutative uh diagram and this property is classically known as a steerability property where we say well if we transform a function just as a function this is a regular representation the group action acting on the domain we could also transform the function by just leaving the coordinate grid as is but transform the representation uh in contrast to the classical steerability uh sense um these do not represent the basis coefficient but the latent Point cloud and because this is a point Cloud we have a a well- defined group representation or group action defined on this representation set simply by transforming the latent poses these POS I'm not sure if I was clear about that but these po poses are elements um in the group so this represent for example rotor translation and we could decide to pick different groups for example the translation groups and then we don't have rotation accurance okay so a little bit about how did we achieve this um well first of all we want a steerability property because that means that the latens are grounded in the same geometry over which the field is defined um and then we we looked at this and this is somewhat known result in E lature many proofs on um equivalent architectures come to similar conclusions when it come to kernels or one argument kernels basically if you have the steerability property that really means this function which is a function of both the coordinates and the latent if it's steerable in such a way that really means it should be invariant with respect to both arguments and okay so we need by invariance of this function so that's one Criterion so weiz okay that's something that we need to cover secondly we want the representation space to be flexible so we want it to be a set and we want the set to be carry geometric information so it almost automatically becomes a point cloud in some getic space and so when it comes to set latent representations for fields we actually took this nice paper as an inspiration the 3D shaped to vit paper this uses a latent set but did not carry any geometric information so there was no way to make this equivalent and that's well conceptually the the symbol fix that we apply to this in addition to figuring out what equivalence means in the context we basically made the latent Point clouds well the latent set latent Point clouds and so that means if you look at the attention um uh equations since we want all this entire field to be invariant with respect to transformation on x and z simultaneously we should look at every instance where we see an X and a z or an X and a latent point and so we have to work with by invariant attributes and this was precisely which we derived in a recent I clear paper on Fast expressive equalent networks um maybe I'll say a bit about this at the end of this talk um but really that's the essence so we have a Transformer architecture which we condition on par wise attributes between coordinates and poses and these par wise attributes to be by invariant and that's what we in this paper we list all possible invariants um yeah and so so this is then summary of of the method so we have an image we could fit a latent Point Cloud to it and this latent Point Cloud generates the image via a cross attention method so it really also means that these latent points they are responsible for uh generating a local part of the image and this has some nice explainability um benefits um same we we appli this also in 3D um for example to fit shapes with s distance functions and we achieve this equiv property uh which follows by making sure that the the attributes that we use are invariant with respect to both slots okay so we started fitting images and first it was a sity check like okay how well do it represent images does it indeed satisfy the the translation and rotation uh e Properties so I said before like the only thing or the main thing to take care of is making sure that these attributes that you condition your Transformations or your Transformer on both the value Transformer and attention that these attributes are invariant and then you can choose which group it has to be invariant to so if you choose it only to be translation invariant uh then you see well if you rotate the lat in point Cloud something really distorted comes out uh but if you translate it it it indeed preserves the translation AC variance and then there's different ways of coming up with invariant attributes and in the paper we deriv which attributes are actually optimal in a certain sense um so then you see what we can steer these fields however we want so if you look pure at a pixel uh sorry Peak signal to noise ratio on cyer SL a and10 um this is really a measure of how well are these neural Fields capable of representing images in the first place and over this entire data said with the shared neural network parameters with with specific latent fitted to the image just like in the funa paper which we took as a baseline we see that we got excellent uh well fitting capacity and recall that for the fun paper we fitted factors and an all work we fitted these latent Point clouds and I think partially why it works well is because of the general equivalance principle that we have been preaching like we have an increased form of weight sharing weight sharing is good um and we have actually um a notion of locality so that really we are able to local um to share pattern over local patterns right so if we yeah are able to construct a local signal then we could also do it here and here and this also improves data efficiency but um the main thing that we wanted to focus on is that this representation are actually meaningful right semantically meaningful so that's what we can test by passing these things to a neural network so in this case it should be like a permutation invariant graph neural network if you process the weights of a neural network if you have an MLP or like a vector representation it could be just an MLP and in our case now we can use an equivalent graph Network that is capable of learning to see uh patterns in terms of relative configurations so this is what I'm really excited about now we make the the whole tool set of equalent deep learning for Point clouds applicable to image data in a certain way yeah so this is then what the results look like um again funa as a baseline which fitted all these vectors and indeed it's hard to sort of reason about the content of this VOR right still you get quite reasonable results on Cipher 10 it's not you know it's not the best but uh if we just out of the box learn or uh apply a graph Network on the latent Point Cloud um we get 82% accuracy this so what it tells us is that with the same fitting Paradigm uh we actually get quite an increase in representation capacity or semantic meaning let's say of these latent representations because now all of a sudden the getic information is explicitly present yeah this is just a nice figure um it's it's about well editing of neural Fields because now we have this locality property we can start to edit really um structures locally and just to illustrate what we did here if you have a fitted Point Cloud using our acal fields we can decide to take uh the points of one image and just swap the two halves and it stitches it together and what I find surprising is that it kind of stitches it in a somewhat natural way right that the neck is nicely outlined it's not just that it cuts like a border like this but through this cross attention we sort of see like a soft segmentation of of parts and I think this is a nice feature of these acal fields yeah so maybe the final part is going to be like another application which I'm excited about and it's the IDE of pde forecasting in latent space again with the same Davids uh bed by an amazing team of PS students and and strats with a lot of experience on on dynamical system uh modeling so the idea here is maybe let me first skip to the next slide so if you look at pdes these are really just fields that evolve over time right they're governed by a certain partial differential equation of PD so if you have an initial State we let it evolve so there's a bit of flow involved there's diffusion going on um so we see this patterns evolve over time and this is what we want to model so we want a neural network oh obviously there's amazing PD solvers but this is just to prove the concept that we could also solve these PDS in a very efficient manner via neural network surrogates and the approach that we took here is that if you have an input field we could encode uh a latent representation like this symbolic geometric representation that I've been talking about an eent encoding we use principles of meta learning and auto decoding to to map an image to its latent representation then we learn an equivalent neural OD so basically an equivalent function that predicts what the next stage should be of this lat Point Cloud uh let it evolve and then read out um the the result afterwards and also this is aent because the norm field is equalent so we have a fully endent equivalent method to process data in a latent uh representation and the results are actually quite uh impressive um so this is our predictions and it stays quite close until quite some steps into the future uh relative to the ground rout now there's obviously very good methods already out in literature that does this the gfo is is one of them um and what I want to stress here is that we come certainly close to um to these methods um but maybe most importantly somehow our method is way more efficient when it comes to data or fitting Solutions or learning Solutions based on less observations because that's the nice thing about neural Fields is that we can arbitrarily sample pixels in the image it doesn't have to be every pixel it's just a neural field so we can do it wherever we want um and so we show that even 5% of the sample pixels we barely see degradation in performance and I think this is really opening doors for very efficient and um scalable methods for you know learning Dynamics in data yeah and this is just to show that this concept of acal field is not limited to images um I said the main thing to figure out is what are the attributes on which you're going to condition the Transformer and basically you can do that for different type of geometries and different type of symmetries that you want to preserve and here we did this for for the shallow water equation on the sphere uh also there we got nice forecasting methods and um it works well also in sort of upsampling uh super resolution approaches we did the same for the internally heated convection uh in the ball and okay this is just a visual of indeed AC variance is indeed equal to data efficiency so we see that we can are way more efficient that we can quickly reach great performance with with less data yeah so I think I have maybe have a couple of minutes left so I just want to give a quick pitch on what we use in all these methods so I already thought about these invariant attributes and I talked about this equivalent neural OD and what we did was a re use a recent um equant architecture for Point clouds um so we presented this at I clear a couple of weeks ago a month ago sh and I ran this project backed by our amazing team so what does that paper offer it really offers a practical method and code both byor and Jacks for acent Graal networks uh it presents a complete set of pawise attributes to build any agent uh scn agre Network and basically we add a more conceptual wouldn't say novelty but a reiteration of the statement that convolution is all you need um okay okay what I'm going to do is just show a bunch of results to imp you know to convince you that uh the method is really uh useful so you appli this for energy prediction of of Mo molecular states and their confirmations um so we really state of the art results um but actually in a way more efficient manner I'm not sure if you see it but neck Whip and this is with their optimized code it takes 20 seconds per ook in our code we we reimplemented that for us it took a minute so they did some clever tricks to speed it up in our code it took only six seconds per eok uh but still outperforms them and the nice thing is that almost all these methods are based on tensor products on CLP Gord and tensor product and the steerability or tensor field principle H and ORS is based on invariance alone uh just like gemet but in a slightly different way and I want to make the statement that pon actually falls in the class of group convolution methods just like neck web and many other methods it's just we approaches from the regular group convolution point of view and the summary here is that our simple method is as expressive as tens field network but it is as simple as egn in the sense that we do not use CP score intens Pro and only use Scala values so that also means we don't have to worry about what kind of activation functions we use uh additionally we we rely on actually also another paper by u a guy Maron together with uh de to show that these methods are or pita method is a universal approximator and another nice perk is that we named it after a Pokemon a fire Pokemon which is super fast and okay so okay we found that applicable um finally um it also works for generative modeling of Point clouds and here I again want to stress that it's important to be able to reason about relative configuration of points and you can only do that if you if your network in its representation carry directional information and that's what our method does whereas egn it doesn't it only works with invariance and Par wise distances but if you need to reason about are the bond angles correct and the distance is correct or like the relative positioning you really need directional information so if you incorporate that in our hidden representations we got like a a nice boost in performance and using otherwise the exact same uh diffusion model yeah so we tried it on aent point Cloud prediction I stated before maybe that this method is applicable to point clouds on homogeneous spaces of scn in general so also position orientation space for example we could treat the bonds as primitive so a bond has a starting point in a certain direction and now we can send messages between edges let's say and this also adds to expressivity but maybe another conceptual thing is well if you have a surface we could think of it as a point cloud of normal vectors and now by sending messages between the normal vectors you can directly take also curvature of the surface into account yeah and other times you need to predict Direction vectors for example in this Dynamics for casting um okay that's what our method is able to do and just to show that it also works for 2D we included super pixel amnest and got state-ofthe-art results on that this is going to be uh one slide in general framework of message passing you want to use getic information and typically you do that via pair wise attributes and that's what our paper derives we derive a full set of pair wise attributes for different cases if I have a point cloud in position space and we want to be invariant to roted translation groups and obviously the only thing that you can Leverage is to PA with distance uh we figured out what does it look like for the group or position orientation space remember the set of normal vectors then we have actually Four invariance that we can leverage and nothing more um and there's a nice results by uh by olver or in his book at least on basically that these sets are basically a set of fundamental invariance meaning that every other invariance could be obtained from this set of invariance so that really means this is all you need yeah um maybe let me jump to the conclusion well so there's a whole bunch of convolution uh so let me skip that in the interest of time um so what I presented was basically a framework for learning geomatic representations which I framed as neural idiogram like geometric representations of things that could be abstract in principle and it reaches equence Beyond just rotations right and this is I think an important thing we show that these representations are more geometrically meaningful relative to the plain Factor representations by this classification experiments and also by showing that we can actually use this in Dynamics forecasting such as in in pdes what I'm really excited about is now for me this feels like yeah now we start to develop tools to make the geometry explicit and it's not that we really do a scene reconstruction it's just that we learn an abstract representation of the scene um but this abstract representation is grounded in Geometry so it still allows to reason about uh you know relative Transformations and all and I think this is uh interesting and this is something that our group is going to work on the next coming years and okay what is a better way to end the talk is is like show show some nice pictures again and so I want to end with the picture of Picasso really we're we're going for this ambitious goal of turning image content into a compressed symbolic geometic representation that still preserve the essence of it and at the same time and maybe this is bit a wishful thinking but I'm think we are on to something is that this could provide a solution to the prevalent texture bias right that where our neural networks tend to focus on pixels colors texture rather than actual shape and that's why I like to to show this Fus uh picture there and um yeah maybe okay with that I would like to conclude this talk and H thank you very much thank you so much Eric for this exciting talk I really like what um the sentence you said in the in our slides like how do explicitly apply geometric reasoning when geometry is um only implicit so it's so exciting and we can probably take a few questions from the audience if there are some um okay so um actually I have a few questions so um from some lowle details to also the high level idea so um the first one is that I really like how you apply like local features and or like combine local features with equivalence so I'm wondering like um first of all like is it also possible like like I noticed in your work that you're like you're using these local features to better represent like Global equivariance on like rotations or transer uh like translations but is it also possible to like combine um apply these local features to like local transformations to like study like local equivariance like for example like multi multiart objects or deformations kind of thing yeah this is a great point and this definitely something that I want to explore uh next is if we have this encoding and currently and this is really early stage work in the sense that our encoding is somewhat naive we just uh initialize these latent points on the grid and then move them with gring descent this outo decoding principle to the right locations but you can imagine that if we spend some time on working out how to do this encoding which now we use meta learning for but I'm thinking of more sophisticated or different a different approach to this you can imagine that if you fit this image um here's the ear that one of the latest let's say it it lands there and another one there but if now we have a different cat and the ear is over there then you hope at least that this latent moves in that direction at least you can argue that if this ear is fitted shifted a little bit yeah this latent would fit also a little bit and actually we could steer that in our Auto decoding by only updating the geometric information and so what I'm saying here is is that this encoding scheme has a notion of uh well dimorphism invariance right or equivariance in the sense that if this is slightly deformed then I expect the local Point Cloud also to be deformed and yeah this would be really amazing if we get a hold on you know how to properly well first of all define theomorphism equivalence because I don't think you can fully do this but maybe in a statistical problemistic sense and um yeah so yeah I think we have some notion of Lo acence because each Dent is locally responsible for generating um the image content yeah thank you so much for the discussions actually I have a couple more questions and also we have a question for uh from the audience on Zoom but probably we're already running into the time for next presentation so I guess we have to cut up here but if you have like more question to Eric you can probably email him I guess are absolutely and you can find me on Twitter to um there's several ways to reach chat and please feel free to do so yeah thank you thank you so much thank you again all right you're welcome um so now um following er um um and the first one is going to be uh leave learning frames for 40 sequence understanding so actually a while ago during the break like someone also discussed with me that like um like a lot of like works here are discussing about like you could variance of static settings like static images static static Point Cloud so whether there could be some um applications or like combinations of equivariant you like Dynamic settings when you have like compow sequences so now we have to work addressing this aspect e e e e e e e e e e e e e e next okay uh so this is the agenda for today uh I'll start introducing the task uh which I assume uh many of you may not be familiar with the task of neuro weather modeling uh and then I'll present two different models uh that we are working on uh to solve this this uh problem uh and the main messages here is I think the time is right for uh applying equivalent uh near weather models or applying equence and ideas from geometry learning to this task and but also I'll show that uh it might pay off to work on scaling uh these equivalent models so um many of us uh design these new models and only try in in small toy problems I think it's worth putting the effort and try to scale to to real large problems right just to start uh with the task that we're solving here so the input here will be the the the state of the atmosphere so this is a bunch of uh physical variables like humidity pressure temperature uh wind speeds uh and we'll have that several uh altitudes uh on the on the atmosphere and uh we can have one snapshot like a t zero or we can have more than one like the past up to now and we want to predict the same variables uh in in the future it could be uh and there are many ways of doing it uh it could be predicting in one step the time the the the variables like 6 hours 12 hours uh 18 hours 24 hours ago or or forward or it could be Auto regressive uh and one thing that we should notice here is that this inputs could be scalar Fields like temperature or it could be Vector Fields like the winds speed uh so this is this could be Vector fields on the sphere so this uh people who like geometric de learning will love this this kind of data uh and I think the most common uh approach uh from a high level are Auto regressive models so we'll have uh one or more snapshots as input and you're predicting a small amount of time forward and then we iterate the prediction using our our predictions as the input to the next step and uh since we are in a computer vision uh conference I I'll relate this to to the problem of uh video uh generation or generating a video from a single image uh or from from a set of frames like complete the next frames or predict the next frames of the video and uh we have really a lot of amazing recent work in this area and I want to keep this in people's mind so this is something uh we might work towards how to apply these ideas to the to the neuro weather modeling task uh the main differences here is that uh our inputs and outputs are not pixels they're not RGB pixels they will be spherical uh scalar and Vector fields and also this weather forecasting tasks it has a very strong physics uh prior right we know uh well we don't know exactly but there are some some part of the physics that that govern this model is is well known and uh we could think about using it uh and the data set that that is most commonly used for this task is called the er5 and this is a reanalysis data set so this data set will come uh this is like the just just the size and the numbers right we have accurate data for about uh since 1979 so this data set has hourly samples since 1979 and the resolution is 0 25 by 25 degree uh and we have uh 37 vertical levels and 16 variables sampled over it I think the most important part of this slide is like this reanalysis so how do we get this this weather data uh almost continuously all over the globe in a uniform sampling uh the answer is we don't right we we get data from a bunch of weather station and and balloons and non uniform sampling and we actually run some simulation models to to predict the rest of the state of the globe uh which is kind of a limitation we'll talk more about this later and now I I I'll show some current approaches uh for this task and I'll put them in this axis uh more learning and more physics so on the left here or the right for you uh are the models that will take into account the physic and the symmetries the things we care about and on the other side we there are some kinds of models that we just do machine learning over the data and don't really care about thewi structure or symmetries so the first is the the extreme of the the the physics based model there's no absolutely no learning this are the numerical weather models uh this is just uh solving the the equations that we know that that govern the weather so this is Floy Dynamics thermodynamics and the equations come from things like a conservation of map momentum and energy uh and this is what is running in most weather forecasting systems nowadays uh it's changing slowly people are incorporating more and more in machine learning but I think it's still the dominant way of of predicting the weather and here on the other stram we have this Transformers uh based model uh which uh will not incorporate any kind of physics or symmetry they they are mostly based on on something like a vision Transformer that you you split a projection of the the the atmosphere we project into a plane splitting patches uh pass threats to to some some uh linear embedding and then just have big transformers uh operating on it so it's lightly uh to the to the right here of the big Transformers are the graph NE networks so here this models will take into account at least the the the the spherical uh domain of this data uh and do message passing uh and uh but in the current instantiation of these models they they still cannot handle the the vector Fields as Vector Fields they will treat Vector Fields as two scalar channels and there's no relationship as a vector and they also have to choose between being either rotation variant by averaging over neighborhoods or by incorporate and in that case if they rotation variant they they don't they are not sensitive to to patter of specific orientations uh or uh or they they work around that limitation uh by incorporating like Edge features or 3D vectors Edge features and then they lose the the actual rotation variance and they are not really equivalent we also talk more about this later and I want to highlight this very interesting uh models family of models called neurog GCM so this is also very close to the numerical weather model so so what they did here is they implemented the whole numerical weather forecasting model in a differentiable way in Jax and they they have just a very simple shall near Network to correct this model so this is very close to a numerical weather model with this option to fine-tune it or correct it uh correct for the factors that that we can't really uh incorporate uh numerically by training from data and using in your network so this is almost all physics a little bit of machine learning uh and things I'm talking about today are right here in the middle so I'll talk about spherical CNN and equivalent GNN for this task uh and U we don't incorporate any physics but we incorporate a lot of symmetry so we care about uh patterns that appear vector vector fields or scalar Fields there there are some local patterns that will appear in different orientations and we want the models to be equivalent to handle those kinds of patterns appropriately now uh we get these kinds of questions a lot so first why why are we doing why are we using geometric deep learning uh in the age of uh large language models and Transformers uh the typical answers are okay we we can leverage the input structure the structure is uh is on the sphere so we should take that into account uh and uh because of we take into account the symmetries of the data we also can have data efficiency and and improve generalization performance uh but this is something that that that should uh scare everybody in this room so the bit lesson if you're familiar with this uh this this I think was a blood poster uh where where rich Sutton uh is saying that U there very a lot of clever people thinking about clever modifications in this models putting more bias in the models uh and they're almost always uh trumped by a new model that has a bigger uh more capacity and more data or better data we will overcome all the the the uh interesting things that we try to incorporate in in our models uh but in particular for this task that we're talking about in other tasks I have some counterarguments uh for example the weather uh uh is data limited when I talked about the ra5 data set we have good data for roughly 50 years and if we want to double that data we have to wait 50 years and collect the the data to double the amount of data that we have so this is still hundreds of terabytes of data because it's very finely sampled and there are lots of variables and it's sampled hourly but it's still limited and the other point here is this point of fuzzy versus exact symmetry so when we talk about natural images and CNN we talk about translation quivalent kind of saying that the distribution of RGB patches over natural images is kind of independent of the position of dis patches which is kind of true but not really uh in the case of the weather forecasting here we have laws of physics as as a prior right so we know that the physics is there are absolute laws of physics that always apply so we could leverage this better in this in this case this taking that into account should help now the question that's why I spherical CNN so this is the first model that I'll talk about uh first is I said before to take the domain to account and this is I'm showing here what happens if you try to project the experential plane and apply a regular filter like a a CNN or some sort of patch based Transformer as those those models that I showed before so we have heavy distortions over the post that that your model will be forced to to handle and and allocate capacity to handle that and another interesting thing of this C Is This Global uh equivalence to 3D rotations so here we we are projecting this uh this uh spaceship this X-Wing to a sphere and these are learned features from the spical CNN and we see that theal rotation so as much as we rotate the input the features will follow and this we Tak some sol average we have an invariant feature approximately due to discretization and this is very useful for other tasks not weather for example if we're doing shape classification or or shape segmentation and the shapes can come in arbitrary orientation uh this would be helpful or if we're dealing with for example molecules this is another application we have quite good success on the molecules is the same no matter the frame of reference or no matter how you rotate the atoms it's still the same molecule still has the same properties so we do really want something like 3D This Global 3D equivalence uh to handle that but in the weather we don't really have that right the the Earth has a specific position in space we know that the specific relative position orientation with respect to the sun uh there are parts of the earth that have sea or mountains and all those things influence weather and they are in specific positions so we're not we don't really care about rotating all the day in the atmosphere that that we're giving the the correct prediction what we care about is a little bit more subtle so uh in this example I'm showing some patterns uh that appear repeated in different positions and different orientations so this is one field humidity I think and we we see some sort of patterns that that will repeat uh in different position orientations and if we have localized filters and an equivalent model the same filters are useful for both kind of patterns either for for uh better recognition or for Generation so this is why we want equivalence even in these kinds of tasks and uh even more so uh in this case here I have a vector field so this is like a wings and now we have this Vector field so each little curve there is like integrating the vector field a little bit and we can see that the similar patterns appear and they are rotated with respect to each other and we would like to not learn this as two independent entities we like to share the filters that can that can represent this or or produce this now uh I'll go on a little bit about how spherical sentence work uh and the main idea here is the idea of group convolution uh and to to explain this uh I'll start with the simplest form of convolution so this is the convolution on the real line uh I have a convolution of two functions on R and uh we can visualize it as excuse me as like I sliding filter Computing inner products and the main idea of group convolutions is to generalize that to to a general group so if you look at these two equations they look very similar so on top we have the normal convolution we have an integr over the real line now when we're doing convolutions on a group we're integrating over the group and uh the the action on the real line is just a coordinate shift so that that's a a subtraction here uh when we're in the space of groups uh this becomes a group action and one more thing we're going to need is the for series and again I would start with the simplest case of for Series so this is a for series of a function on a circle uh and this is what most people might might be familiar with we can expand any function on the circle in the sum so this is countable right in a countable number of basis elements that are complex exponentials with their their free equ efficients and again this there is a very nice beautiful theorem that generalizes this idea to to compact groups so the same way you can expand any function on the circle in this basis of complex ials you can expans expand any function on a compact group on again a discrete countable sum of these Matrix elements that are related or they actually the the group representation so those the linear representations of this complex group and why did we talk about PR series because of the convolution theorem right so that's another thing that that that we some people might remember so if we have those functions on the circle we can compute the convolution between two functions so the for transform of the convolution is a pointwise multiplication of the for transform of the functions and uh luckily this also applies uh when we generalize it to compact groups so we can com we can compute convolution between uh functions on compact groups by multiplication of the fora coefficients uh but now as I said the for coefficients will be matrices right The Matrix elements are are the bases uh are the the group representations are are the Bas and you can arrange them in matrices so each for coefficient is a matrix and the convolution theorem is actually multiplying matrices here uh and uh we're talking about compact groups uh and the group that is of interest to us is the group that acts on the sphere so that's the group s SO3 is the group of rotations in 3D and this is what those representations look like uh they are uh matrices of odd size Square matrices of odd size uh and if you want to compute the convolution between functions on SO3 we are going to multiply these matrices one by one so you can see each Matrix has one frequency right the for series on the circle each complex exponential has a specific frequency here the frequencies are related to to the size of these matrices the larger the matrices the higher the frequency and they are independent so we can uh we multiply them independently each one of them to to compute the convolution and this is one way of doing uh spherical CNN is by uh first you lift the sphere to the group s SO3 and you compute all the convolutions on the group S3 and this was actually what's been done in 2018 by our friends uh Tak Co and maxw uh the problem with this is that SO3 is a much larger space space than the sphere and those for Transformers are very expensive so our approach is will be a little bit different than that and we have to introduce a little bit of more Theory uh first is the for series on homogeneous spaces so the the sphere is a homogeneous space of of the group that means the group act on it transitively and uh we can represent the fora transform of a function on a homogeneous space of the group uh Bor from the for transform of the group and what will show up is some sparcity patterns so those Matrix will not be dense anymore we only use some columns and or rows of these matrices uh so the white here will be all zeros and we can represent the homogeneous spaces is a smaller space in the group that that's how the the fre transforming manifest and then you can compute convolution the same way and you can compute convolution between different homogeneous spaces in this way and depending on this sparcity patterns uh your output could be uh in another homogeneous space or it could be in the whole group itself uh luckily this will simplify a lot for the sphere because uh when we relate the sphere to the to the group SO3 we only need one column so from all those matrices that that represent the bases for S SO3 and the frea coefficients on so SO3 uh only a single column is used to represent the same things on the sphere and uh this might be no surprise so the Matrix elements of each one of these columns are exactly the spherical harmonics right but what else could they be uh if you want to expand spical function in a basis the spical harmonics will show up but why did you define things in that way because now we have a very nice form of convolution uh on the sphere that is multiplying these matrices that that uh are their despir uh representations SO3 and again we have different frequencies uh and we can multiply these matrixes to compute the convolution and what we will notice very quickly is uh once you actually go and do this this matrix multiplication uh imagine f is your function and G is your filter you notice that you will not use all the elements of your filter so this could be two arbitrary functions uh but the result of the convolution will not use the whole function it will only actually use this Central or uh one of the components of this transform is actually the the frequency the m equals zero if we're talking about harmonics uh which is kind of a limitation because in the spatial domain if you're only using that single parameter to represent your cical function you don't have an arbitrary spherical function you have axis symmetric spherical function so these functions they are constants per latitudes anyway this is what we did also in in 2018 uh it is much faster than than the S SO3 base but we have this limitation that uh the filters are kind of symmetric so there's it's less expressive than a fullblown model but we also work around this limitation a couple years later uh by leveraging what we call the the spin weighted aspherical functions so this uh this we're introduced in physics to study gravitational waves and uh they are complex values function on the sphere where the the phase of these complex numbers will change based on rotation so if you rotate your your your domain the only the phase of this values will change so this example here is when and the phase changes according to something we call the the spin weight and uh in this example I'm showing a function with spin weight zero so this is exactly a function of the sphere and uh so this is no different than what we've been talking about before right the magnitude in the phase will rotate as we rotate function once I have I sping one function the magnitude will stay the same as it rotate but the phase is is rotating according to to to this rotation and then there's another beautiful theorem that shows that uh these spin weighted spherical functions also correspond to some sparsity pattern on the on the on the representations on3 and it turns out that each spin that we add will add a column to this representation so when we're dealing only with spherical functions we only have the middle column here now in this example I have a function I have functions with SP zero and one so I have two columns and then when I do the convolution my filter instead of having a single parameter my filter now has four parameters and this filter this operation is mixing this right it's mixing being zero one and another nice thing of this idea is that this since these are complex valued functions of the sphere we can see them as Vector fields on the sphere so this is a way to represent Vector Fields so we can have filters that are uh scalar and Vector fields and inputs that are scalar and Vector fields and the convolution will mix both the scalar and Vector fields and crucially uh it is equivalent to rotations right so you can see is the vectors are rotating the way you would expect right this is something that if you try to represent a vector field as two channels true scalar channels you will not get this property right and this is what we did uh also few years ago in Europe called the spin weighted spherical CNS so this is all old uh work our most recent iteration of these lining Works was called scaling spherical CNN and this is uh where we we try to well there's a bunch of engineering but there's also some some model modifications we try to make this bigger and Tackle real large problems uh with this task and the main result is uh this figure so here uh these spherical CNS that I mentioned are all have orders of magnitudes uh fewer uh numbers so the AIS here like the numbers of convolutions in the model that's a measure of capacity or expressivity and also the size of the feature Maps because for some tasks like the weather forecasting we want very high resolution inputs and outputs so we can see that in both axes we we could scale this model to to orders of magnitude uh larger models than than what we had before and uh maybe one interesting uh thing that we also included in this paper is that I think for most equivalent uh models there's always this problem what can we do at nonlinearity right our nonlinearity needs to be equivalent there's a lot of research how to do good equivalent non linearities I think both the organizers here have have papers working on that has yeah uh and uh we found one that actually worked pretty well for for this particular case that is inspired in in one paper by by malat stepan malat group that is called the phase collapse so here if you have here what we have are complex uh Fields very complex valued uh fields and uh we really care about the phase right we don't want to break the phase because the phase has to follow that specific property that has to follow the rotation so we can do stuff with magnitudes we can't really touch the phases and if you have spin zero we are kind of free to do whatever you want right so what we do here is our nonlinearity only up updates the spin zero because that's where we're free to do whatever we want and it will combine the spin zero itself plus the magnitude of all the spins right so this it's very simple and it turns out works pretty well in practice and it's also probably the the alternative here would be some sort of gated nonlinearity that some of papers do that you predict a whole uh set of new channels to multiply together right to to get some sort of nonlinear operation so that makes it much more expensive right you're kind of doubling your number of channels everywhere just to compute the nonlinear and this compared to that this is almost free okay so we with this paper we we released uh a Jax Library so this is uh I I put a lot of effort optimizing it uh different ways of implementing things seeing uh which is faster but I'm sure there's still lots to do there there there's two ways to make this much faster uh given it's in Jacks distributed training is is a breeze so it's very easy to distribute in as many nodes and tpus and host that that you want and we include these operations these normalization layers nor only AR is the things that we need to be particular about when when we we're building models uh and this enables this problems up to 100 times uh bigger than before and this is available for a while now let's go back to to the weather forecas and show uh what this models can do uh we implemented this uh Auto regressive uh spherical CNN for for weather forecasting and this is the size of the task that that that we're tackling so this uh we're doing 256 by 256 resolution and we have 78 inputs and outputs uh as channels so this follows really closely this uh the the work from kaisler that kind of kicked off this field of numerical weather modeling and and it was a graph a graph neur Network so the architecture also follows like number of layers and and we do a single round of down sampling here so it's most of the channels inside the the model will be 128 by 128 which is quite large and this the delta T here so we are predicting six hours ahead so every step of the model predicts six hours ahead then during training we are predicting up to 72 hours so we predict we roll up 12 times during training and back propagate through all of that and then during inference we can roll with well we can do more but for the for the benchmarks we did up to 10 days so we train like looking at three days of data at a time but then it can extrapolate because you can just keep calling the model over and over over the same inputs and we did that up to 10 days ahead and this is uh one example of kinds of results that we get so this is humidity which is one of the most uh interesting fuels because they're very fine patterns that repeat and here I think we're rolling out up to uh five days and this is an example of the wing so this is the vector field uh prediction that we are uh integrating to to show this this effect here these are wings predicted by by our model and and the color in the background here is the temperature right so this is a just a baring table so I think the main uh bottom line here is that we we try to keep this model as close as possible to to the graph Baseline uh number of parameters training schedule uh inputs and outputs all kind of the same and we do better here in most uh I think all of them up to five days we do better in all of the all of the the measurements all of the metrics uh once you go to seven and 10 slightly worse but that's where the the forecast is is really bad anyway so for for both m models when you're doing 10 days ahead it's it's kind of so now we speculate uh very happy with this result so it seems like uh it's worth doing it uh fully equivalent model this models on the sphere uh spherical SS for weather seem to work but why so we can speculate a little bit so why why what is causing these improvements uh first is uh this I want this to be true to be the reason that is our filters are locally equivalent and are directionally aware so in the graph models you can have one or the other uh and you cannot really have the local equivalence you can have well in the regular graph model not the equivalent graph models you you you have to choose right if you want an inent model only you can only average over neighborhoods but then it's inent you are not directional aware anymore and the way these models this current uh graph your network where the models the way they achieve this directionality and they all do that is by taking uh the relative position between nodes like as a 3D vector and passing that through an MLP and using that as features that then you lose equiv right it's no longer equ once you have that so they cannot reuse the same filters to uh do par recognition or or to predict these kinds of patterns here that that are Circle in red at this Vector fields and and also they can't handle Vector Fields they handle Vector Fields as this two channels right the the west and the north Channel and they treat as two independent quantities uh but there's another well maybe both things are contributing so in the spherical CNN uh we have our filters are actually have Global support and we in fact go through a lot of pain to to avoid that so we have some smoothness uh in the Spectrum to try to enforce the filters to be localized but there still these are real filters from from the 2018 spherical s and you still see that they they extend all the way right this filters here on the top uh they are most of the energy is near the poles but they extend all the way and uh the GNN have a very small receptive field right usually you have one hop and we are talking about like 256 but 256 graphs so how many layers do you need of a GNN to take the whole is a lot of layers so they are much more Loc going have much smaller receptive field uh let's discuss more small versus large receptive field so we used to think uh that uh we should use more receptive fues right so that was the common knowledge at least for CNN we can learn hierarchical representations uh it scales really well we can just have a bunch of layers with three rri kernels and have really lots of them uh to to cover your receptive field and this locally it this local equivalance property that that I was talking about also relies on that if you have a global filter there's no local equivalence right because your filter is taking the global so so we actually do need that uh the downside is that you you need more layers to cover the input uh which for CNN is not really a problem because they are so cheap but for graphs it's actually a problem because the graph convolutional layers are more expensive so you don't see graph convolutions with hundreds of layers the the paper that we're comparing it also has eight layers like like like we do and uh great argument for why using a global receptive field so Transformers they don't care about receptive Fields right the vision Transformers you embed each patch and the Transformers is looking at all Patches at the same time and this thingss should be the State ofthe art in most of the vision tasks so I think nowadays uh Global receptive fu is fine uh and this is what we actually can get uh with our or uh spical filters uh I didn't mention here that I also said learnable receptive Fields uh so by Design they have Global support and but since we are learning these filters in the spectr domain the model can choose uh to to learn a representation such that the filters are are smaller and we actually encourage that so maybe our models are just learning the right size of uh of U receptive field for for the task Leverage The Local advance but uh also take into account the global U representation of the problem right and another thing that I used to say a lot uh this used to be like a I light saying yeah we we're still very far of of being able to tackle and there's no way we can go around the complexity right so if we compare of CNN so B here is the bandwidth so that's like the related to the resolution right directly should resolution uh a CNN uh will be order the complexity order of the the bandwidth squared times the size of the filter square and that filter is also very small so this is order of b square very cheap and uh the fre Transformers that we need for the filters are a b Cube uh and this is like ordal magnitude uh larg right it seems like we can never get there uh and let's give up but now if we talk about Transformers again if you're doing a Transformer on all the pixels of your image that's that's B to the fourth bar and this is what everybody does nobody does it in all the pixels right you have some sort of embedding to reduce that dimensionality but we can have the same for for thisal CNN or nothing stop us so that doesn't seem so Bak anymore because I think before is okay uh there's still some caveats right uh I think there's a lot of engineering to to we're not talking about the constants here in this uh uh in this complexity uh and there is a lot of engineering that we need to make the the for transform and we also need two at every layer we need the forward convolution and backward so it's not it's still expensive but I think it's it's not that bad as as we used to think uh right uh the thing is we need to scale this further uh recall we appli this to 256 256 6 78 inputs and outputs uh and then uh concurrent work right graph cast is the like the state of the art graph NE Network when they came they are using like 50 times larger input so the resolution is much larger and they also use many more channels that mean more vertical levels of predictions so we still have a factor of 50 to to scale these models and that uh since we have graph gas now it motivated us can we just find modifications can we change graph cast and use graphic with iron layers and try to see if that has any benefits and this is how it shows up in our in our graph right they are still like again one order of magnitude bigger than our biggest models in terms of number of convolutions and size of feature Maps now let's talk a little bit about graph uh CNN or or graph NE networks uh so this is the most basic type of of graphic in your network the gcn and this is the one that is doing just averaging over embeddings of the neighborhoods right and this uh model is is very simple uh and it's actually invariant to rotations right if you rotate uh these patterns we're just taking averaging or or some of those features uh so it's going to be invariant uh to rotation and a downside of that is that you cannot tell these two patterns apart right they're just rotations of each other if you're invariant you cannot tell them apart this is not what this weather this graph neon netor for weather is doing they're doing something like this so they they will have this Edge features that will be related to the relative position between uh the nodes but the way they use those features is Again by applying an MLP uh on uh the 3D coordinates of this relative positions so uh it turns out that these models will be able to tell the the two patterns apart uh but it's not equivalent so that means this for these models these two patterns are completely different things and you need to learn different filters uh to detect or to uh predict those patterns and what we're proposing here is equivalent graph NE networks uh and the way to do it is introducing different feature types and introducing in our our embeddings instead of being an MLP uh will be some equivalent function of these types uh so once we introduce these different types we can have equivalent functions and we can aggregate neighborhoods and what will happen here is that uh we will be able to tell this two pattern apart so you see that the feature in the middle node here uh once we apply the transformation there's a rotation the the the vector part of that feature will rotate by the same amount that means we can't tell that the patterns apart and this is equivalent that means that the same filters will work for for both uh patterns and we can uh detect or predict those patterns uh in a efficient way and uh we're very uh lucky that this problem is kind of simplified on this feere because we can consider only tangent Vector Fields so in this case here uh since uh I don't think it pays off to go all the way through 3D Vector fields we could do that too but the models get much more complicated and and more expensive so for for the 2D we only need to care about the each2 only the group of of rotations in the plane uh since locally tangent Vector fields are are on the plane so the our filters are actually quite simple and uh I think we can go a great way uh only considering tangent Vector field and also uh if you look at the inputs and outputs that we want to predict are like wind speeds uh uh and this will be much more prominent uh on the tangent plane than in the vertical plane so that also makes sense for from a physical uh sense to to use only tangent Vector fields and this is a ongoing work uh we don't have results yet so this is being done by by one Sero at at up okay so we are getting uh close to this and so the takeaways I remind you I think the time is right for for equivalent neuro weather models and I it's a very interesting task now we have uh nice benchmarks we have nice uh base models that that you can build uh uh on and I encourage uh people to work on this and also it's worth uh putting the effort to to scale these models and Tackle bigger problems some open challenges so I talked a little bit about the the the data set right that is the reanalysis so we have scattered data from arbitrary weather stations and weather balloons and points on Earth that are actually passed through a simulation to produce the the uniform sampling over the globe uh what if we want to use only raw data if you really want to learn a good weather model maybe we don't want you to tarnish it with with the with the the errors of the simulation of the assumptions of the simulation that then we'll have a non uniform sampling on the sphere and there's a lot of nice theory about non uniform for transforms if you want to do these things on the Spectre domain or there might be other better ways of of handling this and another thing is that uh when I said the results the results seven to 10 days ahead uh that that we showed here they're kind of garbage and there's not much we can do about it it's kind of uh uh we have no hope that predict we can predict the weather really exactly that the temperature that's going to be 10 days from now at some point that means that the the models for longer term they need to be probabilistic uh so what we really want is like what's the worst case scenario or we want a range or we want a distribution that that models the weather at 10 days ahead and that uh brings in all the recent work on generative modeling uh and and how could we apply these right diffusion models how do we do a diffusion model to predict the scalers and Vector fields on the sphere or uh or l language uh based models how do you tokenize and and predict the next token to to simulate or to predict the weather in a probabilistic way okay uh okay this is the last slide I just want to say about uh we're pres presenting uh tomorrow uh this paper is not related to the weather it is related to equivalence so this is uh we are doing intrinsic uh convolutions on Mash to do the noise in diffusion uh to uh generate textures so we have this nice work uh that is one of the first to do this directly on the mesh uh using inic mesh convolutions they are equivalent to isometries uh and that's it I want to thank all my collaborators and uh thanks Carl so much for the next talk and so so sorry for interrupting um you're like I've been hearing people talking about equivariance i' been also hearing people talking about scaling up but it's probably my first time hearing one person talking about those things at the same time and also like I'm so impressed that like the things are implemented in Jacks so there are a lot of like there are existing libraries of equal High torch probably also tensor flow but jazz is also a little bit rare maybe because we work at Google I guess no no because Jack is is great so I also encourage everybody to move to Jack yeah um I I feel weird a little bit like um um over time so probably we can take one question or maybe not okay um yeah maybe we can take one question from I I think you go yeah sorry thank you for a very nice talk you mentioned that there was bit of a gap in feature map resolution for the spin W models compared to just wondering compare resolution how does so the the results that I showed uh comparing against kaer model those are comparable so we compared with this this was Kaiser was like the first graph Network tackling this problem seriously and then we keep everything the same here the training resolutions even number of parameters number of layers this is they're very different models but as much as we can we kept it a fair comparison and then we do better in most most of the most of the variables here all right thank you thanks again well it is if you but um hi Nina can you hear me uh could you unmute yourself okay uh I'll first give a brief introduction so now we're gonna have our next uh invited speaker uh Nina M I hope I pronounced your name right so Nina is an assistant professor professor at the electrical and computer engineering at UC centa Barbara her lab the geometric intelligence lab aims to reveal the geometric signatures of uh natural and artificial intelligence and her methodology interests lie at the intersection of geometry statistics and computer science and her application domains are neuroscience and neuroimaging and also extracting Knowledge from data so let's welcome welcome Nina for her exciting talk thank you very much thank you for the introduction and also thank you for the invitation I'm very excited to see a workshop dedicated to equivariance inv Vision at C VPR uh so many thanks to the organizers for uh putting this together so we are in the equivision workshop so I will talk about equivalance and vision for the equivalence part I will introduce the notion of hierarchical equivalence uh which dep from the classical definition of equivalence and generalizes it is very interesting I think and for the vision part I will talk about both uh computer vision and human Vision so computer vision and natural vision if you wish in both artificial brains and natural brains uh so that's why the title of this Doc is a hierarchical equivalence in artificial and natural brains so Neuroscience applications the main question that this talk will tackle is how do brains artificial and natural brains encode information about the world how do we make sense uh how our brain encode the world if we look at the data if we look at neurons inside the brain the data may look uh like this so on this animation every row represents a neuron and we see a white dot for every time this neuron is spiking so that every row every neuron has what we call a spike train which is this sucession of spikes through time and somehow these Spike trains encode information about the world uh so in our brain not only they encode our perception of the world what see what we hear uh what we touch or feel but the spik strain also allow us to plan and act in this world which is pretty amazing and the question is how how is such Vital Information encoded in this apparently random trains of neuronal specs I'll put it another way how does our brain organize information and our brain and brains in general organize information in many ways and first they do so specially so they are specially or physically very organized um in this animation you can see a brain organoid which is the brain that someone is growing on a chip and you can see geometric structures in this animation this is because this brain this organoid is growing neuronal Connection in a very organized way this geometric structure that are formed by this neuronal cor connection are definitely not random I think they are rather beautiful and a little bit mesmerizing uh but the question is what purpose do they serve Why would a brain grow neuronal Connection in this uh kind of crazy geometric patterns and there is a field of research that studies this question we call it neurometric for neuroscience and geometry and this field looks at the geometric structures that exist both in the physical world like the geometric structures of the neuronal connections of the brain uh but also in data space in data space of the brain how the data is encoded after the brain has processed it so we're g to have a talk on neurometer and how uh and show how brains use this geometry patterns to encode the structure of the world we want to understand how brains use geometry to encode the world okay so to understand this let's look at the structure of the world um because our world the 3D World the world in which we evolve every day is actually highly structured and some of this structure actually a lot of this structure can be Des right by the mathematics of groups and group actions and since we're in the equivariance workshop we'll focus on the part of the world that is structured with uh group actions so let's let me show you one example uh consider a fruit fly evolving in the world and this fruit fly wants or needs to encode the orientation of its head so we're going to consider the orientation of its head uh in 2D in a in a plane parallel to uh the floor say so the fruit fly wants to encode one angle that belongs to the Le group s SO2 or the group of 2D rotations how does it do it here you have a video that shows how the brain of the fruit fly does it so you have a fruit fly walking on uh on air on a little bowl in complete darkness so the fruit fly doesn't see anything and yet its brain is going to encode its orientation in chudi there is this structure in the fruit Spide BL brain called the ellipsoid body it's a a ring shaped structure and their neurons are going to fire around this structure in a way that encodes the orientation of the fruit flight head and the neural activity moves along this ring to encode the orientation of the fruit fly head so even though the fruit fide doesn't see anything there are groups of neurons that fire when the fruit fly has its head at a definite orientation into the space so in this sense we can say that the brain the fruit f is encoding the structure of the world and in that sense it's encoding the Le group s SO2 of 2D rotation that represents all of the possible orientations that this fruit fly I can have so this is a a 3D reconstruction of the the structure that exists within the brain of the fruit fly and in the center you see this elipsoid body this is where neurons are anchoring on the structure s SO2 which is pretty fascinating what's even more fascinating is that this fact actually happens across animals across organisms and ac across substructures uh of the brain the fact that geometric structures in the world become geometric structures in the brain is seems to be Universal in the fruit fly we've seen the this ellipsoid body that enclosed The Heading or the 2D orientation of the head of the fruit flight uh so that's the fruit flight but then in rats uh you have another structure um another subset of neurons called the GD cell neurons that encode spatial position and spatial position position in 2D uh can be represented by the group R2 and this last example is one of my uh favorite favorite one it's in humans uh and it shows how our internal ear encode information about the Le group s SO3 or the Le group of 3D rotation in that case how we Orient our head but this times in 3D so how this one works is that in our inner ear we have these three rings that you can see on the slide and these rings are Hollow they actually have fluid inside them and inside uh this these Rings where the fluid is there is also some hair uh some little tiny hairs so that when the fluid moves it will displace the hairs which in turn is going to activate some neurons and so you're going to have some neurons that activate when fluid moves in one ring other neurons are going to activate when we moves in another ring Etc so that together this group of neuron encodes the three directions in which our head can move in 3D space so together they encode the group of 3D rotation I also find this quite uh fascinating though this is not Vision yet uh and we're in the equivision workshop this is more like spatial navigation how Orient ahead how we are located in space uh so since we're in the at cvpr in this Workshop the question I want to ask is what about Vision how is the structure of images now or visual scenes encoded in the brains so now moving to a visual scene that's for example the image of an office how does the brain encode uh this and what are the groups that will be useful for that so the question we're going to ask are what is the group structure of visual scenes rotations 2D 3D how do they play a role how do our brains encode the structure natural brains uh and then can we design artificial Network that exploit this uh structure so we're going to see uh how we do this in three parts first uh group structure in the first part of this talk is going to be about what is the group structure of visual scenes the second part is we're going to include that group structure in our model of natural vision a model of the visual cortex um and in the third part we're going to include one more level of description which is going to be a hierarchy so back to this notion of hierarchical equivariance that's in the title of the talk going to talk about yal equivariant models of vision I should say that this work is based on two papers uh the first one called equ and sparse coding with Christian shake and Bruno ousen that has been published in the conference of geometric science of information and the second work called visual scene representation weal equivalent spse coding with a larger team across the University of Cali California univ University of California Santa Barbara University of California Berkeley University of Amsterdam and new Theory which is this um upcoming AI startup in the B Bay Area which is very exciting I'd like to give a special shout out to the first author of both paper Christian Shake uh which is an admirable scientist and also has made a lot of the uh visuals that you're seeing on my slides all right so jumping in what is the group structure of visual sces some quick mathematical prerequisites what is a group what do I mean mathematically when I say a group uh a group is an an algebraic description of a set of Transformations for example if you look at rotations in 2D a set of six rotations as shown on this Slide the six rot shown on this like would form a group and we can also give the mathematical definition where a group is a pair g dot where G is a set and Dot is the group law that verifies some properties so that's the group now I've been talking a lot about group action uh so what are these going to introduce the notion of group representation which is what I mean when I say group action the group representation is a matrix description of the same set of Transformations or a set of transformation in our example with the rotations the same set of Transformations so if we take the six rotations that we had before now each of this rotation is represented by a matrix here the Matrix is just a scar a complex scor but each rotation is represented by a complex color that's interesting because now that we have this Matrix description we can use the Matrix representative of the group elements to act on other objects mathematical definition representation of a group is a pair Row V where V is a vector space a row is a group homomorphism basically we say that g the group will act on a space v through this group representation r this allows us to define the notion of equivariance and invariance I know they've been introduced before but for the sake of completeness this is equivariance and this is invariance they describe whether a map f is equivalent of invariance meaning how the map uh Maps a transform object so an object X transformed by an element G and whether the transformation is conserved that is equivariant or whether the transformation disappear that is inance all right so now that this prerequisite are done let me talk about the group structure of visual scenes in an image or visional scene we have objects and these objects have properties that can be factorized into the what and the where so this is an image it contains an object which is this little man here and this object has a property a shape it's a little man that can be factorized into what and where the what is the fact that this object is a man and the where is where the man is uh in this image so in other word in other word um this image can be decomposed into a part that is invariant to the transformation I can move the man anywhere I want in the image it's still a man and a part that is equivalent to the transformation if I look at the location of the men well this one is changing when the men uh is moving so that's a first way where groups enter the picture to describe visual scenes now this factorization is actually hierarchical because we can take our same object the composition into to the what and the where but then realize that this object is made of Parts the little men it me it meant is made out of different uh body elements that each have a location maybe even an orientation in space and then this are made in terms of Subs etc etc so this group structure is actually a hierarchical group structure of parts of subp parts parts and object each of them compos into the what and the where the part that is invariant and the part that is equivariant which lead us to this notion of hierarchical equivalence and invariance uh which generalizes the more traditional Notions of equivalence and invariance uh and it go it goes like this consider sin x that decomposes into K objects x equal object one up to object K and consider now a product group that we denote G Zelda which is a group G that's going to act on the whole scene and then K copies of the group each of them is going to act on the different objects then we can define a hierarchical action of a group element G Tilda that belongs to the product group and it's written as follows and this leads us to defining a notion of hierarchical equivalence and hierarchical invariance of a map five that would act on the scene so imagine that the map five just changes the color of the scene then this will be hierarchically equivalent to uh a change of orientation of the scene and different objects I'm just showing the first uh level here scenes into objects but you can continue this logic and go into parts and so on so that something that seems to be encoding the structure of visual scenes yet current models of vision do not exploit this structure can we make them exploit this structure so let me move into equivalent models of vision this is uh the traditional model the yeah the original model of vision uh it's a model of the visual Vortex of animal it's been developed at Al Housen and Fields in the 1996 and the model Vision or perception as an inference problem so the idea is that the brain has a generative model of the world and that it will invert that generative model in order to perceive the generative model of the world written by these others uh looks like this it's a generative model with latent viable here I means an image I an image that is decomposed into D components each component has a fi that is a dictionary element and a AI which is the latent variable which is how much this dictionary element is activated and that provides a model of the brain because the AI is are going to be mapped to activ activations of individual neurons so here we would have D neurons and we want to model I we want that to be an actual model of how the brain encodes images and so we're going to also include the fact that the brain want to be energy efficient we want to include some energy conception constraints and these constraints are going to take the form of a prior over the latent variable remember the latent variable are these AIS which are the activations of the individual neurons so we don't want the neurons to activate too often we want the code to be sparse and energy efficient so we include in this generative Model A sparcity inducing factorial prior over the latent variables uh for example a lapas prior and so in this model of the visual cortex all hous and Fields say that vision is an inference problem perception is an inference problem I've Rewritten the the model here um so how do we see well this is done according to this work through basian uh inference which predicts how neurons are going to be activated and specifically a so the vector a of the activation of all of the neurons is computed as a maximum as posteriority of the posterior distribution of a given an image I so the model will predict given an image I as input how the how the neurons are activating by predicting this a this ban inference model can be formulated has a minim minimization of a L2 loss which is a similarity uh term with a L1 penalty which is that spity penalty that encode uh efficiency in energy there are many algorithm that exist to perform this optimization but one in particular was proposed by the same authors but most 20 years later 10 years later um and it's interesting because this algorithm which is the last one here LCA proposes how the brain does it how the A's the neurons that correspond to the different A's exchange information so that very quickly the a conver to this maximum as posterior estimates so this is the locally competitive algorithm the LCA which proposes a Dynamics actually so here A D Dynamics uh in time of how this a the activation of the neurons activate through time to in the end encode uh the image on this equation you can see that there is the U so the the U U dot is actually the velocity of U it's a hidden variable and the a which is the activation of the neuron is some nonlinear activate nonlinear function Sigma of U so the Dynamics is written in terms of U uh but a is really the activation of of the neurons and this inference approach has received a lot of interest because it allows to recover the what we call the recipes fields of the neuron so it has predicted what brains actually do if you so what are the the V1 receptes fil where if you stick an electrode inside the brain of an animal record one neuron and try to see which are the images that these neurons somehow maximally react to you're going to find this Gabor looking function like these three Gabor looking images like the three images you see on the top um and the model proposed by UL Housen and field allowed to recover exactly the same images that were observed in practice so which has which is why it has been uh part of the reason why it has been celebrated as a model of vision however this model does not mirror the group structure of visual scene so we explained that visual scenes are really organized they are composed of objects that are composed of part and subp parts Etc but the problem is that the the model that that The Spar coding model has proposed does not exploit this structure in the sense that if an object is moving then the the activation of the different neurons seems to be moving very randomly and that's not exactly mirroring the group structure of visual scenes what we would like to do is have a model that exploit this structure so model that looks a bit more like this when the tree or when an object moves in the image it'd be great if we can model that the neurons activate in a way that makes sense so this is what we have proposed in our paper on equivalent sparse coding in the visual cortex where we model perception as a geometric inference problem so we take the generative model of the original sparse coding model um and basically just relabel the dictionary elements the F eyes but also the neurons the AI so the activation of the neurons we relabel them using uh group elements label so G and this is us to propose a neural circuits that decomposes a scene into the what and the where because the FI G that you have here this dictionary element encodes not only the what so the what will be the gab function this vertical bar but also the where whether this vertical bar where this vertical bar is located in space how it's oriented in space and then we could encode also its size its color etc etc and so this leads us to propose this model equivalent pass coding which is equivalent both the geny model is equivalent but also the inference procedure is equivalence so I've put the definition of equivalence on the top here if you consider the map SP that is the generative model that move that puts that transform the true latent the a into an image then you can show that group action of the true latent gives a group action on the image so in that sense the generative model is equivalent and then if you consider five the map the inference then this one you can show that it's also equivalent in the sense that if you change the image you use a group element to act on the image you can show that the inferred latent so the ones that are the results of the inference process of the Dynamics they're going to be transformed by a corresponding group action so in that sense both the model and the inference is equivalent so we have um realized our goal of having uh incorporating group structure in Sp coding which gives uh faster but also more realistic inference faster it's because the Dynamics the LCA Dynamics once you write it with group action you can actually make a group convolution appear in the equation and this group group convolution can make uh the computation much faster and more realistic is because if you look at inside the brains where you can see that the neurons are actually organized uh in terms of how they encode location so you open up the visual cortex and you can take a little slice where neurons that are closed together are going to encode elements in this visual field that are close together to so by labeling the neurons by group elements we're actually organizing them in the way they're physically organized in the brain incorporating group structure in spas coding gives faster but also more realistic uh inflence though could we do even better which me to the last part about the hierarchical equivalent models of vision remember the hierarchical group structure of visual scene we've talk about the group structure of visual scene in the second section but now I want to talk about the hierarchical group structure of visual scene in this third section so the fact that the scene is decomposed into objects that decomposed of parts decompos in subp parts and that each object part and support has a what and a where the shape it has and where that shape is located oriented could we somehow incorporate this structure Into The Spar coding model now there is something called hierarchical sparse coding that has been uh published in 2020 that incorporate the hierarchical structure of a but yet this model is not equivalent um this more standard yeah IAL SP coding model will lack this geometric information and therefore will be unable to effectiv represent the relative configurations or the Transformations so by contrast we're going to model perception as the hierarchical geometric inference problem going to show what's the generative model that corresponds to this pass coding model and then show what's the inference we propose as a new model of how the brain encodes the yakal group structure of visual scenes for the generative model we're first going to model the parts that make a SC so we're going to say that the image I is composed of some parts and this is basically what we had with equivalent SP coding and then we're going to have we're going to add some levels by saying that the activations that are part of the first level actually themselves de compose into uh the expression that you can see here now that's the generative model and related to that we propose an inference model that runs the LCA Dynamics but layer by layer now that's interesting because it provides a highly recurrent neural network but not the type of neural network that we may be most use used to but the newal network that is highly recurrent with some bottom up and top down connection and even lateral connection within layers so a model that's quite uh different from the strict bottom up or fit forward neural networks that uh we are used to if you train this hierarchical equivalence pass coding model you can recover indeed the hierarchical droop structure so to do that we took a synthetic data set that we constructed based on this idea of hierarchical group structure we constructed these data set using parts parts object and scene where the scenes were composed of different numbers here 1 one3 for example uh the objects were the individual numbers one and three and then the parts were these little bars that came together form objects if you run theal group pass coding model on this through inference you can incover recover you can indeed recover the underlying hierarchical structure so if these are the true dictionary element where at the bottom level we have the bar and then at the next level we have the two objects we correctly estimate uh this uh dictionary element which is uh very exciting from an inference procedure like this whole thing is working uh but it's even more exciting because we think that is what is happening in the brain so let me show you what I mean by that we do have equivariance and invariance in visual cortex uh because it is believed that visual cortex is organized in two streams and this is the so-called two streams uh hyp is which is a model of the neuro processing of vision these two streams are the dorsal pathway on the top and the ventral pathway that you see uh on the bottom now the dorsal path Ray is the part that encode the wear information and neuroscientists call it the wear stream that's what encodes location movement transformation so the group elements and by contrast the ventral pathway is what neuroscientists say en code the what the what uh the shape the size of the object so equivalence and Environ seems to be inividual cortex where the wear stream incl the equivalent part and the W stream will encode the invariant part what about thearchy to find the y in the visual cortex let's see where we have impaired y Archy so there is uh this um condition that some people have uh in which they can describe lowlevel features of objects but not the higher level feature features of an object so they would label the object on the left a rose twig or the object in the middle a fencer mask well for example we see that the second one is um a tennis racket so these people can describe the lower level of features the lower level features of objects they can even copy the drawings of the objects but the perception of the object as a whole the perception of the collection of feature is impaired and on the the right of this slide you have an image of what we think this condition can look like um that that that is if you had this condition that is how you would see the world so you would be able recognize individual objects in the world but wouldn't be able to tell what the overall structure of this scene what what is happening in this scene so if ychy can be impaired if you can recognize the what and thewar of lowlevel features but not read the what and thewar of the global scene then there must be a hierarchy in the visual cortex and in fact we find that in higher levels in higher physical levels of the visual cortex so V4 as opposed to V1 we have some neurons that uh react that uh spikes when some objects are present so here you see data for a neuron that reacts every time an image or a drawing of a hand is presented in the visual field and you can see that we can rotate this hand even show a drawing of the hand uh this neuron will still fire so that is an indication that this higher level of thearchy exist indeed in the visual cortext and that therefore this hierarchical equivalence model of vision may very well be what's happening in our visual cortex I like to finish on this uh slide which is if there there are neurons that could forance well maybe there are neur that comes for other high level principles there was this uh this study that showed that there is indeed one neuron in the brain of these participants that was coding for the for Jennifer Aniston so these neurons full Spike very selectively every time it is shown an image of je Jennifer Aniston in different orientation but would not Spike if shown images of other celebrities or not Spike to shown images of objects Etc so we are at the at the top of the of the hierarchy with neurons spiking for high level Concepts in conclusion I hope I showed you how brains use geometry and group actions to encode the structure of the world uh in the first section we've seen how the World visual world is very organized and that some of this organization can be described with group actions then we seen how we can incorporate this group action framework into a model of vision more specifically The Spar coding model of vision and then we went even further by incorporating hierarchical uh group structure in this pass coding model of vision and not only do we get a highly recurrent neural network that can indeed uh be trained and decompose the world into its Elementary component uh but even more interestingly and fascinating to me I think we get a generative model and an inference process that might very well be how our brain encodes information so brain use geometry to encode the structure of the world and we are using geometry to describe all of these works quantitatively that's the end of my talk I'd like to thank uh the collaborators uh pictures on on the left and if you're interested in all of this check out both our papers but also our python packages we're maintaining two python packages one on Geometry called GM stats and the second one on Geometry in brains both artificial and natural called uh neurometer thank you very much thank you enina so much for this awesome talk um I guess probably people will have a lot of questions I'll start with the one on Zoom because this person asked like the middle of the talk like pretty early so the question is can these relations somehow be connected to a combination of persistent homology and multi parameter filtration I think this is this question was asked like um in one of the previous slides about I think geometries in the world become geometric structures in the brain like kind of like some slides over there yeah I remember seeing this question appearing during the do bre um so yes there is a connection with uh topological data analysis um and actually that's the first step uh when you're trying to describe the geometric structure uh that the data forms once it's processed in brain so let's say you give um an image to an artificial brain and then you record a 100 neurons and how they Spike and maybe you're gonna move this image or maybe this image is a video and so as time goes then the neurons are going to uh have a spiking trajectory and you can draw a trajectory in what we call neuronal State space which in our case would be a 100 dimensional space where each Vector describes a neurons and then we have a trajectory of activation in this um dimensional State space and we want to understand the geometry or the structure of this trajectory and the very first step of such analysis is to ask what is the topological structure is it a ring is it a spear is it a Taurus uh and indeed people use persist andology yeah yeah thank you for the answer um do we have other questions from the audience uh probably you can come to front to yeah hello hello okay FR University of Copenhagen thank you a lot for the talk there have been quite also other works to try to model at least the visual cortex using notion of fiber bundles and things that that it seems that they are all always turning around the same things would be some nice connections because I think fiber bunders are also automatically uh linked to group actions so yes would it be possible to mix this representations yes yes uh absolutely and thank you so much for um mentioning this uh I think maybe the the model you have in mind is the model by petito uh where they use yes yes yes especially for the for the very lower part of the visual C yes exactly for V1 in the visual cortex uh where they use sub remanion geometry to model the activation of neurons uh and yes it's very related and actually it's on our to-do list to to mer both uh so yes 100% because in this yeah yeah yeah yeah good question yeah in mathematics principal bunders are are built on group actions exactly yes and most of the fiber bunders are exactly are associated bundles to this princial bundles so I guess that there should be some sort of very clear path to connect the two of them exactly because even this fiber bundle structure is physically encoded in the brain where you have these Columns of orientation that literally describe the fiber of the fiber bundle uh and and that's exactly be that's exactly why if you uh label the neurons by their group element then you organize them in a way that is closer to the physical organization they have in the brain and then the diffusion process across that physical organization can indeed represent the inference that we think is happening so it's very it yeah thank you so much for this question yeah you're welcome thanks um do we have other questions from the audience okay you can come through front hi thank you for the talk um it's in really interesting to know the hierarchical um equence and I think it's also interesting because I think in machine learning we know that machine are good at low level uh learning and then you also mentioned like um higher level um in geometric is really what I think it's really what humans are good at and I wonder if you have insights in how these lowlevel equirement we learn can contribute to higher level and how can we use that in this hierarchal sense H interesting question so how do lowlevel equivalence can oh uh why the lowlevel equivalence is interesting and how it comp contribut to this kind of higher level equivalence I think I hope that's the point of your question but I think what's interesting with this ychy that is it's um efficient in terms of memory storage so instead of uh storing or having to encode all the possible configuration of my body then I can first encode at the higher level how I'm globally oriented and then encode how each of the parts and subp parts are oriented and I think by decomposing this into a hierarchy you're able to use by fewer neurons to represent all of the possible uh combinations so I think that's how the lower levels and the higher level can play together to have a a better representation of all the possible shapes than an articulated object can take thank you thanks for questions um do we have other questions otherwise um we'll conclude Nina's talk here so thank Nina again for this super exciting talk I guess it's very inspiring for people especially who are interested in like getting Inspirations from how human perceive the world to building Network that trying to perceive the world in a similar way so thank you again thank you so much thank you for the workshop thanks and I think now we're going to have a very short Coffee Break um until 3:45 p.m. and then after afterwards we're going to have the two spotl talks and also the tutorial on how to get started with building your networks that are equivariant to geometric Transformations so thanks everyone e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e and the we do okay I don't think I can um the plan yes I found it yes okay uh today you heard a lot of great talks on equivariance uh in this tutorial the goal is to get you started with equivalent deep learning uh and how to leverage symmetries in your problems especially in computer vision applications uh we're going to start the tutorial with some Theory discussing groups then the notion of equivariance and invariance more rigorously and then we will build equivariant layers that have been built over the past years uh with techniques from uh using the group Theory then we will move on to h building an equivalent Point encoder using these layers and then we will discuss more applications that are in general in the fields of computer vision robotics Etc and we will close the talk with uh limitations and open questions on equivalent bling okay so let's start uh the symmetries are in general uh very Elusive and but very important in uh many tasks and there is a question on how do they manifest for example when you see uh an image and you want to do an object detection you might want to detect the humans in any pose in any scale in any H position in the image uh when you say for example a point cloud and you want to reconstruct it into a an occupant function for example uh the these points might uh because we we provide the coordinate system for the points uh this might be numerically different if the chairs are translated for example but in the eyes of the network we want them to be reconstructed consistently uh independent of their orientation or their scale for example in pick and place tasks as you saw before uh you might want to pick or place an object independent of its pose and independent of the pose of the gripper uh and also as you saw in the hagay Maron stock for for example when you encode an object a neural object like in a neural Rance field when you encode it in a neural network this neural network might have symmetries that describe the same object so for example if you permute these matrices uh in in a very particular way you encode the same object although if you unroll the vector it might seem different so this creates a permutation Symmetry and uh you need to build like methods that uh process the those objects consistently so this idea of equivalence is not new it has been explored in the past even before deep learning uh I would say that the one of the most um successful applications is for keypoint matching where the idea is that you have um for example you need to match this planes the books that they undergo a projective transformation and you want to build features that are equivalent with with respect to those transformations to so that you can match consistently uh and one good approach was building like uh features that are called shift features that were scale inv variant and also rotation and translation invariant uh with the idea that you build expressive distinctive features to match the key points descriptive to be robust to Noise by describing the neighborhoods but also invariant to some Transformations uh also this idea ofab that we will see also later appearing in equivalance has been explored even before uh the shift features where the idea is to build a to build H filters that uh when they are transformed they can be expressed in a basis of some standard filters by only transforming the coefficients of those and since convolution is linear if the filters have this property the convolved signal will also have this property in the Deep learning region I would say that the one of the most successful applications the embedding like the translational symmetry in a neural networks which gave rise to convolutional neural networks uh the lenet for example NE neon even before that so let's discuss a little bit about uh what is a symmetry a symmetry is a transformation that leaves an object in variant for example if you see the picture on the left you see a very symmetric uh type of flower that if you rotate by 36 over 8 uh you will get the same picture and also if you flip on an axis you would get the same picture and this is called uh the set of Transformations form a large like a family that is called d8 um also when you see for example mathematicians have studied those symmet for many years and um for example this there are patterns that are 2D but repeating 1D uh these are called Freeze patterns where you see here the footsteps with a what is called a Glide reflection transformation when the patterns aree in two dimensionals this called a wallpaper Transformations and in 3D it's the crystalographic groups or Transformations uh and the goal of the mathematician is to classify all those different structures uh up to isometries and you would hear words like you can only choose among 17 wallpapers because there are only 17 types different types of these kind of groups moreover these asymmetries appear in a continuous settings especially when you put coordinate systems on objects that you want to be treated the same for example these two chairs uh are just R translated versions of themselves but in a coordinate system they appear numerically very different and you would like and the Symmetry now is the ASM with respect to the coordinate system that if you transform one chair to the other it if you transform on chair you would um consider this to be a symmetry because it describes even though numerically it appears different in the coordinate system it describes the same chair formally a transformation group first what is a group it is a set with a binary operation that is closed if you take two group elements two Transformations and you apply them one after another you get another transformation and you need to satisfy some properties like this uh binary operation need to satisfy associativity then uh there should be an identity element with respect to this uh transformation to this operation and there needs to be an inverse element for every element if you see for example in this figure you will see that um you get this F rotated four times you get back to some f and if you flip for example uh then you can start composing Transformations that still give you uh a closed set of symmetries uh and I want to pinpoint that the the the group elements are the arrows the transformation not the actually the configurations but there is a bje as you can see between those two a transformation group now in our context is like a group this set G contains a group of Transformations and the operation the binary operation is just composition these like ideas have been studied a lot in mathematics and you can find for example in group Theory uh the mathematician studies is the properties of the groups themselves and how they are structured ured uh in representation Theory you study how these objects act on other objects for example Vector spaces that we will discuss and in harmonic analysis there is also like this notion of a generalized pya theory on how to put a basis on functions on those groups so let's discuss a little bit about how a group acts on other objects and the group can act on many different ways but what is an action an action needs to satisfy some property for example when you get a group element and your set X if you apply the identity element then you shouldn't move the object the object should stay the same moreover the most important is that H if you first if you apply like a transformation h on your object X and then apply transformation G then this should be the same as applying this single element G composed with h this single transformation on your uh element X and this single Group G can act on many different objects for example when you see in the main figure in the the second line you will see that there's a scalar field where you rotate by 90 and in order to find the corresponding color you need to just rotate the grid but for example on a vector field like in the first row not only do you rotate the grid but if you carefully see you also need to rotate the arrows themselves another example is for example the gaussian function that if you rotated you create another gaussian function with a rotated with the mean rotated this is rotated as a vector but just applying a matrix on the vector and a rotated uh calance Matrix but this now AC is completely different because it's both on the left and on the right so a group can act on many different ways on different objects moreover if you select like a your objects in a particular like representation you might give rise to more symmetries to a different group of symmetries for example if you try to describe a mess a triangle mess with just with the coordinates then when you rotate the coordinates will appear different so you have a symmetry under all these Transformations if you want to be consistent on the description of the mess but alternatively if you see on the top you might give an intrinsic representation for example you might give the angles or the and the hedral angle and the ratio between the vertical and the side and after a rotation this would be exactly the same description the object does not see any rotation because it's not described in the exic space so there is this idea of a acting on different objects and in particular what we're interested in machine learning is acting on Vector spaces uh and to repeat what an action is on under this uh structure uh an action needs to first uh H satisfy like I keep the property of the vector space this means that when whenever you get a group element you will act on the vector space and return an element again on the vector Space by keeping the structure of the vector space which additivity and scaling but moreover you need to uh um you need to satisfy another property which is that when you apply this transformation on some object uh some some single transformation is composed with g&h then this needs to be the same as doing the transformation of the of your vector by first applying H and then applying G with the same representation and uh this gives a property on the vector space that is more than just adding and uh scaling vectors this gives a property of geometric vectors because now you can act with a group two for example you can rotate your vectors if your group is like um if your space is like finite dimensional this is just described by n byn invertible matrices like the rotation Matrix that we saw before and why do we care about linear actions even though we might have in some cases like spheres or um different spaces in machine learning we usually have functions on those objects and even though the action on the base space uh might appear nonlinear the action on the function space is still linear for example the left regular representation that is induced by this action on the base space dot here which is not linear because it's on the sphere the total representation on the function is still linear for example if you applied it on the sum of two functions you will get the sum of the ER the sum of the functions computed on the rotated domain and the the thing that to keep about the linear representations and in general representation theory is that uh it is important to know that these structur have been studied a lot by mathematicians and that um those representations in particular the linear representations uh can be decomposed into smaller representations so even if some representation appears to be very complicated it's actually structured from smaller representations that leave some spaces invariant in your vector space for example your vector space might be decomposed into onedimensional Vector spaces but these uh representations would keep some of those spaces inv variant depending on the group and we will use that in order to solve the constraints later to keep to maximize the degrees of freedom so that's how we utilize the representation theor especially the de composition in order to separate the independent degrees of freedom and let's jump to an example for example in uh the rotation group is this a symmetry group is the question like the symmetries that we saw before and it is a symmetry if you example take the vector space r three and and impose a norm for example the ukan norm at the basis and you ask what are the symmetries that would preserve the transformation would preserve the norm the handedness of the bases and the vector space structure this would give rise to this group called SO3 which contains 3x3 rotation matrices that have this property that row transpose times row is the identity and that the determinant is equal to one and as we show you can uh rotate many objects using S SO3 for example you can rotate the coordinate of this blue point 3 by just applying the rotation Matrix or you can rotate the whole initial matrix by applying the rotation left and right or you can rotate the color function by rotating the Grid or you can rotate a function that is Vector a vector function on the points where not only do you rotate the grid but you also need to rotate the vectors themselves on the points all these are different actions of s SO3 but we know that S3 is Compact and for compact groups as we saw in the previous theorem they can be decomposed into some basic representations so all these actions can be decomposed into basic representations that will be the building blocks for all those types of representations and this has been studied a lot and I will be very brief into that H the reducible representations of s SO3 are called wigner D matrices there is a sequence of matrices that are H that are uh one by one 3x3 matrices 5x5 7 by seven and all in general the odd numbers that have this particular form for every element for example if you parameterize your S3 group with h Oiler angles alpha beta and gamma then uh you create those matrices that are 12 + one by 2 + one that have each each entry has this particular form that involves also the spherical harmonics which is a basis uh on the a basis for functions on the sphere utilizing this reducible you can decompose any uh linear representation for example the inertial transformation that you saw that is applied from left and from the right can be decomposed into those in after a change of bases this inertial Matrix that is 3x3 after vectorization is a nine dimensional Vector can be decomposed into some representation that is one dimensional and reducible a representation that is threedimensional and and a representation that is five dimensional all in all giving n as the vector the unrolled vector and this means that there are by this de composition you can see that you take this uh Vector space you unfold it and after some change of bases some of the subspaces will be acted only by particular representations and will be unaffected by the other representations so that's how you decompose in general a group a group action into the irreducible representations and let's jump now into what is the uh what is inv Valance and what is equivalence this characterizes the functions so let's say that you have a Group G you have your input Space X and an action on that an output space Y and an action on that and let's say that your for your instance small X you first apply a transformation and then you apply a function or you apply a function on the original input then invariance would mean that uh this output should be the same for every action uh for every group element G for example a a point Cloud classification could have this property differently a equivariance uh for different tasks would mean if you see the commutative diagram on the right would mean that if you apply first the transformation creating a new object X Prime and then you apply the function for example a segmentation mask or you first apply the function and then you apply the transformation of the output which could be different then H if those two end up in the same output then we say that the function is equivalent and equivalence has many properties uh for example the standard technique is to impose the equivalence on linear layers by restricting the degrees of freedom and then designing adhul nonlinearities and compo composing them this way in order to get equivalance from end to end in a deep learning setting equivalence can be seen as an inductive bias also because it's a form of constraint on the function space and and the way that we use representation Theory as we discussed before is is in order to decompose these constraints into the independent ones and the the reason is to maximize the remaining degrees of freedom one note about invariance is that even though our function might be invariant equivariance is still relevant because premature invariance might cause some troubles for example if you take an Environ representation of just the eyes the nose and the mouth uh by losing the relative force between them the two these two uh structures would appear the same but we know that the left structure describes the phase where there's the right one is not so clear that describes the face okay so what's the story so far how should you think about your symmetries you should think about this Trifecta of group space and action what is my group of symmetries what is my input space and my input action and what is my output space and my output action for example if you get a point Cloud that you might want to be consistent uh when you for example reconstruct the into a occupancy function you might want to be consistent with respect to the transformation of the points for example the transformation could be a roto transation in 3D and also in the input of the network you might give a different ordering of the points themselves in that sense the input space the group is this sc3 cross SN the input space is a point cloud and it's transformed by left by a permutation and then a translation and in the output space you have occupancy functions that when you apply the this uh action of the group you get again the occupancy function on the a translated grid Point okay so we have the group the space and the action we would like to uh build layers that are parametric and process this input plus action and produces an output plus action AKA equivalent layers before that is there is a question on why to build equivalance in our networks and there is this idea that equivalence will give us some benefits for example statistical benefits because of the smaller hypothesis space as we said there is a constraint equivalence constraints the hypothesis space of all the possible functions so with a smaller hypothesis space we need fewer samples to learn the best function inside that and also we have fewer parameters which means due to weight sharing the but keeping the same expressivity this gives rise to smaller networks uh moreover if you want to impose exactly the constraints because you have for example physical law that you need to be satisfied exactly or you have some safety uh constraints that you need to be guaranteed this exact constraint satisfaction equivalence gives rise to robustness and feasibility and one last important thing is that uh this constraint is imposed even outside the training distribution uh we didn't even discuss the training distribution uh this is a constraint on the function space independent independent on the input as far as the input you could be acted by some group action and uh you can also see that uh via for example this um image on the left that uh when you rotate when you process it in order to produce some features and then you rotate it those features are stable and the output is predictable so this gives us stability in the intermediate layers and we can utilize that for uh for extra inductive bias uh as you saw in some of the talks before there is an alternative to equivalance that at least that goes hand in hand uh with equivalance which is canonicalization and canonicalization is an invariant description of the input that will later be processed by Network there is some things that uh are important to note for canonicalization for example when you try to canonicalize these two planes that are very close to the to themselves like they only a small rotated copies of themselves uh by predicting the PO for example this is a very non-s smooth function because a small rotation to the right and H you would canonicalize into this object in the middle but the left rotation would be a very large ER large number of the pose so this would give rise to a very steep function if you try to predict the pose in order to canonicalize uh moreover by doing it in a naive way you don't get inv you don't get a robustness to noise for example uh if you take this point cloud and you do a PCA in order to find the some canonical coordinates uh if you have if you add a little bit of noise that could be seen here in the end of the uh plain wing and you do a PCA on that then the canonical object might be very different so these two objects now will appear in the input of the network as very different object and they will be proc processed in a very different way from the network so doing some canalization takes um is important to do it correctly in some sense uh and again the providing like an intrinsic description of a SAE for example in this mes you do kernels on the surface of the mes uh creates a question on what would you do if you had for example two meses and the relative poses mattered as we saw with the Picasso problem where the relative uh poses of the ice and the nose and the mouth really mattered for the problem so in some sense when you want to create a canonical object this is not something trial and involves learning this canonical uh mapping but if we learn this canonical mapping which is an invariant input representation then this definitely involves equivalent de networks so even canonicalization can utilize equivalent de networks there is also the other alternative of doing data augmentations and this has some advantages for example H you get get more than groups you get data dependent Transformations that might be corops Jitter small noise uh and also you don't have a very specific architectur you don't change anything in the architecture in order to do the data augmentation and one more benefit is that it's imposed from input to Output not per layer as equivariance is done however it's very costly during training especially for larger groups because you need to show all the Transformations it's not exactly imposed as with equivariance for example if you have a physical rules as set or safety constraints and most importantly it's not imposed outside the training set so you only get guarantees not even guarantees but even like the uh it's promoted to be the D promote the model to be equivalent but not outside the training set so you don't know what would happen in the in during inference uh one small note is that usually the this inductive bias of equivariance goes hand in hand with locality and hierarchy but this Synergy is not well studied start uh not will stud yet and this is like the blueprint of uh this this paper called this white paper called geometric de learning by Bank BR so using these ideas let's now build some equivalent layers that people have done during this past years and in order to do that we would need to again uh think about the trifecta group space action what is my group does it have some structur for example continuous or discrete group what is my space is this a vector space is it a function space what is my action is it linear is it reducible okay this slide okay so let's say that we have a function on H on a Matrix Group G this function is from a finite dimensional space to a finite dimensional space and we get two representations the input representation is a matrix that is n byn the output representation is a matrix that is M by m then equivalent linear layer which would be described by W uh would be equivalent if it satisfied this equation W * one of g equals ro2 of G * W but this is a constraint for all group elements G in particular if this group was uh continuous group this would be uncountably infinite constraints however we can utilize a Notions like um from for from representation theory in order to decompose this um to decompose this representations and actually find the independent degrees of freedom uh for example there is like this uh very important and very um Elementary LMA that's called s LMA that says that if the representations are irreducibles then they're either the same and the and the Matrix is just a multiplication of the Identity or they are not the same and then the there is no Matrix that satisfies this equation and although this might seem simple this idea might give rise to a very like popular architecture for example with the SO3 um rotations uh you get the your Matrix because this is a an irreducible representation you get your Matrix to be just a instead of a 3X3 with nine parameters you get only one parameter which is a times the identity transformation that means that when you get a point Cloud for example instead of 3 n by 3 n Matrix you only get an N byn Matrix that scales the points and linearly mixes them but not in a way that you can mix them differently in coordinates X Y and Z you can only scale them uniformly in X Y and Z and that is because when you rotate them the X Y and Z loses the meaning because the x coordinate could turn into a y coordinate Etc uh that simple idea gave rise to a very popular architecture that is called Vector Neons designed by our organizer uh and uh yeah ex extending on that there is this H paper that is on more General MLP uh on a general MLPs with General groups General Matrix groups that rewrites this equation into a a value equation and then they utilize representation Theory to decompose this representation into blocks that are independent and then they utilize the generators in order to reduce the number of equations into those that are really uh linearly independent because not all equations uh are linearly independent for example if you try to impose that on for example the Le group s SO2 which describes the 2D rotations instead of a 2X two Matrix with four degrees of freedom you would get a matrix that is that has two degrees of freedom one in the radial component and one in the tangential component if you try to impose it on discrete groups then you don't need to solve it everywhere in the group especially for larger groups this is very important you only need to solve it for the generators and this is the relevant equation that is a linear equation because all these are matrices so uh you can do that via SVD which is like the what the this paper proposes and for example if you solve it in closed form you will see that uh when you get a signal that is four-dimensional instead of having a matrix that is 16 uh with 16 degrees of freedom you get a matrix with only four degrees of freedom this uh when the group action is like a circular translation uh this is the form of a convolutional in 2D this is like a kernel that H is rotated is circularly rotated and will H be multiplied with a four dimensional Vector now if you increase your symmetries and you also permit more than circular rotation you also uh permit transpositions then you have even more constraints and this is the group of permutations of all elements and that is very relevant to for example graph neural networks where the degrees of freedom now reduce from 16 to two where you can only have a self Loop a weight for your own value and a single weight for all your neighbors because your neighbors are interchangeable so you get two degrees of freedom okay so something happened here what about no functions on the we discuss about um um finite dimensional spaces but what about functions those spaces uh I want to be brief on that because we need to move fast into the application but uh if we have functions we need to find equivalent linear operators between functions on the group then uh that gives rise to the group convolutions with it's a generalization of the standard translational convolution where you have your function on the group and your kernel on the group and instead of just translating you also do all the possible rotations like a template matching this might create a this is like very uh this type of um equivalent layers is a very like goes hand inh hand with standard nonlinearities you don't need to create ad hoc nonlinearities as we will see later for other types of uh equivalent layers and it's also a very unified perspective for many groups the group convolution layers and for some groups that involve translation they can also be implemented fast using the standard convolution after some smart index the only problem is that this integral cannot be exactly computed so it needs some sampling in order to do that and smart sampling really makes a difference in the uh in the performance and the same uh goes for a now functions that are from the space like this monaa image from the space to the group when you have such functions then uh this gives rise the equivariant linear layers would give rise to what is called the lifting convolution where your function is on the space let's say Mona your kernel is again on the space but the output after you integrate all the aut translated versions of the kernel you get a function that is on the group for example in S2 group and if you see here on the on the right with the image uh you see like for example uh the group that is the semi-direct product of the translations and four possible rotations you get your Kel which is like the I kernel that tries to detect ties and you pass it through the Mona using like only the translation and then you do that four times by applying this uh group C4 on the on the Kernel itself so all the rotated possible uh copies of the kernel and you do the convolution again so you create four four different channels with all the rotated copies of the kernel and if you now rotate if you say for example during inference the rotated version of monaa the then uh you would still apply the same for kernels but now you would detect still detect the ice as you did before but not in the first channel you will detect them in the second channel so that function is still equivalent but with a different H group action okay and the problem with these layers uh is that uh they create like a memory overhead that is one U negative aspect that uh needs to be uh addressed that while the function is on the space the output function as you can see are four copies and for larger groups it would be multiple copies where you don't have extra degrees of freedom the degrees of freedom are the same it's the kernel but you do a very large feature space which creates like a memory overhead this idea of doing a lifting convolution to go to the group space then doing a regular group convolution as we saw before to go between uh group spaces and then doing a projection operator like a Max pooling on the group is the idea that gave rise and popularity to the equivalent deep learning which is the regular gcns from tacen in 2016 the last uh architecture that I want to stay a little bit on is the functions between the the original space where you change the feature field the dimensionality of the feature field uh for example when you get such function you need to again specify how do they how do the H functions transform and if we have the rot transation group again then you could transform such functions by H if you view it as a scalar field you only need to do the the transformation of the grid but again as we said if you have a vector field you need to also post multiply with the rotation of the vectors themselves not only the transformation of the grid this uh UniFi this unified perspective can be described by these types of actions and if you try to find all continuous equivalent linear operators on those this will look like a convolution but with a restricted kernel and the kernel the Restriction of the kernel is of this form and this is called the stability constraint now if you try to solve it for example for the Roto translations in two Dimensions depending on the input and output representation you get a kernel that is unrestricted in the radial part but is very restricted in the angular part and you can see the form of the kernel here in this figure where for example uh if this difference between the input transformation is zero you get an isotropic kernel if this is one you get a this type of um circular harmonic type of kernels and this idea also was uh also explored first in the harmonic networks by Daniel warl in 2016 now this um this type of networks that are called steerable equivalent CNN uh they have a they can be exact even for continuous groups for example here with A continuous rotations you can impose the transformation exactly if you can solve this constraint and this is the negative part that if you can solve this constraint depends on the complexity of the group itself moreover uh you need to design adhoc nonlinearities for those type of networks which people have done in the past and uh if H we have a connection between the what we discussed before the convolutions that are lifting and the stable convolution that we discuss now there is a connection between those things via the forget transformation uh of course the spaces can be more General than um than as we said than Vector spaces uh for example these ideas have been explored with spherical images from sphere to sphere function on sphere to functional sphere or function on sphere to the group itself and these are described a general theory is described in this paper a general theory of equivalency and on homogeneous spaces there are even more types of equivalent layers for example those that parameterize in the Le algebra and there are even more General spaces for example solved Beyond Vector spaces on reman manifolds or on the aray space with a projective group and very recently in Clifford algebras and now we will move into the part of the talk that is on the applications yeah so after we have seen yeah some Theory ER we will go and see how like a simple case study of how we can build an equivariant Point encoder so yeah the point encoder gets a a set of points as input and uh we will uh assume that they also get a set of corresponding Vector features and uh yeah first we need to identify what's the Symmetry that we want our model to have so the output feature which we will H concentrate on the feature of let's say the Central Point ER yeah what's the properties that we want this output to have uh and before we go into the more complicated rotation equivariant we can see some uh other properties that feel more natural and uh the first one is just translation equivalant so if we just translate the input Point Cloud we want the output to be also a translated Point cloud and the feature to remain the same uh another property is that we want the output to be independent of the index thing that we chose for our points so if we just permute the indices we want yeah the output to remain the same but be yeah for the corresponded permuted index uh so let's say these are the symmetries that we are aiming for and we can see that we can achieve them with a single a simple message passing network uh so let's say first we have the points and we have the edges of the point of Interest which is the central point with the neighbors we just compute the messages using a function that takes us input uh the features of the of the neighbor the point and the relative ER position of the neighbor to the point so this difference here so after we compute this messages with any function Pi here uh we can aggregate them with a simple summation to get the aggregated message and then if we pass the aggregated message through another um General function that can be any MLP or whatever function of choice we get the output uh which is this blue Vector feature so here if we translate the input because these functions that that computes the intermediate messages don't see the translation because they get inputs of a that are invariant to this transformation that we want to be H equivalent in so if we translate we just get the same message and also if we permute we don't get now the same messages because the Ines of the messages change so we can get a different order but but the final aggregation that we do uh it's uh independent of the order because we just add everything so we again H we get again the same output so here we just achieved a translation and permutation equivalance without getting any constraint on the models that we want to use so this F ER that computes the Mage and f f that uh aggregates uh it's not constrain it can be any MLP and we got that because the input representation to these functions are invariances to the transformation that we want ER so this is close to canonicalization so these functions always see a canonical image of the input which doesn't change if we transform it uh now let's say we want to do the rotational quarian and we want the the input to the output sorry to rotate when we rotate the input ER here again we can use the same uh canalization techniques that were described previously but uh again it has a lot of problems uh that we need to address ER like the problems elos mentioned so uh let's describe what we want our Network to do we want uh given the input if we pass it through the function to have this commutative diagram which we can also Express as uh yeah by just uh replacing the order of the vector in the output so if we rotate the input we pass it through the model and we rotate back the output we want to get to the initial positions uh so if we try this to our model to our message passing model uh and we input the rotated version we can see suddenly that the messages that we compute are rotated and this happens because now the the transformation that we apply is not a does not change the input that this function see see so the inputs are not invariant to the transformation we are now interested in and we can see that even if we rotate back H again the output after we rotate the input and we rotate it back it's not the same as the original output which is our goal so this way we get the constraint for the equivalant message passing which is that we want now our message function when the input is rotated and we get the output and we rotate it back to be exactly the same as the initial unrotated output output of the unrotated input uh yeah so we get this equivariant message passing and the constraint uh that we need to put in our models and uh we can see that uh these constraints can also applied in a different setting where we specify more about what are these functions and instead of having a general uh function that takes the input features and the difference between the the points we com we say that our message now is just the the the feature of the neighbor multiply by a w which is a function of the edge and then if we also H simplify the aggregation function to a simple summation we get the steerable convolution and if we put the translate also the constraints that we arrived for the garan message passing we get the constraints for the steerable convol Al kernels uh which can be seen as taking the input Point Cloud looking the positions in the learn Kel that uh correspond to the relative positions of the neighbors and putting this position as messages and then just do the aggregation so we see that in this setting ER for the equivalant message passing we need to constrain the message functions and in the stable convolutional uh in the stable convolution we need to constrain the kernel and uh how we can do that which is the question so how we can impose the equivariance uh one simple way if we assume that the vectors that we are interested in are 3D vectors is using Vector neurons so here the input is not just a vector but it's a it has M it's m stacked 3D vectors Al together and the linear layer that we are interested in is just a um a left multiplication with a W Matrix a learn W Matrix and we can see that the implementation is pretty much applying the linear layer to the input Vector just we need to take care of of applying the layer to the correct dimension of the input which is for example here the second dimension because we have BDS the stack channels the extra three ve Dimensions which correspond to the vectors and yeah the points that we have H now that this is like a simple way to solve the problem but we can also go to a more General way which is using H steerable convolutions and we will show an example of s SO3 steerable convolutions so as we described before what we want to achieve here is to solve to find this kernels W that satisfy this constraint and U yeah this is a linear constraint on the Ws and uh a solution to this is H describe with this equation where you see that the W's if it's they are expressed as a basis of H why here are the harmonics and Q are the KPS Gordan coefficients but you can think of it as a static ER basis you don't need to learn anything and then the only degree of freedom is to learn this five function that takes us input only the Norm of the X which as you can see the x is the uh relative position with the neighor so for the SO3 stable convolutions we want to implement H these kernels that it's some learn coefficients for a a known basis which involves the harmonics and the KP Gordon and this can be intimidated but uh there are pre-implemented libraries and especially you can do it with the e3n library and we have a an example showing how it's it can be done so this Kel pretty much wants to do the KPS G and test product between the harmonics and the input feature so how we do that first of all you can see yeah as we said we can use the implementation of e3 andn and you can see that we can describe our input representation which is more General that what we had uh before so we can say for example this is a syntax thing but this input means that we have five scalar features and uh five Vector features and in the output we want five uh scalar features and one vector feature uh so yeah we can describe any combination of features and after we describe that we can just let the pre-implemented tensor product to do the rest of the job which just need as input the the representations that we want the input the input and the output to have and also H it needs the weights that you can see here uh because uh yeah we want compute these weights using an MLP that takes a single input which is the norm and outputs H for each uh J here the corresponding weight that we learn H so after we defined uh this module we can just do a forward pass by taking the spherical harmonics of the H Edge difference which is the relative position between the neighbors ER passing the norm of the relative position to the length Network to compute the weights and then putting everything together in the tensor project so this example was mainly to show that although the uh SO3 steerable convolution kernels the solutions seem intimidating there are implementations that you can use uh easily use ER to implement it in your work if you are interested ER yeah but uh again ER it's more complicated than the other example of vector neurons but you get the additional expressive of defining the type of input and output representations that you want um so the other part of the puzzle is the nonlinearities that because until now we just describe the linear layers and a general uh recipe used in a lot of works for nonlinearities giving the input features to disentanglement into an invariant part and an equivalant part so if we apply the the nonlinearities on the invariant part uh we can do whatever we want and then we can combine it with the equivariant part for example as Carlos mentioned in the previous talk the equivariant part can be the phase and the invariant part can be the magnitude so we don't want to mess up with the phase because this gives us the orientation but in the invariant features we are free to apply the nonlinearities that we want so this is an example of getting the input uh Vector features passing them through a a norm for example to get the invariant part apply a nonlinearity here which can be a relu and uh multiply it with a equivalant feature which can be the initial vector or a normalized one but yeah it's up to choice and this is the nonlinearity using the test tensor field networks H but we can do more complicated stuff for example we can uh produce two equivalant vectors and uh get the dot product to get an invariant one pass it through are and uh as long as we don't uh mess the equivalent part of the the the the features and we just H interact let them interact with the nonlinearity with a simple multiplication you can do a lot of different combinations to get the nonlinearity so this diagram is used in Vector neurons but uh in general it's still an open uh area of research of finding a nonlinearity that works well it's stable and yeah it's still a something the communities uh searching for and uh we can apply this same recipe of a getting the dot product with between equivalent feat to achieve a nonlinearity and um invariant feature uh to also Implement a tension in the layers that we described before so you can imagine that we have a a message passing function and we introduce here the query feature which also is a derived from an equivariant function that uh takes us input only the feature of the node we are interested in and if we take the dot product ER and apply the soft Max uh we can use this soft Max to to aggregate the new messages that we originally we aggregated with a simple sum but aggregate them weighted by this uh softmax from the dot product so we get this typical attention module where you have the query uh this would be the value er no sorry the query the key you take the dot product and you just uh aggregate the values using the weighted attention score for each uh message and we can see because assuming sorry uh yeah it's here yeah can I think I can thank you so assuming that the the messages are computed by equivariant functions uh although we rotate the input the the dot products that we get are invariant to that so we can use this property to implement the attention because you can see here here the soft Max will give the same result regardless of whether the input is the original form or uh rotate and we get just the rotation on the value part which will uh yeah goes out of the sum so in the end what we get is just the original output FJ Prime uh rotate by the Yeah the output rotate so we can yeah achieve the equivalent property so this uh General attention layer and the previous message passing ones ER are a way to achieve equivalance but as evangelos mentioned another way ER similar to that is going through group convolutions so here we saw an example example of how to do that but for Simplicity we just use the discrete group of 90 rotation so uh how we go with the group convolution we take the input and we rotate the input or you can see it as the Dual of rotating the kernel but here for visual we rotate the input uh to get all the with all the ele ments of the group so we get four rotated versions of the input and then we multiply with a kernel that it's unconstrained to get an output and the size of the output is the size of the elements of the group so how we get equivariance here uh let's say we get a rotated version uh by 90 if we do the same operation and we rotate the ated version we will get the new inputs and you can notice here because uh the group is cyclic ER this rotated input correspond to the second to the G1 rotation of the first and the result of this is that if we apply the same uh ER rotation of all the group elements we will get just a uh shifted ER image of the initial input and then if we apply again the same uh kernel convolution we will get the output but it will be just cycl shifted to the top so a rotation on the input correspond to a er yeah a cycle shift in the in the output ER yeah corresponding to the cycle nature of the of the group that we have we are interested for which is the C4 um yeah so this is another way to implement an equivalant model and it has the additional benefits that now the kernel can be unconstrained uh and also we can use H because it is unconstrained and the output is just a function on the group so it doesn't have the vector nature that we didn't want to mess up before we can use any nonlinearities that we want and H still have this equivariant property so here this H this uh transformation can also be more complicated uh forward passes uh yeah and we still get the property because the input is just a symmetric shift so the output will also be a symmetric shift uh yeah H but the negative which we didn't have with the message passing version is that uh now we have for a simple group of a 90 rotation we have four copies of the input so you can imagine that the uh yeah the memory constraint TR in our Network are uh yeah the memory requirements are higher than H doing an equivalent message passing but we get the Simplicity ER from being able to do any transformation after we leave to the group ER and yeah this um this point encoder can be applied to many applications so we can use it for Point Cloud processing which is a natural one but also can be applied to computational chemistry for example to compute the properties of the molecule which should not depend on the on the orientation of the molecule and also in a robotic manipulation uh similar to a talk we saw before in one of the spotlights where we want the features that describe the object to rotate accordingly to the transformation we apply to the object we want to manipulate and also if we go back to to the images we want uh and we consider the problem of uh to the key Point extraction we want a transformation on the input to not uh affect the matches that we have so this is also the concept of the uh C features but also it's still an area of open research so this is from 2023 and if we go to the image uh domain where we have a fixed grid we can also do more efficient computations because now the The Edge difference that before can be anything it's predefined and uh yeah there is also a a nice libr esnn that can be used to achieve equivariance on grids predefined grids but here you can see a property of the this last example that uh both the both pair of images rotate so we don't have a single rotation but we have both pairs rotate differently and we want the output to uh yeah being dependent of the rotations so this uh property is beyond this the typical equivariant definition and pretty much is what we saw before as by equivalant ER uh yeah so in the typical equivariant we have a rotation of the input that corresponds to a rotation of the output but what if both the input both pairs of the input rotate with different Transformations how we can like connect the two puzzles what we need to do is first to get the the blue puzzle to the initial position then do the original transformation that we had before and then do another transformation which is the T Y the T yellow uh to arrive to the final uh yeah to the final connection of the puzzle and uh if we compose all this transformation together we can see that we are interesting on a new transformation where the individual transformations of the input are applied to the left and to the right of the initial prediction that we had and this is the by variant property and uh yeah this is a more general property than the original equivalance where we have a single transformation acting but we can still use the equivalant encoders that we have built uh to achieve this and an easy solution is extract a equivalent features and then multiply them together to get a bivalent feature and if we assume that these features are 3D vectors we can also retrieve a rotation by applying a procr and the by this operation you can see that we also get the balance property that we want because if we rotate uh the inputs with different Transformations when we construct the bivariant feature the one transformation appears on the left and the other on the right so since the SVD doesn't affect uh uh this property the of the input we will get doing a procus this we will get a rotation which is the original rotation with the corresponding transformation on the left and the right um yeah and these applications can be this by equivalance property can be used on point Cloud registration where we want to register the point clouds independent of the rotation of the input it can be used in pick and place which was the paper that was presented before in the spotlight and also it can be used in protein docking it has been used in protein docking where we want to find how to how uh protein dogs with a lent ER independent of the original position of the molecules and uh this by variance also goes beyond the rotation so we can also see it in the permutation equivariant where we have H when we consider matrices that we apply permutation both on the columns and the rows and we want the output to also permute H by applying the corresponding uh permutations on the columns and the rows and uh in hard for atal they showed that they can achieve this by variance property with a linear layer and they show an an example of the general solution that this linear layer has and uh this also can be applied to different applications but I will go a little faster due to time but you can see that uh yeah let's see say we have input uh images and point tracks we can permute both the images and the point tracks and uh we want the output to be the corresponding one to the transformation we applied but also if we consider weight matrices from neural networks we want ER when we permute both the input and the output columns the network to also ER see that these two matrices after the permutation and before are the same because it's just permuting the The Ordering of the channel so it is the same uh Network and the yeah this can be applied for a processing weight matrices of neural networks uh for either changing the networks or regressing some property that the network has um so to close up we can go through some limitations that we currently have in the space of a equivariant deep learning the first one is the increased computational and memory requirements which is especially for the group convolutional layers but there are a lot of works that try to mitigate that so for for example we have the EPN equivalent Point Network that suggest a separable convolution that does a convolution on the on the R3 space and then a convolution on the rotation space and it combines them to do the whole uh sc3 convolution then we have e2n which also uh proposes to do the convolution on the sphere instead of the whole sc3 and also the recent paper from Eric that presented in the morning H that they defined an equivalent classes that U different weights of the network uh are parts of and this equivalent classes and the networks that uh are contained in them s their weights so they can also achieve equivariance in this way H another limitation can be that solving the constraint that we saw before for the for example for the SO3 steerable Kel can be non-trivial so some recent works on that are the implicit convolutional kernels so instead of trying to solve the constraint of the kernel which just use an equivariant MLP to yeah implicitly learn the kernel so for each input X and let's say whatever latent we are interested instead of having a close solution for the kernel we pass the input through a equivalant MLP and because it's pointwise so we don't need a kernel on the whole Space H it can be more easily modeled and we can uh yeah restrain it to satisfy the constraint that we are interested in finally finally in a lot of task while in an ideal scenario the task have symmetries in practice and when we measure er uh yeah when we we measure the properties that we want the symmetries are not followed and they break and uh this can have a detrimental effect if we want to model non-symmetric data with a equivariant model because it cannot process yeah the extra degree of freedom and uh on that end there are recent work of introducing approximate equivalant networks that allow for the model to partially follow the equival constraint but also have a degree of breaking degree of freedom for breaking the constraint for example instead of a typical convolution we can have an extra learn learned W that uh yeah breaks the constraint of equivariance and uh they have used the W and T they have used these networks to also show that in physical systems uh for different um scale of the system you can see different degrees of H symmetries and equivalance for examp for example in the flow if you scale down H the flow should be equivariant so don't depend on the orientation but in the large scale ER actually the flow is not equivalant because it depends on boundary conditions that create this equivalant error so for the larger scales modeling the flow with an equivariant me method can reduce the performance and they show that allowing the network to break equivariance in some uh uh parts of the network actually improves the performance and this has also been shown more theoretically by Petra and tedi uh yeah so the last limitation that uh uh we can discuss and it's an open question is that uh in general in the whole uh conference everyone tries to scale the models with more data so yeah how we can do that with equivalant models that require more computational constraints and do we need this model in the current setting of big data I think the answer should be yes as we have seen throughout the day but yeah it's an open question that we need to discuss as Community H so I will close with some resources of some useful libraries of for implementing equivariant Network and also some books and courses that you can refer to as a starting point Thank you thank you again for this next tutoral I guess next time when people have a summary of resources our Workshop should be added to the list yeah yeah um I think um our program is done here today thank everyone for coming to our Workshop we cover a lot of exciting topics and we also have this tutorial introducing the like how to get started with EP variant um when when I was listening to the tutorial it reminds me of my undergrad when I was doing mathematics so at the time like we we were required to take a physics class so I was asking people what physics is easier and they were saying um probably quantum mechanics because that is only lar algebra so then like I started to realize like when people try to pretend something easy to model they say oh it's linear algebra although this one is a j but I think it's actually very fascinating that the simply thing in the world that is the linear transform informations can actually model so many fascinating phenomena in the real world like in physics or even in some logical structures and also it's very lucky for us that for computer science like nowadays we have tpus gpus the processor units are actually very good at Computing linear algebra or like the tensors so there are actually a lot of opportunities for us to use computation algorithms nowadays to model the geometric structures in the real world um and also as mentioned in the end there are still a lot of challenges like data from The Real World they have noises and they like computation cost and we still need to tackle all these things in our future words and finally probably like a couple words to complete before I thank the organizers I would like to point your attention since is organizing work organ gave inv talk organiz the work the T geom workl and there also the Symmetry right so I think the main B of e

