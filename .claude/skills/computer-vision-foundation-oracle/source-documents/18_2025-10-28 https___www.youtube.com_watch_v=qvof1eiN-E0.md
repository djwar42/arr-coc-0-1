---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=qvof1eiN-E0"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:04.348Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=qvof1eiN-E0

62910b36-bc15-41dc-89a4-83483eaedc5c

2025-10-28 https://www.youtube.com/watch?v=qvof1eiN-E0

4ac6c158-9311-4837-8b64-bfec7f7bf2db

https://www.youtube.com/watch?v=qvof1eiN-E0

qvof1eiN-E0

## ComputerVisionFoundation Videos

okay so graph structures are all around us probably all of you are familiar with graphs and uh the graphs can be anything they can be molecules with their atoms uh connected in the form of a graph or social networks social media with people as nodes and we can have knowledge graphs and 3D mesas and all of these structures have something in common and they can be defined in the form of a graph for us in the computer vision Community graphs in the form of thing graphs and also meeses and in the general like graphs for vision are more important so we most of the talks today will be about sing graphs but also because nowadays the topic of vision language models have been has been more prominent there will be Al also some Works about using llm for sing graphs and Etc so uh sing graphs before we know them in the computer vision word Were Somehow used in the industry of graphics and animation if you look at this uh uh animations you can see that they follow the same pattern and uh we can say that these two have the same sing graph but uh with different appearances and these have been in 1970s or even earlier done by Pixar and they defin the universal scene description that as you can see these are the same they belong to the same scene uh they have different visual appearances but the same description and sing graphs as we know them nowadays were first introduced in 2015 by Johnson at all and later uh the visual genome data set was released which is one of the main benchmarks in sing graphs later 3D sing graphs and also semantic sing graphs for different applications were introduced also some of these were by Archer federo tomari and ner NAA and later which is also more relevant these days because the models are more popular uh the there were some data sets for temporal sing graphs such just action genome and home action genome that model the videos of V sing graphs and as you can see the number of Publications on sing graphs has increased in the since the early Advent of this topic and it's still increasing and there are still a lot of challenges with sing graph such as panoptic sing graph Generation image generation and manipulation with sing graphs using knowledge graphs and llms for for the for also for sing graphs and singra generation from weos today we have the a combination of Talks by our invited speakers and also the uh Talks by you uh and also the Talks by the people who had their papers accepted we will start we started with the opening remarks with the oral session number one which will we'll have four papers then three papers as a spotlight presentation then we will have our keynote speaker Dr Krishna Morty and then we will have a coffee break with the poster session and the second oral session and two other keynoted speakers the professor fer trisp and Brian perzi and in the end we will have a small uh conclusion and here I would like to also thank the my co organizers Professor ASI which is there in person and I think U and federo will also be there and uh now we can start with the first oral s awesome thank you uh thank you so for the first orall session the four presenters do we have the first presenters uh segment anything for Road Network graph extraction congre no or online all right so we'll probably either skip them or wait until the end of the session the second uh speaker is are they online do you want to I can play the video for their with the second presentation and then uh are you also online for the Q&A the the first presenter is here okay then we can okay I just need to make sure screen is shared uh I I think I I need to share screen can you have you stopped yeah I guess I I just made you cohost okay thing is I do not see the okay where is the share screen button of Zoom H screen is on the other side okay got to this is nor maybe yes yes okay um so good morning everyone my name is sun and today I will present our work name segment anything model for Road Network graph extraction our code has been released you can find it by searching the title so the problem we are trying to address here is to extract Road Network graphs from a image an arrow image that might be representing a large city region we want to predict the vectorized road Network graph represented by vertices and edges the vertices might be representing the intersections and key points the edges could be representing the road segments this can be seen somewhat as a special type of Sy graph prediction if you see the well vertices as the entities and the edes as their relationships so this type of graphs has actually many applications say auton autonomous vehicles could be utilizing them for Road planning and they are the foundation of navigation maps such as Apple Maps and also creating this kind of large graphs is kind of expensive if done fully by manual labor so like it is of act under active research to for automatic and accurate methods to um automatically create them so the segment anything model we are actually trying to incorporate segment anything model to this task in this work so this is actually a foundational Vision model released in 2023 um it is trained for prompt guided instance segmentation and it is extremely good at this task showing extraordinary accuracy and impressive zero shot generalization capabilities this is essentially a large visual Transformer train on millions of images and billions of TKS which gives its which is the root of its capabilities so since it's released there has been a lot of applications of the segment adding model for example object detection medical image analysis and remote sensing and even visual tracking however most of these applications are mostly still various types of semantic segmentation tasks and how to leverage for predicting more structured objects like a graph itself has received less attention ition and in this work we try to answer the question whether segment anything model could help the task of predicting Road Network graphs and if and and how well it can do it so some prior works on the topic of predicting predicting row Network graphs um these Works can be roughly divided into two categories the first one is segmentation based method which is basically treating the task as pixelwise classification saying whether this pixel is the belongs to the road or not and many early earlier Works actually just stop there and call a day and some follow-up Works will use postprocessing characteristics for vectorization um to to get the vectorized graph these methods are quite simple and straightforward and they can be made efficient by paralyzation and also like the volumetric representation of the dense semantic segmentation mask naturally handles extremely complex shapes however like these heuristics could be their bottleneck for performance and also like they are quite complex and brutal and it is actually quite uniform to deduce the vector vectorized graph from the then semantic segmentation mask there are no Universal her istics that works for all cases the other category is actually called graph based methods so this type directly predicts the graph structures from the net they usually apply techniques like the uh transformer for object detection called dater and also like Auto regressive methods to iteratively construct the graph in incrementally so the pro of this type of method is that they are uh somewhat more into end there are less hand crafted rules to to help you get what you want um however like my finding is that they scale quite poorly to very large regions and very large graphs for example the dater really cannot detect more than a a few dozen object at once and also these Auto regressive methods they have to uh build graphs step by step by step token by token they depend on the inference results of previous steps so it's kind of hard to priortize making them slow so our method it kind of has the benefits of the both categories while trying to overcome the shortcomings of both um it directly predicts the um directly predicts the graph structure the nodes and vertices so the overall framework is can be seen as three stages first one is the encoding stage second one is geometry prediction and third one is topology prediction the geometry or the graph vertices are handled by then semantic segmentation we first predict the masks and then use a simple non maximum suppression to extract the sparsified vertices and then the edges between the vertices are predicted by a graph neuron Network um which just wires up this the connection of these vertices this defines the topology so the encoding stage is basically we take the pre-end the image encoder from the segment segment anything model this is basically a plain vit structure it is a stack of Transformer layers interleaving the window and Global attention there are no bells and whist such as like a feature net feature pyramid Network Etc so this encoder produces a feature map of the 1116 of the size of the original feature map of the original image actually so for the geometry prediction stage it it is extremely simple it has a minimalist design because the the geometry decoder is actually just a mask decoder composed of four transpose convolution layers to predict two dense binary masks uh corresponding to the rows and intersection points respectively um it predicts this from the uh feature map produced by the image encoder and then after getting this masks we apply the so these masks masks are the dense masks we want the sparse vertices so we apply a no maximum suppression process to uh basically keep the high the pixel of the highest score within a certain radius so this gives us the sparsified vertices so as shown in this image you can notice those little yellow dots so the geometry prediction despite its simp Simplicity is it actually um it is highly accurate uh you can notice like these SE and complex structures in the tense urban areas this is especially interesting because there are as mentioned no specific architecture design in the backbone Network to really make this network good at these kind of structures so the topology prediction stage uh so what it does is basically after we get these vertices we go over each of them and see whether and and and see it's find its like close neighbors within a certain R radius and see whether it is connected with all these neighbors um so the way it it is done is basically for all these vertices we extract their features from the image feature map through bilinear sampling that makes the feature of the vertices and then for each pair of source and Target we concatenate their their vertex features and also their relative offsets this makes up the feature Vector of the edge so all the candidate all the feature Vector of the candidate edges within this circle are passed through a graph neural network uh which is actually just three self attention layers uh because attention is apparently all you need and So eventually they predict whether this edges are actually connected so the ground truth label you can is gained in this way you can see it as starting a BFS search on the ground Tru gra from The Source node and and and the search will stop whenever it hits uh hits another node or like it goes out of the range so all the nodes are hit by the search are considered connected the others are not and during training this decoder uses teacher forcing teacher forcing meaning it takes the vertices that are the noise version from the ground Tru instead of the predicted ones so the training setup is is also actually quite simple like um I will just skip this you can refer to the paper uh there are no bells and wisos all the loss terms are just plain BC loss no focal loss no no that kind of stuff training is quite fast it takes about one hour on one RTX 490 GPU and also worth noting is that our method can use sliding window inference to handle large regions so basically what this means is that both geometry and topology can be inferred independently first for all the individual windows and then they can all be aggregated together to refine the quality uh I will skip the data set section you can find that in the paper so in conclusion the results is that our method despite its Simplicity it performs in terms of accuracy on par with the more complex recent state-of-the-arts well uh it is actually an order of magnitude faster due to that it lacks it doesn't have this complex characteristics and it's easy very easy to priorize and you can get even more out by sparsifying the sliding window inference so um you can find more appliation study in our paper but in conclusion we demonstrate the power of the segment any Model A foundational Vision model on the task of Road Network graphic extraction it reaches the state-of-the-art accuracy with a much much simpler minimalist design while being much faster and we hope this work provides additional insights into leverage and foundational models for sing graphs and prediction and graph representation learning again if you're interested scan the QR code and thank you for your time thank you uh okay if there are any questions we did have we do have time for questions if if anybody have any otherwise we can go to it sliding window sure so the uh where do I do I have the so what happens is that uh there are two passes in the the first pass the mask for each so basically we have this individual sliding windows in the first pass the masks for each window are predicted in parallel um so you have this individual masks they are aggregate together in a Glo Global manner so you can vote towards the the large image and in the meantime um when predicting the mass we also cach the the image features for for each window so uh after you get this global mask you can apply the same process to get the global vertices and then like uh we the topology decoder goes over each window first it first predicts the edges edges within each window and since now the vertices are Global you can actually vote towards the global vertage for the edge probability so now that uh yes so so basically they can all be aggregated together for for Quality refining perfect couple questions um how does curve questiones curve so like uh first this mask they can apparently handle the curves so like the MMS process it basically like turns the dense pixels into like a set of like vertices like you can control like the distance between this vertic I make it small enough so that it has the sufficient resolution to to handle curves how no exactly that's that that's why the vertices are not the grr labels the GR label is the mask and the vertex is just whatever you get after the nms from The Mask second that don't inter but top each that's a yes that that's a great question so um that's actually qu uh one of the limits of the current work um so but like luckily like well in most cases actually you it can be handled by you know as long as you don't have one vertex that happens to be at the exact location that two layers of rows like intersect with each other like the topology decoder has a good chance of predicting the correct edges that overlaps each other but like if you actually have you get get a Vertex there you you are out of lck then then it doesn't handle it thank you perfect thank you very much so great yes uh for the second presentation uh the presenter is online and we going to see the video first and then hello everyone thank you for being here today my name ISO and I am excited to present our work eax efficient and flexible petline for spatial temporal trajectory graph modeling and representation learning with the rise of GPS and location based Services the amount of trajectory data is growing rapidly this data is valuable but challenging to analyze we need efficient and accurate methes to model it and gain useful insights graph modeling is an effective way to capture the spatial and temporal dependencies in trajectory data our goal with eflex is to provide an efficient and flexible solution for trajectory graph modeling and representation learning and we offer two models Flex B which is optimized for Speed and flex L which focuses on achieving high accuracy spatial temporal trajectory modeling involves virus approaches traditional methods such as basic statical and heris techniques of struggle with large data sets and fail to capture complex patterns to address these issues graph based methods have been developed these methods converge trajectory data into graphs like roow Network graphs and Activa graphs but they come with high computational cost and complexity to further enhance representation learning graph neuron networks or GNN are utilized gns effectively capture spatial and temporal dependencies by learning embodies from graph structures however they require significant computational resource and sometimes lack flexibility to address these challenges we introduce e Flags an efficient and flexible peline with the Dual model approach addressing both the Need for Speed and the need for accuracy in trajectory data modeling Flex constructs graph directly from raw trajectory data converting the trajectory representation learning problem into a task of graph embedding learning formally assume t as a set of end trajectories each vertex VI represents each trajectory TI originally all trajectories are represented as least of longitude and latitude the weighted adjacency Matrix is computed based on the relative distance between each trajectories to capture both Global and local information we compute multiple adjacency matrices based on different K values and then fuse them using attention mechanism for a set of K we can have a set of weighted adjacent matrices in M scale these matrices are then stacked and passed through a sequence of learnable linear transformations to compute the attention ways W the final Fus adj Matrix S Prime is showing here the fusion operation allows the model to leverage features dynamically across multiple scales keeping the original information as much as possible now we have converted the massive original data to a large graph to further reduce the dimensionality of the data the inflex pipeline has a graph representation learning module which uses a lightweight graph convolutional Network to generate accurate graph embeding formally given FB as note features and S Prime as the adj Matrix the GCM model M can be represented as this formula where W Theta and Delta Theta refer to the learnable weights and the bias parameter within the gcn the output of the model indicated as em Theta is the Learned embedding of end trajectories each as 1 * D embedding Vector where D is the perent embedding Dimension to instruct the model to generate accurate graph eddings we implo cosine similarity distance as a loss function here GT indicates the ground troose by Computing ukian distance between every two trajectories in the data set benefiting from the generaliz and flexible pli framework eflex offers two versions eflex base and eflex large with different models employed in the graph representation learning module eax B uses the lightweight gcn for fast processing while eax L uses no toac for high accuracy these two versions provide the inlex pipeline with the ability to handle diverse application needs to evaluate our P Line's performance we conduct extensive experiments on two commonly used TR trajectory data sets collected from The Real World data points Portu which contains taxic trajectories and G life which includes GPS trajectories of virus users considering the complexity and variability observed in a real world traffic trajectories the proteo and G life assets are ideal for evaluating model performances to conduct objective evaluation we evaluate the model performance on top end similarity search task problem specifically given a query trajectory the model outputs the top end similar trajectory based on the embedding under certain distance function the higher similarity search accuracy indicates the more accurate learn in vals of the original trajectory data set by the model the distance function we use here includes H dool frette and DTW following the standard evaluation procedure two evaluation metrics are involved the first one is heating ratio which is hrf 10 and hrf 15 the second one is recall which is r10 and 50 also we compare the results with both non-learning models including PCA SBD and MDS and state-ofthe-art machine learning models including siames and UT trade and t3s this table shows the graph representation learning performance both of solutions e b and EA L outperform existing models and Achieve state-of-the-art results under the House of distance metric effect B achieves a heating ratio and a recall of 55.1% 64. 92% and 98.17% effect effect L performance even better with 56. 51% 71.2 6% and 99.84% iflex B and fxl both showed significant improvement over nonlearning and learning based methods leading heating ratio and recall similar results are observed in the DU life data set our models multiscale K and graft construction and fusion help preserve important Global and local features using the graph convolutional Network allows our model to converge effectively capturing graph level representations and significantly improving performances we also compared the efficiency of eax b and e l on Port two and du life data set using the same CPU environment we Ed the recall as the accuracy metric and measure the time cost under three distance functions the results show that effect effect B achieves competitive accuracy while significantly reducing the time cost thanks to our lightweight GCM design Flex spe captures graph structure patterns effectively and improves training speed FX B being an Innovative design highlights our contributions to learning graph representations on spatial temporal data and accuracy and speed make it suitable for lightweight platform forms like mobile phones webo devices and autonomous driving for applications that don't require realtime modeling such as environmental monitoring effect L is ideal due to its high accuracy this figure shows the trajectory quiry results using the port your data set given a query trajectory our model finds the top three similar trajectories based on learn embeding the results closely match the ground truths proving the effectiveness of e these results validate our approach approach of dimensionalizing spatial temporal trajectories for graph construction efficiently representing large data sets effect also has practical applications like finding alternative rules in navigation we conducted appalation studies to verify key components of our model first our multic scale can and with attention outperforms other methods providing more accurate learning second multiscale graph construction is better than single scale capturing both board and localized patterns third increasing EMB adding Dimensions from 16 to 1024 improves performance making the representation space closer to the original data this table shows the appalation studies on different LW terms where cosign similarity distance are performs L1 lws and MSE laws considering the ability of cosign similarity distance to ignore the effect of data sparcity and dimensionality it is less sensitive to outliers and variations in the magnitude of of the spatial temporal graph representation learning this indicates it to be the ideal loss term for stable pepine training in conclusion eex offers a novel and efficient approach to spatial temporal trajectory graph modeling and representation learning our pep line significantly improves both speed and accuracy making it suitable for wide range applications in future work we aim to further optimize effect for greater scalability and real-time applications such as traffic management and environmental monitoring we're also exploring the integration of large language models to enhance the sematic understanding of spatial temporal data which could improve predictive accuracy and analytical depths that's everything for my presentation thank you for listening thank you uh I believe we have the speaker online and if there are any questions uh we do have the speaker online right yes Z is here okay hi everyone my voice is not actually like in the video I speed it a little bit so to save some time okay yes all right so if there are any questions here or over Zoom um please let us know yeah I think this is a little bit of a challenging setup because if you want to ask a question you probably should step here so that's why nobody's asking question all right uh so then maybe we can uh um move on to the next presenter um do we have flow c d papers alter here I think they Al they did confirm for the presentation right because I have the yes the plan was to do the presentation maybe we can do the uh switch and the last paper in this session the oral session um we do have video Sage yes video Sage we have the speaker here let me make sure I do the right setup for uh screen sh just blocking the The View PR one okay need start thank you B summarization with graph representation learning of SC um good morning my name is Jose roas and I work for Intel FX and I this I did this project in collaboration with stiati from in Labs pleas okay ai ai everywhere is like the new in Mantra and that me as software developer talking to researchers at in laabs um so to really have ai everywhere we need to have more efficient and just more effective models that can be grown on our mobile devices and laptops and the problem is that in general Cent video models works on on short video clips so we need to have and that's due to computation of memory bottle next right so we need to have models that can be run on longer videos or at least that can be run on the same B but faster um so if at the end if we are able to achieve more efficient models then that that means we are able to run those models in our computers on the edge and we will be able to get our data local that's more security that's also more power efficiency and that's also reduce response time so how can we achieve these kind of models well we are lucky enough we have gravity gravity is an open source framework froming Labs so you can with this framework you can take videos and sometimes you can also do audio and text so you you can build um graph representations with the spatial temporal data and then with this graph you can do interesting applications like active detection action segmentation or action localization some of the applications are not implemented yet uh but now now we also have theorization so how can you use gravity to build this representation uh it is easy so we took two uh popular data sets on bation son and TV these data sets already come with feutures sted with Google net model and they already come in segmented with the with terminal temporal segmentation it's a microphone whatever it is oh you did something here last time that was for the speaker that's just the speaker so basically what you have to do is to to take and Define a note for every frame on the video and then the embeds of those notes of those notes are going to be the futures from the Google net Network um then you are going to connect those nodes with a limited number of Adent noes by some temporal distance that way you enforce some sparcity in the model and having this sparcity is good so you can avoid hitting computational or memory bottlenecks um so then what you need to do is to define the Tas as a binary no specification problem where you have to say which frame which rame is important which one is not which one is wor going to the sumary and which one shouldn't go uh at the end you are going to have important score for every frame so you calculate an average for every segment and then you can use uh the nextop algorithm to choose the segments that are going to be in the summary in order to maximize the total uh importance of the summary video and the sumar are going to be uh 0% the times of the of the or the length of the original video so typically there are um three evaluation metrics for video mization U the first the first one is the F1 scores and the seconds the kend St spir and row are correlation metrics so it's been proven by that the correlation metrics are better for evaluation of this mization test than the F1 score however many state they are still using Theon score but as you can see if you use p p random uh predictions you can already get a almost 55% score on the F score which is pretty high and it's close to the state of the R so Otani suggest to use the correlation metrics because that way you can get a SE score with the experienc to but on the other hand if you use perfect predictions and really that using the ground room you can see that you can only get up to 46 with the with the method from oani so we change it a little bit the evidation uh in order to use the form range of the metric um so I to explain a little bit here the thing is that there are 15 to 20 user annotations on the data set for each video and T and I'll did this correlation metric to each of those user annotations and then average the results and that's why it kind of get penalized and not get to the one uh but instead we are using the gr through to compare with in the correlation metrics and we think that's a more um typical evaluation appro of machine learning um so these are the expec results we were able to achieve 33% improvements on this correlation metric uh for the Sunni data set and 11% Improvement for the D syst and in terms of efficiency so we're able to do it five times faster on St the St they are and using three times less less memory so getting it to be effective what was good and now getting it to be more efficient is even better in terms of the qualitative results assessing the the videos just by looking at the summary is a bit difficult because it's subjective uh we have a link in our posters you can if you want to check more examp example videos summaries uh but I think one can definitely say that we achieve comparable result with the state there are on the other hand we have uh the importance predictions on the right and in the middle we have the ground through on top is our our predictions from bsh and we have pgl soon which is uh the stateof the art for comparison um by the way this specific example this video got a better S1 scores on PD soon but as you can see um our predictions resembles way better the ground to importance scores so that's another example why correlation metrics are better so we got B uh graph Bas representation learning fravor for for B summerization so we did experiments into prevalent data sets for B summarization s antiv thisone achieving uh more efficiency and more Effectiveness um and we also suggest and apply a more comprehensive evoluation approach for importance prediction and if you want to check uh our repository for this application and others applications you can SC the QR code and for more details you can also visit poster number 63 thank you very much perfect thank you any questions maybe I I actually had a very quick question uh you talked about long-term long with uh understanding using uh these uh this type of uh approaches and and using S graphs in general any comments on like if you have very long videos then you have a very tall scene graph very very big scene graph over time then how this is handled and then uh does your efficiency uh arguments still hold well not not on this application because this data sets have they is only to five to 10 minutes each uh but now there is another application for active speaker detection I think that we were longer videos so what they did was to split the videos in in shorter clips and then uh they will have a graph for those uh shter clips and do the processing and then they will um like Reen the results from each of the clips if I'm not misunderstanding it would be great if you since your you have some audience on Zoom so thank you thank you so if you can you know standard QR code you will go to a toolbox called gravity that's from in laabs open source toolbox that has hand a long form video representation learning and serves many different applications so far we have enabled half a dozen applications we plan to do more and one particular example that's also mentioned for example active speaker detection so in most of the other models they are processing let's say 4 to 10 seconds at a time whereas with our sparse gra graph p we are doing it at one minute time scale for some of the application it depends on the data set the kind of videos that we are processing or dealing with sometimes it is needed to do the processing over short clips at a time and then we do some sort of processing sometimes it's enough to do all the processing over the unfilled videos but so far we are making our strides towards at least a minute or two whereas the traditional Transformer bed methods are able to look at to 10 seconds at hope that answers yes thank you very much appreciate it any other questions or comments from the audience here or online trying to look at the schedule maybe I can ask a quick question before so how do you handle if there are discrepancies between different shots so if there is sudden only a change in the shot within the video well that's already handled by the data set because this data sets going be segmented using kernel temporal segmentation so this method U look into patterns in the video like changing actions or color or other patterns to recognize the like big changes in in the video and then it defines the shs so the DAT set already so it's done during the preprocessing is that yeah after that thanks thank you thank you let's thank the speaker again thank you uh just double checking do we have a speaker uh on handle ball yes we have an here the paper flow code D unbiased Dynamic scene graph uh generation with temporal consistency and correlation de biasing the assumption is he just he's online an do you hear us he has joined online but the sound is not connected an do you hear us Zoom yeah sorry was UN mute yeah I'm able to hear you here Alo are you presenting online or yes yes I will be presenting online okay yeah hi everyone I am manand kelal I an applied scientist in Microsoft based out of India uh sorry for for the delay in the presentation as for the timings I will be presenting my paper title unbiased Dynamic scene graph generation with temporal consistency and correlation devices so this is a scene graph generation for videos and we will be I will be talking about the contributions which I have made uh in this using incorporating the temporal consistency using the flow-based network and the correlation devising next slide we'll be discussing yeah we'll be discussing what exactly the in introduction to the dynamic scene graph generation and the current state current state of the art and the limitations then the proposed solution one one uh quick note there is a lot of background noise where you are and uh is there yes is it better now yes yes so we'll discuss the Dynamics inra generation and what has been proposed in the current estate with art and what are the limitations we'll then uh discuss the proposed solution flow code and the technical approach we took to solve the current state of the art and and current limitations we'll then discuss the results and the performance and finally we will discuss what we conclude as part of this uh whole exercise of doing the temporal consist introducing the temporal consistency and correlation device next SL so current in in the dynamic scene gra generation Our focus is to represent the video in the form of a dynamic graph such that it can captures the temporal Evolution between the objects for example you have a object one and object two which uh which can interact with other objects in a changing environment as a video passes by for example the object one and object two has a relation in frame one and then after frame n the object one and object two are no longer related and there's an object two and object three which is related along with some other objects as well so you want to EV learn this temporal evolution between the between the graphs which is dynamic across the frames so this is uh this is called a dynamic Sy gra generation and this is this is various applications like when we want to ask about the fine fine grain information into the video it can help in the video question answering task and we can summarize the whole video in a form of a simple simple oneliner state statement we can we can also do that by summarizing a graph and that that can help in a video captioning along with that with the we can assign some text to the video as well so that it can helps it in real time for the retrieval task as well so there are some challenges associated with it that the dynamic relations and the temporal Dy Dimension has an added complexity here so so people have modeled this in a various ways related by Transformer where the input is the where the input is the number of objects uh which which the object has relation with across the temporal Dimension and then it tries to classify the predicates between these objects uh related with the object one and try to learn the embeddings of the predicate predicate and get the uh get the get the different relations as the time passes by in this way it can learn the dynamic relations between the object we'll approach this problem in a different manner in this uh in our in our proposed method for dynamic SC graph generation we'll discuss that in the next SL so as I said the current state of the art has leverage spatial leverage transformer for learning the spatial temporal context in the spatial temporal context what they have done is for a given object they have collected all other objects on which for which the object is related and they model this using the and he model this using the temporal by feeding all this to the Transformer and and this this learns the temporal context and along with the spatial context as well that the all other objects which are in the same frame are beinged to some other objects as well so and learning the predicate embeddings and classifying relations between the object which are dynamically evaluated will eventually result in a special temporal context learned by the transformal model and is able to is learn what how the relations modify over the time for a between a given set of objects but there exist is a class imbalance issue in the data said as well that some relations are very frequently occurring between the objects while some of them are very rare so here not only the temporal context need to be maintained but we need to learn the rare ones rare rare predicates as well so that we will be able to accurately generate the scene graph for a video and hence the not the recall at the rate K matric actually the mean recall at the rate K Matrix is a better evaluation Matrix here so so with the recall at the rate K matric we will show how better how how better our algorithm is in terms of generating the scene graph which is overall correct but mean recall at the rate K will give us the uh give give us a sense that how it is performing across the relations across the different relations in a average manner so it will it will it will give the equal vage to the rare ones as well and hence it is a better metric for evaluation rather than just a recall atate gric so next SL so our objective in this proposed method is to generate an unbiased Dynamic seam graphs and with unbiased we mean that that both the rare labels and the frequently labels are been given the equal importance and and that can so that they can accurately present the video content we introduce the flow aare temporal consistency for example in frame I and I + 1 we calculate the flow between these two frames and using the flow features been calculated we are feeding the flow features to the Transformer model to be able to classify whether the that that object belongs to which category in this way we are being able to identify the changing nature of the object for example in frame I the object is BL and frame I + one the object becomes a little bit more clear then using the flow features we have been able to identify what exactly the objectory is because in the SC craft generation in the previous state of the art it has been identified that most of the object detect object categories detected is incorrect and because of this mislabeling in the object categories the labels detected are also Incorrect and hence we'll correct and hence we try to detect the correct object detect category using the flow aare Temple consistency once the object categories has been detected we then propose a correlation devising algorithm correlation sorry an we have one more minute could you please WRA up yeah yeah sure so with the cor and we then apply the correlation devising and the uncertainty a weighted loss along with the label correlation based loss so the whole idea is with the correlation deicing we want to generate we are accumulating the attention matrices that is uh that is generated by the trans that is used by the transformer in generating the predicate embeddings and we'll take a mean over across mean across the aox of those attention matrices to devise the correlation matrices and then finally we use a mixture logic Network and and the derived weights from that use using the epistemic and the alic uncertainity we then wait the we then wait the LW so that the noisy data points are being vited down and the correct data points are being only being learned we also take a clue from the contrastive laws learned using which is taken into account the label correlation for example if uh object one and object two predicate has a correlating correlation between an object two and object three has at least one correlation he tries to learn the embeddings we try to we we contrast we treat them as positive paay in the contrasted learning and others are the negative payers using negative sampling so our approach consist of an floww object detection and the relation and using floww and the relation representing using the correlation deicing and then we classify the predicates using the uncertainty classification laws with the name even with the under the noisy data even under the presence of a noisy data and use a loable correlation based contact so that we can occur the commonly cooccurring labels yes next uh our results outperform as high as 4 4.1% in terms of mean recall K and and and there is an improvement in terms of recall k at the rate K as well which shows that our method is not only been keeping the performance of the base labels which are already been improving but is able to improve the uh rare rarely occurring labels as well hence resulting in an improvement terms of meanall at the rate and hence it can generate the unbiased Dynamic SC will show the qu qualitative comparison in the paper as well for comparing with other sorta methods as well and the task which we have present we have learned is predicate classification yeah and so this is this is threefold benefits of our method is is the significant performance game bias mitigation and and it can adapt to different Dynamic environments across the video can generate the D thanks uh thank you and for the interest of time unfortunately you're not able to go to question answers we already uh lost a lot of time uh let's thank you speaker sorry SP thank thank you okay so um we do have three Spotlight presentations and I would appreciate if the speakers can line up here uh we have the first one would be open prg do we have the speakers of the other two Spotlight presentations we have one and yours is going to be the touch no this is not you the manga whisper right okay do we have the touchup G um speaker also in the room is that you're the second speaker just yes thank you just uh I wanted to double check so we have all of the speakers and let's get started so this is going to be around four minutes of presentation a couple of minutes of question answers s thanks okay uh I will present open pdhd through open vocabulary 3D SRA generation from point cloud with cable objects and open set relationships um so the task of 3D SRA generation is going from a point Cloud representation to graph based representation where the nodes are the objects in the scene and the edes are the relationships between the objects and usually methods use supervised learning to train this task on the most popular data set for about 20 object classes and eight relationship classes for more F Grand data set it's about 160 object classes and 27 relationship classes of course having like a more fine gr label set improves the representational capabilities of your model however these labels are still set during training time or even during The annotation time of the data set right so we want to propose true open vocabulary 3D SC generation without supervised learning uh where not only the objects are open vocabulary but also the relationships such that we can predict the more specific relationship for each object pair clip was a very similar work in the field of open vocabulary classification clip trains an aligned vision and text encoder and uses the call sign similar to predict um the correct open vocabulary class however we find for compositional object Concepts such as relationships Clips representational knowledge just fails for this for example for this relationship between the guitar and the bed so we cannot do what most open vocabulary 3D scene understanding methods do and just distill the clip knowledge into our model so in detail we want to train a graph Network which takes as input a point cloud and outputs feature graph representation of the scene um and now we want to align this basically with 2D Foundation models here for the note we want to embed the object knowledge of open SEC into our graph using a cosine similarity law and then at test time we can just predict the Open varability Class of the nodes by querying the line clip text encoder feature embeddings to get the op vocabulary classes but for the relationship it doesn't work as we discussed already so what we do instead as you can see in the upper right corner um we go for vision language models which have an encoder decoder structure with a d llm as the decoder uh because we want to basically leverage the world and relationship knowledge embedded in the llms so what we do is basically we cut the connection between the encoder the vision encoder part and the llm decoder part and then similarly to the objects we basically uh use the vision encoder blip model to distill uh relationship features into the edge embeddings of our feature graph and then in the final step from the 3D feature graph we can then together with the already predicted object classes the feature embedding of the relationship edge and a prompt like what's the relationship between these two objects we can predict using the llm the open vocabulary relationship between the objects um so this actually works quite well for for a given point Cloud I would highlight this relationship here uh between the TV and the wall where our model predicts that it's mounted on the wall which is very specific uh quantitively we can see that our SRA conditioned llm model definitely outperforms the 2D clip version for this by quite a large margin for both objects predicates and relationships but we also see quite a large gap still to the supervised learning methods that were spe specifically trained for certain class in The Benchmark uh but this is the same for all 3D open vocabulary scen understanding models um and we just aim to close this Gap in the future but because everything in our model is open vocabulary and the predictions are zero shot you can also do zero shot applications that we didn't even train for so for example here we can create for each note a attribute that describes the appearance shape or size of the of the nodes or we can even prompt affordances between objects uh like for example could you lift object X from object y here for example the llm could say that the pillow you can lift definitely from the bed but you cannot lift the bed from the from the floor because it is too heavy of course uh and this actually concludes my talk um this is actually a full tvpr paper so you can also come to our poster on Thursday um but I'm also here to answer any questions great presentation any questions from the audience also on Zoom if there are any questions uh there is one in the room we need some to this stru so it is fully open vocabulary but of course so we train our model on indoor scenes so the distillation part only happens on indoor scene so there will be a bias to towards indoor scenes uh but generally you can adapt this to any kind of scenario without needing any labeled data have you to use this model to so we haven't done this so this is focusing entirely on 3D sing gra prediction perfect yes dat Benchmark uh so we Benchmark on 3D SSG uh but we train actually on scan amazing was there a question online or no I was asking if there is any question okay thank you very much again thank you our next next presenter will will be jingu right great so improving feature representation through graph Centric fine tuning oh hello everyone um I'm Jan uh this is actually C paper so um so we are doing t g improving feature representations through graph Centric fine tuning so in the task of recommendation systems recommendation it has been like widely used in everyday lives like ecomment content sharing and social networking so one of the key tasks for link prediction is to uh one of the key ways that we solve the recommendation system is to C this as a link prediction task for feature graft so for example um so the co We Gather the co- purchasing informations like given item a which items tend to be bought together with it and the features will tend to be text descriptions images Etc so one of the key paragram that we solve this problem is to use pin models such as bird uh to get it like feature encoder informations and then passes through the graph neural networks however this approach actually is because so for example in this ABS co- purchasing graph uh every feature actually has pretty distinct feature representations and if you pass this through a vision Transformer it will uh go towards like very different feature representation that actually does not really have any correlation with the graph structures but they are often bought together so this shows the like the P of this method params and here one of the key reasons why this paragram does not work is these graph agnostic feature Bings actually prevents the for fully utilizing the correlations between graph structures and no features and so the key resarch question that we want to solve in this paper is how are we going to improve the node features from the portray models such as Vision Transformers for Downstream task such as link prediction and to solve this problem we propose the graph Centric fine tuning uh measur on the pre-rain models so this method is basically agnostic to any type of a train models and we prove it's like it's working for a bird v v Etc and so the basic idea is to propose a matric called fure homop where it decides whether fun is needed and then if the feature homop score is less than 0.5 we would Pro we would propose a graph Centric F tuning of the Transformers and for more details please might poster thank you thank you for the grand presentation any question online no okay uh maybe in the room we have some questions any any questions any comments uh with the audience in the room this was a great uh working presentation I I just wanted to to to see uh quick question how do you uh decide or differentiate that there is the need for fine tuning or not I mean what's the criteria there uh yeah so we calculate a feature homop score which basically calculates the per of correlation between two pairs of features over the connected edges so if the over like an edge the feature similarity score for two features is very high it basically says um so graph network is very likely to succeed in this task so we don't really need to update it features but if the correlation score is pretty low probably due to like the different types of colors or stuff like that then we will need to propose a graph cental fine tuning upon the edges just to indicate that these two features are similar amazing yeah thank you thanks all right and the last presentation in this session before we go to a keynote speaker is presented by right yes thank you hi um I'm ragav I'm a PhD student in the visual geometry group at the University of ox uh I want to talk to you about making manga which are Japanese Comics if you haven't if you're not familiar with the term how to make them more accessible to blind people and this is from like a graph generation perspective so really quickly what the goal is um the idea is that if you have a page like this a a page like this we want to go from there to something like this right where we have got a dialog transcript there are three things we want to identify here what's being said who's saying it and in what order is this being said now let's let's unpack that a little bit right so if you've got a page like this we want to obviously detect all the characters to figure out who's saying what then we want to figure out where all the text boxes are uh want to recognize the character this is to say like these two girls are the same those two those four boys are the same everything else is like no really reappearing um we want to then match the text to the characters who saying it so this is the speaker dation element of it um once we've done all of that then you can think about like okay let's just do OCR to get all the dialogues um and then you want to do some sort of ordering to decide what order things are set in um and for that you kind of need to know where all the panels are so a lot of things that you kind of need to do right let's very very quickly anchor ourselves to figure out why this is really challenging right so um with manga you can have characters that here with changing viewpoints so you can have back view front view top view profile side view uh you can have characters that are drawn with bearing Fidelity so same character look very very different depending on how the autist wants to render that and express emotions um you can have characters that are not actually humans um or characters that transform through magic so one character can transform into another character I mean how to you even capture that um you can have resolution being a problem so characters can be drawn very very small and you want to detect those which is tricky or with text you can have you know this text but there's no really character in that panel there's no obvious of the speakers unless you read and follow the story or you can have texts where um it's it's like an internal thought not really a spoken dialogue so here's the plan of action very very quickly it's we we tackled it in AEP process first being detection and Association and here what I mean is when to detect the characters texted panels and I want to associate the characters with other characters to say they are the same character and text a character to say this text is set by this particular character once we've done that we can do the transcript generation which is just an OCR aing problem I won't talk about the second part of it in this presentation in the interest of time so let's just quickly look at detection and Association um we formulate it as a graph generation problem the idea being that the nodes in this graph that we're generating are the bonding boxes for character six panels and the edges are the associations that I mentioned character character associations or text character associations very quickly this is what the architecture looks like I'll just briefly skim over it which is left to right you have a high resolution manga Pages input which is an RGB image goes through your standard CNN Transformer encoder decod this is very similar to the detection Transformer architecture the det architecture familiar with that um you get a bunch of object tokens that come out of the Transformer which are then processed by a bbx head which regresses the bbx coordinates and also classification head which says this is a panel text character or maybe background once we have that this is like the node generation part of the graph and then we want to generate the edges right what we do is we process these detected features in pairs so we'll take like a text feature that we already know is a text box and we'll take like a character feature concatenate them along with the special token that I won't mention at all U feed it to an MLP and ask it a boundary question is this text set by this character and output number in zero and one really like the confidence value and that really gives us a gives us all the edges and same thing for character character edges so take character tokens that have been detected concatenate them along with like a crop crop embedding embedding for each of them um and then feed it through a binary like an MLP to ask a binary question right that's how we gener all the edges um I'll quickly go over some qualitative results we do have some qu quantitative results in the number and I'm not mentioning data At All by the way conveniently so do come to the post session we can talk more about that so given manga Pages like this you can detect all the panels which I think is not unreasonable to do and text characters uh characters by the way are not limited to humans so it can detect like dog like creatures and all these other things uh but interestingly it can detect uh the the character character so you can cluster the characters on per page Without Really knowing the number of clusters so it's not K means clustering we don't really know how many unique characters we have but in this case for example the purple or pink lines that are showing connected components represent that white haired character on this right image you have you can see like this girl front like full body view but then side view of her face and then back view and it's matched all of that inside like these are all the same character and works really well um beyond that you can see in left image um you get partially visible body parts it's saying like that hand is the same as that hand which is which belongs to the white hair character um or you can have like this octopus looking like creature and I I'll quickly over all of that um it also does text to character matching as I said which is a text to character it's is part of the problem um and then you can I didn't talk about how you do ordering which is also like a graph based approach we can talk about it later if you have any questions on that and yeah that's all I have to say on the topic U it is available on hugging phas so you can use it with like two lines of inference if you'd like to and that's all I have to say thank you thank you very much great presentation question yescome question yes yeah yeah yeah yeah yeah so this is based off evolation for yes yeah correct so so there are three real data sets that kind of serve this purpose there's an existing data set on Mango 109 um that's you can request that on you can request access to it um if you're like an academic and you want to do research for that you can use that um and then we scrape like large 2 million images completely unlabeled and we talk a little bit about like how we use sem supervis training for that and then there's a smaller data set that we get humans to annotate about 50,000 images that we I mean you train with large scale unlabeled data set and then you find tune with with that data set that's not training is not open source the testing is U it's just really copyright issue so maybe we can talk about talk by email if you really want access to that but I can't publicly release it because I'll just get in trouble yeah sounds good anyone else okay thank you amazing thank you awesome uh thanks everyone and uh I especially want to thank uh all of our speakers in in uh or session one as well as very well done presentations in the in the uh Spotlight presentation of the last three on awesome how I there yes so uh I would like to introduce our our first Speaker Dr Krishna Morty he is a post at MIT working with Antonio talba and Josh tanon Bal he completed his PhD at Mila and University of Montreal and his research is on building computational World models to help embodied agents uh to perceive reasonable and act in the physical world he will give a talk about a structured World modeling with 3D singra we are happy that you have accepted our invitation and we look forward to the thanks um yeah thanks for having me and before I start can anyone confirm that people hear me zo in the room yes yes can hear you yes loud and clear I'm just going to share my screen and yeah I think the screen share came through yes so thanks again for uh inviting me and for all the great organization so far uh so yeah uh today let keeping up with the team of Workshop I will be talking about some of the work we've been doing on 3D graphs and maybe towards the end I'll show you like a quick sneak peek of what's to come in the next few months but uh overall just zooming back uh I like to think of my research is teaching robots to understand the world around them so imagine you have a robot in an environment like this and robot's given a wide variety of tasks including ones that it's never been trained to do before like uh you know help helping someone find their keys or like finding shoes that im an outfit or so on and to do like these tasks may seem very very uh simple and trival for humans but these are extremely complex uh if you consider all the challenges that robots need to uh encounter uh through this enti process so first step is of course uh knowing what is better in the environment like creating some sort of representation uh organizing the environment so that you can uh take these queries ground them and like then figure out how to address these queries like once you know what is where you can interpret the context in which the queries are provided and then you can plan actions to achieve specific outcomes in the environment and uh in general we've been seeing this trend of like scaling uh you know bigger models more compute more data and uh it's it's resulted in this sort of unified transforma architectures or solution patterns train a large data sets which have shown a lot of progress and in some ways uh scale is essential but uh it's it's probably not sufficient if you think of all the applications where we've seen progress so far then these this very densely correlate with scenarios where we have lots of training data like we have billions or trillions of data points like text images videos on the internet uh through which we can train these mods but you start thinking of 3D content for example there's there four or five orders of magnitude few uh data points for them whatever data point uh might mean cont through content uh and most of the data we have there is static and this becomes more and more difficult as you go towards the real world like think of Robotics you have an order of magnitude fewer dat points including like robot control actions so there there's always this tradeoff of you get closer and closer to the real world it's more relevant to robotics but the complexity also increases while at the same time like the amount of data you have to train these models also decreases so in a lot of what I'll talk about the the solution pattern is that we never some sort of structure in the data or as part of the model that helps us learn faster from less data and once it's structured we talk about keeping up with today's theme is the notion of three seam graphs and uh specifically the take I will provide over next 10 minutes or so is like we have all of these impressive uh Vision language uh and what not like models can we uh take all of the capabilities of these models and lift them to 3D for uh 3D perception and robot task planning so there there's there is historical context for singr being used in uh robotics or computer vision for representing the environment so there there's a lot of traditional slam uh or even like to an extent structure promotion approaches that represent the environment as not just a bunch of points but as a collection of meaningful objects or like a bunch of categories of you know semantic segmentation uh attributed to each point and scene and more recently like two years ago uh We've we've been seeing very strong scam graph uh like three scam graph proposals for these environments where you have multi-resolution representations uh like the highest level could be a building or like City Block if if you want to go like one step higher and building comprises of rooms rooms comprise various places and uh there's objects and agents which just sort of Agents is a complimentary layer uh but objects are organized into places which are organized in rooms which are organized in buildings and so on and uh before all of the recent advances in Vision language models a lot of these uh scene graphs and semantic slam or object being slam approaches were close Set uh in a sense that all the classes you need to know uh beforehand like a train time ideally so all of the objects of Interest are are sort of this closed world uh you only ever a reason about them and uh they also are largely unimodal like you take a class ID and that's the only thing you can address these objects by and solutions tend to be very very s specific like these SE graphs constructed just off of specific like sto or in some case rgbd data uh but if you now look at like oops okay I think you can see this now uh and since 2022 uh 2023 like last year and a half or so we've been seeing this significant shift towards the open set setting which is the assumption that you do not know all clouds of interest at train time uh but you do assume access to very large train set that a lot of related concept of classes are covered in the train set and we we're also seeing approaches that are multimodel you can have queries in the form of text or an image and you are able to localize those specific semantic categories in the scene so in this talk I will focus on uh open vocabulary uh 3D perception and the specific angle is we will be using offthe shelf models without any particular training or fine tuning so we will be open set we will be multimodal but we will also be training free uh and the reason we want to be training free uh really stems from the fact that uh we get a new model or or like multiple new models each day or each week and we just want our performance of the overall system to scale as newer and better based models become available so it's the setting where someone does all the expensive work of training a large model for you and you are trying to figure how to engineer using these models for three perception and I'm going to talk about uh one of our uh recent Works called concept graphs where we build three scene graphs from data and the the key idea here is you have a sequence of input images and you can run number of open vocabulary detection segmentation algorithms to segment these and often these segments are s over segmented but if you run 3D mapping over time with semantic and geometric consistency criteria you can organize these automatically into a bunch of meaningful objects so what you see is on the uh top left the original input scene uh top right a segmentation and you can often see how the cabinet is over segmented or the sof is over segmented and then over time you just reorganize it into a consistent object and quick way we do this we we have we we can also optionally use detections in the pipeline uh you have post rgbd images so we assume camera poses are available which is sort of crush for now but there are many ways you can get camera poses uh now think of iPhone we can use AR kit to pain camera poses and you get open vocabulary 3D masks or optionally detections as well and you take all of these to build 3D map uh you fuse detections over time by associating semantic and geometric consistency criteria but you also take these objects and run them through a captioner and image captioner to get captions for each of these objects uh and you fuse those captions across views and this gives you eventually a Json like text descript destion that uh you can then pass along to a language model and we've demonstrated uh its application in a lot of Downstream cases for example for here there's a play Jackal robot uh task with qu you're finding something to where space them party I'm not sure how clear it is on your screen but you can see that the robot and what robots view it ends up with a ster that says NASA written on it or in this case there's a spot mini trying task with finding something healthy to eat and there's mango that it goes to pick up of course it's a plastic mango and you can't really eat that mango but that that's uh that's what the something Healy to eat PR resolves to so can and again here is like a fulls size autonomous car drawing itself uh to a shelter or or white underpass to Shelter From the rain and uh another example here uh videoos a little lagging that uh when you ask abstract ques like get me something that goes well with Ronald McDonald outfit the robot navigates these pair of R white sneakers saying well Ronald McDonald outfit is red and yellow and the red and white sneakers go really well with it and you you can also combine queries that mix text and visual context so you can show this picture of Michael Jordan and ask something this person would play with and it finds a basketball scene so so far so good like this is a in in some sense it's a nice integration of TW mapping uh with modern language models so all we did was you take uh 3D scene reconstructions you caption objects and then you construct scene graph out of it and then you pass it along through language model that acts as DK planner and uh there's a number of extensions of this work both from collaborators of ours and also from other researchers around uh one particular aspect I will focus on for the next minute two is uh this notion of Inc in more structure and Performing more complex 3D understanding tasks uh using this kind of a framework uh and the SE graphs are the core of all of this where you uh you can think of pipelines where you have these old rgbd images you can construct three scene graphs and these are all open vocabulary uh like the nodes are open vocabulary objects you do not have to see them in Prior your train time the edges can also be uh collection of relations that you arbitr specify in our case we use physical support or containment as the two relations of interest but you could also imagine having just about anything else and uh this is converted to a textual description but in addition to this textual description you have a library of uh let's say spatial like computer distances is a bigger than b computer volumes get that a closest object or trajectory planning a bunch of logical operators and also a bunch of neural operators so that there's like a view synthesis operator which is a g Splat trained over the scene which can synthesize noral views given a camera post or this describe around operator which given a SC graph node and a distance radius generates a description of all the objects uh using language model and uh an image QA agent as we call it which is under the hood another image language model that that can answer questions about an image and the idea here is can we synthesize instead of having neuronet directly output uh objects of interest can we like synthesiz programs by leveraging these modules so that we have more prescriptive uh 3D understanding uh an example of that uh for sake of time I'll just run through one example so one example is situated reasoning where we basically say if I were to walk from the table to the cabinet would I pass by the aut so this task if you think about it requires you to situate yourself in the scene uh and imagine you are uh walking from the table to the cabinet and then replaying the scene in your head of ss and uh then answering the question would you pass by the orent and then uh the way our uh approach answers this question is do a call it plan trajectory function like plan in the trajectory from the table to the cabinet and then call A View synthesis function to generate a bunch of intermediate views and for each of these images ask the question uh to an object QA agent and then consolidate all these responses to come up with uh a single response that yes you will encounter to aut men trajectory so this is an interplay of both these prescriptive symbolic uh operators and neural operators like the object QA and uh organizing their outputs to form a coherent response there there's a bunch of other questions I will not get to this point but uh if we have look at how this fairs with respect to a baseline that does not use any structure or uh so if you do not use any structure the struggle like the scene graph significantly helps and if we use the operators it bumps up the perance even further so with this again like I I will just step back I'll leave you with some of these takeaways like using opencity structured scene representations will uh is is a very promising approach for enabling new capabilities uh across like for what perception planning but the key here is retaining uh algorithmic and symbolic structure that allows performance to scale uh you get new models each week and you want performance of your overall system like three perception and planner to scale as these new models become more and more capable so with this I will take any questions if you have time or yeah I see that copy break is next otherwise thank you very much for the great talk yeah please uh yeah thank you I I just wanted to announce if there are any questions uh in the room please raise your hand and it would be great if you can come here and ask your question because we need a mic but in the meantime o there and online folks if there are any questions anyone in the room maybe one small question from my said how do you decide on the concepts uh so uh how do you choose them is there any way to automatically extract like choose the best ones or I guess you define them beforehand right oh good point like in so in in the detector free variant which is like just using open vocabulary segmentation uh there is no premeditation so we just get segments from something like F sand which gives us like a bunch of masks from each image and we just fuse these images using geometric and semantic consistency all time semantic consistency just means we compute like clip features for each segment and then we use that as an additional Factor uh in Computing our over overall like data Association score and uh so in that case there's really no premeditation like everything's decided by what sand to this a group as single segment and over time if the clip uh similarity is uh high enough they get grouped into a single object it's not just semantic similari also like you also have to look at the fact that they're close enough spatial proximity holds too uh but we also have other variants uh which are very useful for creating like task specific uh pipelines for robots like if all you care about is reorganizing books you probably want something that can detect book so we have a we injecting a prompt into the overall system and uh but our code is already like open source if people want to play with it like you just say these are the categories I'm interested in mapping so you can effectively convert this to any number of close set systems by just appropriately prompting it thank you any questions in the audience there uh not that I can see any any hands raising so thank you I think this was a great uh talk and let's thank our speaker again thanks thank for so I believe as this is the bre right the yes yeah the poster session and break and the posters will be presented in the ark building perfect and yeah the numbers are on the website of the workshop which is 60 260 yeah N9 right great and we are going to be back at 10:45 I believe right yeah okay amazing thank you thank you as all stay online uh just in case but we'll uh I mean my laptop is online sir I will share a slide for the poster session uh okay so maybe I can leave the laptop connected hey or if you want you can also disconnect it okay yeah the the the la is connected so let's have it on e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e what e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e son do you hear me how yes I can hear you I can hear you too yes I mean in the room yeah yeah I can hear you and everybody should be able to hear you too I was just having problem finding the unmute button okay so okay now we start the second part of the workshop the first uh talk uh in the second oral session will also be online I will start playing the video shortly and the authors are available online for q& and the second author I would appreciate it if you're if you're here just let me know the second presenter in this oral session hello everyone today we present our work eego SG learning 3D Sy graphs from entric rgbd sequences in this paper we focus on the task named 3D SGP 3D Sy graph prediction the 3D SGP task is initially proposed by the below paper where the network is trained to predict 3D Sy graphs on localized object that is given the class agnostic instance segmentation of a 3D scene the graph prediction Network would try to infer a semantic s graph from the point Cloud the instance segmentation of the 3D scene will be provided as a class agnostic instance mask cim as the input to the network and the network would to conduct prediction on object and their in between relationships most existing 3D SGP work expect to take the high quality 3D reconstructed environment as input such as Point Cloud where their graph prediction Network could be built upon however this Assumption of high quality reconstruction has limited its applicability in Mobile or E entric applications especially when accurate camera posess or slam techniques becomes unavailable or unreliable due to the hardware limitation and the power con ition in real world use cases to this end unlike most existing 3D SGP approaches we propos a new learning Paradigm that learns the 3D syn graph from unposed entric videos via a reconstruction free manner our approach would take unpost RGB or rgbd sequences as input focusing on the mobile or entric app applications we first revisit the existing reconstruction based 3D SGP approaches as well as their potential shortage next represent ego SG our reconstruction free solution for 3D SGP to infer SC graphs from 3D environments this approach highly rely on the Reconstruction step where the additional Associated camera camera poses will be provided as input next the perception stage which includes the feature extraction and graph formulation steps more specifically pointing coders such as pointet and pointet Plus+ will be used to extract pointwise feature from the 3D environment and the scan level C will be used to conduct Point 2 inst feature formulation noticeably the C would only review the point2 instance mapping information but without releasing the speciic object classification label with C this approach completes the 3D object localization the next popular module to be adopted for 3D SGP is the graph reasoning step in graph reasoning it has two functionality the feature engineering and feature refinement first it would transform the node feature and use it to initialize the H feature next it would pass both the node feature and H feature into a triplate gcn to conduct attentional reasoning eventually the refined node feature and age feature will be passed into two MLP to perform the of sing graph prediction for node classification and age classification however their performance will be largely limitated either when the slam techniques becomes unreliable or the ground truth camera posess becomes unavailable in the real world use cases to this end we present ego SG our reconstruction free solution for 3D SGP ego SG would learn 3D Sy graphs from ego entric sequences without any Reliance on either the camera posess or the slam techniques it would first perform the eego view perception on each frame for pointwise feature extraction and node feature formulation next it will perform the graph reasoning on each frame for better future refinements similarly in ego SG we also utilize cim to perform the crossframe node Association for 3D object localization eventually the graph reasoning module is also utilized as the global level to perform the video level graph prediction since the perception and reasoning process of the infert graphs are performed on each Eco View and the environments we term these estimated graphs as the ego SG to differ them from the typical 3D Sy graphs which are learned directly from from the reconstructed Global 3D Sy for the experimental setup we first select the 3D SSG as our evaluation data set which consist the 20 class object of NYU N2 format and seven class relationships such as attached to built in and supported by for the evaluation Matrix we follow the 3D SSG to compute and Report top inac ises for object predicate and relationship predictions for the evaluation of the relationship tripletes Sr o it measures the overall recognization of the subject as object o and their in between predicate R by multiplying their scores jointly next we release our experimental findings the R3 denotes the system Reliance on the 3D scene reconstruction p d and n indicates whether the system take the ground truth camera poses deps or normal information as input or not the first Baseline to be compared with is the 3D SSG which is also the first work proposed 3D sdp task it would take the entire 3D scene reconstruction scan as input and it would utilize the ground truth camera posess and depth informations the next Baseline is the singra fusion although it will take the sequence as input it will still require the slam technique to perform the incremental s reconstruction for the final s graph prediction during our preliminary study we observe its high Reliance on the ground true posess and its performance would largely drop when it's shifted into the estimated posess next represent our reconstruction fr result the OS SG with unpost rgbd frames it's largely outperformed its counterparts with the compatible settings especially when the camera poses become unavailable we are also interested in further released Assumption of the ground true steps information so here we utilize the Medias to compete the depth estimation and list its result as a reference here here we present the qualitative result for more visualization please see our supplementary we'd also like to further discuss over the difference perception scheme unlike the existing reconstruction based 3D SGP approach our ego SG is compatible with both 2D and 3D prrint visual encoders such as resnet or Point n++ unlike the Reconstruction based approaches our encoders could will be flexibly switched when dep's information become unavailable one more interesting finding to share that all Point based interest outperform their competitors by a large margin yet maintains much less numbers of trainable parameters it validates that the depth information encodes vital 3D press for 3D Sy graph recognization and point encoders are capable of extracting them efficiently to better understand the contribution of dep's information towards the 3D SGP task we further conduct the following experiments where we find using estimated depths with Point encoder seems a good option although it's still for short compared to the ground true steps with Point encoder it would largely outperform the RGB alone with image encoder result at the end we present and identify three popular failure cases when utilizing depth estimation for 3D SGP the first one is over smoothing the estimated depth might be in sensitive to the object boundary the second one is far position the depth information at further positions might be easily omitted during its optimization and the third one is illumination interferes the scale of the estimated deps could be impacted by the illumination changes with more advanced depth estimation meth to be developed ego SG alike reconstruction free framework could further broaden its application to build 3D Sy graph from more conventional RGB videos such as the large scale egocentric RGB videos on YouTube thank you thank you for the presentation chy can you unmute yes okay hi nice to see you nice to see you too thank you for the presentation are there any questions there in the audience if not yet I can maybe first start with the question so your method relies on point clouds what happens if the point clouds are too sparse have you tried with that or for the for the sparse Point Cloud I think we uh since we do since we do the perception on the Ecentric view so basically we we build the partial Point Cloud for each frame and the the entire Association is completed with the c mask so basically as long as the the the the detail information can be preserved as the current frame we can be able to extract the node uh node feature from the current frame and we leave the the the the following step to the to the eventual uh Association stage thank you and maybe a followup question since Point Cloud are kind of in a graph is structure themselves can be represented as nodes did you try using a graph neural network directly for the whole process no for the for the association one in our supplementary we try different like two exploration so one of the one is like we we try use theistic stage to to like without any training process to just to add one one node into the other and we just we just try this kind of um uh a preliminary study for that yes thank you any questions in the audience perfect thank you any questions here in the room uh no so I think we can uh thank the speaker again and uh thank you move on to the next presentation the second uh presenter is in the room um and we have have let's see I think you have the PDF file right for the presentation let see if I can share a screen and also little challenging even that it doesn't when I do the screen nobody can see that so the solution would be to all right yeah wonderful right good day everyone my name is julan noin I'm from the machine learning and computer vision Lab at University of Oxo and we got this one oop yeah yeah that's thanks yeah sorry yeah wor um yeah and we've uh published paper review and efficient implementation seing graph generation metrics so before talking about what we actually did I first want to Define singr for our paper because as we've probably seen also in this Workshop singr mean different things to people so in our case we've singra consists of relations and instances and an instance is either a box or segmentation mask and it has an additional class lay so human or Blinder SL whatever and the relations represent the interactions between those instances so we've chose the name instance because object is already taken for subject object in a relation so we thought that instance might be a better of name at least for this presentation um yes so now that we settled this um what our goal was for publication was to make life as a SC graph generation researcher better and easier and therefore we identify two problems that you have when you're researching a scra generation and one thing is that if you I know at least when you're new to the field you notice that we have all these nice metric recall mean recall and and so on but they never really like precisely defined never so it's just written in words also good written like but um for a really formal definition that's often missing and you have to delve like three layers in code and then maybe find this and be lucky but we we thought this maybe would be good to collect all those commonly used metrics are used in SRA Generation Um review them combine them in one single uh space paper and then formalize all these different steps are required so that everyone can just read this section and understand immediately what what this is all about and the other thing is that when you're researcher on SRA generation and you want to write or evaluate the SRA generation results uh you have to use some way to evaluate them right and you can use some existing repost and try to surgically extract this code that is used to evaluate um these Metals models or you use SG bench which is which is a python package that we developed exactly for this use case so that you have really nicely separated um module that you can use to evaluate all the models without thinking to how do I integrate this in my code base how how do I get this running so yeah it's really the idea was to make life easier for you as a researcher on this topic so we've develop um sbench supports multiple metrics obviously to extend them wouldn't be much of a hustle so if there's something missing uh I think I see no problem why we shouldn't extend that so just talking about SG bench python package a bit so what we do is we have sorry to interrupt you but on the zoom we only see the first page if you have changed the slide okay interesting oh now we I don't know why the screen sharing was paused so okay yeah now now we see the cor okay sorry yeah okay if I now change slides you see it yes yeah okay wonderful um yeah I think yeah this is what was the slide before but I think I so told everything so let's go to the next slide um the python package tool so what sbench does is we have the relations and the output instances and we have the ground truth relations of the ground truth instances the first step is to match ground truth instances and output instances because you have to like is the predicted human also the ground human and so on and you have this mapping you can map the output relations to use the same reference point as the ground Truth uh relations and you just have to compare the predicates right so the the more uh more difficult or more computationally more challenging part is to to match these instance onto each other and then just counting how many relations have been uh recovered that's the easy part so that's why we first do this matching process and then with these matched um instances we can easily calculate all these relations uh right so what you can you do is you can just pip install and then SG bench and then use the three lines code there to evaluate your model um just as a a warning so these these lines are a new version which hasn't been updated yet but uh you can expect it in the coming week or week after you can also use it as a CLI tool um just put in The annotation file BR with file and your um model output file and comes with us in the next slide and then you can get some results back on the the metrics so about the model output file it's also something that we wanted to improve for researches on S graph generation so you can have these uh these pickle files that tend to be really huge you can only be read by your code base because they're maybe using some class references and so you need some code for that so we thought would be really nice to have a file form that anyone can read anyone can write and it should be fast small and efficient so we thought for at least for panoptic sing generation you need segmentation masks um and that's why we need some image files so I think the most commonly used way is to just store numpy arrays as pickle files but that's obviously really resource inefficient and another way would be to use Tiff files for example so why did we choose Tiff files instead of PNG files for example um it has it supports arbitrary number layers so you can have overlapping segmentation marks so for example you have 100 instances in an image and you want them to partially overlap you canot really do this in PNG you have just three channels um so we chose Tiff could have thousand layers supports compression which solve the file resource uh requirements and it can be read by I mean virtually anything that supports like these files or any program language you want to use supports I think tip files it's really high chance that you can read those uh and right if you if you're do not doing panoptics s generation you obviously don't need those TI files but if you do then yep and to store the relation files we decided to use Json as the file format um just to have a format that anyone can read anyone can write um we thought about using hd5 or sq but for hd5 you would need an additional Library which would try to really really try to keep the dependencies to a minimum so that it really works with your code base and sqi also be maybe nice but I think Json files are just easier to work with if you can human readable you can edit them it's built in everywhere and yeah that's why we decid to use Jon annotations so if you look at this you see that this file format that we devis for SG bench can be read uh can read and written even without SG bench so you're not locked into our solution here you can have I don't know uh C++ implementation where you want to evaluate these files no worries no problem you you don't you're not locked into some python class representation or some binary format where you depend on our implementation yeah and the good thing is that we can using this file format compared to these traditional pickle files we require much less space and on the dis because we don't store all these images as bit maps and we also Only Store the information that is required to evaluate these models so we can still generate the same scores the evalation scores with these files but um while reducing the number by quite a lot so and the last thing that we've built is um it's an evaluation server where you can upload your these files and they evaluate other scores for you so if you have this then uh so with this web service you can guarantee that everyone uses the same evaluation code um which I think is quite beneficial if you want to prove that you have um try to trick someone with your uh evaluation method for example uh so anyone can register takes a couple of days and then it's unlocked and also if you um upload something there you can add links to your project page for publicity whatever and we'll also open source the web service code yes so we hope that these changes will improve your life as a SC gra generation researcher and you have I'm open to questions thank you thank you for the presentation maybe I can start with one question so does your benchmark only support uh the semantic sing graphs or if the sing graph is also in a different uh setting for example if it doesn't have uh it has undirected relationships or only zero and one binary relationships would it support it or not um probably yeah TR okay finding the microphone at the moment play with this the fine it's just you can probably you okay just speak up yeah okay I'll try um so the question was if it only supports semantic segmentation and that's it and we can well obviously we're limited to uh the to to some um applications here Al some problem formulations but there is no problem in extending this to I don't know binary relations or uniform a uni directional relations so at the moment it supports the Box um sing gra generation and semantic s gra Generation Um but I see no problem and at extending this to to other fields U also I think it would be quite trivial to do so thank you any other questions there or on Zoom any other questions in the room uh I think we do have a couple of minutes and I actually had a very quick question so you talked about a number of metrics that's a great uh resource I think putting all of them together but uh have you thought about scene graphs that are applied to um video understanding like for example action graphs that are repeated uh nodes but the relationships change over time does it any uh does change how you defined the metric so so these are developed for 2D singra generation so that was the the main goal so everything that you can use for so so you could calculate those per frame I guess using Library would I don't see any problem in there uh however if you want to extend this to video then you of course have to um some of these Concepts don't apply I would say so there's no it's not built in a at least yeah okay great thank you let's thank this here again let's great and I believe our next uh speaker yes we have Professor trap online so let me see if I can hi can I share screen or should we do I will do a short introduction then you can share screen okay perfect yeah our next speaker is Professor fare Tris he is a professor at Ludwick maximan University of Munich and also with the Munich Center for Mission learning he team has been pionering in M learning with knowledge graphs temporal knowledge graphs and S graph analysis and he's also a codirector of the Ellis program semantic symbolic and interpretable machine learning and we are glad to have him today uh to give a talk I stop sharing and we can okay can you see my screen yes but it's not in presentation mode how about now yeah okay great perfect yeah thanks a lot uh for inviting me I hope you can hear me well and um thanks to the organizers and I'm sorry that I cannot be uh in Seattle prob very nice place and Conference so my co-authors here are H Le um zahan sharade who is now with Deep Mind in London Dario konopaski now with eth and uh hun Puma so we um thought about um so I like this uh proverb from G you only see what you know so G looks at at this uh scenery and um he sees maybe some mountain range and then he recognizes that that this is Munich and uh then he knows from his semantic memory that Munich is capital of Bavaria and uh and maybe he has some nice episodic memories about visiting Munich so this uh completely enriches his vision experience and also um really make him understand what what he's looking at so so we want wanted to look at the question of also from a cognitive biological Viewpoint so um how are they connected how do they interact um how how are semantic and episodic engrams implemented so these are memory traces uh in the brain uh how do they uh interact how does perception memory interact and U why are they Subs symbolic but also symbolic now we we have some maybe visual impression of a past memory but we can also immediately talk about them and short short out exploration in direction of reasoning and language so this is like a small part of some introduction to our larger work on the tensor brain um so this is a very simple model so um an engram consists of an index and it can be like one neuron or a small set of neurons and this index I here this is for Sparky is connected to the representation layer um also called Global workspace and uh the connection weights here are the embedding of that of that index in this case of of Sparky and and these uh directions are bidirectional so if um if we we initial activate this index it will form the embedding on the representation layer and if the representation layer accept is excited it will maybe activate this this index and um so so an an engram always consists of an index and the embedding vector and if the index is a concept like an entity a class or attribute it's a semantic memory and if it's a Time instance it's um it's an episodic memory so how does this uh work in in practice so um so we have these two layers and um so to recall an episodic memory we activate the past time index here in the index layer and this um index then activates the representation layer with the embedding of this memory and and this also goes back to earlier um processing layers in the brain which we call embodiment so we always have embedding and embodiment um this information is then propagated in time and um so so now we can immediately so this gives us some some some visual impression maybe of this past memory so of this past episode and but then we already starting the decoding so uh we we we recall there was a Sparky in the image by decoding the sying vector um Sparky then is activated and reactivates the representation layer so the brain knows that Sparky this is now about Sparky and then the object is detected maybe a Jack and then the predicate looks at now so we have this uh iCal triple structure of of knowledge graphs and everything is fed back to the embedding layer and then also to the embodiment so so this is how I our idea of how episodic memory and this is repeated many times now for the same at for the same uh memory we can uh detect or decode many triples so in uh together with this Subs symbolic impression of this memory we also have the um the triple expression of what was going on and uh semantic memory is very similar only that we start by activating the subject for example in this case Sparky um we also have an embedding for Sparky and then some understanding of what Sparky looks like what he acts like in this um uh in this embodiment process then maybe a Jack is activated and then owned by so we know that Sparky is owned by Jack respectively the brain looks that and perception works with the same machinery just uh in another operational mode so the input comes from the vision activates the representation layer um and that then decodes in the image uh the the the subject the object and the predicate again maybe finding in the image that Sparky uh looked at Jack and so here it's mostly a bottom up operation and not so much top down operation but everything works with the same machinery and if we don't need the predicate for example if Sparky and black is activated then it means um that spark is black so we don't need the predicate so we have sort of a little bit of a shortcut here which also leads to generalization so in this uh in a layer representation so this was unfolded in time um let's say we have bounding box content from the bottom up and then a deep convolutional new network which activates the uh representation layer and and this then activates the index layer and this goes forth and back and the connection weights are the Bings of the corresponding indices and um this whole processing is also supported by another layer uh the dynamic context layer which provides the nonlinearity and and some nonlinear mapping um and so the ideas are what we proposes uh the the the brain doesn't really know what is true and false it only knows what is brought to the attention of the representation layer by the internal observations by this uh generative uh sampling process and in the episodic memory we can decompose um as we discussed before we can sample an S given the T and O given the S and the T and A P given the S and the o and t and we read this um repeat this this sampling many times to decode or recall an episodic memory and we we argue in the paper that this is proportional to our confidence that this uh statement is actually true and and we have a similar relationship between uh with respect to semantic memory only that here we are starting uh with the S um the subject we want to have information about then the object and and then the predicate and also here we can argue that this is proportional to um our belief that this triple is actually true but the the the the inner language is the sort of this sampling language in the brain and we can also look at this um as um generative uh uh recurr network uh here on the right side with the um the representation layer the green one which uh and folds in time and then the input uh from the image bonding boxes and uh then so the only difference is how we deal with the output so we are predicting the output we are sampling the output and this information which sample was generated as fed back to the representation layer so that's uh the main difference to to a recurr new network which we are proposing here because the Brin should know what we're thinking about I should not just say uh I'm happy that you decided that we detected Sparky in the image the whole brain needs to know about this so it needs to feed back into the embedding layer and then also in the embodiment uh we also have a some thetical reasoning why um this um so this addition of this sampling which is we are proposing not some complex interaction but uh the different sources can produce samples like perception but also episodic memory semantic memory and we form formalize it in this dially Fusion model uh where we combine observed data with semantic memory which is essentially something like a like a prior uh in particular we propos that a perception um generates samples from the perceptual posterior so the probability of the tripos given the perceptual input um recent episodic memory uh from the past posterior so this are both posteriors and the semantic memory is something like the prior uh so what is our default knowledge about things but we don't combine it in terms terms of Prior likelihood it's prior and posterior what we are proposing here so here's an example about semantic memory which is about things we know uh so we we know that lines are dangerous we don't have to have a personal experience and and here we have two bounding boxes in this image one is identified as Sparky the other one as a bench or more concretely we have several labels for this bounding box it's Sparky but also dog ml living being young white and for for this second bonding box it's a bench Furniture non- living being old and other color and for the triples we have dog sits on bench living being sits on furniture mammal sits on old white sits on bench so it's sort of this back of the decoding sort of idea and uh from the semantic background we we we add that spark is owned by Jack and Sparky is loved by Mary so this sort of the triple background is brought to the attention uh to the brain when this um in this case Sparky is detected in in the image and and we can uh generalize statements because a subject can also be a class or an attribute and then we can uh because maybe the the the dog in the in the image is unknown to us then we can just know that it's a dog um but also since the the subject can not not just be an entity but also a class we know for example the the dog is a member is a living being they're often young uh dogs are often on grass they're often behind a person or or here a person the person often wear shirts or wear glasses uh so we can also add this default knowledge or background knowledge about Concepts now here a mammal mammal can be a dog with 40% um memal are often on the street in the data set and and even we can talk about um description of the color black so 25% it's it's a human being and or something is is on a person maybe a hat or something so um so we can also model this sort of more General background knowledge not just instense oriented uh scene graph are typically concept oriented whereas the knowledge graphs are typically instance oriented and we did a a cluster analysis or any analysis of uh the embeddings and we found that they're quite meaningful uh in particular I mean all these things are nicely clustered so trucks uh planes jackets buildings and so on and also the embedding of the concept truck is very close to the center of the cluster of the trucks which we did not enforce now so but it means the sort of the algorithm or the the model understood what what is similar that that like a tree a concept tree should be close to the instances of tree and we thought that that's a quite nice result then episodic memory is about things we um uh we uh remember um so and and we distinguish between a recent episodic memory uh so that tells you why you are where you are why you are in this room listening to this talk um so here for example this poor Mouse if the Mouse um forgets that it it's in this cup because the cat was chasing it it will not live very long so so recent episodic memory gives you orientation about um recent uh events and so you don't need a complex State Vector to do that you just have to recall recent events to know uh why you are where you are and uh remote episodic memory is more about decision support um if you have a a bad experience with a lineon you don't have to um you don't want to repeat that experience so it gives you um decision support and guidance in in in acting and we also propose that um episodic memory can also be active or a nice concept for the future for example if you read in the newspaper this evening so you don't have to have a big predictive system there's a football game and it will rain then you expect uh bad driving conditions so this concept of um having episodes in the future might also be quite interesting and also our Ed for the time index is uh meaningful we don't see this amazing clustering but we see for example here we have party sceneries here we have sport sceneries so also here the thematically this uh organization of the embedding vectors U might be might might make sense we also looked a little bit at reasoning so um so embedded reasoning essentially is reasoning by adapting the embedding vectors and if you if if we know that spark is a dog then we get the embedding Vector maybe close to to darkg for Sparky and that can add additional knowledge and when we refine our information about Sparky we will also refine the embedding vectors um we also want make a clear statement what symbolic reasoning is so symbolic reasoning would be reasoning which only uses the indic say without the embeddings so if Sparky is a dog then we can inere that Sparky is a mammal and we don't need any embedding vectors for that currently we don't have a model for that but I think it's nice to have a clear separation of this embed based reasoning and symbolic reasoning then we also have symbolic embedded reasoning which essentially means when we say that if Sparky um triggers dog then dog triggers mammal and we the brain can conclude that Sparky is is a mammal so so we have these two triples which trigger each other and we get a little bit of this subass generalization uh we we propose that this is I mean this triple language we have been described first of all it's also going on in animals but it's also like something like an inner language like a nervous inner language um which is cleaned up in some adaptation process of course but that natural language of course is much more sophisticated is more about justification of what we did arguing that what we planning to do is the right thing so it's not the same thing but we would think that the the outer language is sort of based on on the inner language to some extent uh this is how it might look in the brain so um um so so we have people found these indices so these famous Jennifer Anderson neurons in the temporal cortex and so so ideally it would mean or some interpretation that there's a single neuron or maybe a small set of neurons which are uh which are responsible for for representing this person um um of course there has been a lot of discussion on this result and then we have these results from Hood from Berkeley which sort of maps surface of the brain to different semantic Concepts um so so that would be what we would probably interpret as the as the representation layer of the global workspace and also interestingly the hand argues that the brain is a so slow sampler now you might have noticed that we always have this serial sampling process going on which slows down things but language is a Serial thing so maybe it's not so strange for for language for for the brain to have also a Serial bottleneck somewhere so uh in conclusion I think time is running up um so we we showed that a perception and uh and memory can be formulated in one um um in one model essentially in different operational modes of of one model um so they rely on the same uh brain way as we're proposing and um and there's always an interaction between sub symbolic and symbolic um recognition so there's no symbolic recognition without some sort of embedding or embodiment and also and vice versa um if you have an embodiment it's automatically embedding it's almost almost always decoded into into triples into indices into symbolic representation um and um but finally maybe the statement um uh uh so we uh so we would propose that this is quite a plausible mechanism for some of the brain functions but the brain does many different things simultaneously and uh maybe the same thing even different ways uh IT addresses a particular problem with several strategies and um we would propose that uh maybe the model we are proposing is one way how it solves some problems of course vision is much more complex has many more aspects but maybe other modules are more important for those uh functions and every everything is then sort of integrated in some form of awareness and um so yeah we are quite proud that this was published last year in neur computation so if you want to uh read up on it um here's the reference so thank you very much for your for your attention thank you very much fare for the great talk uh are there questions in the audience there anybody has any questions uh I don't see any hands yet from here maybe I can start with the question so there are some data sets recently like ego 40 that are egocentric do you think it would be interesting for those data set if in those cases the subject would be the person looking at the object then would it simplify the ban and yeah of course that would be very interesting I mean essentially it is egocentric what we are describing only that we had to work with the data we had at that time because perception of course is a very personal thing and it involves the experience of the uh of the agent and um and the prior of course maybe can be changed or adapted in different ways of course if you for example if you have a um we have we have some form of uh perception and also sequential perception uh but if this is uh very rich of course you can combine the different recog recognition processes um as in any video processing so um so that would help a lot and then also would uh refine the prior but we uh we always uh make the distinction now so our uh prior is the semantic memory and that changes slowly now that's that's not going to adapt very quickly so if you if you if you want to remember something and maybe um somebody was not married and then and yeah then in your sematic memory this person is still not married but then maybe he married last week and then you have the memory of this episodic memory so the this by this prior changes slowly now but the perceptual prior which might bias your perception in some direction that that can be a different mechanism and it would act uh interact with a perceptual um processing and not so much with the memory so we think the memory is a slowly changing thing and if you if something contradicts your memory you would remember the episode which tells you that oh yeah no it's not true anymore something has changed thank you any other questions on Zoom or in the audience I'm going to wait seven awkward seconds okay we waited thank you again for the speaker thank you very much thank you for my attention okay I will quickly share my screen our next speaker is Dr Brian perzi it's he's already there in person he's a research scientist on the graph mining team at Google he works in the areas of data mining and knowledge Discovery machine Learning Network science and natural language processing Brian's research at Estonia B focused on applying representation learning to applications in social network analysis and natural language processing and his talk today will be about giving a voice to your graph thank you for joining us in person thanks for the introduction as on so we should probably maybe mute that and then I'll unmute myself sorry this is really hard to do on the screen okay uh yeah so thank you so much for the introduction is that um and I'm going to talk to you about something today that has almost nothing to do with scene graphs and I don't leave because I think it'll still be cool um and the uh the application it's about but it's about more General than than like kind of Beyond sing grass right so um I'm not going to talk as much about the visual element but instead like how can we best represent structured data of any form for large language models um and a general generative AI models in general right which all kind of have this discret interface so uh I'm part of the the head of the graph noral Network team in Google research um we kind of have like a dual Mandate of applying um graph structured models around the company and so we do a lot of that we also do a lot of research in the area and um then we kind of build infostructure uh to helpfully make applying the models easier that good you notified the workshop organiz I can I can just talk loudly yes all right I'm just going to talk even more loudly um so the motivation here right so we have this powerful new Tool uh generative AI models they allow us awesome new capabilities in many different modalities like language vision and video but there's there's weaknesses that are stopping us from applying them everywhere uh you guys have probably heard about this a lot there's the hallucination problem right a model will confidently a model will confident say something that's incorrect uh then be we go beyond that we go to the freshness problem right so if you train the model these models are expensive they take a long time to train in a lot of resources and and then they're stale like the instant you stop training them they don't have new information right so there's a freshness problem and then there's also kind of a like a privacy problem where you can't train the model on data that you don't want it to tell everyone right so if you have some some data about yourself and you don't want the model to just kind of regurgitate that to everyone um you got to some somehow you know not train the model in that data but have it available for it and you know getting back to it there's also this you know problem of cost right like that they're expensive to train it's expensive to do inference um so like anything we can do to make these models more efficient is desirable and so how can graphs help um so graphs can help kind of in in two ways so a lot of the problems uh that I talked about with the um kind of the freshness and the um the privacy like trying to trying to put more complex data in the prompt this is where like graphs are used to represent knowledge um it could be the knowledge of uh how things are related to each other it could be the knowledge of what's going on in the in the scene of a video right but like graphs are this nice kind of human interpretable way to structure the knowledge represent it and reason about it and if we can get if we can use graphs to to like to represent this knowledge that we know to be true and we can get this into the the context of the large language model uh or um somewhere else in the large language model better we hope we can improve a lot of these like helping the model have access to the right data at the right time the other thing is that um you know graphs allow us to decompose the data so the the hierarchy and the structure of the um that the graph itself allows us to maybe do clustering and work with it at a coarser data uh a coarser level of the data than we would otherwise be able to and so this is another Advantage for um for graph models so graph models versus Transformers graph models are they're more parameter efficient you know a graph model can do more with less and they also um they have a higher You Know sample efficiency so you with a couple examples and a good a good graph you can learn something but if you want to Transformer to learn that you're going to need a lot more data and so there's four uh ideas that I want to talk to you about quickly today and I don't have a lot of time to talk about any of them unfortunately um but I if you have other questions please um please follow up the first idea is graphs is text right can we encode graphs like graph structure data in text in a way that makes uh makes it easy for models to use it and then the second idea which is kind of related but like graphs is embedding so if we don't want to encode the graph as text we think there's a better representation out there um how can we learn a better representation uh of the graph for the model and then we'll get to the next two after after talking about the first two so yeah question one how to best encode graphs is text and and this is this work talk like a graph at iclr um we're really interested in text because it's the dominant interface for large language models you know this is kind of like a new capability to be able to talk to the machine and get an answer back and so a lot of people are using it this way and so we in this work we explore uh graph prompting and graph encoding and the analysis of the effect of the structure of the graph on how how well the model can do um it's kind of funny because the model's very simple so we're just going to takee text and put it into a gen the right so it's it's a paper at iclr that has no learning and no representations I don't you know blame them not me um so okay so the framework itself um we uh we're going to vary this graph encoder function and the prompt question the way we're trying to get information out of the model uh and what does that really look like well the prompt is now going to be some verbalization of the graph and then there's going to be some verbalization of the connectivity information and at the bottom there's going to be some sort of like question to see whether the model really understood what we were talking about and it's got to be a graph Centric question so we'll ask it about the degrees of the node or whether things are connected stuff like that um that's all going to go into a frozen model so a lot of what we do is working with frozen models and you then we're going to get the answer out and so you know we'd like to find the best encoder and promp question to you know help the model answer these tasks and to kind of bring it home this is my favorite example I think from the work um but like what's in a what's in a node ID right so we have a picture of a graph here we have a lot of different ways that we could describe it to to uh you know one another but what if we Ed fantasy characters just something totally weird right okay so this yellow this is Daenerys right for my Game of Thrones people uh bran yeah okay so that's bran maybe that one's in the head so we can give every note sort of like a you know a domain name about um from some fantasy story and we can write like a little verbalization of it that says like oh here's a friendship graph and these characters are friends with each others blah blah blah um would that actually how would that affect performance you know kind of crazily uh that was the second best zero shot encoder The Game of Thrones encoder right like over many other different choices now um the other one maybe interestingly is like the using politician names what's the what the best encoder on this particular model um and so like you can think when you when you're telling a model a story about like political relationships and who's you know who's uh related to each other like that might put it in the right head space to to think more relationally about it now that's um you know as we change the prompting type as we change the information that we give the model and we give it more it it turns out that that's you know different kinds of promptings we varied all these different kinds of prompting um different kinds of prompting you know okay using the the you know the story book version of it um doesn't work as well as the abstract version when the model has more context lower is better here um this is like the like the ranking average ranking basically across all the tasks so so an abstract representation works when we give it context but if you don't have as much context the model was actually doing better with like um telling it kind of like a story with tokens that had more refined representations and um you know uh to show you like and it I guess you know you might see these results and say like hey why do I care about this at all there's no way this could work okay so we've kind of been doing a leaderboard uh over the last year or two for these kind of metrics and in the beginning I think I agree with you like so this is accuracy numbers so like 100 means that the algorithm you know it's working like as well as an algorithm could always finding the answer if it's low you know then okay it's doing like a really bad job so you know we're doing a bad job on things like um accounting edges is really low and some of these some of these tasks we hadn't measured um in you know September uh but if we look at what models are doing nowadays like so this is like uh from a month ago uh with you know larger and more capable models um in general you see like there's a lot of these bars that are you know they're not all at 100 but they're getting a lot higher right and um across that the the big models like uh gp4 uh the big Claude model Claude Opus um and Gemini are are all doing you know really well on these t and they're getting they're kind of getting better so every time we go out and we measure we see that um models are getting better at doing graph Centric tasks um on these syn synthetic generated data um so you know there could be a future where we can encode graphs as text and the models will solve everything who knows maybe um there's another question though like if we don't want to do text can we can we do better and so in the the next line of work what we did is looked at learning and encoder so it's kind of the same model as before but instead we're going to um learn this thing called a graph token it's going to use a graph neural network to encode uh the graph into a sequence of tokens so the verbalization of the graph is going to be these generated tokens that go next to the prompt and then these are going to go into a frozen model so the model itself is not adjusted but we do learn the encoder from the model's gradients uh so it's a like a variant of parameter efficient fine tuning and um just to quickly show you like show you what the these kind of results look like so uh using an like an older model that we did some of our early analysis on um so okay that's Palm 62 billion and then we could take we can take a model and learn something that's better than it but if we if we take the same tiny model and add graph token to it it outperforms the large version of the model so the large version of the model has substantially more parameters it unfortunately I can't tell you exactly how many but you're got to have to believe me that the small version of the model and the large version of the model are kind of widely different um so just by attaching a little graph Nal network with like a million parameters to it we're able to um sub like boost the capability the graph capability Way Beyond what the LM can do by itself so learning the the encoder like the the graph verbalization can work a lot better than the um the text based prompting that I showed you and can make small models very competitive uh and you know because I don't have a lot of time um uh basically every one of these results has a paper with a lot of extra information in it so this is kind of like the tldr version but what else is that the intersection of graphs and large language models there's two other ideas that we found really exciting lately one idea is on the like basically Theory resolve on the graph uh capabilities of Transformer models so where where are gnn's the right thing to use and where are Transformers like either just like a raw Transformer or like a trans like you know large language model you know Train Transformer uh were are they the right thing to use and then the other idea is how can we use graph s to to actually generate data for evaluation and training of Transformers to make them better to better at tasks and so this is um the intuition here is that uh maybe maybe I'll get to that in the next the next slide because there's one or two slides on it okay so when we when we want to uh when we were looking at what we could do with graph algorithms and Transformers uh we wanted to try to understand what um when was the right time and place to use each of these methods because that's kind of our that's what we do so we're we're looking um when we when we look at the Transformer results kind of on the next slide here we're going to be looking at the text o like the token only representation um of a graph and so this is there's no text but there's a like a discret token for every vertex and a discret token like pair of tokens for every Edge um and then there's like a task token at the end and this this comes from the uh token GT representation so we we took the graph um like QA tasks that I talked to like I talked about in the talk like a graph section before and then we we tried to figure out how much um like how big like wide and deep a Transformer model had to be to solve that um that perfectly and so we came up with this complexity hierarchy of graph algorithms on Transformers which are kind of based on are they looking at the global view of it or are they doing like a local search those are kind of the two two kind of large paradigms and the experimental results well I mean I think they show things that are known but maybe maybe I hadn't seen them people say them in exactly this way so right obviously Transformers excel at Global tasks right they can kind of learn the connection between any pair of nodes and so um on connection or on tasks like shortest path and connectivity um you know we we see the Transformers doing well and then like we see that you know pure gnn's have capacity tasks so when we take those tasks from before and we train them on a thousand samples um you know we can see that uh the the gnn's are doing well so like the mpnn model is like perfectly Sol no degree perfectly solving cycle check basically um on global tasks we see that you know there's less of a gap with a thousand example so the the bias of the mpnn is still making it better on shortest path but um you know uh with more data Transformers can become competitive so if I if I just go to the next kind of columns where we're looking at 100,000 training examples we see that okay on the local test the gnn's are still winning um but the the Transformers got a lot better and the Transformer on the global task can just like wildly exceed the what the GNN is able to do and so you know it it kind of hints towards the regimes where we want to use GNN which is like ones that are doing like local tasks where we don't need you know and we trust the graph structure a lot to help solve it but if we want to do something uh larger or more Global um we would want to use the Transformer and so um I just want to note that these uh you know the Transformer model here is 60 million parameters and the DNN are much tinier so they're 100 thousand and so finally uh and maybe my last minute um this this idea of using graphs to help with synthetic data generation so um synthetic data you know Transformers have basically trained on all the data that we have available you know movies videos text right there like these things are data hungry and we're kind of we kind of run out of data um so we're looking for new means of getting data so synthetic data it's um it offers a couple advantages one it's harder to memorize and so this is good if you want to evaluate them because it you know you're trying to basically see how much has the Transformer stored in its parametric knowledge versus how much does it really understand like how to reason or like how to do a more abstract task um and you know we hope the graphs can help here so we have kind of you know um the way that people were evaluating temporal reasoning was basically like um giving kind of like a sequence of real world examples and then asking the model about it but the model already has been trained on a lot of real Temple or examples so when we when we go to like an abstract version of it where the entity names are masked and the the type of the relation is masked um we can kind of get a better a better understanding of what the model is really able to do now how can graphs help here well um we're using graphs to come up with all sorts of weird temporal dependencies so like it's kind of like making random knowledge graphs so we can generate graphs we can assign entities and relations to the names and edges and then that gives us kind of like a like a knowledge graph or like a like a relational database and then from that we can we can kind of look to find like a question in it like oh okay if there's these two abstract entities and they have like a time period like this like how do you you know what was the gap or what you know who was uh doing something else at the same time so we're generating these kind of random questions um and just to show you what like different graph generators actually are creating like different databases that they look like so we can using all kinds of random graph generators get all sorts of weird structure that people weren't evaluating on um there's a lot of results in this paper too but I don't have time to go into them so the conclusions here are um adding graph information um can help Frozen llm solve graph tasks the way that you represent your structured data does matter a lot and um you know while Transformers are less sample efficient than gnns with enough training data they do get a lot better so um I think that's that's kind of like the takeaways uh all the papers I talked about are are right here um if you have any other questions I'd be happy to answer them now or after the workshop thank you very much for the are there any questions on Zoom or in the audience any questions oh yes maybe I missed this detail but how large are the graphs that you performance kind of scale the size all right great question the question was how does performance scale with the size of the graphs and how how large were they so in order to on okay so this is I get this question a lot we were evaluating older language models that had smaller contacts and we wanted to be able to store a complete graph in them so um those were the initial limitations uh that led us to generate graphs of with um 20 nodes and up to 400 edits H there's no reason we can't do more than that now with larger models that have much bigger context windows but we were evaluating kind of like the two generations ago like large language models when we started um in general though as the uh I I didn't have the slide in this present but as the models have um like there's more in the context window the they do worse so you'll see this degradation and this is like a fairly standard result like the more kind of unrelated information in the context the harder it is for the model and it's the same with the graph algorithm so as we add more edges um generally performance goes down and uh you know I would expect that to go with more nodes as well but we have better reading like edes right now when you uh when you have vers the text Ty how how General is that over a particular class particular typ uh yeah great question um so the question is when you're using a like a graph token learned encoder how General is it um so both in the the paper and uh and in some backup slides I have some some um like some analysis of the G generalization we've found that when you train an encoder on one task um it it can generalize like both the the embedding itself um so like we'll we'll train an encoder on like using one task and then create an embedding for like you know a bunch of graphs and then test it on another one and we do see we do actually see a lot more generalization than I expected there so it's not like they just set one task but if you train um you train something for connectedness then it can do something with like biparting ice and something with like shortest path like um in some cases the generalization isn't uh super high but um I think that you know there would be ways to even make it generalize better so we didn't try to get it to generalize at all and we F we saw you know some generalization um and you know I think that there's a lot of Headroom there that could be explored further maybe one followup question here so do you hear me uh uh yes maybe okay so I was wondering how would these embeddings handle domain shift for example in case of medical Knowledge Graph or or medical graphs if you train on normal text and then want to do you hear me uh sorry can you repeat that question I was wondering how would the embed graph embeddings handle domain shift for example if you want to train on normal text and then uh or then fine tune or test them on medical Knowledge Graph or medical text in general yeah I I think that's that's a really good question um so we did we did the audience because so yeah sorry to repeat the question how does how does this kind of representation handle domain shift um and between different applications like if you trained it on biomedical uh could you reuse it on something else like citation networks well we've done like an initial evaluation here and um we found that first interestingly the type of the relations at least in the text form the types of the relations didn't matter as much as you think so we would ask the same kind of question twice and kind of vary the type of relations and then see like what the diff was in the performance and um the model could kind of still answer it so we we could do it with like a regular graph or change it to be a heterogeneous graph with like different kinds of edges and the model would get about the same performance um so that's one result in the talk like a graph paper in the um in the when learn the encoders we we kind of have a choice um because if we make the the Learned GNN encoder more like a more heterogeneous Network then then it will it will be less able to transfer for sure because like the GN the GNN encoder is going to have to get around those um those relation types but it it seems like the the large language models don't um aren't as like specific or peculiar about it thank you any other question last question thank you so much again thank you again for thank you all actually I have a question have they ever ask you how to talk like a girl okay um so so we are let me share my screen we're now at the end and before you leave people are leaving I would like to announce the best paper award for the workshop uh which goes to oops the segment anything model for Road Network graph extraction congratulations to the authors are they in the room no okay well I'll accept the award okay uh well thank you very much everyone uh for attending uh we hope to see you again next year uh and thank you again to all the speakers and to the presenters uh and to people who submitted to the workshop yes we are recording uh and uh they will be available through the cvpr platform and after three months public you're welcome okay thank you again to everyone and see you at the rest of the conference thank you thank you

