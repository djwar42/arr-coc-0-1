---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=4B8cCdptNMw"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:00.445Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=4B8cCdptNMw

84416e4a-7edd-430a-bcb2-41858daabb18

2025-10-28 https://www.youtube.com/watch?v=4B8cCdptNMw

e5fa3220-d2ec-49ee-9aa6-4f3027740286

https://www.youtube.com/watch?v=4B8cCdptNMw

4B8cCdptNMw

## ComputerVisionFoundation Videos

okay hello everybody uh we are still waiting for the first speaker to show up but uh yeah we will start this Workshop as scheduled right now so I'm Ki from cyber agent and welcome to the workshop on graphic design understanding and generation which is a to be called G and this Workshop is focused on the Technologies all graphic design here I mean graphic design by you know these things Flyers posters advertising presentation or newsletters or whatever the document that uh convey visual information to the viewers and graphic design involves you know couple of complicated process for creation or editing because it interacts with visuals texture messages and then the intention that the designer want to comp and you know Ed we would like to have our AI to be able to answer this kind of an question yeah think of this scenario uh here's one guy asking for I saying that I'd like to create a banner for an insurance product could you suggest design ideas then AI response maybe you know we are still not able to do this but no that we are able to do this this you know sure what about this and then the AI suggest one Banner design and then the designer you know continues to ask on HM could you move the main offer a bit upwards and use funer Font with caraton that align with the corporate identity then probably right now as of know the year 2024 AI systems are not able to you know edit anything about this graphic design right because you know uh still the gbt 40 is not able to handle graphic design that in this way right so here are bunch of know challenges involved around a graphic design understanding and generations yeah think of that you know yeah think about this question does text to image models so all of the graphic design programs I believe you know most of you know you answers no to this question the specific know you know technical challenges behind uh the graphic design is that often the time the graphic design requires vector format which is uh you know usual format for designers like you know know the for example the PowerPoint presentations have their own you know the format a you know Adobe Photoshop has its own format and all of them are not the so-called Vector graphic format and then these formats preserves the documental structure instead of you know the individual dots of pixels right and then this in vector format are preserved high resolution within easy editability and and then controllability another thing is like you know error free typography we are not you know we should not uh save the you know texture information in pixels right we should um save uh the know typography information in vector format like you know what kind of font or what kind of font size to be used in uh displaying the graphic design other uh specific things related to graphic design is usually a graphic design has some intention that conveys that wants to convey something like you know the product images or the copy of the you know product and then yeah there are usually existing materials to present and we need to uh nicely um integrate these existing materials to the know final graphic design and then usually also you know there are brand guidelines and copyight issues around the design these are you know specific challenges around graphic designs to list a few and then the topics of Workshop is listed here we are you know focused on multiple and uh technical disciplines across you know bunch of topics like multimodel document understanding layout analysis font and typography analysis col parate recommendation and so on and I hope you know the uh you know audiences here are all interested in these technical topics okay uh before going into the presentation let me just introduce a few of the state of thee art research in this uh topic uh this is one kind of you know the emerging topic which is a text to template generation and usually on text to image models and generate image in terms of the ler graphic but here text to template uh task is um aimed at generating that document structure instead of ler pixels and uh know representative work is the entitled core by know J J and then his Works um where uh he tries to generate um the complete graphic design structure from the texture prompt this is one kind of Representative recent Topic in this work uh in the domain of the graphic design another uh thing I would like to note is the vector fun generation this is you know the another kind of you know that important topic where we would like to generate uh font or typographic information in vector format in of the last format okay uh let's move on to the next thing so yeah I believe couple of there are a couple of fundamental techniques to think about graphic design understanding and generation the first and foremost is the structural modeling as I repeat many times the graphic design involves a structure like the layers or path information which are in uh nowadays uh represented by a sec structure which can be nicely handled by Transformer based architecture yeah we can think about say Photoshop layers or know PowerPoint objects in PowerPoint slides as a sequence of elements and then I believe and Transformer based architecture is nicely fits into this data structure to handle all of them another thing is that you know this domain still lacks a large scale pre-trained models like other fields like LM for blms and then we are still in need of you know a nice Benchmark data set as well these are I believe fundamental challenges and then techniques we need in this domain okay and uh in the long Le uh we probably need to think more about the uh kind of Upstream of the uh creative workflow uh we are you know mainly talking about uh you know the uh Technologies in the surface or know the very end of the uh creative workflow pipeline because you know we are yeah think mainly you know the computer vision researchers think about generating a specific data structure at the end of the process but in reality cretive Workforce starts from the you know very beginning of the creative briefing with say customers and then designers right and then this Upstream uh conceptualization stage probably needs more about you know the semantics and then yeah then we need to probably interact more into semantics like you know what kind of concept they want to convey or what kind of uh motive to use and then these are probably the uh domain in maybe NRP or whatever they know the uh research field dedicated to semantics but we probably in the future need to um de dive more into this uh uh legim of the process and uh in this Workshop we have three um wonderful invited speakers uh Cher ja and sh sh and Jo yeah from P University okay yeah we are looking forward to you know seeing uh speakers to present wonderful talks uh here is a summary of our program this is the opening uh session and then the next we we are going to have an invited talk by chel followed by Si and then Joe okay and then later we are going to have paper Spotlight presentations by three speakers and then we go into the we will go into the poster session at the end and the poster session will happen in the AR exibition Hall which is another building so be careful it's going to be five minutes work uh we would I would like to thank all the organizers and you know preparing this wonderful Workshop thank you guys and then yeah uh please enjoy this Workshop thank you so we are going to have the first invited speaker chy hi are you ready sure we need to have a zoom uh streaming are you able to connect but here is the zoom information oh I need to have the Wi-Fi on yeah the the Wi-Fi information is also here organizers work to or yeah be careful to or or have a zoom okay oh yeah toap yes yeah and then are you sure you sure Shing check theing the right I just share person than har is a reod at Adobe research based in San Jose who work lies in the intersection area of computer graphics Vision machine learning and HDI research interest is mainly focus on datadriven graphic design with minimal data content generation and manipulation she is excited to develop algorithm to en to enable designers to predesign and publish Works to top venu such as ASV and class let's welcome Cher for giving us talk here and S for the intruction hi everyone I'm car from research so today I like to share with myself about shape graphic design creation and since I'm from Adobe hope everyone knows AI is artificial intelligence not Adobe Illustrator um so I like to uh starting from like more definition about the graphic design uh so what is a graphic design so according to the Oxford language is definition the definition is the art all schill of combining text and pictures in ad ments mag things or books um but from my understanding is not really enough for this concept I would say like it's not limited to advertisements marketing Sol books like the slide I'm showing also one the graphic design and also when you going into the commercial uh like shopping online you will see also those banners and graphic designs so it's kind of broad concept rather than just like the the go and also it's not only limited to the text and pictures it covers a lot of elements actually such as the Thum the thumb face uh the photo illustration texture on the itcon and also the color and layout so each of these component plays a really crucial way for delivering some like the impression to the audience so designers will use those kind of um techniques for trying to anle some like impression or like doing some like on purpose objectives when they design those kind kind of the things um so as you may find creating a graphic design is kind of a challenging task because there are so many elements like each like the fun face there are maybe thousand of the fun face and the color maybe there's a color where you can choose any of the color in it so it's a really large there maybe have more than thousands or hundreds of the kind of the options you can choose so with this kind of large exploration space you really def finder also want to create something like a purpose like when you define a banner you want to attract people to buy the um the things from you or like if you want to create the event poster you do want to attract people to coming to this events so there will like when you design those graphic design you need to consider about these kind of objectives or purpose in mind um except the kind of the objective you also need to take care of the kind of Aesthetics where the harmonization of the design uh so it makes the design more really appealing uh trying to make them more um like like people enjoying the design um so according to this kind of a consideration it's kind of tedious process you need a lot of the true and error process before you may have your own satisfied results so it's kind of challenging when you gr when you doing the print of the the pl so what uh so before the Deep learning I very popular how people address those issues with those technical papers so I would like to introduce some of the like representative work they're not expensive but like we can have a look um so for the fun uh initially when the people doing the fun or text when you want to add some like the fun face use some fun face they will do some retrieval so they can use two ways one ways like they were just using some similar fund and trying to retrieve the funds which is similar to what you are using or they can use some attributes such as like the different whether they're sing whether they're s whether they're round or something or they also like from the odonovan and and his sper um like this paper is poory fun selection using cruce attributes they do use the Crossing with Amazon tur for this way you can easily get the fun like from this kind of the simple um attributes for the the features and for the color um so starting from like after Newton finding the color spectrum like when he write in the book of Optics there are like a lot of masses such as Max they're trying to like study some like the color serum and for this work the color harmonization uh then uh Daniel was like writing in the 2006 with his spors this is trying to predefine some the templates and they kind of assume all the C in this great area they harmonized with each other so they're trying to make this templates for helping the design or the image to be color and also there are also some like work they're trying to study the color palette and they uh so it's still the Peter from and they are trying to learn this kind of col compatibility from the data set and trying to learn those patter so it's easily and he also kind of develop a method with the linear regression trying to predict those scores if you can see for telling people whether this color palette is moreing or not um so this is a early stage when they trying to study the color related uh the um and regarding to the layout uh so earlier work they having a lot of work related to the layout retargeting such as this paper from Kuma and his and her CER at 2014 they're trying to retargeting the web design so since the web design you know they have HTML so they can do the Dom easily for maing those elements and tring to retargeting from the two web pages and it's kind of a sell transfer um and before even earlier people are more rely on the templates and D dynamic programming but uh like after like there those masses when in the 2014 around that of layout paper they're trying to learn something from the data also but they learn really like the simple heuristic rules like they will learn what the balance Factor what the harmonized factor and then they can use like when they having the new design come in U so these are the two words uh a really great work from the Jacobs and his auor and from with hisers for the design layout and so after introducing some of the words before the Dearing I will like think some like a takeaways uh so the first thing is like um if you look back carefully about those words you'll find those Pap really they just study a single elements single design element rather than study the whole design and they are relying on some handcrafted features or human files such as you know like when you study the color maybe the few features when you st the funs maybe they have like the the Cur curvature where the S features so this really rely on the human expertise when they design those features and it's also less creative because it's really manual effort and also you can see a a signal that people are starting to learn from the data set but the data set is still really small steing from hundreds to thousand data points and uh there also based on this data they already starting to use some machine learning techniques such as fdm and the linear regression which doesn't need a lots of the data um so some of the meth they're doing like in optimization way and it will find like if you just using optimization driven method the sometimes the time can slore so it takes time for you obtaining the satisfi result you want so this is like some of the features when it comes to the uh the research words before the learning so what if we have the Deep learning so deep learning is a kind of a subance of Mion learning that uses multi-layer neuron networks we call the Deep neuron networks trying to simulate the complex decision making power of the human brains so it's like the diagram I show here uh among them there are two really typical neuron networks which is really common when we do the gra design research the first one is again which is the generative Deval Network and and also the V which is varation coder so people really rely on this kind of two techniques when they're doing the graic design creation so similarly still taking this kind of design examples first let's see some like the fun face the stud with the Deep learning so rather than using those manual design features the uh the work they are trying to learn the features from the data such as U for the big F they're trying to learn the video features from Pixel format of the font trying to do the classification so they the model with the Reconstruction and the classification test and trying to learn the F representation in that way and also uh the work with his coauthors and trying to learning the implicit Bel shape representation um through the contion neur network oh sorry this work is like they're trying to represent the uh the the kind of the uh GS with a lot of combination of The Primitives trying to uh represent with the curves for um like the fun pH uh recognition those stuff and for more application driven task and we have developed this method trying to um predicting the B pH and their attributes within the B design so when you having web design you want to design a Tex and you need to find a Bas we will automatically recommend the one fit into the so they are some application for the font and furthermore there are some really fancy stuff when they using the G model and V model trying to do the F sale transfer I really like this kind of work they're trying to transfer the cell from the maybe reference FS into the package the whole set of the alphabet and they can even transfer from the images with some like this kind of ch um and it look really cool and so similarly to the font I would also like to introduce some of the work related to the icon shape uh so in the 2020 we develop this kind of algorithm and interface called ion which is trying to create the compound ion that is a combination of the small together U like driven by the um the kind of the model and you can give this like a prle prom and you can generate this compound icon and more recently there is a word called inter Vector which is a really using a really clever idea they're trying to learn this icon generation or shape generation and what they doing they're trying to like just assume each of the shape is like deformable Circle so we'll de the circle of the control point and trying to generate those kind of the shapes uh which really interesting to myself um and for the um for the color perspective um rather than like previously if you remember for the previous T work they usually just use a fingle palette which may have four to five PS but later on they find this is not enough for the graphic design because they have so many elements which is multimodality so to get more customize and expressive color combination of the graphic design um there are some Works they're trying to learn the color distribution of the paletts not only from the color itself they also considering those VAR elements such as the layout and those kind of arrangements uh so the Apper word uh from the the closet is trying the col info colorizer is trying to learn those color palets for the infographics and the below work the color recommendation for vector graphics documents this work they're trying to um predict the color pal for different components such as the images shape and text they all have their individual palets when they doing this kind of the color and for the layout um so researcher are trying to learn the model to um kind of giving the alement have where the number of elements they're trying to lay out the the kind of the design so early work they also based on the G model and V model for layout G it introduce a novel like the frame framework that is it rendering the graphic design even like discrete space and then you can having the pixel signal we triming the model and also so uh there the the right work the classify generation modeling for graphics uh layout they're using the V to do so and to condition on more high level Concepts such as the keywords um and also the category uh the below work is General from with her coauthors they trying to uh using a g model to encoding those mod or that inputs so as you can see those kind of design uh the algorithm is only for the different elements and starting from with the different and more data come in there are more Works they trying to uh kind of understanding the design as the whole or trying to create the design as the whole so in our work as 2018 uh we're trying to learn like uh we just rely on the online dat set and trying to learn what's the personality of the design so the personality means like whether the design cute whether the design uh ROM and it can try like after with a sound like a weekly learning and can recommend those elements for like enhance the personality of the design and it can also transfer the personality among the design such as in this case they can make the upper one more vintage based on the reference and make the bill one more D Dynamic and later on people in this community takes a lot of efforts to TI and collecting those well created data sets such as has from yeah they like dat set which is really M for the gra community and then you can having more kind of possibilities when you working on this domain and more recently FL DM treats the graphic design as a set of the multimodel elements trying to predict the properties of elements including the elements T the position the selling attributes for those image Anda uh elements under UNIF architecture so then there are some takeways for this kind of early deep learning uh deep deep learning stage so you can see people are kind of already taking advantage of deep learning model such as g v and also large scale online data and they are trying to learn adaptive and the customized features from data rather than using some human like handcrafted features and also because of the graphic design data is really hard in the label like if you using Amazon you can only have a limited number of Don notations so there are lots of work there using with confir s or with by learning trying to learn those features and generations uh and also like you can support the many creative t such as the fun fun St transfer we showing previously um and also there's some limitation at this stage because the data set is still kind of limited and it's only limited to The Graphic Design data set you can already imagine how limited it will be because each graphic design needs a lot of human curation for having the high quality data so that reason the data is due kind of limiting skill your to will be a few thousand that will be already a really large skill so then um in recent two years there are so many Works related to the CH large language model I know I think everyone already know that and maybe using them we like I'm seeing a lot of amazing results generated by those models uh such as a me journey and also the Adobe Firefly and for lar language model the CH the really representative everyone already know so it's kind of below people's mind about how the the content has been generated and used and also like there are lots of the otherg such as L you can always like customize based on the different content and you can see this AR go so fast with it the I around and you can see like within just a few years there's so many large language model has been happen and evented from the different company all the optimizations so then people are curious why they can be used in The Grand Design so actually they already having a lot of products they already using this kind of techniques in their real products such as a canva this is a feature I just uh borrow from their web official webss so they kind of collaborate with like d trying to use those kind of gentic features in their graphic Des workflow and also I would like to show you uh this is our like a go for that called go Express it's focused on the graphic design and they also having this trailer for advertisement play with it there's no sound okay see mic yeah or I can play on my end yeah I think can do let me mute e already a lot of the features are in the support that and also there are some other that from like Microsoft has designer so every company they're trying to using like this GI large langage model in their real use cases so how it happens why some way they can be used in the graph design so this is the kind of relative comparation about the that we're using for the graphic design the S as you know for a few thousand even for the 20 already really large for the graphic design itself but for the model we train model diffusion they using so many data like over the the data so you can see like when you're seeing those SC it's kind of really not comparable and also for the general training data for those L language model they also contain a lot of content from design like the icons the graphic design Stu so they actually learned the features those R image features and also ra multi features that we can use for the gra design real applications so in the remaining time I would like to quickly show a few of the work maybe already show some of them for the recent research Works they using the uh these kind of models um so for the icon these are two of our wordss uh related to the like you representing the icon in the SVG pass so the first one is um the tax to VOR generation with neuropath recation this will be present to be the graph 2024 so this is trying to Genera in the um the v in pass with the uh instruction not instru like the prompt and the input and uh so when you have L icon you can customize later because it's theg pass uh and also the blow one is the test guided the vector graph is customization so in this case when you having the initial Vector you want to J more you can just using the prompt and we will just customize for you they will still share the original identity and still following the prompts and also in the s space that you can customized with the existing softwares and in this we also have one word called Nal trying to neuro implicit Vector layers for T Vector generation so rather than representing them in SV space where the people space we're creating this kind of the neural layer in place representation for the vector so we compare also with the Rel Vector fusure for the conative result and we do have a more kind of vectors like uh the Anon like shap and also for the layout uh recently there are a lot of Works they using diffusion such as layout DM U they're trying to using the this create this Bas diffusion model to model the layout problem and also in this here we have a word called the ver layout composer so rather than only using either pxel or thetive space we kind of combine this two space so kind borrow the ideas from the both vector and the Piel because for the vector they will better know the alignment and for the ver better know the the whole perception and also there are two really great in works for trying to generate the design as a whole so when you're just giving some instructions they can direct giving you the design with under kind of a thone um so this is the core and open core and open core we having the post your interest to have a discussion there uh so after introducing a lot of this work like some finaly to it I would say like when we go back to the title we're saying shap the graphic design creation if you listen I would say definitely the answer is yes you can see already evolves a lot when there more kind of the new um like newm coming in um but like uh it's I don't think it's already enough because you will see like for creating this graphic design there are so many stal elements and it's very subjective such as there are lot of the kind of when you having the Tex having the line there's also line space and word space that you need to consider so it's really a complicated problem so we still have a long way to go and also when especially if you are a noice when you do the graphic design sometimes you will see whether the design looks good or not you are not really confident so for theist people really don't know how to don't know how to evaluate the design so I feel like if we developing some methods trying to help people know whether the design looks good or not what's the advantage or disadvantage so kind of things that would be super important and also it it will be great inove the transparency of the model itself to the user then the user will know why they make this decisions from the North or South uh and also I would say there are also a lot of the efforts we can do in the HCI Community for the human Computing reaction trying to make the those kind of tools more user friendly and easy to use to increase the efficiency when the people do the grand B uh so but I already really glad to see the community has GRS and a lot of the really good work happened recently uh so that's my major things I want to share today and thank you uh if you do have some questions where you want to discuss with me this is my hom page you can scan it thank you so much okay uh thanks Sher for the wonderful presentation uh we have about 10 minutes to you know take questions from the audience he go ahead the question thank you first for the amazing presentation is SVG just a du facto standard for VOR graphic representation uh that we should just carry on in the age of AI or should we come up with a new way to represent graphics with text that is more AI friend and whether Adobe is doing any work in that direction yeah I would say like um basically there are already lots of the ways to represent Vector such as you can use the to repres Vector you can use SVG and you can useing those curvature and you can also deform the circle like with those kind of the points uh so far there's not a really uniform like it can do everything good representation for the vector is still under exploration because Vector is kind of the you need to like maintain the editability for the vector right when you have inventory you want to them easily so I don't know why X is the best best way for representing the um the vector when you train the model because when you train a model you always want to learn easily to learn the VOR and also still maintain the ability so there's still kind of the efforts or the Future Works oh it doesn't work can you hear me okay yeah maybe I don't need a Mac okay so then there's still like some Future Works we can do trying to like ensure the model easily to learn the vector while still maintaining the avability so for the so for the learning because the back data is really not that much and also they're not in the uniform shape when you're checking those SVG even the same has like the circle they can use in multiple ways to represent it in SVG so that's why I don't think I'm not sure whe SVG is the most yeah good way for representing it and for the Adobe yeah as you can see we are having some like an internal work and we rep that will be present on the Wednesday if you interested you can come and talk through the post section and do trying some internal efforts trying to find a way to represen in the and trying to learn them yeah and also there's another line in the work is like you learn in the pixel space and then you do the vectorization that's also another way walk around yeah thank you actually this is first time it's super interesting yeah yeah um so my question is like when it comes to manipulation graphic design so we are talking about AI so we are talking about something like tax I so ta the best solution to like mation because from my perspective for example you show that you can make some gra more Dam but if I want I say I want to make it less dyamic or more Dam Bas on the so is there a client can t to this or is there another another way that may something like we have we Expos some API and the VOR and when it is two it is super dyamic when it is one it is yeah yeah yeah it's really good question uh and I think it depends on how specific your idea will be if you are just like having really high level idea then T would be a really good way for you to express the customization because you can just easily to describe with and heads and those description but if you can have some more specific idea you have your own speci in mind of how to maybe do the animation or the dynamic you can I think I think there like earlier where they do have some like a drag me you can just some control points where give some like the maybe flow with some arrow and they will just do the animation that way and recently we also had some work now in this conference but they're trying to do some like the animation transfer with some example so you can just use the example as a guys rather than using some like because some sometimes it's hard to describe those animation words any other question from the audience oh go ahead see can device but I'm curious about like uh like such kind of like uh design generation like uh would like like like the people who see like like be like or say like U making like certain like to evoke certain emotions from the like the audience or like uh giving them like certain like imagings about the recently and I think they're not delber to do so like our ear work at 2018 we're trying to learn the design those kind of personality like whether you can evoke some like whether you it's really cute it's really romantic so sell but recently I just feel the large language model or the model they may already having certain capability but I haven't tried maybe you can try on those kind of already B product I would say they may already impa a certain like emotion like if you see I want a cute design maybe they just use some cute Bon cute color potentially because when they training on large scale data set they may having this kind of feeling already but they're not deliberately trying to learn that but at least you can try but that's a really good direction because you like when you having a design you want to evoke some impression from the audience so that's a really good direction to study yeah uh so usually like when you're doing a generation you will have some the data set as ground shoots so they would just meas the similarity for those BR shoots and the images and when you study like a certain like design element such as font or the color they also have their original BR shoes when you do the like test so they just using like similarity as the Matrix but generally I would say yes the graphic design is really subjective when you like Marin maybe a certain like the WEA they're attive pleasing they're really human subjective thing so also there are some work they're trying to collecting human opinion through the crossing platform and trying developing that man that way through the studies or something yeah okay uh any other question okay yeah go so usually the layout they just like either they taking the number of elements where the elements have as input where the design content the output will just be the coordinates or the sometimes they will have the element T also so it will be like either the four bounding BS the coordinates or maybe the diagonal if they assume you just just a rectangle and later on for more Recon work sometimes when they generate the layout they also generate content so in that way they may generate the video features or maybe just the image itself as output yeah thank you okay okay uh any other uh question from the audience okay no more yeah thank you so much all right thanks again the speaker for wonderful presentation okay uh we are going to have the next speaker uh who is C and yeah she's preparing for her presentation okay uh you is five and you just need to connect to the it works yeah yeah perfect yeah oh this is interesting maybe you need to switch the here sh Buton yeah you can start or maybe we actually have two more minutes start maybe you can start yeah maybe now can introduce is given by the doctor she and she received PG degree fromi and she's currently senior in Microsoft and she has been producing many insightful papes on graphic design and I'm personally also a big fan of sh and I'm very excited here at the PO okay let's welcome sh okay so my is model so what design design is a for elements in and the details in a way that can accomplish a specifical goal uh for example in several design the goal could be umw Lance and congestion in mechanical design the goal could be a specific functionality and also in graphic design the could be the Aesthetics or or whether it conveys information effectively and is similar for architecture designs um um um Des creation is actually very difficult because it C cost lot of and it also require many years of training when you become become a professional designer and so uh in the past we have many softwares and tools to help out cre a designs uh for example in mechanical design we have Auto Safety and auto desk and in graphic design we have Photoshop camera and P Point um with this tools and the softwares U design creation has been changed a lot uh first uh firstly uh we can have more accurate and precise design conditions with this uh tools and also uh with the virtual manipulation from this TOS we can red reduce um cost and um this s well also provide a good documentation which could improve um communication and knowledge sharing um but um this is the past what have been ched by softwares and CHS um so if we could have we could um create a new age of Des creation um what what do you want I believe everyone will want efficiency enhance efficiency which means we can generate prototypes more quickly and um um our care action can be automated and also we would um expect um a boost in creativity uh which means um a system can provide new ideas which um we have never been considered by our humans and sometimes um we often expect personal liation which means the projection um is provided based on the understanding of user preference target audience and the context of design and we could find uh there is a rising interest in turn such a new age of design creation uh we could see U there are lots of companies and comp communities are discussing and even uh investing this area and in this talk um I want to discuss about the RO of foundation models in the in leading the way of creating a new age for the DI creation and I would like start from the uh strength of foundation models uh first of all we all know that um Foundation models are very flexible it can be used across diverse test and if we have S capability in design creation uh we can have a better efficiency and also a boost creativity and also um Foundation model is good at um percept percep user in very smartly and sharly with such with stability in like creation uh we can expect a better experience in personalization and also um there are extensive and cross disciplinary knowledge in the foundation model U if we have L if we could leverage that knowledge which could better efficiency creative and personal personal at the same time um in the beginning I want to take graphic Des as example to show um our exploration in the in the ru of foundation model in creation so uh what is graphic layout and it is arrangement of video elements um for example for the mobile UI here U we it has uh it has elements such as image text and buttons and each element has um its top and left coordinate and weight and heat and there are some other C such as web Banner on slides and Mags so um in the of GRA L generation there are many tasks for example the refinement completion and the and the generation condition on TS um actually U currently different tests are so so by different method uh here I give with people about um what technology has been adopt for different tests we can find across different dimensions like include format model architecture and the learning method and there are many many uh Technologies for different tases but um if we think the model they are actually very flexible um which can be used across test um one key factor um that contributes to um such flexibility is to use different as unified input output and for test so um this Mo motivate us to uh unify graphic Lev generation as a sequence generation problem um our um the first component in our framework is a conference sterilization modu which represent and Conference by unified six format then we leverage a typical Transformer encoder decoder to generate the out at last we further introdu a decoding space restriction mod which can Pro the invisible options in the predict distribution and for the conization we have a b observation on which um both layout and constants are about elements and their file attributes including type left toop coordinate weight and Heat so uh with this observation our method is for um the first step in our method is to um Define a set of vocabularies about attributes um for example for time the the vocabulary can be um image text and Buton um for coordinates the vocabulary can be infered from zero to um 255 then um after have started our vocabulary uh we can conate the tokens from different vocabularies to construct layout and a single constraint and further all we can concatenate sequence of different constraints by a fixed order to construct a combined constraint U here are some examples for different C about its input how how it input sequence looks like and how it output sequence looks like um after the after the Transformer enoder deod structure the decoder um predict the distribution uh we developed two bling strategy to have the model um predict better layout the first one is about Conant grooming um where we choose invisible values that while the con and we also have the pro probability on it proves the values with a very low capability because adding SE element to the sequence might lead to very bad layout and also we introduced a back treat back checking um checking strategy which DRS back the decoding space if when all the all the values are approved We compare the method um called lout former Plus+ here with the Bas lines um not that all the Bas lines only handle one or two task two test while the our method can handle all tests and our method also um out form out form Basel on all tests um next um I want to uh go more closely to look more closely about the input constraints um we can find that the in the input constants are actually the um rigid guidelines U taking the generation com types as an example the user are restricted to provide the exact types to the this and such re guidelines for each TX actually are not tored for user is and it is teed for computer um considering Foundation model we can find that they can quickly get what the user truly want even if the input is complex and weak sometimes so uh this this Mo motivate us to propose a new task uh to provide a friend friendly way to control gra lout generation weord effect to layout um here example on the input could be um I want to show some several pieces of news each with Each of which has a title and a summary and there should be a c and T and there are several challeng challenges in this T the first one is ined CS uh for example here the title and the summary uh actually refers both to uh text boxes and also um the constraint may be combined together for example here are constraints about types like for typle summary and there are also constraints about the position like um a heading at the top um Additionally the con might be uh incomplete sometimes uh in this example the user do not see I want P image but as a news page window is better to have image on on the page so um f model actually uh motivates motiv motivate us to think about this um this new new test but also uh give us a way a poent potential to solve this test um our proposal is to uh introduce intermediate orentation to the CLE the problem with the umentation here are two stages in our proposal um the first is the par stage which U translates the nature language to the intermitting dentation um this stage can be finished by fish model either F tuning or prompting and the next stage is the place stage which U translate the constraints um the expens constraint to a layout and as we have discussed in the last work actually there are good solution for uh explicit constraint so uh for the par stage the most important problem here is to how to design the intermittent age dentation uh here we follow two design principles the first one is the expressiveness uh which ensur um that it it be able to express diverse your constraints um the other one is formalism which ensures that it is friendly enough to exist NP Technologies uh with that we can leverage fation model to finish the part stage um for the BL stage U we adopt the sequ to sequence model introduced in the in the L work but with two small tricks the first one is for Combined constraints we simply serialize each constraint and concatenate them in a certain order the other one is for incomplete constraints and for this problem we introduce the categorical attribute to indicate whether element is given by user or Auto completed and for the for the next stage the the most important the most challenging um here is um we need lot of data with intermitting mation as input and L as output into CH such model but such data is very expensive and hard to uh collect um on the on the other hand we have another observation um we find that label layout contains um very uh very abundant patterns so um our proposal is to uh exploit on label layouts U which means um we first synthesize intermediation from lout by a set of a set of htic based use then we research sythetic data to TR the B um but there are gaps between the synthesized interation and the real inting representation iner from the user description uh look at the example at the right side here in the syn intermittent Mission um there are large images um produced by the her based R but actually the user did not specify that so to mitigate such um gaps between synthesized data and real data we further introduce fin step which use label data to um train the model um because this is the first work in the Tex to layout task we adopt some uh methods from related to Ma and compare it with our method and as show in this picture we can find the our our method CH performance uh the last topic I want to discuss about um data efficiency um graphic graphic La is very um very big concept there are many tasks like uh explicit con uh explicit constant generation text to layout and content generation content a generation and also it has manyas such as document poster Android and web layout so U for each Tas and domain the model needs to be trained with enough data and this will be a very big challenge here because we cannot collect so much dat um again we can look at the function model we can find that uh with the knowledge acquired in the pre-training uh they can perform different tasks without further training or many new um this makes us this makes up to ask this makes up to explore a new topic whether we can simply R Foundation model to generally gra it out we find there are several challenges um the first one is how to L design knowledge included information model even if we know there design knowledge and the second to to for this test we propose the input output model um to for to Sol this challenge um the next um the other CH here is how to make Foundation model understand different typ and domains for example I want to model know I I want a document La not Android so to solve this challenges we propose the dynamic demonstration selection mod here um for the input output a mod um we for input concience and sequence um this is to follow the excess of previous studies introduced before and this is also to U align with typical input format of foundation models um for for output the layout we formulate it as a HTML code um it is because um HTML code is a systematic and a structured way that can describe any type of graphic designs and and also we we think Foundation model might have div L knowledge in the form of of hdmr code because the training data contains um a lot of HTML Pages um for the dynamic demonstration selections um giv a user constant with s with closing data from the training set as the next demonstration and here the distance is measured between your constraints um they it to um find the most relevant context and also working and also to help Foundation model better um comprehend your con maybe speak a bit louder oh okay okay um we also includ layout in um in order to have Foundation model better capture the characteristics of the target um putting all this together we get the final framework uh first um there is a preamble which um briefly describes the basic information of the task and then we have the demonstrations reted by the technology mentioned about um and we we uh conate the test input uh note that here for the other demonstration the input cont and output layout follows the uh particular format we have we have given in the input output format par input output format paraphrase stage when we send when we send all all of this to Foundation model um the foundation model is the design they with the format of code and here are the not we compare our method which is the TR 31 with all baselines which are training phase ones um we can find that our method achieves comparable performance on most task and even better than basid on some tasks and another interesting thing is on Al HML code is Prim used for web UI in Practical scenarios U we achieve good perance where it on mobile UI documents and posters we um we also make studies of dat side um for the B like u l former Plus+ they are training based method we change the size of training data for our method L pretter is a training3 method so we change the SI of retrieval Tool uh here is the IID score of different method on the with different data s uh we can find um to achieve comparable performance Le pumper use much less data than L former Plus+ or for example 500 data um is enough for L fter to good performance but former but for need more than uh 10,000 data to achieve a good performance and as a summary and in the Deep di with start from the recognized stren of foundation model and then we U pro we propos new method um inspired by the foundation model to make achievement in graphic L generation uh here we first consider the flexibility of model and this motivate us to use sequence and UniFi input output presentation which makes us to make a unified framework to handle different Val tests and the second uh strength we consider is the sharp perception of user intense um this with that we take we we we we think about taking Foundation model as a transmiter um this makes us to propose a new Rand way to control out that generation the third strength we consider is the extensive and cross disciplinary knowledge included in the foundation model uh with that with uh with with that uh we explore how to PR the foundation model and finally we propose a data efficient and training free generation framework um in the next I want to um discuss discuss discuss about the future how to push funders of the current Technologies and desire Creations um the first interesting thing in my mind is I think there is a debate on layout lications um one method is to represent layout as a sequence which takes the object as a unit and generate object level attributes um but U such with such technology we should rely on numeric understanding ability which is even difficult which is difficult even for fation models um for example uh if you really want to know whether two elements are aligned in in a l sequence we need to calculate whether it's on top and the left coordinate are the same um another way to represent layout is to uh use it as a r image which takes a pi pixel as unit and generate pixel information um with that approach We R we actually r on V understanding ability which seems to be more intive and mirror human behavior for example since we have such a layout image we can just gain and we know whether two elements are LED we do not need to calculate whether the two ordinates are the same um the first approach actually produce edable design but the second Ro produce um nonedible design so most work currently is focused on the first approach Which models lout as a sequence just like the threeory work I shared before um but personally I think if we could introduce um rather image information into the model we could have more interest in finding a next interesting thing is about how to go s generation to editing and here example uh actually uh user may have many many uh requirements after she get the first results for example the user may first um request the system to produce a string page after again that Jim wants to um Pro more product items and even more even want to add a description for each product and may uh try to change the way how the how these products are arranged so uh EDI is crucial for PR youth that um it is really studed um I think um are three channels here and the first one is we for anything we we actually need a more strong generation capability of foundation model for um their generation and also the data collection and synthesis is much more difficult for editing compared to the generation task and also women need a new learning objectives and valuation because on the editing results are diverse and personalized um a next uh a next interesting thing in my mind is about how to go beyond the graphic lines um here are some here are some questions um actually there are many designs Beyond graphic design like cular design mechanical design and AR architectural design um so can this design also be powered by models as graphic L design and will there be a model that can handle all all this designs in bi single model um before sharing some personal views for this question I want to first introduce um some observations um the first one is actually for designs in other fields there are attempts to formulate that as sequence for example sa in mechanical design and house plan in architectur design um another observation is actually different Des my Fields share um some common line principles um for example um balance and as symmetry is important to all Ty of design um for mechanical design baling weight distributions ensure theity and for architectural design symmetry um often is often used to provide a sity um there are also design principles such as flexibility adity and minimalism they are all shared across different lines and uh uh however uh I also find that um each design field has its unique needs and domain specific knowledge for example graphic design in graphic design we would expect it should be athetic op and con information effectively and for circular design or we want to optimize its Valance objection and density um for for architecture design we would expect a precise relationship among architecture components so um my personal view for these questions um for for the first question can l in other field Al be powered by by Foundation model um I think it is promising um especially when we do not have mils of data um the Reas is that um the the first one is um converting the design into SEC formats we have model to Pro them after and the and the second format has been studied by works recently and the second reason is that the com design knowledge might have been inced information model in some formats just like the HTML code for gra line and the last one I think is important to consider the spe specific knowledge when we want to um use Foundation model and um my personal View for the last question U will there be a model can handle lives in all the fields um I think uh it's impossible but there are challenges in the DI predation U the first chall here is the complexity of different types um unlike the language which u m de always on text in the field there are many many different types like uh 2D Graphics we have introduced and also 3D structures and the netl in the circuit design and another change is that we need to carefully balance subjectivity and objectivity um for example me mechanical design is consider more objectivity but a graphic might be more subjective um okay uh that's all for this test if you are interested in our work please visit our page for uh more work about graphic L generation uh we also have work about capturing CH from layout and um in in our uh early work we also focus on on human computer interaction perspective on AI power like thank you okay uh thanks uh s for the wonderful talk uh we are going to take a couple of questions from the audience right now uh any question okay yeah thank you for presentation in the first work you mentioned you use some alistic rules to syze the training data when um how much data is required to achieve a satisfactory performance uh that the diversity of the amount of the data matter uh yes uh we use one million data foring uh we also try to dat but I one I not discuss and the diversity is actually very important so we have many versions of RS to improve performance uh so how did you Del the political rules did you like uh ask the suggestion from designers or you just come up with them by yourself uh we have a regular meeting with designers to improve the we will give them some examples gener by from our model and they give our feedbacks so it's also um an interesting Pro eff with okay uh other question okay Des any IDE that there are some design in in Comm from different aspects of models and and layouts so ask about how to find the common design inside and just bring them all together into AI Foundation model um um this is very um actually difficult problem because I'm working on um I can share my um personal approach actually um I to work on all of these designs one by one um I work on graphic design in the last year I will work on on mechanical design this year I I hope I can find a way to unify all this designs by such hard work thank you yeah sure I have I have been working on T programs myself and I found interesting vention they have something and also I'm interesting one specific problem that I have encountered in when I tried to utilize some function mod gener so it is numerical problem I mean uh if we for example if we ask the G something G to place two box and one box is just add only another box which means their two surface will be like exactly uh align like they f each other fix with each other so it requires for example the uh lower surface or the upper box should be 10 and then the upper surface of the lower box should be 10 or in some other system it should be like 10.1 so my observation is that Al some here general prision for example this small box is uper the box is down but it can it always have PR it cannot uh produce some precise numbers that will like achieve my goal so is there any problem uh like this occurs for example in 2 or does this because you have some smart Solutions um actually uh I think it's Al we also have this problem in 2D design so uh this is why I mentioned this Bas on presentation um the the problem you mentioned is because we always mod all 3D designs as a sequence um but um as I just mentioned I think we should have method to introduce the the information of R image or r 3D designs to the uh to the to the process when we want to prance um but I think um gbt is a Clos uh model it's um we we may choose some open model to St this problem for how to incorporate oh yeah yeah okay so it is like even if we do strage the problem of do course for example in the r image how can we tell that these three contents are aligned or c or not so if we do want it to be then and for example if we the image how can we assure that those images output are aligned like by centering or by other GRS so this is my I think this is kind know a Comm problem that's in our C and um I think thank you yeah we should have a you know poster session to you know discuss okay uh any other question by the way uh no no more okay um maybe I have one question to you uh so you have been working mainly on the you know layout part of the design what what do you think about you know the generating content together with layout information do you have any you know kind of you know potential ideas like you know generating say lter image together with the layer component any perspective um I actually I have some um videos when when I want to okay when I want to make it I can share some experience um I think your wife um thing is the most difficult because we don't have okay okay all right all right thank you uh so any other question from the audience no okay then let's thank the speaker again yeah so we are going to have 30 minutes coffee break from now on uh I don't know where the coffee will be served maybe you guys can figure outside all right uh see uh in 30 minutes okay maybe I need to first move this screen to the other display hold on second okay okay okay yeah your screen see my SES yeah I can see your slide here let me just uh set up the uh projector yeah it's working now all right okay good we are three minutes and and in the beginning I will give an introduction okay problem maybe we can show something is there space Maybe he is turning on the camera or no I don't know I open my camera uh you don't need but if you want yeah we show your face okay maybe I will say hello to Audi yeah yeah and then close it right on second I don't know if we can arrange the one side by side speak then present this here yeah working that's great great uh sorry we don't have a camera for the uhia ah no problem we have I don't know what roughly 20 to 30 people in the room maybe okay 20 people okay yeah e e okay Shall We Begin yeah let's start the rest of the session okay uh hello everyone uh we are pleased to have Professor Jan as third K speaker Joan is an associate professor in P University his research interests include computer Graphics computer vision and artificial intelligence he has been working on the topic of fun synthesis in the last few years he has published more than 90 academic papers in top tier journals and conferences he was the principal investigator of several projects founded by national natural science science Foundation of China Beijing Nova program of Science and Technology Etc and have led his group to successful developed two practical systems for the generation of Chinese fonts let's welcome to hello hello okay thank you for each introduction okay hello everyone um I'm Jo uh from ping University and I'm glad to present my talk for czes where let me start a video okay okay for this is uh um where deep Genera models in this Workshop I'm sorry that because of Visa issue I cannot be there in my in person so I need to give this St all right and thanks for the invitation from the organizers okay we know taex taxes are everywhere everywhere such computer forms are wiely use in our daily lives appearing in books and newspapers uh print and advertisements social media and more and but however three uh representations can be quite diverse from two dimensional to threedimensional and from images to graphics and from static to dyamic so want to create a computer forms how to create a Compu forms traditionally you have to um first spend several months Maybe how to make the computer forms uh step by step and then okay um and then you you need to draw or write all characters out uh like this you need to drop them out and also uh or write them out and package them into the F Library like this yeah so this is a process of uh of making a computer phones but for English yeah it seems a piece of cake okay already uh 52 uh letters right so taking maybe several hours or days you can create a englage form but for Chinese it has become um tougher over numbers of characters included in the form Library increase so let me take uh Chinese forms as an example the official character set Tob 19030 uh 2022 consists of 87,00 887 Chinese characters so let me give you a a brief uh a examples of how many Chinese characters are there in the for in the Chinese form so what's more many Chinese characters are are really complicated super these character are really complicated here are some examples of those complicated Chinese characters so we also need to keep the St consistency in the in a form so therefore making a complete Chinese form library is usually uh an impossible mission for ordinary people usually a Chinese cap form may take like four years for the group uh with some with several skill font designers so our goal is to create computer forms easily given a small number of gri Bon or designed by the user or phone designer using our system a phone Library including maybe uh handwriting fonts printing fonts C graphic FS uh fonts with spe special effects uh in those designed input font styles with arit arbitary large numbers of characters can be generated automatically including Chinese phones FS Etc so for example feeding like 100 characters uh written by the Asian Chinese craphy um so they can obviously cannot but he cannot write other characters they haven't write before right but using our system we we will be able to automatically synthesize the grip in the C Graphics writing style uh for all these Chinese characters so furm we expess that the system can also be used to fible add in fonts by modifying their F attributes um however like a decades before even through the Deep learning has U made significant progress in testes like object detection image recognition and classification very few people uh who have imagined that a generative relative text could be effective accomplished using neur networks but in in recent years we know that uh with the development of the game vae and diffusion models more and more researchers has began to apply a deep neuro deep neural networks for generating images and Graphics or videos so um so we we we can summarize those kind of works like this okay so we also we we can summarize existing forms of input and outputs so different combinations of these uh forms uh so can can have a various uh applications including the wellknown test to image image to image sketch to sketch and also we can have a attest to grief and grief to grief and so on so deep generating models can have thoroughly rized the traditional Paradise of the stre modeling and image synthesis so achieving many astonishing results so in this talk I focus on how to uh synthesize grips where deep generating models so as we know uh the mainstream deep generating models M include Auto Inc ve G and auto autographic models including Transformer and a FL based Genera models diffusion models and physical uh physic based generating models such as nerve stf Etc and also hybrid methods so due to the time concern I think many most of are very familiar with those generative models so I I will not elaborate on each of them so using those deep Genera models uh impressive performance has been achieved for general purpose of image uh synthesis but uh currently problem still exist in some real applications such as uh form synthesis so there are some major problems uh that is the resolution of grief images since by existing models is relatively low and so many critical local details are missing and for Chinese grips with very complicated ships since Strokes are often incorrect or missing and how to directly synthesize Chinese Vector forms is still ongoing and challenging problem and due to irregular data structures of those Vector grips and due to B mention uh challenges exist existing in grips so even using the most advanced large scale multi mul multimodel multi model model generating model such as um like d party and R Etc and the issues of content accuracy and sty control ility in grief ciz cannot be satisfactory uh addressed so in in our Institute a Institute of ping uh of computer technology in ping University I lead a group working on ph synthesis for more than 10 years so now I want to I want to briefly introduce a major progress made each year in the last decade and that uh let us uh just uh reval those history uh in in the last 10 years and actually 10 years before we we did not use any neuron networks because the since the results generated by the by those NE networks are really are really unsatisfactory okay so we might use some traditional computer uh Graphics computer Graphics Technologies so like in in 20 in 2012 and we propos an automatic stre muring and for sty Fusion algorithm for Chinese characters based on as rid as possible interpretation and non rig Point registration and so and in 2013 we developed an automatic Chinese phone generation system based on optimized component assembly Str strategy and capable of automatically generating a complete Chinese form library from uh like 639 input gri images uh carefully selected by by by us and and then we also released uh release a here we also release a website our website website uh this is a this is our website and free and uh uh publicly accessible online platform for creating Chinese character Chinese Li Chinese phone libraries in enabling ordinary peoples to easily create their own handwriting Chinese character phones so through this platform we have collect huge amount of handwriting datas Chinese uh of especially Chinese characters in the last 10 years and we also have we also propos the algorithm by exploring the uh aesthetic uh aesthetic rules of Chinese craphy we automatically can we can automatically evaluate the visual appear appearance of Chinese grips and use this to enhance the grief synthesize result and in 2016 uh we propos a sty learning base Chinese handwriting for synthesis system and for the first time the handwriting phone library in a users personal style with arit arbitrary like large numbers of Chinese characters can be generated automatically and before that actually this is a a really challenging problem and cannot it haven't been solved by uh before before 2016 and for the first time we have present such a uh impressive results so specifically in this world we utili utilize some shallow artificial neuron network not deep deep neuron Network and we just use a very shallow uh Aon and to precisely model stroke structural style and stroke ship style respectively and then in 2017 we the first time we used the Deep generating model and and this year also the very famous work pxel to pxel uh was published so was reported actually and so we we also use this the similar uh framework and but use some modification so we we we propos an end to end Chinese phone generation system based on deep generative adversor Network and the KE idea is to integrate a new font sty feature reconstruction model uh into into the P to P famework making it one of the earliest work to use a deep generating model for Chinese phone generation test so as we can see let still exist many artifacts in synthesiz data and here you can see lot of artifa and then in 20 in 20 uh in 20 18 we propos the Chinese fun and interpretation methods based on manful learning and de neural networks ensuring both gri colle colle and Inter inter interpret abilities of f Styles and actually from from 2017 we all use the Deep generated model to solve those problem when we use scan Transformer and and also ve and diffusion model and then in 2019 we publishes talk talk papers we propose Artis net and for the first time achieving automatic generation of arbitrary large scale artificial artistic style form and based on one stage field shop learning the core idea in Worlds sty content DEC comping and local data argumentation A major advantage of this method include minimal training sample requirement and sinon transfer of texture effects and and phone Styles and the high quality of synthesiz gri images and in the same year we also develop a structural information guided Chinese phone generation model based on deep static networks and the cor idea in Wass two stage generation process in a straty that se separat writing trajectory and out counters and in 2020 we propose propos attribute to font that utilize a visual feature transformation model and attribute attention model and and semi supervised learning mechanism to automatically generate fonts with personized sty based on user given font attribute values and and the next year and we we develop the F synthesiz method based on deep reinforement reinforcement learning the keep idea is to apply the Deep reinforcement learning to obtain the TPS interpration parameters for strok uh scarington transformation in different styles and then recover the uh uh rendering rering details and in 20 20 in 2022 we propose a counter a rare layout generation network uh which take gri images and their corresponding text as input and synthesize uh aestic uh layer for L automatically and last year we adop the static uh Transformers for high quality high resolution Chinese gri image synthesized uh and the idea is to apply the uh parallel Transformers to avoid uh to avoid the accumulation of the prediction errors and utilize the seral seral Transformers another one another Transformer to enhance the quality of synthesized strokes and recently in this year we propos a QT form an efficient Quant tree based diffusion model specifically designed to handle test of efficient F shot form sensor size and ke idea is to design the Spas quot tree base gri representation to reduce the complexity of the representation space with a quote uh quote tree base representation so in this manner our QT form compared to the existing approaches can conally high resolution gri images with super uh Superior super quality and more visual ping details so meanwhile significantly reducing both parameter size and the computation cost okay as mentioned before up to now the G image sensiz test can be well solved and since the vector form are more wly use in computers so is it possible and how to automatically generate a a vector form so first let me look at let let's look at the difference between the G images and Vector gpes most existing font generation models only am to uh generate Grave images uh also most of our work are also focus on G images but as we can see the details become BR and when we zoom in we can see uh many details are uh so where our am is to directly synthesize uh Vector forms which has a scale Inver representations that is if you zoom in and you can see the uh always Shar details so also Mor uh Mor you can we can see that Vector grips can have significantly Less storage requirement compared to the elected G images like uh a Chinese phone Library maybe containing a 7,000 characters only have two megabit okay and if you uh if you if you s the uh 1K plus 1K C images uh BMP image file you will occupy like one MB so keep challenges of this Vector for syze are threefold uh including un structur data format and less of the human design and for example if you have a same curve in the the gri gri and actually designer can have a different ways to represent this curve and also are some common requirements of the grief synthesiz as I mentioned before so to okay to summarize uh there are two possible ways to address the problem solution Vector form synthesize the first one is to directly generate Vector grips and the other one is to synthesize gri images and then vectorize them so in this talk I will briefly introduce this for workor our reason reason work and using our latest method we can generate uh very high quality Vector grips and the first one is a de V form publ gra age 2022 and the method is as we can see here the meod is capable of directly synthesizing high quality Vector forms given a few reference Vector G as input our method can directly synthesize the whole Vector form in the same style and the keep ideas are threefold first we design the Dural modality uh learning strategy which utilize both image aspect and uh sequence aspect features of forms to synthesize Vector grips second we dou the techn techniques of the image synthesize uh sequence sequence modeling and differential characterization and to ex exhaustively expore expire uh dual modality information so we provide a new generative paradig to handle and structure the sample possible synthesiz result together optimal one which is fly refined under the guidance of the generated structure data and why it is complicated the problem is complicated so this is a illustration of our data structure so you can see the table this table show the drawing command of the uh commands of gri D and using this relative coordinates the left figures uh demonstrate the gri ships and the argument points of the drawing the Commons using the absolute absolute absolute coordinate so m r c and not move line curve commands respectively also the crosswire and anti cross wi other determine how to F the outline and this is the overview of our method our model is designed to fully exploit the Dual modality features which are first project into the latest star code and afterwards we synthesize the target target images and commands respectively the differential differentiable uh Riz look status is employe to align the synthesiz back to data with the images so here are some uh technical details okay we use the CM LM and we also use the dity Fusion us the conc concat and and and for connect for connection okay and we also you uh employee the k l to normalize the style lat space and we use the go mixture models for cinate prediction and we also also use the propos the neuro defensible uh rizer uh uh brief as ndr ndr because uh we know we introduce the mdn to represent the loss and so inly inevitably brings a local location location shift and M A generate Vector grips looks and coordinates so we use this we propos Le rer to M A delect alignments between the gri images uh uh so here are some example of the ndr outputs when we fit it with the synthesiz drawing commands during training and the whole loss function of our main model is formula as the sum of the imagely construction loss command Cass prediction loss and command coordinate prediction loss neuro resterilization like reconstruction loss and the KR loss for the lat space and during inference uh phase during stage and we proposed a new generation Paradise first gener gra images and the lat uh Goen distributions of the of distributions of the vector grips based on the reference gri and then generate uh candidate Vector grips to sample and next uh we use the dvg to dvg to fix the uh instruction types and qu and quantities of these Vector grips and adjusting the coordinate to align the candidate Vector grips with the grip images and finally select the vector grips with the best alignment as the final result so let's see the experimental results and given a few Vector grips as refence our model so our model can synthesize Cod Vector forms automatically and with the forms are synthesized by few s learning and more Vector forms can be generated by smooth interpretations in the sty lat space of our model and the interpretate interpretate uh interpret Vector forms are maxed by the re Das rectangle and we also we compare our methods with three reason recently uh propos approach and we can see that uh our meth significant aut performs are the existing approach and these are operation studies operas studies I verified our the effect our meas and we also compare our measures with some with with the uh with ad image Chas TR AIT and we sending our high resolution images synthesize images into the AIT and then we can see that uh Adobe image taser still can have a uh some problems like the always OS Moon the local details and also uh so many details are missing okay so but our me synthesize Vector grips of our meth can have a very shock details right but our method still can have some problems uh still have some limitations especially when handling grips that contain extrem s uh strokes and and also losing smoothness after refinement and refinement process uh is somehow time consuming also is uh they they still have some problems like over over smoothing and also uh the quality is uh heavily depended by the quality of the synthesized images so can we can we directly uh directly synthesize high quality Vector gri without refinement and here comes another work publishing 3pr 2023 and the motivations are two fold first the original Deep Vector form result refinement suffers from and second the original defect form with refinement has both over smoothness uh like here we Mark out by the green circle and the under smoothness see the blue circles and proposed deep learning deep Vector for v2 successively adjust these problems and here are three major contributions uh first we develop a transform Transformer based Genera model accomplished by a relaxtion representation of the Vector aliz uh and second we propose to sample uh agas points in addition to control points to precisely align the journal and the target outlines and finally we design a Content based self refinement modu and to fully utilize the content Contex information to further remove artifacts and here is a pipeline of our de Vector for v2 and the inputs reference grips inut uh restor images and the vector outlines so in the figure a subfigure eight and this is a d this is a dual Branch architecture based on Transformers and since I'm to synthesize the target vector gri and subfigure B so the so the self refinement modules which is designed to remove the artifacts in the initially synthesized Vector grips and the sub figure C in addition to control points ag ag agar agly points are sample to align the synthesized gri with the corresponding Target while the biz curve alignment loss and and so due to the time limit we did we don't want to go to more details okay and this is a how this uh relaxtion representation works and also we we need to sample some AG oxel oxel points in addition to the control points like here those how we we can sample those uh uh additional points and we also this is a context based self refinement modules and here we show how you can f remove the uh artif by sell refinement instead of the refinements additional refinement St using the VT and Etc and here's the L functions we used this is the Bas study results and verified verifying the effess of the propos the relaxtion representation the B curve alignment loss and the self refinement modu see okay we also compare our our methods with our approaches as we can see our method achieve uh achieve obviously advantages in generating high quality englage Vector forms with various styles and we can also apply our method to generate Chinese Vector forms and but for this work we we can already handle some very simple Chinese characters we just select like 100 100 or 20 100 or 200 Chinese characters with some simple relatively simple shape uh actually currently we cannot still cannot directly uh synthesize Chinese character with very complicated shap and qualitative results also quantitative results also prove the super superiority of the propose method and we can also also implement the style interpretations for both englage or Chinese fonts and also apply a method for font generation F out font generation and uh also can apply this font generation for uh Chinese Vector FS okay this is a demo of the generated uh Chinese factor FS see there are difference against the uh image synthesized by uh those trative models they actually they are writing sequence drawing sequence basically okay uh I will briefly introduce this work uh uh Vector form uh SDF also published in cvpr 2023 as we know inic Tri representation like SDF has been improved effectively effective in ship modeling and Analysis therefore we would like to see whether it is possible to reconstruct and synthesize high quality Vector forms using SDF the KE idea are twofold uh first we design a new imp uh implic shape representation which can be directly converted into a common uh commonly use Vector form for mats uh consisting of the basa curved and lines and second we use a True stdf Values as a strong separation separation instead of all little ler images and this is a pipeline of our method and here I saw some formulas regarding how to compare that thef uh SDF one second uh formulas regarding how to use the uh how to compute s SDF for the given gri that consist of better curves okay uh I think may I think we are very familiar with this formulas so I want I don't want to go to more details and so due to a complex uh gradient back propagation process so directly calculating the real uh sided uh distance from the sampling point to the uh parabolic uh curves using this formulas is inflexible when training our neuron networks so therefore we desire a uh we we try we design through the uh distant function to address this problem and we uh stdf stdf can be easily localized uh leize leiz into the gri image that is using this function and like this we can easily converted into a gri images and also to fully exploit the gri vector information we repeat the above calculation on all agre position and uniformly sample points uniformly sample points near the gri Contours to obtain the gri SDF and Contour Contour sdfs respectively and this is the gri SDF loss and the Contour SDF loss also we use a regularization loss which are assembled into a complete loss function and in order to synthesize uh Vector forms for practical uses and we need to convert the parameters of the hyperbolic curves generated by our stdf decoders into the B curves B curves so here shows how we can com assemble those elements into into this into a complete outlin consisting of the many consisting of many bz curves the basent studies results verifi effect effective solve the proposed SDF losses and the control SDF l so here are more operation study results and both quantitatively and uh qualitatively and here we saw the comparations of uh different methods for grief reconstruction so as we can see our method can recover more details more details and the qualties is also better and to further demonstrate the potential of uh V form SF for Vector formont generation we direct directly directly extend uh extend the popular F shot sty transfer text from the image domain to Vector domain and and see this is an end to end channable uh because the proposed uh Vector form stdf is defensible oh here some results of our method create out perform the defect form uh due to the time limit I will quickly go through our list work and list work is under still under review and the key idea key motivation of this work is that is that if we can generate high resolution for example 1K plus 1K gri images like this high resolution gri images so uh many well preserved local details can be still can be included in these images so if such kind of grave image can be synthesized then it is possible to generate high quality Vector forms which is in indis indistinguishable from those manually produced by a personal a professional phone designers like this one and okay the propos HF HF H form and to handle the test of the F shot F shot B trines phone synthesiz with higher resolution higher quality faster speed and higher resolution okay uh so taking a subset of the calcor in the desire style as input our method can output High Fidelity high resolution G images of the remaining characters which L can be vectorized into the high quality Vector forms and like this one okay the propose method can be used like f shot personalized font generation and also the artistic gri imagees synthesiz and uh with higher quality and faster speed and higher resolution compared to existing uh especially diffusion based model and this is overview of the method we have three stages and um all based on division models and we use the visual model generator gri High L first generator LW resolution uh but high fality G image and then use a uh diffusion model to generate high resolution uh gra images um using the St St uh St guide style guided I sty guided diffusion model and also we we we use teacher student models to to to let the uh one step sampling become possible to speed up speed model and we also integr uh integrate the prior prior knowledge of Chinese calculs into this model uh to enhance the quality and I want I don't want to go to more details okay here are some experiment experimental results and this is the setting of our experiments and here are some qualitative uh results comparison results obvious our measure performs much better compared to his approach and you can see that intuitively from these figures we we can our our methods can handle different types of the phone uh phone Styles and still uh show some very impressive results uh we can zoom in to to see this okay and also here more results and we can see that our proposed method was consider considerably well when only 10 reference G images are fit into the model and more we can see that our one step uh this models can still feul mimic the output of the multi-step teacher models so while the teacher model does not produce value result in the one step sampling but our student models can do that in the one step sampling and providing uh proving the effectivess of our design okay and this figure show that our model is able to extract and recover style details from high resolution style reference and compared to our general purpose image super resolution models and from this video we can see that uh very high quality uh Vector grips can be generated compared to the ground truths almost a similar number of of control points and the high quality of the grips and actually we also conduct conduct a lot of uh many user studies uh including us study for ordinary peoples and also uh Prof professional phont designers and so let's suggest that our generate this result suggest that our generated Vector forms are significantly better than those uh for the compared methods and are even compartible to designer create ground Tru Vector forms and we our me can also be applied for the test of artistic uh artistic G image synthesiz achieving a better performance compared to exist approaches okay so let's go to the discussion pack uh so all our we have we have been working on the pH synthesiz topic for more than 10 years but that still exist unol challenging problems for example how to handle a complex complex Vector grips for example TR this grips can we directly synthesize uh the vector grips um and and S achieving the style transport and style editing for Chinese VOR forms and also how to incorporate with a large language model to make the F generation test easier and and also more user friendly and so yeah that's all okay so finally I would like to thank those amazing graduate students I supervised in the last decade including and Li okay that's all uh that's all for my talk thank you and and any questions hello thanks Professor ran uh we are gonna running we are running out of time but okay sorry sorry yeah is there any audience asking for a question maybe only a single question no okay maybe uh uh we don't have a specific question but okay later yeah the audience yeah maybe you have any question maybe you can contact me vi the email okay okay thank again the speaker okay thank you thank you thank you so much byebye right thanks okay we'll move on to the next session uh hold on second okay uh so we are going to have three uh paper spotlights and uh first is uh s post layout and the presenter will be uh Tanaka okay he I think yeah zoom zoom is presenting your we can catch the sound here okay then good ahead so today I will explain our studis and realation of Sci posters SCI post maybe you having a speaker on this no yeah go ahead so creating a scient scientific poster that sumar this papering for automating this task using ml models is promising applications research onic post the high comple and man of the for scientific post generation is are not public available or the data license is unclear so go the gold stand B with realizing necessary so before explaining about our our this set I will briefly introduce existing lay to this setet is one of the welln this set for real analysis and generation it contains scientific paper images automatically aned with bounding boxes and polygonal segmentation across five categories text title L figure and table Lio is a real set for while application design but there's no real for post be C license so I will explain the method to construct our this set first we downloaded posters in PDF format from research then we kept 7,943 posters under C by license then the PD files were converted into being format at DPI is equal to 100 we excluded posters with file sizes below 200 KOB as the main consisted of text which was unable for real analysis finally 7,855 posters remained we found that most of the corrected posters were in the biomedical field then we annotated the layout of posters we recruited professional data annotators to manually annotate the docal of the posters we expand public five Cate annotation standards to nine categories to acquire fine grade annotations of VAR title info section text table figure caption and unknown title indicates paper or poster title post info indicates Po and the Au affiliation section indicates the section title text indicates the paragraph list indicates the stories table indicates the main body of table and the figure indicates the body of figure caption indicates the captions of table or figure and indicates advertising information or low of App application affiliation and here the examp example of the collected T post and annotations we can see find gr and accurate realation and using the corrected we conducted the experiments re analysis and re generation so first we explain about the re analysis we used re are Bly on the for analysis experiment we measure the performance of the models using the mean a PR the intersection VAR from 0.5 to 0.95 of powering boxes and here are the results the unknown category wased due to sufficient number of elements the table shows that re3 perform in all categories except for table and the result show that both models show high performances in the title and allo categories we attribute this the regularity of the title and allo blocks since they are always at the top of the poess and there is usually only one of each block per post however we found that compared to the result on pet and the pap set in which M2 I is over 90 in all categories both models show the performance throw so indicating the complexity of our dat set next we will about real generation experiment we conducted the generation experiment with various settings for the information to be input into the model generation condition on types gen aims to generate from the number of categories and generation types and sizes JS aims to generate re from the n and size of the categories generation condition relationship CHR aims to generate Ray from the number of categories and potion relationships between the categories and the completion means generating complete from a part of and and refinement means generating a new R from a rout that needs Improvement we used the r DM and for prop and the for the re generation model we used the four methodics to evaluate the model performances in the generation experiment maximum IU m is a measure of the highest I between generated R and the Real Alignment indicates how well the elements in the real are aligned with each other overlap is overlapping area between two arbitrary elements in the Inception distance FY measures how similar the distribution of the layout is to that overs so a higher Mi value means higher performance for the other three methodics a lower value means higher performance here are the results Miu was low for all models less than part in power net in contrast all models perform effectively on alignment so indicating that they can generate align layers and layer prompter was the most effective for overlap here indicating that it generat with the least overlap and L DM was the most effective in terms of FID indicating that it can generate layers most similar to the r on each setting and R PR at performs the other models in the refinement setting the bot one indicating that it can generate R similar to real RS from noisy layouts and here is a example of lay lay promp generated layout and Ray layout in the refinement setting it can generate similar routs to the re R then I will conclude the presentation we built a new T set called CYO player which consists of 7,855 s posters downloaded from the website all posters in C are manually annotated with categories of elements such as titles and F tyos is available commercial research because all the PO that are under the City by license for analysis experiment although some elements could be deed with high we found that R analysis on cost was more difficult than on paper foration experiment although existing models could generate aligned layers we found it toate R that are similar to real RS then our future work will investigate and end methods for generating post from scientific scientific papers that's all thank you okay maybe a short question from the audience any question okay go ahead uh about one or two months two months okay all right right thank I don't know because I just asked the professional company okay it's input HTM being for what three so you have given a paper how to how to inut ah to generate layers so so the input is a paper and the output poster H yes but this is a future this a fish FR I mean that in this study I just generate lay for from the given for example categories where something like T information but in the future I want to build the system to generate players from papers okay uh let's thank the speaker again okay then the next speaker be J okay you and Sh can you stop sharing the screen yeah sharing your screen okay working oh yeah okay yeah you can sir hello I'm I'm going to talk about our paper Des to cont generation cont generation isual orex content to indidual content generation mod like additional T content the output Genera layout can be converted to this fing the syntax or to generation this model can extend to test like onement generation following this our objective is a visual and textual content layout generation existing layout generation method highly Focus on visual content while resarch on texal content is still unexplored this method typical in compar two generation types General model and in compact running in general model there are two limitation data inefficiency and numeric optimization above all as you commonly know gener model require a large volume of data and however pain import layer of test PA is too expensive due to the copyright issue this make it challenging to apply layout generation to L applications next the Genera model address layout as a simple Vue so if we train from scratch without gener strugle to generate well aligned layout like this generated P about in cont in contact running method the defent in contact running method May visual information by b b or bment the bound B is to simple to repr present complex visual components we want to embed more fine visual features in our paper we tackle these three limitation for data inefficiency and numeric optimization we use the design knowled of the language model because the language model is trained on design test like you additionally language model can be extended to actual contact generation for for more efficiency we use we propose new orentation method utilizing the def for limited visual awareness we train visual adapter for theayer Generation with finally trained visual adapter we can acquire fine gr visual awar this is the overview of our method and our objective is to develop a language model based multimod are generation system in our texure we utilize the visual encoder and adapter and language model in first stage training we train the visual alignment and second stage we train language model on layer generation lastly the layer generation is converted to C sequence in first stage training we train the visual awareness we trained theor phing the visual in COD and langage model for training we constructed 20 Millions we create data that composed of the Ron and conceptual caption and LCB data set in second stage We additionally train the language model to lay generation task during training the network and image pair and output HTM form layout layout for training we utilize the ring cation to prevent castrop for getting additionally we deform the layout to HT sequence the inut sequence H the three components the Tas definition is about element contraint generation uh this allow for each modification by changing the test condition and the test contraint is about test content generation we can inject any test contraint in here lastly the HTM form is the test dependent input with the MK element this is the entire input out example this H for complete layout following the we can train the language model for visual texture content or layer generation additionally we propos no argumentation method to T the challenge of limited data for this argumentation we use the depth map to preserve s object in here we estimate the depth map and make a caption and inject them to the control deps for generating multiple images to reduce the old example we selected the case sample from generated images and we utilize the as similarity meure and the IM on the right demonstrate how our orentation technique can produce diverse IM with preing the S projects about experiment we use the two data C verion two and both and we evaluate on a measurement compos of the graphic and content measurement additionally we utilize the visual and cod training foration we select three sample from 10 generative samples we compared our method to three method D layer prompter and RM as you can see in this figure our model shows the high score on over meure additionally we assess the impact of our orentation method uh this table shows the Le identifying the positive effecta of our orentation method in unusual case RM shows ex actually high for in three in this three metric and this metric improve and generate layout on ground to location in here we can identify the information of the key because poster data that don't have clean background we paint the layer elements and as we can see in this P setting make in patch so we is exploting in Pat We addition experiment to identify this problem for this experiment we manually imp the and generate layout on imped Imes the result shows generate layout on imped area like object SE and from this experiment we demonstrate the reason for the performance of RM come from the Reliance on imp painting ARA not not generalizing to post image so far uh the for evaluation we use our orentation method this picture shows our orentation the impa and left we can show the impa we can see the impa on the right figure uh this is side effect of our augmentation and we evaluate on augmented data set for the evaluation as a result our model shows the state of the art for in nearly measurement and further our model also can do all type of element conditioner generation uh just by changing the test definition I introduced it this shows the result you can generate diverse element condition generation and thanks for watching hey question okay question from the audience okay go ahead uh so yeah she is asking what kind of architecture you use after Dino uh we just L adapter that uh adjusted in Rec modal langage models okay any other question okay then let's yeah move on to the next speaker thanks are you able to connect to zoom you need oh yeah e sorry everyone what let me oh see this they showing the box I don't this yeah maybe something is wrong um Zoom is miroring the home screen maybe we first need to connect this adapter to create a Target screen then then and then you know you can set up the zooms here I think or just miror everything yeah it's Swit I think yeah you can first stop uh screen share on zoom and then you set up your slid first I think Zoom will have to uh watch the uh choose the target screen if there is one or nobe well if it's not working yeah that's why yeah we get the other view okay okay yeah that's fine okay all right sorry about the delay there um okay so I am CIS wiggington with Adobe research um it's a pleasure to be here with you today um unfortunately our bir author uh sonit Beast was not able to join us today and uh this work was a collaboration with uh him he'll be uh uh and so let me just uh move through the introduction um so with document uh designs there's a variety of different types of layouts that we might encounter um some documents are visually Rich um some are uh denser in text and they're made to communicate different things some are supposed to be understood at a glance and some need deeper reading and so this motivated our work that we wanted to be able to um both process um the visually Rich documents but also text Heavy information dense documents in our work um so I talk uh briefly through some related work here um the version one of this work um was a pixel-based approach and as we heard in some of the uh prior talks today there's some discussion of what the best way to represent documents are whether that's pixels or um some kind of sequence of elements um other approaches uh include Flex DM which is an excellent work I believe uh co-authored by some of the organizers here today um um and in this work uh uh the uh model will do a variety of tasks um predicting uh Missing um aspects of the document such as missing layout or uh different styles um in the documents um we see layout diffusion and layout Gan models um these are state-of-the-art Works um they focus primarily on layout but maybe not other attributes that we care about such as text um or um other content in the documents and uh finally uh there's many related Works which I'm including here one other one is layoff transform which forsake of time I'm just going to continue um so to jump into our method um we build upon prior work um that outputs uh that represents documents um by a category or label such as title table um encode the position of the layout in XY um we also include additional information um into this represent representation including um the actual text contents of the documents and font information such as the font name um font size um other attributes of the font uh being italic or bold um uh to represent the font uh overall it's a fairly simplistic framework uh in this paper we used uh the gpt2 architecture uh fundamentally you you could use really any decoder only uh language model approach um and as you kind of see in the bottom section there uh we represent uh a table for example as first the table label and then it's XY coords and then it cells hierarchically nested inside um this is all flat into a single string um for the examp for the case of something like a paragraph This would be labeled paragraph also with its bounding box coordinates and then um the actual contents of the paragraph in terms of the text and any embedded font information that would be there um during training inference this is just kind of what you would classically back from a decoder only model um kind of the one interesting thing is we follow the approach of the layout Transformer and use the kill Divergence um we found that this um just particularly helped with some of the smaller data sets uh where overfitting and overconfidence uh might have been an issue um so jump right into our uh results here um we built an additional data set um we call this the uh Pub gen net this is uh a derivative of the publ net where um in publ net we had layout information but there's additional information embedded in the PDF which is not fully leveraged um this includes um the actual text content so the words that were in there uh Word level bounding boxes uh and all the font information so the fonts are embedded font size um bold italic that kind of information is often embedded in the PDF so uh basically we leverage the original PDFs um from the publ net um use their annotations and then were able to extract out um this extra information from the PDFs that we we found useful um and so we looked at uh two tasks uh in this work um the document completion task um so this is basically given some set of subset of the document we want to you know predict the rest of the document so uh maybe leaving out some of the elements and predicting the next uh elements filling out the rest of the documents and then just uh single multiple text box placement so if we wanted to say we have a design where is the best place to add a text box um that's the task and that was um similar experiment to what we see in flexium and the cor data set um okay so for some results um to start off we we see that uh our model is is not uh performing completely up to par to the multimodal flex DM um in this case it had uh for the single box um we don't have as strong of a multimodal input but I think uh the interesting thing here is given uh the Simplicity of the decoder architecture and the uh the input that we are given it's it works surprisingly well and then uh when there's multiple text boxes that gives the decoder model additional context uh and we get much closer to approaching those results and for the BD store um we do have um very good results so I think this is just showing the promise of the decoder approach uh for this task um additionally uh when we apply this then to completion this one we evaluated on our um Ginette and we compare it with layout Transformer and layout Transformer Plus+ uh we heard a little bit about uh some of these in earlier talks today um and also uh in some cases we are not performing state of the art but we're very close um it's interesting to see that uh our results uh without text versus with the text uh there's a significant Improvement and I think that just tells us that uh including the text in the generation provides the model with lots of clues that it's able um to do and leverage and then we we in fact do get um sa results um uh in some of the other metrics that we see here so uh here's one qualitative example right so if we were to leave out this table and we were to say we want to complete the rest of this um it would generate uh the table with with the textual content and everything um take of time I'm just going to move on um so I think the the key highway uh highlights that I would like to uh just emphasize in this this work is that we want to design just a simple autor regressive decod oron um document generator framework and I think um we see promising results with this um and I think this is a flexible framework that can work on both uh structured text Rich documents as well as um Creative Design documents um I think the you know as we think about what future work this will um apply to uh our approach we could see was kind of struggling in some of the cases where multimodal would help a lot so Vision language model I think we see a lot of uh really great state-of-the-art uh models that can do multimodality I think applying um these techniques to those would work and I think there's a greater need also for um evaluation techniques um metrics that Encompass kind of coherence readability relevance and visual appeal um to to these document properties um so uh thank you for your attention today um yeah thank you okay maybe let's take a question from the audience uh I can also take questions session yeah that's right anybody uh no maybe we should go to the poster session there great right thank you all right uh so the poster session will be in the arc exhibition 4 which is like a five minutes work from here uh so uh thanks again for attending this workshop and then let's enjoy the rest of the poster session okay

