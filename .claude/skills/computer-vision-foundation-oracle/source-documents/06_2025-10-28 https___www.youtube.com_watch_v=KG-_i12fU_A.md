---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=KG-_i12fU_A"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:01.633Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=KG-_i12fU_A

b341451c-bdc4-430d-833f-5a6d65d1d2e7

2025-10-28 https://www.youtube.com/watch?v=KG-_i12fU_A

f2090635-be68-45a7-b67e-90845fbffb6c

https://www.youtube.com/watch?v=KG-_i12fU_A

KG-_i12fU_A

## ComputerVisionFoundation Videos

can zoom hear me now yes we can hear you yes amazing thank you all right so yeah so the organizers okay there are like uh old organizer I think first is like Ador truls and mu also then me and joined like sometime later than our like Italian Branch uh fa faan look and this year also Emy joined us for the mostly like she contributed to challenge and so like to the workshop and we have our competition now sponsored by like academic institution unlike all these times before a company gave money now actually C in fra give money okay so here is plan so now I will talk a little bit more like about the image matching and like uh what there then we have this block of uh three invited talks uh then we'll have coffee break then we have a paper talks and uh um like most of them like should be doing life you would be able to do questions and answers and like we don't do posters and it's actually terrible like this way we other Workshop I did we do posters and it takes like 15 minutes to go there and then back and then P session is over um yeah so then then after paper talks we would have coffee break again uh and then we go to our challenge which is like um I don't know like important part of the workshop every year uh with the challenge talks which also they would be doing live but unfortunately most doing in Zoom because of like time in issues and I think we have one uh video recording and then we'll have like some closing session yeah so six years uh like I think this year we were a little bit more selective we have like uh much less accepted papers and it's not because we had a um like less submitted papers it just like it happens that we were like super selective like maybe that's good maybe that's bad uh we did igc paper about image like menine how to bench mark in 2020 because like there were like many many metrics and way how to do it and like it doesn't align align in the practice because for example K up is like Still The King and like all these like Nerfs gaus and splatting they need poses and like usually these poses come from C up but when you look at the other like um benchmarks usually okay like hel up is bad saf is bad like no they're not super bad actually like they're great probably something B metric so we published a benchmark and the data set like this many years ago and like since then we don't publish papers we trade on competitions kago but like this year we actually plan to do one more paper and do like one more data set hopefully like bigger to actually go a little bit back to like standard academic setting and uh you like I think all the live streams from the previous workshops are available if you're interested in something like you can go check and then for like competitions usually there like I do Recaps for every competition on my blog and then like there on KAG and like many many things so like it's and I also tell my students to like if they do image matching just check Workshop materials yeah and so what is this Workshop about uh like we like have like local features in and Beyond as a subtitle and the thing is like we want to do like rigid imagine we want to reconstruct rigid objects which like now we have like many like deformable things and tracking and every but actually we believe that sfm and like 3D Reconstruction from images is far from soloft like very very far uh what is solved is like current data sets which were actually created many years before that's why like they become easy but that the data set become easy doesn't mean that the problem become easy and there are also like new ways how you do this and okay we we show these slides every year so we were like lot things we are not consumed by Deep learning yeah but uh what is funny is that although there are more and more deep learning like or learned features uh like for example and super glue like amazing approach which like blow up like our leaderboard in 2020 and it's still used until now uh but this is only part of image matching and sfm solution and like what about the whole system well people use com up people use geometry people use these like classical things uh so maybe dorans like still haven't eat everything and maybe maybe like next year we say okay we cancel Workshop so hey CH GPT 3D all done we are out of business maybe I don't think it will happen um yeah so and still now they like now 3D is like very important thing in the current cvpr and like I really glad because okay diffusion is super cool I'm using this but like my heart is in 3D and like I really love the gaan platins and everything and they are built on the image matching but like besides we have all this topic like if you check social media and everything so we have Nerfs foundational models and like that's it like you don't hear about anything else but actually there are like more cool things and one of the like actually we have two recent cool things which use de loing for sfm part which is like uh super cool like finally we had like all these learned features may be matching but sfm is p up maybe not anymore we have duster which was uh which is invited talk and also challenge participant we also have V gsfm which is a one of our winners third place solution like really great work we have an a zero uh from NE antic which did not participate in challenge or Workshop but we hope that they do next year so and maybe next year would be fully consumed by deep learning solution and actually be able to go forward from the coma yeah and uh like and maybe even if we have all this cool stuff winning challenges another thing is production like Google Live View and like things they still use this for large scale and that is because it's so very efficient and uh what is funny thing about like classical Thing versus deploying so like V gsfm uh when the outer was first trying to run it for our challenge he could not because of out of memory error like he could train this on a100 but could not run it on T4 which is like standard machine on kagle competitions and uh while you can say Okay T4 is very old but it's also cheap and for like scale you need to be cheap and fast and this also like very long way to go yeah and we iterate on our challenge I would not go in and do this so this is like a little bit recap of those of you who didn't know so these are like standard academic challenge more or less we have website evaluation server like 13 team 20 team 100 and then in 2022 we move to kagle yes and since then we are are on CLE we are super happy about this in terms of like people participating and uh all the time like in all Co things actually people who were not working on 3D but working on deep learing and Engineering they were like among the winners actually proposing some maybe better solution than academics which I believe also exciting but on the other hand they were using solution proposed by academics and some academics were also among winners so this is like uh really how it should be and like this year we have and like not just sfm thing uh but we have a uh like hexatone so we have now six different categories which we would cover later but like they are like super different like transparent objects like repeats nature scenes because before we were like considering all like Photo tourism Urban like you know now boring stuff um yeah so and like we do the sfm one of the issues of kagle is actually hard to run sfm within the Jupiter notebook constraint on kagle like it's hard to compile it's hard to run and like we like acknowledge these difficulties but I think for at least for a while it's definitely worth it to go and have this much wider Community there and hexatone here is like uh some examples so we have seasonal changes we have illumination changes wide Viewpoint changes as usual like some transparent repeats and like what's more interesting is actually some of the scenes especially in test set combine all these uh maybe not all but many of these challenges together for example the scene is nature scene it also contains repeated structure such as very similar trees and it is from the winter and summer and Autumn and also it is at night like no no no method can Sol solve this which is exactly what we look for we like we're trying to to give a like Northstar to people to work on sfm like no it's like not getting this light latest like 99.999% accuracy you can actually uh improve things like by order of magnitude from 10% hopefully to I don't know 80 or something and what we hadn't change since here uh we still had this great PR price fund of $50,000 uh one more shout out to like our uh Italian part like I think that was like Fabio and luuka who bring this uh cultural preservation think especially that some of images in our data set depict objects which unfortunately do not exist anymore so maybe these images are like one little way how to preserve it maybe some like rebuild this although it would not be the same but I think it's also good to have like some like more cultural thing yeah uh kagle challenge sponsors Google as always and now our University gave up a lot of money so like thanks to them and yeah so I think I'd like to ask uh next speaker no so yeah thank so come here you would need to connect to zoom oh should I connect to zoom yes e e e e that's interesting and video as well please and video okay sorry does that look good so I think I'll get started do you think that's okay so this is so amazing it's really good to see all of you hope you're having a good day so far um and I'm going to talk about oh I don't know I'll talk about a new data set that we've made called Mega scenes and in general some of the challenges in 3D reconstruction at scale okay um but to start things off with I am a huge lover of architecture I guess I would say I love Cathedrals and I remember when I was a kid I saw this book called Cathedral by David MCCA it's this amazing book that I guess if you picture yourself as a person living oh I don't know a thousand years ago and you want to spend 100 years or more building a the cathedral this walks you through all the steps so it shows you like okay here's all the tools you need here's all the Craftsmen that are involved um it's beautifully Illustrated uh you know to start with you have to dig you have to dig some big holes in the ground you have to cut stones and lay them in for the foundation etc etc at a certain point you know you make some progress and you have to construct this huge wooden wheel and you have to like raise it up to the top of the cathedral so that you can uh raise blocks up to the top uh and it just it you know you start to realize that it's not just the shape of the cathedral everything has a function everything has a reason for why it is that way everything has a name too it's just it's a to me it's really amazing to see how this all works for real Builders that's really inst instilled an interest in in architecture and also an interested in like or or an obsession maybe with reconstructing things okay uh here's another book that is really I came across more recently called the story of buildings uh with these beautiful illustrations by an illustrator called Steven B and in this book you find illustrations like this this is just amazing to me this is the not Cathedral um it's why is it so cool well it's just this beautiful cutaway diagram exploded view diagram you can see the inside and the outside of the cathedral you can see like things split apart like the roof so you can see inside of it um you can see people for scale uh and you can see everything is labeled like everything is semantically Rich now this is the kind of reconstruction I would love to create with computer vision um and also have it like know the names of everything like it could Point yeah these names to me are very romantic like flying buttresses or vaults or these like Parts in the front that are the tyms and the arav volts you know it's just so rich and and reflects a real you know deep understanding of what this building is is and what it is for and why it exists and what parts are what the parts are for okay so I think my current interest in life is automatically reconstructing 3D models of all the world structures to the richness of that diagram that I just showed you um and ideally from open data right from all the photos on the web all the information on Wikipedia and so on and so forth you know information that's freely available online and you know we can do parts of that today we can generate shape so this is like a CO map style reconstruction of the notom cathedral from many many flicker images with your camera poses and your point Cloud um and but it's you know it's it seems like just a step towards that that problem so you know and we can do better than Point clouds too so I'm about to show you something that the author gave me permission to but I don't think think this is online yet I don't think this is even an archive but so um I used to show Nerf in the wild as an example of what's the state-of-the art of how well we can create photorealistic renderings of of data from you know from internet collections uh but here's this is a I think this is a gausian splatting based version of that Sono Pung showed this to me and said I could show it so I think this is okay I again I don't think it's online yet but this is just a I think this is like a gausian flatting based reconstruction of the Trevy Fountain from lots of Internet photos and it also allows you to control the lighting so in some ways we've come really far in some ways we still have a lot of a long way to go but I feel like maybe in the community and or at least parts of the community there's a feeling that 3D reconstruction is solved you can just run coal map and I don't know gou and splatting on data it work well all the time you know if it's solved you know you can kind of reconstruct the entire world from internet photos maybe we about very close to being done with this vision of you know that I let out of reconstructing every scene richly from internet photos uh so you know there's sort of a question you know people like me who like to reconstruct things is it time to retire what's the like what's left to solve I remembered my the my PhD thesis actually when I was defending my PhD someone asked okay well it looks like you solved the problem of like Photo tourism what do you do next after you reconstruct everything and I didn't have a good for that then but I think I have a good answer for it now okay so the first thing you realize actually is that okay so we've been embarking on this project of reconstructing everything and I'll talk more about that that LEDs to this Mega scenes data set that I'll tell you about but of course maybe it's it's just um obvious in retrospect that once you start Rec once you start running a tool at scale like hundreds of thousands of times you you realize all the flaws in it um like once you start running coal map at scale you see all the ways that it breaks and it turns out well I guess the biggest way it breaks is if you don't get a reconstruction at all because there's not enough visual connectivity but maybe the second biggest way that it breaks is repeated patterns and and symmetries and so if you okay you know maybe you go to London you see oh this is a beautiful cathedral St Paul's Cathedral I'd love to reconstruct it okay so you get a bunch of photos of it and you run Co map on it and it turns out you you get reconstruction like this okay so what what do you see here okay so there's a uh we have this Dome shaped point this we have 3D points in the shape of a dome that looks good there's a big dome here and then we have cameras okay reconstructed cameras in the shape of a dome that's good probably people are taking pictures from the dome but these are not in the same place right like you'd expect these to be collocated and what went wrong is is you know it's pretty clear from looking at this that this building has so many symmetries and repeated p patterns I don't even know the fold of symmetry of the dome but you know all the sides look alike that is just unable to disambiguate all those um all those ambiguities and create a good model and this is not just a one-off problem every building in London I think suffers from this problem and lots of buildings and lots of other places too this kind of visual ambiguity problem okay so uh we set out to solve this problem uh because it was really annoying and it really felt like this is a big hindrance to reconstructing everything in the world from internet photos and we're not the first to try to solve this problem but um this visual disambig disambiguation problem has been around for a while but I think it's an instance of a very interesting kind of fundamental philosophical computer vision problem of visual twins of things that look alike but are not alike it's almost like yeah it's very very philosophical so here's two images that are that look the same but aren't the same they're actually opposite sides of the arc Triumph and if you run your feature matcher you know whatever feature matcher you love it's going to find lots of high confidence matches and it's going to think that these are the same thing because all the structures on the top look look very very similar Etc okay so um like I said there's other things in the in life that look very similar to each other but are not the same thing so I remember this was you know many you know 12 years ago I guess I was thinking about this visual disambig sorry my mic seems to gone off I'll just try and hopefully you can hear me I was thinking about this visual disambiguation problem in structure motion and I came across this very interesting National Geographic article that um was about twins you know and the subtitle is alike but not alike and it made the point that I hadn't really realized before but you know nominally identical twins are not actually identical for many reasons like in this case like maybe probably the pattern of freckles developed independently it's all like stochastic so that's different um but you know over time especially as you age as identical twins age I think they start kind of diverging a bit from each other and you can you know the nose might be slightly different they start to develop these differences that make them not identical and it's kind of the same with like images of architecture too like oh may you know two sides of the building will have different like smudges of dirt on them right so we should be able to tell apart things that are alike that look alike but aren't alike anyway that's just a problem that I've been intrigued by for a while uh and so one name for this you know sometimes you have twins that are identical sometimes you just have Dopp gangers you know someone who looks like you maybe someday you'll run across your doppelganger uh just who happens to be identical to you and it's very fun in these articles but um it's not so fun when you run structure motion and they cause you to completely get the Reconstruction wrong okay so for this fundamental Vision task of same or similar it's not just philosophically interesting but it's the difference between you know these doppelgangers or evil twins are the difference between a good reconstruction and a poor reconstruction so okay now how do you tell them apart well if you're a person you might say okay I'm just G to stack them on top of each other and blink between them and now it's like a Spot the Difference game and indeed if you do this you can see H actually they still look kind of similar but the if you kind of look carefully you'll see the um you know the statues here these sculptures are different the buildings in the background look similar but they're actually not the same they're different you can tell them apart if you look carefully enough um and yeah probably like each of these individual bricks one of these images is pretty saturated but you could probably like inspect like bricks and see oh this brick is different from this brick they must be different structures and it's very interesting it's like the opposite of what we usually think about in feature matching or image matching like in image matching we're looking for like positive evidence of okay these images are matching each other and you usually do that in the form of finding lots of sip features or lter features or D whatever features that look similar uh but here you're trying to do the opposite you're looking for evidence you know um evidence that they're not the same it's like negative evidence of them being matching and that's something we don't do think about as much but I think it's very important okay so okay we did the classic deep learning thing uh is okay first we have to build a data set and then we'll try to classier once we have that data set but it was very difficult to build a data set of these doppelganger pairs these full these Furious matches because even a person might have trouble telling oh is this the same side of the building or a different side of the building so uh we relied on I guess the wisdom of the crowds to do this for us um there's a website called Wikipedia comment this is a sister site to Wikipedia it hosts all the images that appear on Wikipedia and a lot more images besides um and it's a very cool website that again is like underappreciated in computer vision I think unless secretly all the companies are like using all this data um and what it has is categories of things uh so of course I guess like Wikipedia this is all crowdsourced so people who love to categorize things I don't know maybe Librarians who knows you they have curated Wikipedia Commons to be a semi-organized resource of visual content so there's a category in Wikipedia commment called Cathedrals and under that category is every Cathedral known to Wikipedia um you know for for instance there's one for the Noam Cathedral uh and you know you could get lost easily looking into this data it's so rich um you can find a subcategory that's historical photos of of the not Cathedral you can find categories that I don't know um archaeology of the noon Cathedral views from the noton cathedral um and you can even find things like uh a category of the organ and a category of sub parts of the organ like the shamad which are the pipe organ pipes that point out horizontally it's just it's I don't know it's it's very lovingly crafted web page I love it a lot um anyway among the things that people categorize on Wikipedia Commons is Viewpoint so you might find in certain categories like the arct Triumph a categorization of you know into which angle they viewed that from so someone carefully determined which angle these images were viewed from and categorized them accordingly and if you see a subcategory that's named back right views of so and so and another category that's front right views of so and so then you can reason about okay those should not overlap each other they should be visually disjoint sets of images um and so if you then match images across those categories and find lots of geometrically consistent feature matches then you know aha this seems like a doppelganger I'm going to put it in My Doppelganger collection uh and and you can run that on lots and lots of Collections and find many doppelgangers that way now you might say okay well this is kind of silly why did you know street view and things like that are categorized by viewpoint but I really I'm you know there's a part of me that wants to do everything using open data uh and if we can do it with internet data then I'm very intrigued by that okay so we Ed this idea of like Viewpoint categorized categories to build a data set of 76,000 images with 1 million image Pairs and 178,000 Dopp gangers um and here's some example Dopp gangers that show you different places that where like the front and back are kind of similar but distinct um you can tell like oh this can't be the same Viewpoint of the Alexander n Cathedral because this Dome is in the wrong place uh even Walt Disney Corporation you know maybe they're very obsessed with standardization they built the same castle in two different Disneylands one is in Japan and one is in Florida so yeah you get like matches between those that are clearly you know these are thousands of miles apart they can't be the same structure and you could probably tell them AP by looking in the background like one has a mountain one has I think a nice Florida sky in it um okay so you can find lots of toic kers and this is great um you can then turn a classifier to determine whether a pair is a doppelganger or a good pair um you know there's some thinking that goes into doing this actually I'm very curious I think we tried using chpt to solve this problem it's surprisingly good but I think not as good as our as our classifier anyway just a bitter lesson is that like chbt can solve a lot of the problems we used to think are hard okay anyway uh so what we did is not that but we give a classifier the image pair after first having matched them so we provide it with feature matches you know here's the location of features here's the location of matches so that I can reason about things like oh I see there's a lot of matches in the top of the building but not the bottom what's up with that right and can make determinations uh accordingly we also align the images with the aine transformation using the feature matches so it gets to like easily compare you know kind of flipping between the two images with corresponding points to looks the same or not and then you feed that into a classifier and you know output yes or no this is a good match or this is a true pair or a bad pair okay so let's see so we evaluated this of course we kept some secret landmarks that the classifier never saw during training so we get a good test set and here's the performance of just how well can it tell these images look the same versus the IM these images are are the same okay and so we compared it to some baselines like the obvious thing you can do is just count the number of matches you get and threshold the number of matches like if you have more than 100 sift feature matches or whatever the right thing to use is these days you get you say it's correct if it's less than 100 you say it's incorrect um and so uh those baselines seemingly perform pretty well our method our train classifier is this blue line here uh it gets about 95% and you can say oh that's not a big Improvement why are you so excited well okay the problem is here's how you're going to use it you're going to take this classifier you're going to take your graph of image matches that you got from Co map or whatever structure motion system you're using and you're going to classify each Edge as good or bad and you're going to try and prune that graph of bad edges and you're going to reconstruct with left and even a single bad Edge could screw up your whole model right so you really want to be very good at detecting um bad matches and so um you know 95% is much better it turns out um for this task uh you can see some examples of correctly classified pairs here on the left we have negative pairs so these are doppelgangers and you can see that okay maybe something interesting is that it's very confident that this is a doppelganger it gives a low score that means it's a doppelganger it's very confident that this pair is a true match and that's correct it is and it's able to do that but despite these images may be seemingly looking much more similar than these two images right in terms of alumination and other qualities but of course it's been trained on diverse enough data that it does it knows the difference between illumination changes and seen geometry changes now I say it knows that I just assume it knows that because it's able to do this task I I don't actually know what the how what cues it's using to do this maybe there's some secret like watermark on all doers who knows or maybe uh like or maybe it's using Sun Direction you know there's many possible cues it could be using okay now uh so then you can use this as I said to prune the graph of all bad matches or all matches the classifier deems is bad and then run structure motion on what's left um and you get some uh pretty okay yeah run filter the match graph run structure motion and so here's some before and after comparison uh here's the arc to Triumph you might say this looks like a fine construction any problem but what it's done is only reconstruct this is vanilla coal map it's only reconstructed one half of the building whereas there's plenty of images to go around on all sides of the building so it's just you know seriously glued both sides onto each other whereas after running doppelgangers um it works much yeah you you get a full 3 360 reconstruction as expected um here's another this is a uh this is that Alexander nki cathedral which vanilla coap produces I think one too many domes because it's getting all confused and discombobulated um our reconstruction corrects that and builds a a correct model with the correct number of Domes and images on all sides and here's some other examples so I think the like what do people say like the inness challenge or like I don't know the image net the image net of 3D construction is Big Ben like the canonical bad example Big Ben is something which has four-way symmetry it's all it's very confusing to reconstruct um and it's it's not obvious this is broken but all these models that are from vanilla Co map are broken in some way or another um and if you run our disambiguation first you get much better results again I think if you were to remove the cameras you'd see a like correctly four-sided Big Ben uh Bell Tower here actually I think Big Ben's something name of the Bell but people call the tower that anyway okay any sorry random um now I should say I'm very proud of this work but I was demitro I think G gave me some information that like for the image matching challenge it turned out that like this doppers didn't work that well I can believe that so we've also done some tests that suggest that it works better on internet photos weirdly it well maybe not so weirdly but it seems to work better on internet photo collections like the ones I'm showing you as test images test cases then on your run-of-the-mill images captured by the same photographer I don't know exactly what is in the image Channel what the distribution of images is but I think it's biased by what we've trained on to kind of these Landmark internet photo collections it hasn't for instance seen a lot of Office Buildings I think so actually we've trained we're training something newer um that I think might might work even better than this like initial version okay I thought I'd give that caveat thank you for the like feedback okay um good well now at least we have something that works on internet photos more robustly than vanilla cold map and so now we can said about this project of you know running on everything uh build 3D models of everything that Wikipedia knows about and so that's what we did we oh I don't know what's the right way of saying this we have this ancient cluster at Cornell by this Cornell work from Cornell students led by Joseph Tong and Gan Cho um we have this ancient cluster it has hardly any gpus the gpus it has are like you know five generations old but has tons of CPUs and that's perfect for like if you don't care about Co map running as fast as possible it's we have many many no one uses this cluster that's the beauty of it like all the gpus that we have that are new are kind of constantly being used by everybody like totally oversubscribed we have this nice cluster of CPUs that we've been running you know secretly uh coal map on for many many months and we reconstructed this um data set called Mega scenes which is over 100,000 structure reconstructions from around the world consisting over two million open licensed images and many many sorry registered images these are only images that got reconstructed there's many many more images that are not reconstructed now I think another thing demitro might R me about is well it's 100K that's not Mega you need a million to be Mega but okay that was like true of our Mead dep paper too but there's the problem is there's no like good Greek if anybody knows a good Greek prefix for like 100,000 please tell me because I can't find one I did a lot of searching but I can't find the like canonical prefix for 100,000 heilo what's that hecto kilo hecto kilo is that right okay so we kind of rounded up and we're calling it Mega okay and it's just you can yeah yeah okay the number of images is me okay so maybe it's poetic license maybe it's okay anyway it's certainly orders of magnitude larger than you know kind of the previous data set that's kind of like this is called Mega um much bigger than that uh okay uh yeah order mag to large than Mega sorry my screen is not totally in sync so I'll have to keep looking at this screen so you know an inspiration for this is there's a group from Google who did a very similar thing but in TWD no 3D so there's this data set from a few years ago called Google landmarks data set V2 this is a data set for landmark recognition and retrieval so it's like recognize if you're looking at the Eiffel Tower versus oh I don't know the L um or one of hundreds of thousands of other categories okay so just 2D instance retrieval um we used the same uh source of data as they did um and we used data from Wikipedia Wikipedia Foundation just exactly what I was telling you earlier with a few different pieces thrown in so um if you want to reconstruct everything Wikipedia knows about first you need to like a list of those things and the way to get a list of those things is to use a sister project called Wiki data which what is Wiki data so it is a kind of Knowledge Graph like Google has a Knowledge Graph there's these things called Knowledge Graph which kind of are encyclopedias of everything with connections between them so this Wiki data is a big database which has for instance a category called tourist attraction and you're then able to do an API query that will enumerate all of the tourist attractions uh that Wiki data knows about everything from the Acropolis uh and the entor zoo to whatever landmarks start with z i don't I can't think of any off hand sorry but there's a lot of them um and then for each of these categories of here's a place you can then go to wikip Wikipedia comment like I told you about earlier which has all these images indexed by subcategories uh like Berlin Cathedral and all the subcategories underneath it you can download all the images underneath that root category and then you can run them through structure motion with doppelganger step thrown in for uh models that are ambiguous and you get some nice structure motion Point clouds and registered images you run this enough times and you you get 100,000 uh 3D models um here's a visualization of some of them and by the way this I think this just came live yesterday and the paper will be an archive today I think uh but you can the data is now open you know the website is live you can start downloading data and using it um here's just some visualizations of some of the um uh models in the data set again it's not just your like noted on cathedrals of the world it has a very heavy tale of lots of different places that um you might be unfamiliar with as well as every as well as the famous ones too um so uh I guess if you were wanted to be critical you could say okay to a first approximation this is a data set of religious buildings because that's what the most common thing on Wikipedia Comins but it has a lot of other stuff too it has and uh I don't know tombs uh lious Etc this is just a count of the number of scenes by the category you know the wiki data category um and also to a first approximation it's a data set of European places uh this is a heat map over the world of uh where these landmarks come from because they're all geotag that's another thing Wikipedia knows about um you know so it's heavily biased uh G geographically but it does have you know at least worldwide coverage to some extent okay so yeah I I don't know I encourage you I'm going to show a live demo actually if I can so okay we had our undergrad research assistant Brandon build a nice web viewer for this so it's like a CO map viewer on the web that you can explore this data set so I've pre-loaded a few uh okay so you can type you can start typing in this box and we'll autocomplete the name of different landmarks uh this is like that famous Sphinx in Egypt this is one of the first things I reconstructed in my life actually when I was a grad student it still works that's good uh what is this oh it's uh okay here's some cameras okay this another Sphinx okay it knows about other sphin in the world like there's one in the Lou uh okay that's nice so has museums okay Sur I don't even remember what I uh oh this is that Capel br this famous bridge in lucern uh so you know outdoor scenes uh I pre-loaded this is the pantheon but not the one in Rome this is one in Paris uh it has indoor scenes as you'll see in a moment when uh I think the point claws are a little lagging the cameras maybe because there's so many points okay anyway you see where the cameras look like in the inside of the pantheon in Paris uh okay I'll move oh there they are there those points yeah yeah know there's nothing more than I'd love than a good point Cloud uh even if it's kind of sparse I I just love it anyway uh uh you know I said there's a long tale so I don't know this random address in New York City 11 West 54th Street must be like one of those buildings on a historic registry uh there's a point Cloud for that too with cameras registered in fact cameras that you can see are taken on the opp side of the street and things anyway uh yeah you can just go here A Space Needle I'll just skip that you all know what that looks like um yeah you can start typing I don't know does anyone have a landmark they want to see anything come to mind oh do you have a suggestion what's that okay CN Tower okay yeah oh so yeah you'll see it like autocompletes for you uh there's a lot of uh you know interesting uh landmarks here uh CN tower now I can't I haven't like inspected all of the landmarks so I don't I can't vouch for all the qu quality but we'll see uh okay it should load right Tred yet there you go oh and yeah you know of course like I said the other problem with Co map is that it uh okay so this is okay the points should appear in a second uh I think you're looking at like Toronto me this is probably the top of the CN tower views from the CN tower is that right does that make sense okay yeah so there's other components here um I don't know a lot of them are kind of small like this say only 39 cameras so this might be kind of unrecognizable some other oh yeah May another view of Toronto from some other prot I don't know anyway what's that oh maybe this the island looking towards the CN okay so yeah I mean like like I was saying with coal map you never know where you're going to get you often get models broken into sub pieces like the indoor the inside of a scene and the outside of a scene there's no visual connectivity so they won't register together um actually that's something I'm I'd love to work on more is like how can we reconstruct like full reconstructions even if like we don't have visual connectivity between all the parts there's a nice paper called 3D jigsaw puzzle I think Ricardo Martin brola I I see Rick in the audience maybe he was a coor sorry I can't remember all the authors off hand but that explores that problem okay thank you uh good okay so that's just a a taste you can play with it yourself now I have to say many of the landmarks are super small like two cameras so not all of them will be as beautiful as that CN Tower but what we lack in all of the landmarks being amazing we kind of make up for in we just have lots of posed cameras for very diverse places in the world okay good um so you know uh I guess maybe the implicit problem behind all this is you know just if you're enumerating open problems is you know what do we do with all this 3D data um I think you can do all the things that you can do with data sets like Mead depth or any or photo tourism data sets uh you can maybe learn better feature matching because of the scale of data maybe so we don't have dense depth maps without something we're working on adding to the data set uh monocular maybe you could learn great monocular depth prediction if if it's not already solved um you know this kind of data especially if we have dense depth maps you could you know train a better duster I would imagine with this or any kind of geometric 3D reasoning task and and just I think you know it'd be great to see all the ways people can find uses for this data I like this data set a lot and hope it can get some usage um I guess I'll talk about one particular application that we described in the paper which is view synthesis sorry and it I I talked about this talk has heavy overlap with a talk I gave earlier at this Ari for cc um uh uh workshop and it we also have a lot of overlap with the talk will give it a UMD 3D Workshop in a few minutes so you know you can just go to one of them anyway so um I'll just quickly because I have to end it two is that right okay um you know what I'm just going to I'm actually going to um because I think I started a little late I'm going to um I'm I'm just going to say we can train zero one two3 you know you all know 0 one to3 it's like you can train view synthesis using posed images right so you give it an image you want it to render it for a new pose um you condition a diffusion model on that new pose and it it can do this task very well after fine-tuning stable diffusion um it doesn't work very well on scenes because it's been trained on object data from obverse but if we fine-tune it on this magazines data it works much better um I think I'll leave it at that and just you can if you want the details I'll talk um you know you can come get me later or read the paper so I'm just going to skip I'm very sorry I'm just going to skip through this whole section I think I'll just show some pretty videos um you'll have to bear with me for like minutes questions yeah I have seven minutes I know I just want to leave yeah thank you thank you uh you know it seems like my computer's Frozen though so I'm gonna try and I might spend seven minutes like debugging who knows oh there we go okay you know so I'll just show some results um if you take kind of a a method that's not been trained on scenes there kind of a a variant of Z 23 that's been trained on more data called zvs um you don't get so the idea is to render a trajectory shown on the upper right corner starting from a single reference view so so single view to novel view um things that haven't been trained on this kind of data on scene level data don't work very well but if you fine-tune these view synthesis models on Mega scenes because after all one of the other things that this data set is is there a massive data set of posed imagery you can train view synthesis models very well with this data um and if you further condition on this warped image conditioning them showing up on the right corner you know um it gets even better I'm going to just skip over the details so there's some time for questions okay now I just want to talk about some open challenges so one open challenge is as I said the ambition here is not to reconstruct just geometry soup or Point soup I think the ultimate ambition is to reconstruct things that have the depth of information as this illustration right things that know about the names of things things that know the function of things things that could simulate what people do inside of this building I think 3D reconstruction is very far from that goal but I do think that I'm sorry my slide is partly included by the zoom I do think that like oh it's clear the way the field is going people are training larger and larger models and models that are capable of more and more things um I think you know a very obvious idea is to try and train some kind of model that combines Vision language and 3D all in one by by taking you know the fact that meines exists in this Rich context of knowing everything that Wikipedia knows about um I think there's an opportunity to kind of you know structural promotion has been kind of separate from all the other all the work on large language models and so on and so forth but I think maybe there's a way to integrate all these things together maybe it's time to do that uh just to show yeah one nice thing about data on Mega on on Wikipedia Commons is that it has captions and and language so in addition to reconstructing 3D you have this semantic information like okay who knows who knew this thing was called a tricks here you know Wikipedia did okay um so you know eventually I don't know how long this will take but I feel like like it just it seems to me the right thing to do as a community for people who care about structure motion is to rethink the whole Co map Pipeline and we're starting to see that with things like a zero and flow cam flow map you know work that are rethinking Co map but I I think we're not quite thinking big enough because what I really want is you know there's so much data Beyond image data there's texts there's Maps there's floor plans there's all sorts of Rich multimodal data that we can use for reconstruction if you feed Co map a floor plan it will just discarded because there's nothing you can do with it right there it doesn't fit in the like classic feature magic Paradigm despite a floor map being kind of or floor plan being you know if anything the most useful image for understanding the structure of we building okay so you know how to fuse all these like different threads in the computer vision Community together is a big open problem um another big open problem is reconstructing 40 scenes 4D not 40 4D four dimensional uh so you know Cathedral is a good example you know there's a tragic fire there in 2019 um we need to be able to like reconstruct things over time uh we have some work on this called neural synchronology that I'm just going to skip over uh because we're almost out of time so there's lots of progress on 3D reconstruction happening driven a lot of it is actually driven by better image matching kinds of Technology but I think what we're missing you know some of the big open challenges are complete models like the inside and outside of a building together semantic basically aware models that know that something is a flying buttress and know what you can how you can interact with scenes uh you know sort of what the next thing Beyond Co map is um how to transcend that Paradigm and we're seeing work like that already um and Beyond like just geometry but like fully Graphics writing models that can be relit and interacted with like I can go up and light a candle inside the cathedral um you know 4D reconstructions both over like scales of years but also like I've I you know I've reconstructed the Troy Fountain many times in my life but I've never reconstructed a version with people Milling around and water flowing there's lots of kind of dynamism that's lost in 3D reconstructions uh and I think generative 3D scenes is also a clear uh you know open Direction you know scene level generating a whole generating a million Cathedrals like the notam cathedral and as detailed as that but are new and imagined so uh thanks to all the people who led this work and collaborate on this work and thank you for your attention do you have a question yes yeah why POS negative so if you have plent of imri to drops s wouldn't kill you but as you said one in corre link they put two sides on on top of each other so I would expect this be like said differently oh yeah so are you saying like the Precision recall tradeoff like you want oh I don't like this like okay as a user you us want standard and about scene with that's true I I think that's totally fair and we should yeah I think we haven't you know kind of properly calibrated this model perfectly but I think there's another to me I think maybe even bigger kind of win would be you know if you classify everything independently it's like as if you think every Edge is independent of every other Edge which clearly there's a lot of correlations like there's a group of edges that are very similar to each other and very and all bad and all bad the same way so I think you know maybe an even better way would be to like find these like structures in the graph so maybe you did a lot of hard work and the point is that maybe with a little bit of extra you can get a lot of more music out of it music I love that yes I love music and besides now9 yeah good point yes any other questions thank you so much and next question please come here and also ask to mik sorry I didn't repeat the question I'm really sorry about that yeah yeah thank you again thank you very much everybody this is very thank you for can excent and share like cool yeah I think you it how do I okay it's good it's good yeah sure no no no no it's good here but not good on The Zo they see both maybe you can put maybe you can put uh PowerPoint into reading mode as opposed to this presentation mode which is almost the same and you will have just one window that click on this on this one yeah yeah this one looks almost the same and now on Zoom it's correct because yeah good good is that good yes yes this is perfect okay okay thanks thanks for the invitation to come to the workshop and today I'm talking about how to do a slam inside the human body uh I suppose if you know about Isam I suppose most of you know about orlam so that was made in our lab so essentially Islam is taking a set of features solving a structure for motion but in real time and then for every new image you match features you locate your camera you add feature to your map for a key frame and you are able to recover the trajectory of the camera I would say easily okay in most situations okay this can work a building size no problem inside uh inside a building outside a building so what's the next Frontier what about inside the human body Okay so so what are the difficulties there why does Islam work so well nowadays mainly because of the structur formotion that needs essentially to solve that Association that comes in several in several colors image to image that Association map to image that Association and place recognition and all of them are based on the assumption that the world is rigid all also most of the time we have a stereo or we have inertial sensor that help a lot to make system the system more robust and more accurate our view I'm presenting result from the endom mapper European project you can see my t-shirt um is to try to transform any endoscope in a smart 3D device to solve mapping to solve localization inside the human body to help navigating the doctors to be able to perform measurements to be able to study the coverage of an exploration to build peroperative models or to do medical robotics that's our goal what's the difficulties okay man-made environments are easy for Islam nowadays they were not easy 10 20 years ago when I started but now they are easy you see the the first video you have essentially aesthetic scene you have good texture lighting is more or less constant okay and look below the the video below you have a medical image where the scene can be deforming so ransack gone okay will not work anymore or almost not work you have ugly texture very texture and you have light changes all the time okay because you have an endoscope the light is attached to the tip of the endoscope you move you approach the surface is more light you go further away it's darker all the time you have light changes that are not only Global but also local okay so in summary the main challenges I I want to talk about today are the formations of the scene po texture occlusion dirt Etc how to address Place recognition what to do with illumination changes and what about monocular endoscopes and and how can we go behind farther away than using just monocular images so what's the problem with the formation okay we know in a structure for motion if we have end points and M cameras so if the world is rigid we have a number of observation that is much higher than the number of unknowns so we do run sack remove loud lers everybody works and everybody's happy what if the environment is not rigid the problem is that every Point gives to observations but every point for any state for any instant has three unknowns gone runck does not work anymore you cannot reject appliers so you are crapped so you need to have some kind of deformation priors or shape prior so it's a problem that this has infinite solutions so we need to add some kind of regularizers to be able just to find an A solution that makes some sense Okay so this is a paper that is just accepted so it will appear in a few in a few weeks in on i e transactional robotics H that addresses this problem how to do non-rigid slam is already available on archive so this is an example of a real colon sequence and this is how our system is able to work in this kind of uh things okay first problem we have many illumination changes if you look at the green square it's the same position and in two different images the light is very different okay so if you want to use um photometric tracking you need to take into account that the light of each feature may change independently of the others in this case it's the main reason here is probably viting in the camera so the camera moved a little and then well you can just add two parameters for each for each feature you has to estimate a gain and a bias for the feature on top of its Optical flow okay so this can be done and with that you can track features resonably well but you need to do this composition differently for each feature okay then what about priors previous work uh proposed to use priors in the form of a global mesh in the environment Global meses tend to have a planner topology and they assume that the surfaces are continuous and that's not really true in many medical images you see in the in the right part is an image of a colonoscopy a real human colonoscopy and then the the environment is not flat the topology is not planner and the surface is not continuous you have some holes some parts that are not being observed in the in the image so for that we propose to use a representation using just feature points plus a graph that represents which things move together so the idea here is that close points should the for together unless they don't okay so you do a a previous hypothesis you have an image then you build a graph joining the features you have in in your image and you start tracking these features then in this graph we has decided to remove edges that went stretching too much so essentially if the elongation is more than 100% bigger than the first distance yes we just cut this this Edge okay and second um mathematically we use we represent the formation as a what do we call a vastic model it means with an elastical term like in a spring okay these are the equations of the spring in the left part plus a viscus regularizer that is like a damper okay so to uh perform the formable tracking you have the standard data term that represents the repr projection error and the second term that represent the regularizer introduced by our vastic deformation model that has two terms the elastic one and the Visos one okay so the idea is to estimate the pose of the cameras and the ations of every point in its time step in in it time stamp minimizing this error that accumulates the reprojection error plus the deformation regularizer and this is the result this is an example with a lever with two parts and you can see here that we are able to track the motion of the camera and also the deformation of environment here in the right part you can see how the environment they form in this case one part of the liver with respect to the other the nice part of this Dynamic deformation graph is that even though we started with a whole graph then we were able to cut the parts that were not really tied together that was for tracking then we need to do mapping to do mapping we can do the same thing kind of a bment but adding a regularizer term that is the term that comes from the deformation of course the hesan of the bandas becomes more populated because you have many terms related relating each points with his neighbor okay that's these are the green parts that are the VIS regularizer and the last regularizer are this orange Parts okay so the pointto point MCH is not diagonal anymore so the problem is a little more complex that bandas men but is still doable this is an example with a simulated sequence a colonoscopy a simulated colonoscopy so you can see how we were able to estimate the trajectory of the endoscope and at the same time the small deformations of the environment and this is a true colonoscopy with a real person okay and more or less we are still able to track to build a map and and perform tracking and mapping uh in this kind of of environments here the color in the graph represent how much each part of the scene has been deformed red means no deformation while the yellow and and and green means uh bigger deformations okay that was about the formations when we started this project we thought that the formation was our worst enemy and then we found out well at least in colonoscopies the formations are not too bad because I don't know if you know when they do a colonoscopy they inflate everything with CO2 and then once it's inflated it stays more or less in the same size okay once it's inflated so our worst enemy is not de formation our worst enemy is occlusions water death I mean if you do a slam in this room you never hit the walls if you do slam inside a colon you always hit the walls always all the time okay so it's almost impossible to build a map of the whole colon you have the slam system working for one second two seconds 3 seconds that's it and then map the your tracking is lost what do you do start again the operation no you just build a map of 3 seconds stop this map and start a new one okay so to be able to address this problem you need a multim mapping a multi map Lam system but we already have one it's called orlam 3 right okay so it's easy let's go let's try to do that well it the truth is that our features are not that nice inside the human body they is very easy to track to lose the track of or features but miraculously the good old fashion SE features work much better inside the colon or inside the human body so we took this orlam 3 replace orb with C features with a a GPU implementation to make it faster and it works reasonably good we have to tune a lot of things of course if you have done some slam you know that for any application you need to tune up a lot of things that's true but well at the end in a simulated colonoscopy in the image you can see on the right I mean in an easy one we were able to do a map of the whole column say in only 11 different Maps okay with a coverage of 93% of the images that means that 7% of the images were crap because the camera were hit was hitting the the wall what about a real colonoscopy well this is uh our system working in the seum area so the that's the end of the column or the start depending on which side you are looking from okay and then as you can see we can build a small Maps sometimes we can we are able to merge several Maps together okay and at the end we get in this case a coverage of 38% of the images so 38% of the images were used for our map the rest were crap and we were able to build a map consisting in in this case kind of 96 sub Maps Okay only 96 sub maps in a whole column okay okay then what can we do with a map like that now 96 parts that are not connected between them because there was not enough matches between each part to be able to put them together okay let's move to the next problem Place recognition suppose I want to be able to solve the localization problem inside the column let me say what what for okay a practical use is for example the doctor does an does an exploration to try to find cancer in the colon and then he finds a poly okay he finds a polyp and he was about to remove it okay wait H are you sure the patient did not take any anti-coagulant and the anesthesist said no he's still taking coagulant you cannot remove the poly today okay okay so we will not remove the PO today we need to do a second operation in two weeks to be able to remove this polyp that now we are seeing will we able to see this polyp in two weeks maybe yes or maybe not that's the problem so that's why you need Place recognition inside the colum you want to come two weeks later to the same patient and to be able to tell go 3 mm or go a little further and here must be your PO okay so for that in reality you only need a topological map or a metric topological map if you are able if we are able to build a topological map of the colum it's enough to say go further or go backwards to find a certain place you saw two weeks ago okay so for that yes at last IA this year we presented color mapper at this assist system that builds a topological map and is able to use this map to localize the camera inside the map how it works it takes every image uses a convolutional network to obtain a global descriptor and then we can use this Global descriptor to know if two places are similar are the same or not typically this is not enough so in this case just by comparing the descriptors you can get highest score or lowest score for some images just to be more sure that they belong to the same place we do a secondary test extracting in this case features using lofter and trying to find matches and we if we find enough enough matches then we know for sure that it's the same place okay so we do we use that during map building so we add images to a first place while we are able to match the images is the same place when we find new images that don't match we start a new place okay and then we will see here a summary of our topological map in this case in this case the topology is simply linear okay it can get a little more complicated this is an example of verification if of whether two images belong to the same place or not and this is our final map okay you can see funny things here for example some images are blue this is because at some moments the doctors use a narrow band Imaging that are is used to get more strength to the veins and also to do some polyps in the image to be able to visualize them them better okay so our map in this case is just this sequence of places okay so two weeks later this was a real patient that was this quation problem then two weeks later we have a second exploration and for that we use bayian localization the good old fion basian localization just matching features using the descriptor of matching images using the descriptor obtained by the neural network trying to uh compute which is the approximate location of every image inside our map that is this sequence of places okay okay so the idea is to be able to use that on assisted navigation so suppose we want to reach a place that we want to revisit and then if we if our localization says we are in this place in the blue place it will say go forward R forward to much go backwards AR that's the idea to be able to provide assisted navigation we are still not there but we are quite close okay what about illumination changes as you can see in this image things that are close to the camera in this case are almost burned up and things that are far away are very dark when you move a little the camera the lighting changes totally you will say wow that's a nightmare that's very complicated for computer vision okay but think twice here darker means further so we have 3D information wonderful we have a monocular camera that is given 3D information just because it has attached a light in the camera let's try to exploit that so for that we need a model well our the illumination model says that the illumination goes down with the distance to the square okay so this the inverse of the distance squared okay so to be able to take profit from that you need to calibrate your endoscopes to do photometric calibration of your endoscopes and then you need to know the radiance of the of the Le of the liid emitting the this the light the spread function of the light you need to know the viting of the camera you need to know the gamma that was applied to your video and then you have the cosine the typical cosine term of the reflectivity ETC but if you are able to calibrate your endoscope and to build this illumination model you can use it to retrieve 3D okay so the idea here is we have an image and we have proposed a neural network that is able to obtain the Alo of the surface the depth and from this depth we compute also the normal okay if we have Alo depth and normals we can do rendering take it into that darker that things that are farther away are should be darker in the rendering so we do this kind of rendering and then we synthesize an image and we can compare this image with the original one with the true one so you get self-supervision of death for free no multiple images needed no just the light in your endoscopes provide selfs supervision for dep here you see this Bing table okay in the yellow part you have supervised methods methods that were trained with the ground with the ground truth depth and we were able to match the accuracy of methods that were trained with ground truth depth the red box are methods that are trained with multiv view for example the wellknown monod technique our method is much better than that why what is much better than monod for example because to solve monode you need to register and obtain the relative post of the two images and that inside the human body is not easy okay it's quite complicated while in our case the illumination is giving that self-supervision signal for free okay this is an example this works in colonoscopy this works in gastroscopy so you can see here live reconstruction for every single image in real time once the network is trained you can throw the network in images and it will give you the 3D reconstruction up to scale okay up to scale mainly because we don't know the absolute gain of the endoscope of the light of the endoscope okay for example this can be used to measure the size of a polyp here you have in a colonoscopy a poip and this is the 3D reconstruction and as I said our reconstruction is up to scale so we did a trick is just insert a tool here we know the size of the tool then we can recover the the global scale of everything so this reconstruction in the right now is with the correct scale thanks to this small trick okay this is another example you can see the 3D Shape of the of the polyp so you can measure the size that is very interesting for doctors because if it's bigger than 10 mm then the the the following of the patient is different than if it was less than 10 mm but you can also measure volume or what area whatever of course this can be also be used for robot navigation If You from One image you can obtain just the 3D this can be used to reconstruct the 3D of of our environment and this could be used in future for automatic surgery okay that was 3D reconstruction just form a single image what if we want to do Reconstruction from a set of images okay so one technique that is now very popular is neural reconstruction either nerve or in this case we are uh using NOS okay so here we have this is a phantom a silicon colon okay that's why it's easier there's no water no dirt no nothing okay and this is a neural reconstruction we were able to obtain out of it how to do that well if you just take nerves or nose and try to do that out of the box this doesn't work why because the Assumption when you use Nerf is that the illumination is constant while you take your pictures okay and this is not true inside the human body we know that darker means farther we know that things gets darker with the square of the distance from the camera and the point of light okay so also a nice thing for this application for colonoscopy is that if you do your reconstruction if you use a technique that builds aign distance function assign distance function represents a watertight surface okay the where the function where the value of the function is zero is the surface negative is typically inside and positive is typically outside in this case of the column okay so a signal distance function is a particularly good representation for this kind of application so what we did is building on news neural implicit surfaces a news uh for the ones of you that don't know is a deep neural network that is able to regress aign distance function to perform with this sign distance function to perform volumetric rendering okay so the representation is an implicit 3D surface okay what happens if you do that without taking into account this term essentially it doesn't work we will see results in a moment so what we did is taking the same architecture and just replacing the volumetric rendering just taking into account this simple equation the things that are farther away should be darker okay so you put this into uh the software of news you get light news our our version that was published on mikai last year and all of the sudden everything works okay of course you need to put into that the endoscope photometric modeling so the calibration we did for every endoscope and then you can see here this is a part of the secum the end of the column you can see here the Reconstruction if you don't take into account the illumination that line with distance is essentially crap okay and just by putting this simple equation into the model you get this reconstruction much more accurate and it makes sense is it is watertight okay so the modal here is neural networks are nice but if you know something throw it into the model because they will work work much better than if you try to learn absolutely everything from data of course learning from data in medical imaging is not that easy so we did in this project in the endom mapper project we did a lot of effort and then we got a data set that we call the Endo mapper data set that has 100 colonoscopies real colonoscopies from top to bottom okay 100 for neural networks right now is nothing if you want to try a sophisticated model okay okay okay this also this method like news obtains some kind of also we results it's able to have hallucinate areas not observed by the doctor not seen in any image okay in this case we have done a small motion of the endoscope from this image to that image so it's a very small motion a few centim kind of 10 cm or something like that and then this is the ground truth section of the column we know because it's a silicon colum that was just printed and in the right we have the 3D reconstruction that our system was able to to obtain the shape is not perfect that's true that's mainly because the texture is also very smooth okay but then if you project in this reconstruction which parts were seen on some images and which parts were never seen this is the colors here some parts were seen almost 20 times the blue parts were seen only once and the gray were never seen okay and even though the system is able to estimate a shape that is rable that is watertight why because we are using a sign distant function and this has an implicit assumption that the surfaces should be water typ and you will say okay what's the use of hallucinating surfaces okay because we can show the doctor which parts he has not seen here we are seen the column from the inside and the red parts were never seen in any image okay so probably the shape is not perfect it's not correct but we can tell the doctor here is maybe in this exploration maybe there is a 20% of the area that you never saw that's a very important metric to evaluate the performance of doctors when they are doing a colonoscopy because an area that you have not seen is an area that may have a cancer polyp that will remain there okay so this this is what we call coverage analysis okay something wrong I don't know okay okay so I just want to thank my team nice depending on The View some some of them disappear okay and if you are interested interested in in our work you can just download the papers most of them are on archive or already published this is my my Google Scholar page and you have them there all the all the papers and I'm happy to take any questions you may have thank you there's a question thank you nice J there's a question on the chat it's asking about FPS of this FPS there's like a concern if it has maybe it will be slower yeah FPS okay for the first part so for the the formation for the nonrigid slam system FPS is around 5 per second something like that so yes currently is not real time 51 per second it's not like in Orest Lam that you go at 30 Earth without much problem so we are not totally in real time right now for the other parts I've presented the one without re without de formation the multimap system that part is real time at 20 Earths at 20 images per second no problem for the death reconstruction let me show it for this dep reconstruction in the lower left this is totally real time a few m seconds per image and the light news is a several hours to compute this reconstruction probably if we use News News 2 or if we use gausian splatting we could get much faster reconstructions but in this paper we were just using a standard news and the time to train this network to obtain the model was several a few hours there's one more question about photometric calibration whether do it at all and how if yes how do how to do photometric calibration okay everybody knows how to do calibration you print a pattern and you move your camera in AC cross around the pattern and that's it okay photometric yeah photometric calibration is the same thing only you you buy a lambertian paper and you print the pattern in the ver on paper you do the same motion that's it okay and then you need to choose a model we use a single point model so we are modeling actually most endoscopes have three sources of light three L LEDs we are just modeling like one more or less close to the camera so the parameters we learn with we calibrate are the position of the of the light source with with respect to the camera the orientation of the light source the spread of the light source mixed with the viting because separating them was very complicated and that's it everything is explained in the papers uh that you can download from here that covers the zoom so I will ask one question like all these things like you didn't say it explicitly but the fact that you are inside the body means you have full control of illumination and this is asking for structured light and photometric stereo yeah true but it's sort of difficult to you said just a second ago three lights technically if you could switch them in a fast sequence you could have three images that's true that's true but the the problem is that nowadays you have millions of endoscopes around the world in in thousands of hospitals and then you need to check all of them okay like with that you can go it wouldn't mind yeah of course not not so sure they are not so eager to introduce new things in Endoscopy in my experience please please repeat the question after because um you're in a tube so is there any um way to use the fact that you're in a tu you use a very strong domain right yeah so the question is in in a colonoscopy we are inside a tube and then it is possible to take profit from this PRI yes it's true but we are following more the orlam approach we build a slam system that works anywhere okay so if you use prior years for the colonoscopy then maybe you go to gastroscopy and it doesn't work anymore or maybe you go to broncoscopy and then it's a little different okay so we pre we try to prefer to solve it in the most General way we could just to be able to widen the application field yeah for like classic PL system like U might AEL is there anything okay could you just repeat yeah the question is in for in robotics you have you can use inertial sensors or other type of sensors and what about medical devices in in endoscopes it could be I mean using inertial sensors will be interesting uh I'm not convinced that the type of motions you perform will be that nice for using an IMU also hitting the walls maybe will distract the IMU measuring forces but we have never tried because in reality there is no end endoscopes with IMU and we are not good at building things doing har just use the time thank you again okay thanks so good to see you e e e e e it's the okay no sorry yours is is muted I am not muted okay now you hear me now all right let's go so back from the beginning I'm very glad to be here to be able to present and I give the opportunity because it's Workshop to also talk about master which is very mysterious and I start with duster and we see where it goes so duster is a joint work with from the University and colleagues from neor Z it stands for and constraint seral reconstruction and the dream we had when we started duster was really to have a single n network that would be fed an input image collection and that would output dense 3D reconstruction and the unconstrained means we we don't have any prior information about the camera poses nor their calibration parameters and this is very departing from the traditional NDS setup where you consider cameras at input but in the while if you want to do photogrametry the first thing you need is to estimate the camera parameters so let's say you start with colmap your favorite camera parameter estimation basically the pipeline from colmap it always struck me as paradoxical and a bit frustrating because first you extract the correspondences and from those correspondences you incrementally build a sparse representation of your scene along with the camera parameters with the bundle adjustment and it always it was a bit frustrating because it really means that you need to obtain a 3D scene to get the camera parameters in order to get a 3D scene so basically you're solving the problem in order to solve the problem and this is very frustrating and it's even more more than this because if you look into correspondences I I can argue that it's inherently a 3D task so a correspondence you can Define it or I can Define it as finding two pixels that observe the same 3D point and it really means uh it's not because you're not explicitly telling the coordinates of this 3D point doesn't mean that it's not 3D task and I'm pretty sure it is so the question we had with d is how could we unify all those steps in a single entity that would solve everything at the same time because when you look at correspondences really they are 2D coordinates that observe the same 3D point but when you look at post estimation the camera parameters Define a mapping between pixel coordinates in 2D to Ray in the 3D world and it's the same for mvs actually you can Define mvs in many ways but let's look at the Maps a depth map is really a mapping between a 2d coordinate and a depth value which is exactly corresponding to a 3D point if you know the camera parameters so in all these sub problems you realize that there is a common denominator which is there's always a notion of mapping between 2D coordinates and 3D space so really the philosophy of duster is what if we just predict this mapping so what the representation that we chose for such mapping uh it's the point map so the point map in a nutshell it's really like an image so it's a 2d G of 3D values but instead of storing RGB values you store XYZ coordinates so really a point map it's strictly equivalent to point Cloud but you still retain the 2D information so you know which pixel goes in 3D space and this is uh we found it very interesting because if you have a network that processes Point Ms the network can learn priors on the geometry of the scene like if you have a planar surface or right angles but it also can reason on Pixel consistency if you have neighboring pixels in the image that share the same appearance they're most likely to be in the same region in 3D and lastly the point map encodes this mapping as I was saying and this mapping is critical because from it you can extract the camera parameters you can extract in six and you can extract rotation and translation I'm not going to detail too much much that but it's super easy so this is what we do the question now is how do we predict such Point Maps so we really like to play around with v architecture so let's put ourselves in the simplest M view setup which is two views we have a VI encoder that processes each view in a sis Manner and then we have a set of decoders so I separated the decoders here because they're not doing exactly the same thing we'll explain later but first just to mention the decoders they perform self attention on the tokens of an image and cross attention between the images very much like super glue style of image matching so the first decoder is predicting the point map of camera one in red in the reference frame of camera one which means the red camera is centered and looking in the Z Direction and the second decoder this time is predicting the blue point Maps which are the point map of camera 2 but in the coordinate frame of camera one so this means that both Point maps live in the same three space so this is a dense representation of The observed points of the surface but camera 2 can can leave anywhere it doesn't have to be at the origin it can be really depending on your training data everywhere so I'm talking about training data we are in a supervised training setting so we have a ground roof Point maps and we very simply minimize a regression loss now that we normalize for scaling vales but it's that's it that's that's just it is super simple we really like this idea because there's no hacky trick for now so if you look into this approach you see that we never explicitly told that these were pinhole camera models actually we train with pinhole camera models but it's never enforced but we can verify that the predictions respect the training camera model so the prediction actually respect the pinhole camera model in our case and we can can extract intrinsic information from that in a perfect world you could also train this on orthographic camera models or fishi or 360 cameras even as long as you have supervision I I don't see why it should not work this is only speculation so now we have a network that takes a pair of images as input and is able to Output a dense reconstruction in the form of a pair of Point maps from which we can extract the camera parameters and this this is uh basically this was our dream we have a network that is jointly doing reconstruction and Camera POS estimation an interesting thing is we started playing around and the question we had was what happens if you have only a single image as input well duster is a binocular Network so you can just fit it twice and you end up with a monocular F reconstruction and this was significant for us because in in in my imagination at least there was always this idea of orthogonality between monocular 3D reconstruction and multiv reconstruction and monocular 3D reconstruction was heavily data driven learning from prior learning prior from data while mvs was really relying on the parameters of the cameras that you had as input and you use them for triangulation you use them to search along epipolar lines for the correct depth information and so um in my opinion this is a very nice unifying formula of monocular and mtiv reconstruction so we started playing around a bit more and we tried I'm pretty sure you have already seen that but I'm going to show that anyway because I love it um these are two pictures of Jerome's office and as you can see you can as a human infer where the cameras where and what is that seen But if you look in details you don't have any corresponding points you can't you can't establish pixel correspondences because there is no pretty point in common between the two images this is the result that we got with duster um so here for it was really crazy because now we can have a dense reconstruction with the camera parameters in in in cases where I I couldn't I could never imagine doing that with pixel matching and still cost so that was very exciting for us I literally couldn't sleep for a month because of that uh so it was a it was really nice so let's cool down the excitement uh duster has limitations we found a couple of them the two main limitations are that it is a data driven regression so a data driven means if you have test images that are not present or not represented in your training data then you have no guarantee on the prediction and as you can see here there is a human standing is moving uh and we don't have any human data in our training set so the predictions are uh not great and also not consistent between you so just to mention here you see we have four images as input when we have more than two images we do what we call the global alignment I'm not going to detail it much here because I don't think it's really exciting but uh we have a method to align all the point5 in the same reference system coordinate system so the other point is duster is regression based and regression is known to predict the average of the possible outcomes according to your input which is known to be inaccurate especially compared to uh classification losses that you could use and so you can really see uh on the mvs evaluation that we did on the DTU we don't have access to the camera parameters for our predictions but you can see the overall Cher distance in millimeters is 1.7 which is nice in absolute value it's a nice reconstruction but compared to state ofr is really it's really a poor performance interestingly we found that you can extract correspondences from the predicted Pond pipes literally you can find the nearest Neighbors in three space and this give you matches and from those matches we do visual localization for example we realize that using 2D matching was much more accurate than using the regression output directly and uh the problem we had is that duster was never trained explicitly for matching and so I'm pretty sure you you know where this is going uh this is why we did Master so Master again it's as simple as we could make it it's a work that we did with Yan and Jerome at neor laabs uh and it's the main philosophy is simply to equip duster with the ability to perform pixel matching so let's summarize a bit I find from duster uh the the three main Novelties of Master are that now the point M predictions are done in metric scale actually we realized the normalization was not needed when you had metric information in the ground proof so why not just predict metric scale Point Maps Okay this is a small nty but the biggest one is really but now we have an additional head that predicts local feature maps and these are the same resolution and the images but slightly higher dimensional and those local features aree designed for Pixel correspondences and we train them with an info and C loss which is basically minimizing the distance between corresponding features and maximizing the distance between noncorresponding features in fature space and this is a contrastive loss that is much more uh robust and much more accurate for establishing correspondences the last contribution we have I'm not going to talk much about it but we it's very compute intensive to perform dense matching between local feature maps in high dimension so we have a fast nearest neighbor algorithm that we use because why not make it much faster Al it's improving with our Fus okay so in terms of training we are still in a supervised setting and we have uh overall 14 data sets 50% are real and 50% are signific and 10 have metric ground truth so for the other ones when we compute the loss which has us the previous normalization which makes Master as metric as so we initialized from a pre train duster which is itself was initialized from poo this is a previous work that we did for PR trining binocular bit architectures for geometric tasks so oh it's not showing oh too bad uh we performed many experiments uh these are experiments from the archive that we made publicly available today uh I'm going to look into M free ization and multiv reconstruction you can look in the paper for more evaluations um okay so starting with map free map free is kind of a novel Benchmark where the task is to predict relative transformation between Pairs of images but the the trick is the translation has to be metric and this means pixel matching alone cannot do that because you have the scale ambiguity in the relative transformation and you need to have a way to estimate the scale which is kind of convenient because now Master predicts metric Point Maps key um and so it's working really well so what we do is we estimate the scale with master and we use the correspondences for relative post estimation you have many ways of doing that what we are looking at here are the results of four methods on the M fre Benchmark uh the left image is the reference image that is cck and the right images are the sequence that your your you have to localize so these are the green trajectory of cameras that you see on the right and the reference one is the black one the bottom so the prediction is the blue camera and when it gets close to the ground prooof we mark it as a blue dot so the more blue the better end the bottom rows are the error in rotation and translation and as you can see uh when the viewpoints get closer super glue and lter get reasonable performance but otherwise they are completely he lost and for master uh the the robustness to large view point changes is really showing and it's really showing in terms of quantitative varation on the publicly available Benchmark on the public Benchmark and when we submitted it we improve the performance by a significant margin I'm not saying this likely because the margin is significantly significant um yeah we were really thrilled about that and uh there's no dirty trick is really we predict metric Point Maps we have correspondences and we extract the essential matrices and and use the point map for scale also they provide training data which is also helping so let's look into mtiv zero 3D reconstruction uh because I was showing very poor results from duster before and now I'm going to nice I'm going to show less poor results with master so these are the qualitative uh reconstructions that we can get just to mention that the architecture is not task specific so we very simply triangulate correspondences in 3D using the ground proof camera POS and quantitatively speaking it's we go down from 1.7 mm in Cher distance to 37 mm which is very good and very competitive with st of V art especially considering that we do not train on the D data set so for me it really shows the generalization capabilities of Master but more importantly it shows that matching is still far superior to regression in terms of quantitative evaluations so let's talk a bit about the image matching challenge now uh we we tried our best it was it's hard it's really hard so these are results from the lizard sequence I'm going to show them again um um I'm not going to detail too much how we obtain them but just know that we do not use SK map for that we have a new Global alignment oh I need to optimize the sharing to have nice videos okay sorry online people sorry audience um so we have a global alignment that works with uh in this case 49 cameras so we make it scale to larger image collections because the previous one was having a lot of troubles with the memory usage because of many things so in the end we have a mean average accuracy of 8225 on the lard sequence which for us was uh really good and uh better than what we could get with colap but the problem is we we don't really know why but uh the performance on other sequences were a bit disappointing and especially on the test set and we have basically no clue because we don't have access to the test set u just to mention the church sequence um the Symmetry ambiguities are still a pain uh you see for those two images the second image is the black cone inside the church and so as a human you can clearly see that this camera is not well localized because it's literally inside the church but for a Max matching algorithm we we process all pairs uh separately uh it doesn't make any difference so that's probably something to be done in the the global aggregation of Parise predictions if you don't have pipeline so in the end for the sequence we got roughly 25% of mea R accuracy but on the test set it was very disappointing and yeah symmetries we can't handle them for now uh that's it for my presentation uh we have code available for both duster and po soon Master it's not yet available but it will be you can play around with the deos uh many people already did and we are very thrilled about that the more you play with it the better we know the limitations and the weaknesses so that we can improve it in the end just to summarize uh Master is the new generation that is improving duster it's able to estimate focal length both in monocular and multi setups uh it's able to estimate metric camera poses well as metric as possible if we can and uh we have metric geometry for large image image collections which is also known as mapping which we could not really do with duster uh and also we get the correspondences which are extremely accurate in our experiments except when you have long-term changes because we don't have a lot of training data with long-term changes and hopefully this will change with the proper license please um yeah thanks for your attention and I can answer questions now I have actually like did two questions and first is uh like speaking about like symmetries and this stuff like do you think it's enough if we have like like big data like as Noah was presenting this like double gers or like you need something else like modifi the training pipeline Som power um so that's a tricky question because sometimes uh symmetries so when the scene is changing you have part of the pixels that will not correspond but when you have symmetries you will have pixels that do not correspond and in some cases you want the pair to be consistently matched and in other cases you want the pairs to be rejected because if you use it for your Global optimization it will destroy everything and and this is a very tricky question if you have a pairwise network so ideally it would be good to have a lot of supervision from the training data but importantly it would be better to have a holistic approach that would take all images at the same time and that would output the whole reconstruction and if it is Data driven it might be able to learn prior from the training data but the problem of having all the images as input is that for now we have vanilla attention which is quadratic in terms of the number of tokens complexity um and so we can't really train that Al you might have different sequence length so how do we train a network with sometimes one image or sometimes a thousand images these are questions that need to be answered uh we are only three so we don't have time to test many things we try to do our best and hopefully you guys can also help we are releasing the training data for duster now so you should be able to reproduce our results overall thanks and Y lot of data it to yeah that's at first we wanted to use so the question was uh is it possible to scale The annotation pipeline to to have a very large Network train of very lar image collection and uh we are thinking about uh including self-supervised training let's say you have image pairs but you don't have the 3D or you don't have the corresponden and you could still learn something relevant we are thinking about that but it's really non-trivial and you probably need to mix supervision and self-supervision because we had experiments with self supervision where the output was just not consistent with what we knew was the real scene uh but this is for sure to be explored uh probably in a rendering pipeline way if you render views from the geometry that you predicted you could probably do some subsis training uh but this remains to be answered and just to mention creating pairs of images of the same scene is already a rather strong signal it's hard to build theirs so it's depend on what you call Self thanks yep yeah repeat question you pleas a repeat question for so the question was how is dust you're able to to solve this problem right so to solve the The Impossible Machine case uh what is the data driven approach so we don't really have control on what's going on inside but my intuition is that D is as a very strong prior for mular 3D reconstruction and then it has also a prior to align the floors because the floor are usually planner and then to align the walls and to create right angles because we like to live in few big places globally and uh my intuition is that the the network Lear the prior to align those such that it corresponds to the distribution of the training data so if you train with square GRS with flat walls it will probably align the corners and align the ples and you have I don't remember the name but you have other methods that do that uh in a handcrafted way they detect planes they match planes and they align them such that you can also have this kind of reconstruction I know it's used for Real Estate uh reconstruction when you have few views to reconstru the inside of the building I don't remember the name sorry so if I understand the question yeah also yeah uh the question is how do you handle repetitive appearances or like if you have two floors of a building that look very much similar it's just the floor number that changes how do you uh how could duster handle this and again if you have sufficient training data you could try to train it but these this kind of reasoning really relies on small small details and this is an extremely challenging task like to to be able to disambiguate it's like the church problem to be able to disambiguate from the front from the back you need to see small detail so I didn't show the hardest pair but some pairs you only see a very tiny of image that is discriminative enough to tell you that this is the backside of the church and those small details uh are extremely hard to catch it requires a high level of resoning and I'm not sure D architecture would solve that but we don't have the trining data so it's hard there yeah yeah thank you you can continue one last question please and we go to C yeah good you can fight no we have one one UNC we know that in the model should be con M that's the one okay okay that's already a really hard question so I try to answer the question was um so we know in in the training we have pin hold camera models so is it better to use a scene coordinate regression or a point map regression in order to model a pinhole camera model when we know that the 3D points will always lie on the Rays of the camera so there are two answers to that the first one is um okay I'm try to think quick the point map is an overparameterization of the camera model so it's really a depth map but it's uh free from any intrinsic parameter formulation and so if you think about that a Dev map and an intrinsic value a single one is a parameterization for a set of fing points if the camera is at the center of the coordinate system but the point map is really adding a lot of parameters so it's much less constrained but hopefully during training your predictions will go closer and closer to the pinhole camera model if you have pinhole training data the other thing is there is also probably a notion of convergence of your algorithm it's easier to converge in a less constrained setting than uh having to find the focal length it's it's really a bad parameterization I think for 3D point to have death map plus focal because it's really constraining the the cone of rays along which you are supposed to find a 3 point and uh in terms of convergence it might hinder your training algorithm this is my take mostly intuition based yeah thank you thank you again thank you and here we have a little break for like get oxygen to this room like caffeine in our veins and we'll be back in 15 minutes uh with our paper talks and then one more break and then challenge stocks yeah youever okay welcome back and now we have our um like paper like we were like super selective so this like really great Workshop papers and it's my pleasure to introduce Johan who is like I think in like this and maybe couple more years he's like one of the guys in sfm and image matching so yeah why it doesn't work yeah hello hello thank you for the kind introduction uh how close do I need to stand to this to make sense maybe like this okay hello everyone nice to see many faces I'm here to present the paper uh the name of the paper is D do and there's a V2 on it which indicates it has been upgraded and the rest of the title is uh kind of a homage to caras because I like caras but yeah what we did in this paper was to analyze the doo detector and see what can we make better what do we need to change uh and make these improvements this is a joint work it's not only me so you can see I'm Yan there's also George in the back and there's also sjun who could not come here sadly but he is also on the paper I appreciate all my co-authors you're sending your elbow oh my elbow okay that's the that's the wrong so no no no it's okay just just like have to tell the camera oh okay okay okay this still a little bit more okay and now you're are fine okay now it is a face appreciate oh thank you okay great now I even move it a bit here okay okay so now the online people might be able to see me hello uh okay so I'm going to give a very brief introduction to the doo detector so Doo detector is a learned feature detector that we made recently it was presented at 3dv this year uh yeah it's a keyo detector basically what it does is it's kind of trained to detect uh sift detection that survive sfm so basically what we mean by this is uh someone ran sfm on a data set maybe it was Mead depth it was Mead dep uh and we take all the detection that got connected to a 3D track basically these are somehow good detections they were consistent over views they made this nice track we like them uh we call this the detection prior because that's kind of what we start with in some way it's also trained simultaneously to maximize some kind of self-consistency this was actually the original thought of the project was to make a detector completely uh away from a descriptor the descriptor came way later in this project um but basically just take two views maximize the consistent so we have like detection logits we try to make them consistent between the views we do this by cross entropy and we do some topk whatever it's not so important or it is important but it's not so important maybe in this talk and then we also regularize it uh to spread the detections out over the image so that it doesn't get too uh concentrated on any specific region okay uh so there's also a question mark there I don't know why there's oh this is supposed to be an arrow so it went from V1 to V2 but now it's V1 question mark me to but that maybe also makees sense the question mark will be explained uh as we go along uh so as you can uh maybe see maybe you don't see depending on how far away you are uh we have some detection shown in the first model and we had some detection shown from the second model and if you look kind of blurrily they kind of look the same but then you look closer and you find there are like a bunch of issues with the first detector we made so one of the issues is like really obvious detections such as on the top of this building uh on the antenna or I don't know what to call this the cross you have things that really should be detected but it's not detected in the first detector uh and this is due to data set issues and all this type of stuff I will get into it and we have the new detector and it turns out wow it's much better it detects kind of all the points we wanted to detect it's kind of well spread great that's a good news and okay it doesn't only look good we actually do some quite major improvements on a bunch of uh benchmarks so just compare uh different stuff so we compare basically we keep the descriptor fixed we only change the detector so we haven't actually retrained the descriptor here we just used the good old uh DD descriptor and yeah on Mead depth 1500 and on the on the IMC 22 Benchmark we get some pretty big improvements uh what's quite cool is the if you look at the bottom right here like 78.3 is actually quite a good IMC 22 score and the cool thing here is we're only using a detector and a descriptor none of this dense matching or semi-d matching really cool okay uh yeah another cool thing but maybe not so cool thing if you start thinking about it is uh so what's actually different in the new method is one thing we did is we actually started looking at what happens during training we previously we just kind of looked at okay we we know we have the nice repeatability objective we will maximize the repeatability it turns out uh the repeatability increases on the test set it all looks good but actually when you uh solve for pose so if you have post lib any type of nice ransack actually what you see in these curves is the orange curve which is the like post area on the curve it goes down quite soon in the training and the repeatability keeps going up so what we basically did is we just kind of chopped off the training time so before we trained for 10 hours now we train for about 15 minutes on an A1 uh a100 sounds really good but you can already tell there's something a bit sketchy here right um okay other good things I mean I would just keep saying some nice things like if we look if we compare qualitatively again maybe uh to disk compared to DD we can see some differences as well like this has this kind of nice property that the detections are kind of well spread out over everywhere uh but it doesn't really uh it's not really discriminative it doesn't really converge to any really repeatable points while the first DD it finds these really repeatable points but as training goes on what will happen is it will take only the really really repeatable key points and ignore large regions of the image despite us having this regularization and all that uh while with the the v2d do we get repeatable and kind of well distributed key points you can kind of see that it takes most it still ignores some regions so there's still kind of work to be done uh yeah so also we don't only just cut the training time uh because then we wouldn't get accepted maybe uh so we also do some other stuff uh so we do uh nms uh so non-max suppression which is everyone kind of does at inference time we don't do it at inference time and actually we still don't do it at inference time because it still reduces post AOC but it's good to do during training time so we have a I talked very briefly about a topk objective in thisa at this time we do the uh uh we do non Max on this to ensure that our targets are not yeah uh are not clustered basically um and of course similar things exist in previous works so if you look at R2D2 they have a a peakiness loss there we also experimented with other types of losses such as a peakiness loss uh didn't work quite as well and actually these key Point networks they find really weird solutions to minimize these type of losses um I'm happy to talk more about that later if you want uh we also did some minor improvements so basically we just updated the test time Aug sorry the train time augmentation so we did a bunch of rotation augmentations to make sure that the detector is robust to Big rotations this is actually already kind of in the steerers paper uh but we integrated it into this framework as well also one thing that you might not know is that the top K objective in the original DD or probably you haven't even read the original DD but there it says the topk is done per image but actually it was done per batch uh if you look at the code uh now we do it per image I promise and it this is good this is good uh and yeah the thing with doing top K of our batch is the original thinking was it can be good because some images basically have less interesting features so we should kind of have it over a batch but actually what can happen then is that the network is not very good at certain images and then we let it kind of ignore those images and that's not such a good thing if you want to train a good network uh yeah we also trade uh tried a bunch of things so don't try these uh if you want to improve it uh so don't I mean you can you might be much better at this than I am but what we tried was basically Maybe we can do some post ha stuff maybe we can modify the Logics in some cool way to make uh stuff better to get this nice spread didn't work well uh modifying the detection prior so maybe instead of sift we could use super point or something like this didn't work well either uh maybe if we do like a bunch of detectors as priors and train that it could work better but that's such an ugly solution I think uh yeah modifying K changing regularizer changing learning rate it turns out 10 to the minus 4 is the best learning rate don't change it um resolution doesn't matter and uh yeah this non-max suppression during inference was also not useful still uh and I'm thinking maybe it's not really good for cfty type detectors it might be better for Corner type detectors but I don't have much proof for this yeah uh so there's some bad news I guess in this which I implied from the start which is basically that this repeatability metric is kind of inherently not the right metric to optimize if you want a good keyo detector what you really want to optimize is the posst uh area under curve or something that's actually the downstream uh thing you want and there are Works in this Direction I mean there is already differentiable ransac in multiple forms uh uh but I think not many people have gotten it to work well with keyo detectors yet I might be wrong um I've seen some work on like detecting key points in 3D which uses post errors but they are still kind of tricky to work but I think this is kind of the thing you want to you need to find an objective that actually aligns uh with what you want in the end so that's something I would hope yeah okay final slide uh yeah you can find the weights in the skit tab Link at posat D or you can use cornea uh yeah and this are the new weights over here it's a B2 uh you just change that line and hopefully it should work let me know if there's some bug somewhere here there might be uh I heard one of the IMC teams used it so I'm happy that they worked out for them uh yeah if you're going to get in touch you can uh yeah just get in touch here or at me or open an issue on GitHub I have a lot of them um okay uh another final slide uh we have also a poster for Roma later super late on Friday shitty time uh and an oral and a poster for steerers with George that's on Wednesday so definitely check those out uh also one one last thing so okay a key Point detector detects okay that makes sense the points that it detects are called detections that makes sense that makes sense a key point the scriptor describes yeah yeah okay the points that it describes are called descriptions yeah oh some people uh accidentally misspelled this as descriptors I don't know uh how this misspelling got into our literature but it's actually called descriptions okay I think that's it no okay uh yeah I will be working on matching again this Autumn uh looking for more people to collaborate with if you want to collaborate on something let me know and uh let's do something okay that was the that was actual thank youfor there is no time for questions well super overtime I also want to comment that how our Workshop started we said that like reputability is not good metric yeah yeah now we've gone to Circle we it again so people yeah fine yeah I'm here uh I have even a keynote here so you and do this yeah okay uh hello everyone hello everyone uh my name is H and uh in this session we prevent a f based form attention for the semi dance M dream and this is work down by the joh work down by the apple and the Hong Kong University of Science and Technology okay uh so we first do a brief recap of the uh different Bion Paradigm maybe a little upd dat because it doesn't include the D master that is just mentioned but uh so let's recap uh so we briefly we categorize meion prodam into three parts the first is a traditional sparse meion Paradigm uh which includes a keyo protection and matching and then we have the uh semi dance regime which means that we encode the four four resolution image into a fish map and we do some f orentation typically through the cross view Transformer and then we do the matching and uh do refinement to get sub pixel level accuracy and very recently we also have a very powerful par which which mean which I think is a refl reflation of the fly dance regim in the uh optic flow like method uh that we encode the uh feature mapping to a cost volume and we use a decoder uh to get a full resolution uh match output and do sampling uh the typical one is like the RoR and the dkm which is very powerful performance and uh our work fors into the second that semi dance regime why we don't do the fly dance uh because we find that if we want to obtain very good result in the fly dance regime need have backbone and decoder and many par many param parameters in the network uh but we also want to so we want a light lightweight Network and we also want to benefit from the uh dense regim so that we don't uh be restricted restricted on the key point so we choose the second one that is a semi one with a relatively lightweight uh Network and also uh we don't need to detect key point so uh as a very as a very typical method in the sem regam is lofter in uh C 2021 uh the basic structure is that we extract the uh feature map from the usn and we conduct across and self attention among the tokenized features and after that we do a Vanner or NE matching and after that we do the refinement uh so we think about the very important part of this Paradigm that is how to we defined uh the Transformer or the tension in this uh cross view operation so uh since that we know that the when lar attention has a quadratic complexity and our image has run pixels uh so if we want to do a full resolution is very uh it's almost impossible and to prevent this uh there's something like a linear approximation in the original of the paper and also we can uh similarly do a resolution reduction while the Ping operation and uh actually there's a CPR work uh in 3pr 2024 about this Paradigm that is just do it in low resolution and Achieve good results but actually this is a global attention that means we sacrifice some uh granularity for the efficiency uh to achieve a high resolution output we want some fine gry interaction that's we called Global local attention that means we do globalization at a low resolution fut map and by some means like we regress intermid optic flow through the tension process we can locate which part of the image correspond to other part on the opposite image and we just cast the uh attenion pattern upon the opposite View and the sample local attention span and we do attention between this p uh spam PS so that we call the global local tension and uh in accv uh we prevent a previous work to adopt this kind of things like the global local attention and after which we fuse this uh message from the global and the local scale uh to form the basic operation of an attention framework uh and we think in this direction and we find two things that may be May hinder the further performance Improvement and one thing is how do we how do we Define the local attention space SP so our idea is that we want to maximize uh the overlap ratio between the cells and Target uh attention span so that means but that is not very easy in the two view geometry because we have a deformation across Lo like scaling sharing or other translation so idea is that we can estimate a piece-wise deformation field from the intermediate optic flow we uh predict and use it to shape of our attention span and this in this work we choose a fine field uh to shape this uh to define the deformation field and another issue is about uh how to refuse the global and local attention uh message actually we know that the local message we obtain can sometimes be unreliable due to either due to the uh inaccurate of the flow regression or just because this two image some part of the image P doesn't overlap at all so what we want to do is that we need a reliability score reliability metric to determine how to determine the weight uh that we put on the local and the global message so we prevent this work that is a fine for it's a deformable uh attention it's a semi dance matching with deformable attention and also a selective Global local message Fusion so uh we just work through the briefly Works through the pipeline and actually what matters about how we how do we design the global local attention in the cross view M cross message exchange so the first St we don't sample the uh input Vision map into a relatively low resolution and we conduct the global attention uh in this in uh in this stage and we know that in global attention uh we have attention Matrix uh that depict how similarity which part in one image one correspond to other part and uh we take inspiration from some uh dance method that use this similarity Matrix to weit the positional encoding token on the other image and regress uh intermediate active flow uh so after the intermedial flow regression we upsample it to the original resolution and uh use use it uh to shape our to regress our uh deform deformation field uh so this part is about the uh flow regression in the uh Global attention stage we have a a hat that is the tension Matrix in the uh Global attention stage and we have the P which is the position coding tokens and we have convolutional uh predictor that you predict the up flow from the Ved uh position tokens and uh we uh inherit from our preious work that uh model the flow as a gion model that give us a probability of of the estimated flow uh flow field which means we uh we don't only have the uh flow regression flow coordinates but we also have its uncertainty so we use it to uh estimate a deformation field which means that uh since we have a piecewise uh op uh flow field and for each patch uh we can uh we can Feit fit a local fine six fine parameters that a fine Matrix and it constitut our deformation field and for each Sal tokens we use it deformation field to cast the cast it upon the uh Target views which forms a deformed attention span and we just conduct attention between each of the uh span PS so after down the global and local attention uh we do a Fusion and as we mentioned that we want to uh put different weight on different part of the fusion stage so we just use a because that we we can also obtain uncertainty score for each predicted flow so we use it to design a metric that uh put more weights on the more reliable prediction and use it to uh weight two parts in each location uh so after that we just do the uh normal uh normal operation in the tension that is to fuse S2 stage and pass it to the fit for Network to obtain our updated features and also we have a refinement stage uh which means we uh to optain a set pixel level accuracy from our cost mates and uh in terms in terms of loss design uh we have a cross entropy loss which is uh designed for the cost uh matching and F matching loss and once you find that is uh if you apply additional special software law special software Max laws uh to oppon the uh assignability assign Matrix we find that it will uh further improve the matching completeness and uh we also conduct the post estimation relative post estimation on the uh two classical data set the scan at and Mead deps and uh we have obtained uh High result and both previous semi state of artwork and uh we also uh test the generalization ability of a method that means uh we use the model trainer Meads to uh obtain the poe on scet and we find that uh if we compare with lofter the macap train model can achieve higher accuracy than the scanner train lter and uh we also test about the accuracy on different motor size uh since we have a so we have a lightweight model that is the third row with only 2.1 one million parameters and uh almost one set of the fobes compare with L but achieves High results on Mead deps and uh to thetion study uh we find that if we directly replace the focal loss with uh the special s Max Pro uh we propose we find that it will degenerate performance but when combined with the FOC loss uh it will bring a very large uh performance Improvement and we also validate a different part like the low quention and select efficients and a de deformable aine deformation uh in our method so uh the last part is about some visualization uh we can see how the Sal patches are projected is a target one use deformation field estimate uh which covers both scale change and and sharing change and uh about the mat results thank you uh so the code and they'll be released uh in this link after the uh after it's prepared so any questions I think think we are get over time as same with first talk so no questions here uh so like next speaker and I would like to ask you to be a bit mindful of schedule uh yeah and next speaker would uh tell tell you how to match from space yes thank you hello everyone I'm so I go like this right there um I'm Gabriel Berton and I'm going to talk about our paper Earth match there's not much technical novelty but I promise the task is very cool and the insights are interesting as well um so I'm going to start by telling you that astronauts take a lot of photos of Earth they are up there in the International Space Station there's a Cupa and they take photos with a literally like DSLR kind of uh camera and right now we have over four million photos taken uh in over 60 years and before going up there astronauts undergo uh photography training so they tell them what they should take photos of and how to take them because the ISS is moving very fast so it's not too easy uh these photos are used for uh scientific research like long-term climate change study and emergency response so for examples they take they take photos of wildfires floods as as they happen and volcano eruptions so uh for example by just turning the camera oblique they can uh they can measure how how high the the volcano eruption smoke is and that can help for example to divert uh flight traffic for example um but this photos are not do not automatically come with the GPS coordinates with their localization so these are sent to to Houston to NASA and there are scientists there that manually localize these photos manually and then these localized photos are then sent to the authorities anywhere on Earth um localization as you can guess is a very difficult task because uh the search space is very large this is what an astronaut uh sees from above Earth ISS is 400 kilometers above the Earth surface and they see an area that is roughly as big as North America and the search space of the people that need to localize these photos is is just as big and these photos cover as little as 0.005% of the visible air and so localizing this photo is as difficult as finding an needle in a Hast Tech but these are actually very important so 300,000 of these images have already been man localized by hand and not to collect some data set just because they're useful um and so this is in the words of NASA Earth Observatory this is a monumentally important but monumentally timec consuming job um sorry there was a video of a person actually doing the manual localization here um okay uh so how do we localize these images at first we do image retrieval so we sample a lot of images from Earth these are photos basically we cover the whole earth uh these images are taken from satellite images like Sentinel 2 type of images that they're open source we can sample them at different resolution because when astronauts take photos we don't know the resolution we but we know the camera lens but we don't know the extent of the covered area so we sample them at different resolution different rotations because astronuts don't know where it's north so all these photos are pointing all different dire directions so we rotate them we create this giant database of 700,000 images and then we run image retrieval and uh with image retrieval well we can get a candidate so as you can see here on the top right there is the astrona photo which is the query that we want to localize you can see the occlusion there from a part of the ISS and then we get a prediction like the most similar image from the database to our query and I'm not going to go uh too much into this uh but uh because this is from a paper that I'm going to talk about as a poster session on uh Thursday morning so I'm not going to talk about the image retrieval part I'm going to tell you that these are the results of some qualitative results of image retrieval you can see on the left the left po image is the query and there are some predictions well these are correct predictions so we're happy with these but these are these are different query and these are wrong predictions so um we're not happy with this and want to uh automatically know which are wrong and which are correct so this is an issue of um um of a image retrieval is that it does not provide confidence can I move this okay okay um image retrival does not provide a confidence score or it does but it's not unreliable um so we actually do want to obtain a confidence for each prediction so basically by postprocessing the candidates from the retrieval we want to get a confidence want to obtain a pixelwise localization and that's because for example if they take a photo of a wildfire want to know the extent in terms of latitude and longitude of the Wildfire so like precisely per pixel and uh finally we want to run a fair outof distribution image matching Benchmark I'm going to tell you what that means very soon um okay so to get a confidence score and to pixelwise get a localization of the query images we do image cor registration so basically we take the query we take the candidate we pass it through a image matching model we get a homography and then we get a we can transform the candidate and get an image that looks more similar to the prediction so as you can see the colors are very different but the first image like the query is the same uh location as the the image in the end after the transformation and like this we get a confidence level because the number of inliers is a good proxy for a confidence and we also obtain pixelwise localization so for each pixel we know the latitude and longitude thank to the homography um in practice the way this works is actually we do an iterative process so we don't compute The homography Once and do the transformation after we do the transformation we do it again and again a few times and as you can see at the beginning the two images are very different from each other but after a few iterations they start matching perfectly one on top of the other and then we know that the prediction is actually correct and that we can confidently say that the yeah the prediction is correct with high confidence um so well I showed you a little bit the pipeline there's the retrieval the homography the the the image matching but so we were uh now we had the question which matcher should we choose should we train our own and well before training our own we wanted to try previously existing matches and we found that surprisingly they pretty much work very well like this is a very much autodistribution kind of task uh most of these methods are trained for on buildings outdoor kind of images and then we test them on satellite images and somehow they still work um so we tried 17 different pre-train matches and we upated a number of things like image resolution and number of key points which is something that usually we don't see in literature because we we only care here about maximizing the the results and I'm just going to show you this one result which is the one I found the most interesting just with a subset of the methods that we benchmarked um by changing the image resolution well obviously small resolution means better results but what we did not expect is that with different resolution the best performing method changes and it changes very much and you can see here that with four different resolutions there are four different best performing method and personally I'm very happy that we did this oblation because otherwise we could have just fixed the the resolution to uh 768 for example these are squared images like seven like 512 by 512 pixels and would have said that one of these is the stateof thee art which would have not told the entire story um and obviously to run uh this code with 17 different uh models we had to uh to to make a repository to make a codebase where you can easily try different uh models and so right now we we're actually supporting 23 of uh 23 models and we we just show 17 um in the paper and these all come very easily to use with a unified API uh I hope uh this open source we put it on GitHub I hope it's going to be helpful for many of you uh you can just get clone the repository and just run Python main.py and just pass the matcher name uh everything should be running out of the box uh so to summarize um we produce a confidence level and pixelwise localization for these astronaut images by postprocessing the output of the retrieval uh we ran the largest autodistribution image image matching Benchmark and uh we provide a codebase to easily test dozens of matchers uh code and data sets are available and I hope that the code will help you in any kind of project you might have and I hope that the data will um push you to at least try to work on this highly relevant task so thank you for your attention now we have time for one question who would be this slly person no when you spoke about maximizing the number like dependence on the number of key points matched as a function of resolution the graph was in percentages yes so how does number and percentages yeah okay so here I only showed the the percentage is the percentage of image that we correctly localize and on the xaxis they show the image size in this plot we don't show the number of key points there's another plot for that yeah yeah thank you again thank you and additional thanks for making everything open sourced I love it and next one Cross lofter or something softer lofter lofter great hello everyone my name is iora and I will talk about our work zofer cross model feature matching Transformer well matching local features across different views is a fundamental problem and visible termal image matching is a special but challenging case it's special because the termal image is always robust against the adverse light and weather conditions for example in this poy image you can almost see nothing but the termal one is more crisp but it's also challenging because the texture car characteristic and the nonlinearity instan differences between the RGB and termal Spectra and termal image terminal cameras typically have lower resolution and field of view and this despite many handcrafted learning based methods are available the performance is still suboptimal as you can see in this image there are many differences because of the texture characteristics and in literature up to now there is no available image meure that supports both multimodality and multiv view based on the lofter algorithm we have introduced the lofter algorithm and we follow the we have three contributions the first one is we follow the two-stage training approach with pre-training first and then fine tuning with augmented image and we leveraged the mask image modeling to address the multimodel data set scarcity and second one is the we follow the fine matching pipeline which for the pre-training phase and we enabled the one to many matching which we follow the ad matcher algorithm in here and we ensure the reliable texture matching at reduced scale of one over two uh in this imagery you can see we now available in the one to one to many Ming for the 108 scale and our chart contribution is we introduce a new data set the visible termal image matching data set called meter and it has various Viewpoint differen and weather conditions such as the Cloudy to cloudy or sunny to sunny or another combination and the to gain robustness or differences on intensity we introduced the mask image modeling based on the pmch algorithm in literature and for training we have used some available termal data sets which is called as K multis spectular pedestrian detection and we randomly create some masks that that cover 50% of the image with 6 64 to 64 size Pates and we follow the fine tuning but there is no available termal data set with depth information so we have created pseudo termal data we applied the modified version of cosine transfer on RGB image and the The crucial part is to gain endurance against nonlinear intensity variations for this we directly use the Mead dep available data sets but we have converted them to PUD and as you can see our modified cosine transform algorithm is the simple but effective algorithm this is orinal RG and this is original data and theal is very look alike to the termal imagery and I think there's something wrong with because of the what's Google Slides there are some I mean difficult for to do it in real time have to yeah like okay so yeah okay let me go on like that we have used the CNM feature extraction backbone and we get the one8 one4 and one or two s scales of the features and we have used this one or8 scales features for the course level matching module we use two 108 scales features to get the one to many match avail aable in the course level matching and we feed this mates to fine level matching module along with the one over two one over four and one over two level matches and we use all of these scales in using the concatenation self attention and cross attention at the end we get the one to one one two scaled matches and we feed them to sub pixel refinement module to get matches over the sub pixel level and here are our results on the relative post estimation on meter cloudy cloudy and cloudy Sunny data sets and please note that there are not many available algorithm in the which with the code algorithm in the literature the only ones are the red fit and LOF are M MTV and we tried them with RGB to termal but the other ones are trained with original RGB data and our proposed softer algorithm on average increase their performance by around 28 points and here are the some qualitative results and these are also verifies our findings as you can see our sofware gives much reliable and less error results and it also works with different homographies and these each align visible and termal image pair from these data sets is used to generate five different image pair through homographic transformation and as you can see even though we Chang the homography we still get great results and this also quantitatively verifies our findings so to conclude the leveraging the two stage training and mask image modeling zofer significantly outperforms the existing methods in visible to termal image matching benchmarks and adop addition of one to many matching provides the robustness across different viewpoints and texture diversities which are the original challenges in the visible to termal matching and achieving the we achieve the state of darkart performance on a newly introduced data set and we increase the area under curve score around 28 points which validates the effects of our methods and for the future work we are planning collecting large amounts of data consisting of visible and termal image for training purposes and we try to obtain the ground Ro pose and depth of the collect data with sof and finally we are planning to utilize the foundation models with s gener ation ability so thank you for listening the code will be available in a short time and also you can access paper via the link or the QR code now if you have any questions I can answer them question I I think text just no no no just one please canach Beed to yeah actually as you can see from this image we lost lots of the texture patterns in the termal image so we have to just focus on for example The Edge or something like that because it's impossible I believe to focus on the paintings or something like that and for the second question yeah I believe it can be adapted to the trans yes so our final talk rgbt anyone okay so this is I download this one moment yes this wait wait wait yeah here we go pleasant good morning good afternoon to everyone I'm amula Pand and this work has been done under the supervision of professor suana chapaya at lab for video and image analysis IIT hydrabad India and the title of my paper is our deep learning models pre-end on RGB data good and for RGB thermal image retrieval so this would be the outline of my presentation starting I would like to introduce with the introduction the problem statement RGB image retrieval the conclusion and the future scope so coming to the introduction so in an in image retrieval for a given query we ideally would like to retrieve a visually similar image from the reference database and the applications majorly involve Place recognition surveillance monitoring autonomous navigation search and rescue operations however the existing methods mainly work with the RGB data alone but if we wanted to work on all day all weather scenarios raying solely on RGB data will not give us trustworthy Solutions and hence we wanted to look into RGB thermal image retrieval where the query and the database come from a different domain so coming to the problem statement if you see the figure two column 1 and column two represents RGB and its corresponding thermal imagery which are taken at low level illumination setting and at adverse weather conditions so if you see the visual information that we get from RGB image is not so informative here and we see that uh corresponding thermal images are giving us better information and helping us understand what exact exactly the scene is about so uh here the objective is to design a reliable rgbt thermal image retrieval solution to perform in all day all weather scenarios especially when RGB image is not unre reliable and we see that uh matching multispectral images has been a major U interest in the community with the focus on RGB and N imagery however as n band is very close to the visible spectrum we see high correlation between the RGB and N imagery like if you see the Shadows here or the design that we see are they are present in N imagery as well and we feel that solving RGB and N imagery is much simpler than solving RGB and thermal imagery which is f like far infrared imagery and moving forward in the related work uh it it is mainly classified into four different categories with the first three categories mainly focusing on RGB and N uh imagery and people have focused on uh different kinds of matching images uh like a they have been trying to match the pair wise images rather than matching images in a image retrieval setup so the first category comes under classical keypoint based techniques where they try to focus on gradient based techniques edges and all and the second category comes as learning based keypoint techniques where they try to use the Deep learning methods and try to extract feature descriptions and the third based technique is a image patch matching similarity where they try to match the uh given patch instead of an high resolution image and the fourth category is a visual Place recognition methods which try to employ image retrieval techniques and majorly the state-of-the-art VPR methods have uh had uh data sets which have geotag information and all the models are trained on data sets with geotag information uh so the ideal setup here we would like to train a VPR model for RGB thermal image retrieval task on RGB thermal data sets however the lack of RGB thermal image retrieval task specific data sets that is missing of geolocation information the existing data sets is stopping us from training the models training the VP like models and so this made us look for alternative data sets which are existing uh and are called as a benchmark data sets for Fusion based object tracking or Fusion based object detection uh so all these data sets uh we have used for evaluation and these data set doesn't possess uh location GE location information except the boson data set here which is used for thermal geoloc localization application but however ever this boson data set was taken in a desert sort of area in in spite instead of City kind of setup so uh the so these challenges May has stopped us from trying to train VPR like models on the available RGB data sets and hence we wanted to see the suitability of the existing pre-train models which are trained on RGB data set RGB data sets for different task for RGB thermal image retrieval task and so this is a overview of the pipeline uh G given RGB image as a query image and a thermal imagery as a reference database we pass them through the pre-train model which is train on RGB data for different specific task and we try to gather the feature descriptions and then try to get a Best retrieval image based on the least distance and we say the best retrieved image by the model is a correct match only when the index of the best retrieved image is exactly same as a query index and moving forward the these are the pre-train models that we have considered to evaluate the suitability uh for RGB and thermal image retrieval task and these are the main contributions of our paper an extensive evaluation of the pre-train models a demonstration of why the F models are doing better than the stateof the VPR techniques and an explanation of the factors contributing to the superior performance through different qualitative and quantitative evaluation techniques so starting with the quantitative evaluation we use recall at 1 and recall at five to evaluate the efficacy of a pre-trained model over RGB thermal image retrieval task and if you see here these are all image and train models trained for classification data classification task and these are the VPR uh methods set of theart VPR methods which are trained for Place recognition task and these are the multimodel methods which are trained for different tasks like classification object detection Etc on multimodel data and sgm is a baseline we treat it as a baseline model which was trained for thermal geolocalization task on boson data set which is basically a desert kind of data set and if you see the results here the uh bold represents the higher higher recall rate and the underline represent the second best recall rates and we have observed that squeez net imag pre-end model is consistently giving us higher recall rates over all the data sets except the boson data set and on boson data set we have seen that the Baseline model is giving us the highest recall rate as it was trained on that and we uh we also see that despite being trained on RGB and thermal imagery the Baseline data set Baseline model is failing to generalize over all other data sets because of the nature of the data set it was trained on and we I have also seen that the stateof theart VPR methods are also not doing great here one reason could be that VPR mainly focuses on background aspects of an image rather than the foreground aspects and we have seen that the data sets that we have used for evaluation has more of foreground changes rather than background and uh we also have seen that the imag pre Trend models are doing relatively better than the other pre-train models one reason could be uh the commonality that is present in the classes that are present in the foreground of the evaluation data set that we have used and the image data set and uh we also see that squeez net is doing better uh over and above other pre-train models we feel that one reason could be that uh the delayed down sampling in the architecture of the squeez net is aiding here uh in giving us the highest recall rates and moving forward we have used uh Central kernel alignment an index that measures measures the similarity between neural network representations of an RGB and thermal gallery of a given data set so these are the set of pre-train models which are doing better among all the previous models and so we have considered these moving forward so we if we have uh when we have evaluated the cka we have seen that squeez net is having the higher similarity rate over all the data sets that we have used meaning that the feature presentations of an RGB Gallery is much closer to the feature presentation of a thermal gallery of a given data set using squeez net model and moving forward uh we have used centroid condition uh to uh understand the model's ability to distinguish visually similar locations from dissimilar locations so for an example if you say here we have used temporally cor correlated data sets meaning that say if we uh we wanted ideally the C centroid feature representation of this location to be closer to the CID feature representation of a same location in the opposite domain and far away from other locations so with this condition we have evaluated the pre-train models efficacy and we have seen that squeez net is able to retrieve 32 locations correctly out of total 42 locations and we have seen other models are doing relatively less and we have seen that the Baseline is giving 35 locations out of 42 locations and uh so moving forward we have used qualitative evaluation using distance plots so ideally in a distance plot we want ayat query image to have lower or minimum distance with the ayat reference image in the thermal gallery and but then if we see uh different models we have seen that each and every model is giving us minimum but at a wrongly wrong index I would say and we have seen that squeez net is able to retrieve the exact correct uh index from the thermal gallery for a given RGB query and this Behavior has been uh uh repetitive under different samples and on different data sets as well and these are the top at one retrievals uh by different models and uh at different scenarios so if you see this is a low level illumination setting this is at adverse weather condition this is at high illumination setting and at a again a low illumination setting so in all in most of the cases we have seen that despite the temporal correlation that is present in the RGB thermal data sets that we have used we find that squeez net is able to retrieve exact same exact same match for the given query image and where the squeez net is failing we have also seen that all other models are failing and so moving forward we have used the heat maps to understand what exactly the model is looking into u meaning that uh for making the decision so here if you see the column one represents a query image and the ground truth image and the column two here represents the saliency based similarity map for the given uh column one pair and uh we the column three here represents a query image along with the top one retried image by the respective model and the column four represents their respective sbsm map so ideally we would want the model to look into the uh major uh part of this image meaning say the road here but then we see the model are looking at some different aspects and we feel that this is resulting in their unreliable behavior and we see that squee net is uh more or less looking into the major part of the image and so coming to the conclusion uh with the given challenge of unavailability of uh VPR like RGB thermal data sets we have chosen to evaluate the existing pre-trained models which are trained on RGB data for different uh which are trained on RGB data on different askask for RGB thermal retrieval and interestingly and surprisingly we have seen that some of the pre-train models are doing well cross doain generalization and we have also seen that squeez net is being as a clear standard in various uh qualitative and quantitative experiments that we have conducted and when we have done the parameter comparision we also have seen that squeez net is uh taking like very less number of parameters which could be an ideal solution in a realtime application where there is a resource constraint and we also emphasized on the lack of VPR kind rgbt uh data sets and the future scope we would like to explore the early CNN layers uh and then we also would like to explore the recent Foundation uh Vision Foundation models for image retrieval task thank you we have a for one question uh okay then I have a question and is that like uh are you planning to also release a like maybe data set and fix the thing that there is no such data set yeah yeah we have a plan but then yeah we are working on it yeah great thank you thank you now uh I think there should be a break there still will be like five minute break between competition mostly like for biological reasons like but it would be just five minutes not 15 Okay and like see you in five minutes for the challenge uh talks uh they like very interesting not much worse than papers or actually better maybe okay very good okay now now you can go okay uh so before start to present in details in detail the the the challenge and the result and I want to thank again the sponsor Google and the CTU in PR for giving us enough money to uh to get a good price for the best competitors and now what's new in the this last edition of the the challenge well as dimitro anticipated we are now on exatlon that means that instead of using the usual categories uh we have in data set and common data set we try to uh enlarge these categories we have six overlapping categories overlapping because some machines can belong to different categories we have the usual historical preservation night versus day and temporal change aial and areas to ground images symetri and repe trctor and for this time we introduce more SC with natural environments and transparencies are reflection uh in the next slide you will see more than six categories eight just because we split for the analysis night versus days and temporal changes as well as aial and mixer AAL ground but these are the main also we uh introduce a new evaluation Matrix with respect to the previous one this is in some way more clear to understand in this geometric meaning is just we align the cameras and then we compute the uh error of the rep Proctor with respect to the ground through cameras and uh this kind of uation also allow to have only one thresold for uh the diff for the valuation for to setting the the different accuracy in the in the metric H instead of using two threshold one for rotation and one for one for translation and if always go well that mean that we start with arct for motion we have uh rotation and translation and then we compute the camera senters this should be good uh here there is a just the training set splited in categories actually as I said the temporal changes and illumination going together but is much simple to draw uh uh Square so and also for the analysis so you can see here the planning set with the split and uh as you can see we can go further in division we have on one side transparent object that go well with the symmetries and repeated distractors while there are the other stuffs that go together especially a historical preservation with the aial images and natural images with temporal changes and illumination here there is a a sample of the super set from which some scenes were extracted for the test set for natural environments and you can see there is a lot of variation temp temporal changes also over in the in the illumination and the uh the S trctor is not the typical building so is quite there are less planes insides and on the right you can see also The Spar 3D Reconstruction from the uh original super model that uh has about 3,000 images and on the top right you can see the best resolution the best submission results after the alignment with the the and in this case is what sufficiently well but there are some uh points identified by the red and the green that are outside now for the transparent object this was really a challenge not also for the the competitors but also for us to provide a a good reliable G through so uh since it's difficult what we do was using a thir table and then acquire images on this of this ring well you can see on the top right there is the result for this category is and using the uh the new matric the P two comp comp scores are quite High while with the the other mats are low that's because in the end the top score get the correct camera Center because what in practics they do was to distribute distribute along uh the ring the cameras and obtain the correct camera order so the camera centers were correct but the they got a wrong rotation Matrix you can see here in some details the blue is the ground true and the r one is one the the one the ones activate and uh in so way uh the top submission trick for the original aim of the post estimation but if we focus on the the aim of the challenge was to estimate the corrent cameras if they were right and also because they the only the these two methods were able to provide the exact image order by the way for the next Edition we are working for fixing this in the new matric H now here there are the result as taken from kle for the top score you can see again that the leaderboard is quietly determined by the the transparent scenes here and now going in more detail if we also uh do the same using the previous matric we can say that there is some movement in the in the lead board in the rank of the leaderboard and uh so here there are considering also the uh old matric the best competitors and we have got vgg structure motion Z ju trdv and moton has the third best one by the way the uh current energy voltage that was the top rank in the new Matrix score quite good also here and so you can see is the is the F fifth ranked and now uh here there are the details of the result for the different categories and uh We There are three slides each slide uh put together the uh categories with eye overlap and you can see here that with exception of the transparent image uh considering a transparent image because here there are no transparent image the two Mets correlate quite well and in this case for historical preservation and a r to Grand image uh here there are the top scored meals and we see again that vgg struct for motion and this case then match and motono but also current voltage are seems to be the best for this kind of a category overall while for the nature temporal and illumination yeah this is in the original designation design one single category but for the analysis here we split again into here the results are quite similar and but there is not there is no this m dance match as the best one but they ZJ 3dp and vfm moton and current energy and voltage and finally for the transparent categories that means transparent image but also with symmetries and repeats here the result is more more or like similar to the one of the uh this slide here so uh this analysis also Al also done to to give a a a reason how the challenge T was selected and uh I think we can start then with the present with the talk or if there are some question I don't know yeah so first of all congratulation to all the winning teams and I guess everyone here will be curious about how they make it so yeah let's first inite the um yeah the first winner current energ voltage here the gyms here I guess so I need to share my screen uh okay uh hello everyone it's a great pleasure to uh come here to present our solution first place Solution on Koo uh so let's let's start with the architecture first so basically we emphasize uh two different components in our pipeline so the first one is the feature extraction and matching stage for non-transparent sces and the other component is image post estimation stage for transparen sces you already know what the transparencies mean and what the non-transparency mean so let me me just uh go next so for the matching stage we have different images in the scene some of them are not rotated not in a natural orientation so we fix this orientation uh aligning both images in the image pair and we feed all the image pairs to the uh to the menion stage to the multiscale to the multiscale crop so we extract the matches from the entire image as well as from the cropped regions and finally we assemble the matches and we use them uh for submission and in more detail we uh use the a light detector and the light glue matcher with a limited number of the points we also use similar to the previous here we also use uh uh key points and descriptors cache it helps to reduce uh the time execution another finding that we uh developed is the multi multiple GPU acceleration we found that we can reduce the time uh the time running significantly using both T4 gpus on kull with a mixed Precision uh enabled uh also we use the different scales for the images so the best combination is uh 1,00 and 280 and 2048 uh it was also useful to employ the repetitive uh scene reconstruction to address uh scenes with a low number of registered images uh together with that we applied the horn alignment to estimate the transformation matrix and the final thing is the omn glue which is going to be presented on cvpr later uh so we got the best private submission with the this model this uh model but didn't s it the relation for non-transparency as follows so the best cross validation score is uh 27% uh across the training Data Center and the best leaderboard score on the kle is 28 so the scores are the best uh across all teams on the Kel so you can see that the best cross validation score is a Ensemble of multiple uh of images scaled at different resolutions so it's quite big but for the best leader Bo score it was enough just to make uh to use used to scales the interesting part and the main contribution in our work I guess is transport sces we realize soon that we cannot uh use the conventional svm method with the matches and col map for the transport sces it just didn't work well so our assumption was is to compute the poles for each image in the scene and measure the rotation the rotation Matrix directly so this is the trick that was uh showed in a previous talk and it helped to boost the score significantly so our idea the final idea was is to compute the P of each image to be located on a perfect circle uh with a equal space between the images so the basic pipeline is uh first we have to find uh neighborhood images so the images that are similar to each other so the first approach so we try the optical flow the cray scale pixel level difference the structure similarity measure index and and the number of matches uh the best score that we got is uh 9% for the cylinder it's 92 and for the cup it's 62 so the general pipeline is to compute uh is to compute the similarity for each image pairer get the distance Matrix uh get the shortest path between the images solving the traveling ment problem uh after that we can place the images in the right order around the circle and finally we get the rotation Matrix that's all the interesting fact that we developed the second approach and the second approach actually doesn't use this uh trick with the rotation Matrix so basically we we had an assumption that uh we assume that we can order the images by looking at the number of the matches so the number of matches tend to be higher for the previous image and for the next one uh and also we use the high image resolution to extract all the differences in the in these transparent objects so the highest the highest resolution for the transparent objects it was about uh 4,000 pixels and this approach gave us uh 3% accuracy so the cylinder is 77 and the C is 41 so we iterated different ideas some of them worked on the cross validations some of them didn't work uh which is the multicrop different detectors and matches uh different kind of the models for the image retrieval uh different refinement methods uh non-max suppression uh we also tried uh to run the dism equation methods but uh we did not find uh uh a good a good score for these methods we also applied the so-called 3D ran linion to eliminate the incorrect matches uh and the final is the motion the blueing so some images in the non-transparent scenes had blurred images so that's why we try to eliminate this artifact so finally we got the best public and the private score on the leaderboard on the kle uh we got a huge trick with a huge score uh with a transparent trick the execution time is just less than five hours while the the maximum limit on the kle is 9 hours and the total number of submissions said more than 300 yeah thank you yeah this is the last slide so uh I didn't cover every detail of our solution so if you have any detailed questions just uh go by this link and check out and I'm ready to hear any questions do I understand it right that you figured out that in the training set everything was on the rotating table you believe that this will be the GU the case in the test set and that's the biggest like this is what made you the best uh yes that's right the interesting fact that all the images in both I guess in both transparen sces were already sorted so if you run uh if you just use the same order as the images are sorted in the folder in the training folder and compute the rotation Matrix you already get a high score this was a signal for us yeah any other questions yeah so thank you for the presentation thank you so let's invite the second team um okay let me re hello everyone the video way okay where is the video find me you just open this right like could just show this guy and okay hello everyone it's very pleasure to present the second place solution today we are the location team from auton Alibaba our team members include one and our Mentor here is an overview of our solution we started by designing a new Global Fe nature that adapts to the scenes generating stronger representations for Downstream Tas for regular scenarios we created an iterative optimization scheme for St for motion based on the minimum spaning Tre this approach use concise data associations to build a Rec reconstruction modor skeleton and then we add redundant associations to interative enhance the model's accuracy for transparent scenarios we hypothesize that the camera capture images by rotating around the transparent object therefore we simply transform it into ranking test and directly output the appropriate posess along the circular path specifically we first constructed a stronger Global feature for image tra our basic idea is to adaptively enhance the discrimination within a s this is important for retrieving highly similar samples another ination is that mismatches caused by repetitive textures might be better addressed at the retrieval stage therefore we aim to further enhance the semantic strength of global features by introduce stronger semantic Elements gued by this motivations we first extracted extract P features and Patch features from an image using Alid and D to then we concatenate the patch features to the point at each corresponding location and finally we generate a global descriptor based on the clustering and the V algorithm it is worth noting that the clustering provide a mechanism similar to F tuning the backbone additionally since patch features inherently possess stronger semantic than point we directly introduce d 2 to further in enhance the semantic effect in our tests this method out perform uh state of the art technial such as n analog Doo and salad without considering transparent scenarios and with all other configurations being the same our method also Auto perform net BL on MC metric we will discuss it further impact in the following sections on MST AED sfm and transparent scenarios our approach for local features involves an example of multiple feature detectors and mates we experimented with combinations of lit sub point and doot with dis and sift we found that the V2 version of the doot detector produces richer and more evenly distributed feature points this provided us with the best score in the data Association phase of sfm extensive feature matching is typically undertaken to enhance the Buton and the accuracy of sfm however increasing data associations Inc with repetitive textures can result in more incorrect matches this issue was evident in the Reconstruction result of church things in the training SE in the left image the front and the back of the church were incredbly match together due to similar textures resulting in the P of a church fa failing to register any images in the direct sfm P plan as the contrast our MST did cost to Fan approach effectively resolve this issue so on so on the stage one we built a similarity graph with images as noes and similarities as edges the MST from this graph provides optimal data Association for the initial sfm the left image shows the nose one to8 forming an MST pass around the church separating similar images one and eight in the tree on the stage two we use initial camera poses from stage one for geometric verification filtering incate matches and refining the model this approach keep stage One's cost benefits while improving the fan accuracy finally the two stage MST 8 sfm helps us achieve an improvement of nearly two points for the transparent SC we also use Global features to generate a similarity graph when calculated the mean cost pass to determ the image shooting sequence arranging all images in a closed loop to determine their positions interestingly the TSN visualization SS of a cylinder Global feature revealed a highly Ed 2D layout significantly aing in recovering the image capture sequence eventually the solution for transparent SC improves SCS by nearly six points we also tried some methods that that didn't work as well for example we attempt to add a few redundant edges to the first phase of the MST to balance the recall and position this method achieve a good private sces but didn't improve the public scope we also tried transion methods like the lofter series and Optical flow but the results didn't meet our expectations additionally the due to time constraints we didn't manage to try TTA and multikill matching technical thank you very much here yeah okay let let's move the uh the other team um okay should I share my screen oh oh you already have the PowerPoint okay that's good yeah maybe you can share your screen yeah it'll be great uh could you first close the screen share Reser I can sh mine yeah oh is see like this yeah yeah thank you this check um but I start now or we need to wait for more does it look correctly yeah yeah everything good okay um hello everyone my name is Jan and I'm a p student at the University of Oxford with your geometry group we are glad to share our third place solution in RC 20204 um the solution mostly used our recent work with ggsf the team members include me mingal CH and David um actually to be honest I'm I'm a bit shocked that if I understand it correctly uh does it mean we should be uh the first one if using the correct metric for camera post estimation instead of camera centers and I'm still shocked but let me introduce my our solution first yeah so we did just fin our work on VPR 2024 which has been accepted as a highlight basically it is a fully differentiable a framework and we train it end to end specifically given several frams as input we first conduct Point tracking to get some 2D correspondences then we fit the image Futures and track Futures into a deep camera predictor to estimate all the camera parameters simultaneously finally as in traditional sfm we do triangulation and a BN adjustment to to optimize camera parameters and 3D points at the same time um basically we just follow the traditional incremental sfm Pipeline and convert it to a differentiable way the whole framework Works quite well on various types of input data and especially as far as we know it is the first time that a DP s method can outperform Co map in photo tourism data you know Co map itself was specially designed for photo tourism data um let's have a look at some results for example on the test set of MC 2021 um it is also called MC photo tours in data set there's a clear performance gap between the comat based mths and the dip meths like post diffusion we observed that the main performance difference is due to Joint optimization although these deep meths have their own joint optimization steps they did not work that way as traditional ba therefore we designed with ggs f as a whole pipeline that with all components differentiable including the joint optimization as you can see compared to previous de Mees here I um for example 12 here our performance jumped a lot such as a us at three degrees inut from 12 to 62 it achieved sortal results among all the meths no matter the dip one or the classic one you can see the clear jump in all the metric and here are some qualitative visualization of the reconstructor S the left is MC 2021 and the right is co 3 do set here is just a brief visualization and if you need more um visualizations you can visit our web page we are encouraged to test our method on IMC 2024 okay let's go I first had some trials in my local machine and it works quite well a like plus light glue seems to be the most powerful combination in MC 2024 while in my local machine wjs F Works clearly better than comap plus light plus light glue with an a against um with the Nu 0.27 against 0.24 we had a good start but the bit lesson is I was spoiled by meta cluster we are required to run our code on the cargo cluster for IMC 2024 whose GPU is at most 16 gabes instead of 80 gabes um actually it's our thought that I should notice this earlier that most users do not have a high memory GPU so I think the uh such a constraint from the cargo cluster is reasonable I mean uh 800 8100 GPU could be a luxury for most people so what can I do learning from my trials for MC 2024 I'm developing a low memory version which seems to save the memory by half I hope to release it within this months although um you know sometimes it's just lazy and sometimes the stuffs are out of my control and the second option um which is also one we used for MC 2024 is combining com M plus wijs F we have three ways to integrate wijf into comat pipline we can use the point tracking module from wijf to provide more tracks for comap we can refine sfn tracks which were buildt by comap and we can use wijs f to get the camera poses of the unregistered images these three ways improve our public score from 0 point 17 17 to around 0.20 turn saved us from a ranking of around R 50 to top three I mean in public leaderboard we are um oh in the private leaderboard we are third using the camera Center metric so the first way uh additional tracks for each input frame we view it as a quy image and find a several nearby friends by this we can track the 2D points of the quy frame and look for the corresponding points in the key nearest frames then we fit the tracks into coap as matches we believe this is helpful because a wellknown topic in sfm is how to CH to the P West measures to your track and a such chaining cannot be guaranteed as accurate this is widely observed in a lot of experience we have conducted so we think directly searching for tracks is quite beneficial and complimentary this hypothesis is verified in ic 2024 and we are glad to say this is efficient for a s with 50 frames at Tex Fu in 2 minutes totally acceptable for our IMC 2024 solution the second way was inspired by the Winer solution from theu 3D W last year in w ggs f we use a cost to F strateg for Point tracking for example um with some as as can using here we first we first uh attract some C tracks over the Image level over the whole image then we crop the patches with a size of like 30 by 30 around the cost tracks and refine the point locations within this range it means if we run um I mean at the same time if we run comat it will have some three Spar 3D points and a corresponding 2D projections this 2D projections are actually sfn tracks and they can be refined by our fun tracking model so that's what we do and in our cargo sub Mission we only refine the 4K Sant tracks for efficiency but it is good enough refining each track will take around 0.03 second so for each SC it is around 90 seconds in total acceptable for us and the good news is we can do this easily and totally in Pyon we do not need to recompile coma for it I believe that in um in recent years users are freid of modifying C+ code or compiling by themselves yeah I think this um important for a lot of usage and the Third Way is quite straightforward we all know that Co map usually cannot register all the images of a SC so we use wi F to localize those unregistered FRS the general idea is for each unre frame we can find some close frames that have been registered by comap given this we can build a small subset and run with J F over it easy and S some but sometimes the unrejected images may be too challenging if so we skip the bond adjustment uh step in wgs fan and only pick the predictions from the deep camera predictor once again we ensure the whole process is fast enough for each unrejected friend will take only three to 5 Seconds um and you know unre the number of unre friends will not be too much yeah overall such integration works well it is very happy to see our method can really work in MC 2024 whose data is totally unen to Wi GS F and the super challenge um but personally I still hope to try using WI is for only in the challenge last year MC 2025 so um thanks for listening and thank you again to the organizers for this great challenge this has become a standard and mastering Benchmark in matching and sfn I would say it is a fantastic job and yeah one more thing is we are going to present our poster this Friday morning Chris and David would be there to present the poster I may also be there probably in an iPad or laptop if the internet connection is good enough so feel free to say hi if you have any question or want to discuss any topics there and feel free to send us an email or create a GitHub issue I check GitHub issue every day thank you and question time uh any question yeah I'm sorry to hear that you can only use iPad no I no yeah since there's no question um well uh yeah maybe I have a simple question so do you have any slide to make uh you know your awesome with GSM more time efficient so like do you have any insight like U how how much room left for you to improve the time efficiency um yeah I the question I heard is about the tie efficiency or is it about the memory efficiency well like uh maybe both memory and space the same so have a you know L larger footprint in a memory and also looks like um yeah more computation intensive right if I understand yeah um first regarding the time efficiency yes we do believe there's a lot of room to improve because uh for Simplicity I keep all the code and implementation totally in Pyon which means okay um it's will not it will not lead to uh maximum speed and an idea here is that probably we can uh write some like coders or something to accelerate a lot and another way is uh currently we use uh around 4K tracks for for each corer in I personally believe is this is too many probably we can reduce it to quite a lower quite lower number and which means uh ideally we can further reduce the time for uh the time for inference yeah this is for the first question and regarding the GPU memory um yeah this is a big problem um I did learn some lessons from MC 2024 and I hope we can release a new version quite soon it will save the GPU memory by half but even by half it is still not that friendly so I hope to we can serve this um in the second version of wijs fan a potential solution is that um we can modify with g f to like kind of closer to traditional s which is um the wide and Concur thewi and concur or a cluster by a cluster a group by a group by doing this way we can further reduce the memory consumption Z I hope this can serve some consense thank you yeah thank you for the answer and thank you for the great uh presentation again um yeah stop the screen sharing and give it to the next one so let's move to the other team I guess the team is the zg you 3D um is any uh like Au here or are they remote can I hear me oh you're in the room great you want me to share the screen or you share screen uh I will share my screen sounds great yeah I guess you can share right like yeah go ahead can you see my screen yeah yeah okay okay can go ahead I will start it hi everyone we present our six Place solution in IMC this year our team member in this compation includes and here is an overview of our pipeline but General things we continued the winning strategy from our IMC 2023 solution using dfsm a two- stage and a hybrid matching strategy for robust matching I will provide a detailed introduction to dfsm later but transparent since we identified the patterns in the camera trajectories for transparencies in IMC 2024 by sampling the camera centered positions on a circular trajectory and then recovering the order of each image we were able to estimate the 3D poses of the images let's first talk about our solution for General sces in the matching phrase we first construct image pairs by the global descriptor then we detect image Rotation by performing a lightweight Spas key Point detection and matching on rotated image pairs the pair with a max number of inliers are used for the next stage matching based on the rotated pair and the first stage in lers we cop the overlap region and reside image to align the scares then the second stage of matching is performed using both detect based and detect free matches last year when we used the semi- matching model such as loter the cost to fine maging process of lter resulted in the multiv view inconsistency problem this year we adopted dance matching models such as dkm and roma although the matches generated are all as the pixel level since dkm and roma sample matches from flow they also have the multiv inconsistency problem therefore we also addressed this issu through a cost to F architecture okay in cost mapping phrase our idea is to sacrifice match accuracy for for consistency by match merge and reconstruct a cost model then we improve the post and point clock accuracy by our iterative refinement pipeline in the C sfm3 we first aggregate matches of each image by its matching pairs then for each image we perform anms over aggregated matches and the merge matches to the point with a Max match confidence we reconstruct the cost model based on the merged matches then given the cost model we refine it for higher post accuracy by our iterative refinement phrase our idea is to iterate between multiv matching refinement model to uh refine feature tracks for higher precision and use geometry refinement to improve process and point color Point Cloud accuracy based on the refined checks next let's dive into Future Track refinement module the inputs of this modu are cost to Del locations of feature tracks and the outputs are their refined Tod locations uh features of corresponding image patches are extracted and transformed by our multiv view feature track Transformer then tracks are refined by the multiv view feature correlation more details can be seen in our search paper here are our results we used super point plus super glue for SP matching and roma or dkm for uh dance matching due to the original Roma being too time consuming we replaced the fish extraction backbone of Roma with vitb and ret Trend a model the results showed that our R TR RAR could achieve better results however we found that on the leaderboard our scores are quite low we believe that the transparen things are the cause of this issue so we have tried to process the transparen things with special meth methods by observing the characteristics of the camera Center trajectory in local transparency we designed a specific strategy for transparencies that is generating a circular camera trajectory and recovering the order of the images the first problem is how can we distinguish this transparency on CLE we use the token card which is a segmentation model to perform foreground segmentation on each image if the foreground areas segmented by from all images are roughly consist consistent it's indicates that the camera trajectory for this SC is approximately circular and we mark this thing as a transparency and the next step is how can we recover the image order for transparencies we designed a strategy based on the image similarity Matrix plus tsp algorithm to recover the image order the key step is to estimate a similarity Matrix that is closest to the ground truth we try two methods the first method is using a retriever method meod such as net we lad to calculate the global fature similarity of images and build the similarity Matrix the second method is employing a SP matching with run approach using the number of matches generated between two images to represent their similarity here are our results if using a lik with NE resonable matching to recover the image order we achieved the highest score of 0.225 on the public leaderboard which ranked third and the private leaderboard score of 0 one91 which ranked six regrettably although the strategy using sift with nearest neighbor matching to recover the image order only scored 0.207 on the public leaderboard it reached the highest score of 0.201 on the private lead about which ranked first okay that's all thank you for your listening thank you for the presentation uh any question for the audience yeah maybe I can ask a quick question so what are potential uh Works to improve you know uh the pipeline can you say something about this uh well uh this job I I mean dfsm is uh done by my teammate sh her uh uh in my opinion uh I'm I'm new to this competition as an uh member of the their team so in my opinion uh I mean the feature of uh sfm uh all the Improvement of this pipeline should be uh turned to uh such as wsfm uh we need to do uh end to and optimization to uh make our model more robust and uh uh less affected by noise yeah thank you for the answer um yeah thanks thanks again uh for the present presentation I guess now let's move to the uh last team okay I will stop my sharing thank you thank you very much last only video okay yeah let me share my screen hello everyone today I am presenting my solution that secured the eighth place in the image Mar challenge this year let's dive into the details this ride provides a overview of my Approach I started by finding similar image pairs or image matching this solution improves to stage of image matching using a lik an light GL model the stage improv various Transformations and open transform to enhance keepy key Point detection additionally I performed the Reconstruction using mple model cameras including simple radio and the simple pin hole finally I selected the largest map and extracted the camera position and orientation from this map this slide explain the process of pass stage of image matching first I focus on the problem of rotation tolerance with a like under light gr method I executed image matching on the original image and their Lo version according the central Ro it was observed the number of matches key Point dropped significantly when the image were rotated between 45 and 60 to address this problem I adopted the TTA processing by loaded in the image by 90 I perform the image marching by rotating one image to Z 19 118 and 27 degrees then concatenated these key points this slide explains the process of the second and the stage of image matching in this stage I enhance a number of key points by correcting image rotation using a p transform but uh from distribution of key Point detected in the P I calculate the bonding box right green and crop meage one with bonding box next I estimated The homography Matrix from the keing points using this estimated Matrix I appli the up been transform to image two to collect it rotation related to image one and then apply image marching to the corrected images through this process I observe the increase in key points by a factor of 2 to 10 here I present example of several images on the left are the original image pair and on the light of the transform image appears the first example is an image of building hidden behind the tree leaves the second is uh is rotate loing and the third is Al Imes in this cases I confirm the up Transformer using the homography Matrix could correct the image rotation and result in an increase number of key Point against the P stage this FL explains the Reconstruction process with Cola I noticed from the ground TR data of training data set that two type of camera model are used simple radio and simple hole as summarized in the table therefore I execute the Reconstruction using both simple radio and simple P camera models from on the maps generated by B model I adopt the map with the largest number of images this R highlights the performance Improvement achieved through my method I started with a Bas rine M A score of 01544 by incorporating TT and rotation 19 and image matching F stage and correction in the second stage and uh reconstruction with marriage camera model I incrementally improve my score to 01799 on the private leader board just conclude the explanation with my Approach I extend my gratitude to the competition host KAG and the public notebook compor for the support thank you for your attention yeah yeah I guess uh it's time to W up our Workshop um let me share the the slides where uh so yeah sorry actually I uh it's not I didn't make the slide so I can just try to to um yeah read this ler um well what's next uh after this Workshop um first of each AR magic has pros and counts uh and we are thinking about how to improve it and also we are wondering if we can truly do the large scale because large scale since now has 100 uh images to allow Evolution running time compatible with cago constraints and thir question we are asking is that uh should we move to an Open Bench Market with open data this would be this would probably lead to wider spread and Community collaboration and support including issue detection and fixing as well as new and better image imine challenging and solutions and of course we always you know I like to have a like a round uh discussion and yeah please you know send email to us uh here and let us know and thank all for the uh like join this Workshop yeah so recording

