---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=34m5mH1Ug7s"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:00.254Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=34m5mH1Ug7s

246159b3-34bb-4cd1-adcb-8f34f3bbc18d

2025-10-28 https://www.youtube.com/watch?v=34m5mH1Ug7s

1fe02481-ae75-4d01-ad20-024d576ae461

https://www.youtube.com/watch?v=34m5mH1Ug7s

34m5mH1Ug7s

## ComputerVisionFoundation Videos

good afternoon everyone and thank you for joining us uh for our tutorial very very excited to be here with you so um let's get started uh my name is Orin Aiden and um I'm going to be one of the one of the presenters of this tutorial I'm an assistant professor at St Louis University good afternoon everyone I'm I'm Philip GS I'm with the oration National so be also one of the presenters today we're going to be covering the this agenda that is going to detail a little bit now I think everybody had enough of our Dynamic header I think you looked at it a million times already so I'm just going to scroll down a little bit uh and assuming that you've taken a look at the overview since you're here just talk a little bit about our schedule and what to expect um we will quickly introduce basically what we're going to cover today in our opening statements and we're going to start talking about Earth observation data and I'm assuming that you're aware that there are there are a lot of great um Earth observation sessions tutorials that are either happening right now or about to happen um and we want to focus on the data first what do we measure how is this different than some of the other types of computer vision data that we're working with uh so to really set the stage first with the data and you're going to have a small Hands-On section and if you go to that link that's that links to a collab so with this tutorial all batteries are included so to say so you can just go to this collab notebook launch it the only requirement that you need to have is an AWS account it doesn't need to be like a premium account you just need an AWS account if you want to follow the clicks but if not you can still follow along in the notebook and the notebook will be there for your reference um we're going to have a copy break and unstructured Q&A um what's going to happen there is if you are following along the notebook but if you want to explore some other options if you have questions specific to this notebook I will be here um and also um one of my students who's supporting this event Donovan will be here uh to answer your question so that we can answer any like data IO related questions oneon-one and after this first half we will quickly we will start talking about machine learning methods machine learning tasks how does it fit into the data that we discussed um talk about Foundation models and going from Models talking about applications what does this mean for sustainability what does this mean for resilience and we will talk about some Hands-On examples um sorry applied examples um and we will end with some remarks and an exit survey I know nobody likes surveys let alone exit surveys um because that J stands between you and the exit door but it'll be very helpful input for us um to make this U Workshop more tuned to uh the needs of the community uh if we if we do this again next year so that being said um we will get started jump to this me jump to the slides here so let me let me start with a question how many of you know knows what is a earth observation what this is useful for did you have okay so just start with that you know what this is useful for and you're curious about it you're here to learn more great how many of you have any experience playing with this data like getting your hands dirty a little bit seems to be about half and half so that's is still within our expectations so myself I have experience with this data for about three almost four years now so I come from more conventional electrical electronics engineering background computer vision that since 2015 16 has been dominated by Deep learning and so on kind of Follow That Trend but also more working with the O creation National Lab with what we call application oriented type of research so that includes parts of like how do you select your data how do you do the labeling do the training and the planer models but also some postprocessing and assessments of quality and so on so those are some of the things that we're going to be touching particular at the end when we talk about this type of real world usage of these things some of these steps Beyond of just training and deploying your model for a single image so to say uh but we start what we want to cover is giv a little bit for those of you that maybe are not familiar yet very new to the Earth observation uh type of data what we're talking about here is essentially this idea of gathering information about the physical chemical and biological systems of the Earth and in particular this idea of remote sensing is very useful for scenarios like you cannot simply go to a certain location like in the middle of the Arctic or the middle of a desert or middle of a disaster scenario and try to take uh whether photos in like on the street level or getting some other sensors so having this ability of leveraging satellite or AO imagery by an airplane like doing survance of the place can be very informative for responding like to disaster scenarios or doing more this kind of larger scale assessment of this features to guide policymaking so so they has a lot of applications that can range from agriculture Disaster Response human planning humanitarian assistance and so on let start these examples here with some of the work we do at the National Lab which is working with a lot of uh folks from the geospatial side whether intelligence or population modeling that you want to understand where you have maybe a constructed areas where you have buildings in particular you you can think of well population you already know the sensus of allocation but that gives you just an aggregated information a very high level if you want to have a more fine grain estimate of where population are leaving and maybe do that of more finer uh temporal resolution than just rely in a census every four to five years that can be a very informative proxy that you can then aggregate with other types of geospatial data and construct this kind of very high resolutions uh mappings of populations across entire countries or even the entire world as we've been doing like with the land scan project at right now and then you can go beyond that and look as well at Road mapping that is very important when you think about Transportation networks you can think of scenarios like flood let's say what areas have been blocked or not so you can think about eight going to those locations and you can think about more fine grein attributions of for example the buildings try to understand whether something been used for commercial purposes or residential purposes because that can be very informative for folks more like from the geography side to understand maybe the wealth of a certain area or what are the risks and vulnerability assessments and how you can start to think about policy making around those features some other very interesting applications are more around uh environmental or land use understanding land use land cover classifications a very classic problem in this domain of computer vision and Earth observation you can think of things such as understanding where you have areas being used for agriculture or areas being more urbanized the level of urbanization and then start understanding over time patterns of deforestation or uh uh what areas are getting more densely populated and so and in particular I think these types of works on uh Disaster Response is a very uh rewarding space to be working one and what it's unfortunately will be more and more important when we think about climate crisis scenarios and the ongoing conflicts we have around the world there is increasing frequency and need for this type of mappings in ways that you can as fast as possible provide information to people on the ground doing humanitarian assistant or also thinking about accountability purposes in some other scenarios other scenarios you can think as well is uh trying to map uh maybe populations of features that are unseen so to say you might have maybe some uh volunteer based information like open street map that for some areas people can go there in this sort of open Wikipedia and select where you have features like schools but this tend to be sometimes incomplete so you can have works that leverage uh satellite imagery or types of imagery computer vision techniques to actually put shed light on these uh structures that are unseen so to say mapped and a lot of work more on um conservation and so on one uh if any of you were maybe in the morning in the earth Vision session there was a great you know talk talking about how computer vision and machine learning General pair with EO data already plays a big role and continue to play a lot of role on trying to understand these aspect such as uh compliance with regulation of the usage of the land where you have some uh maybe illegal uh plantations or you can also have this work like was done recently few years ago by NASA finding vegetation where sometimes you wouldn't be expecting so to say to see like what you have in the Sahara and the sah and this kind of information can be relevant when we think about not only sort of deforestation but also thinking about uh overall the carbon cycle and also those risks that you might have maybe how much fuel have in certain area for wildfires and so on similar again here this aspect of trying to conflate mappings of one type of vegetation and what should be the information have for maps in terms of Regulation other types can be more around uh Wildlife monitoring as well which is a very illustrative example of how hard it can be to play with Earth observation data it's not that nice uh picture of your cat in the very center of your image like you see those image Nets and the like things get much more complicated in terms of scale distribution and so on and all of this that I show was mostly just with RGB things are not only RGB things can be radar for instance I'm going to detail more how radar can be more advantages compared to just Optical imagery but this is one example of one challenge the xu3 that was uh one or two years ago on trying to map illegal vessels so these are vessels that sometimes they're not emitting what they should be in terms of communication by radio and they are maybe fishing in zones that they shouldn't be fishing so it's a very hard problem to actually have some monitoring what's going on and plenty of other applications that we can think Beyond just this imagery part but also understanding climate patterns of whether you have uh what is the forecast you certain features or also this idea of super resolution you can think of it helping getting finer grain Maps as compared to corser maps that you would have constructed from other sources so all in all one of the things that we're going to discuss is how there are many analogies that are some categories of problems that from the broader computer vision that they're still here and there observation but there are plenty of extra challenges on how you handle those so those are some of the things we're GNA cover in this tutorial we don't have the time half a day to do everything Hands-On so we're going to have a Hands-On session in particular like how do you can use this type of data and start to play with it but then we're going to try to cover many bases of resources you have available that you can search out there data loading visualization processing the types of techniques used for each of these problems the extra tweaks that people do and so on and then the other thing that starts to be fun to think of is like these extra challenges I mentioned that it's not only RGB so you start to have issues even if you're just dealing with the RGB with the optical spatial resolution can change a lot you can have uh images from certain types of sensors that each pixel Cor on to maybe half a centimeter which is great image to work with for satellite purposes if you're AAL maybe you're going to reduce to tens of centimeters and so on but then you also have uh sensors that you're then operating maybe a kilometer or 100 meter per pixel and ideally you don't want to be training specific models for each of those so this generalization issue becomes a big problem or even if you have the same Spectrum resolution your view angle might change according to the position of your satellite and that can easily again break models that are just trained for one type of of uh of uh view angle you can break even your mind too if you try to delineate the exact footprint of a building here in this view angle it's a pretty tough problem even for a human to to handle and then this different diff uh multiple difference in the amount of channels you might have from hyperspectral data and so on so to illustrate just a bit further before we dive more into specifics of the data I just want to show more examples of this contrast between the conventional so to say conventional computer vision data sets and the things we have to deal with so here I have a series of examples that you're looking at the same area with this type of sensors so you can see how is more familiar to us like the red green and blue kind of thing but then you can start to think that there are many features that can be interested to start looking at the infrared things such as more understanding better vegetation patterns and how it start even like how do I visualize those things I can process as a human like three bands kind of things so I replace my red by the near infed or what do I start to do and then you deal with bunch of code bases that you cany start to use from the broader computer vision you put your data there and you see that's not working because things are hardcoded assuming that you have three Bandits things are just not like this when you start to deal with other modalities you have other types of data as well that you have some sort of combination of them like the ndv this a vegetation index you start to aggregate information from different types of uh bands sensors in your spectrum to have then a better way to visualize and handle this data and then you get to the fact that you have different sensors being better for different uh benefits or different uh disadvantages one classic example that we have is this notion of optical and synthetic aperture radar so when you think about Optical dat is that's what mostly we are used with it's essentially what we call passive sensor you're taking pictures that essentially the reflectance of the sign illumination that you're you're measuring then right uh but then you can have issues such as cloud cover so you are very interested in trying to get images and process as quick as possible for a certain I don't know flood event or wildfire that happen a certain location you get the image and then you look it's pure cloud and that's a problem that radar doesn't have because you have an active type of sensor so essentially you are emitting pules and measuring the reflectives so this can be great for those scenarios where you need to assess changes as quick as possible and you have these issues of like cloud coverage in particular like in the tropics but you can see that it's a much harder type of data to work with there are issues such as what we call uh this Speckles such you can see like this kind of cross in the middle and there are issues such as like when you're measuring reflectance from the radar you can imagine that you have different structures of different heights they might reflect in such a way that a single po or pixel in your sensor is actually capturing reflectance from different parts of the surface so that gets pretty tricky even the physics underlying it and how you can try to enable this kind of reasoning across the different types of data here's just another illustration same location different types of data how things can look wildly different so ideally you would like to have a multimodel kind of model being able to handle that and that's another thing we're going to hit quite a bit today that multimodality can come in different flavors when we look about this type of data we tend to think nowadays of multimodality is uh synonym for RGB and text but no you have multimodality issues that can arise from different channels different spatial resolutions temporal resolutions so on and so forth points pixels many many variations I already covered this uh I will let orun speak a little bit more to that but essentially this aspect that you can have different sensors capturing focusing different types of spectrum because they can be beneficial for different types of applications and then I just want to hit in the temporal part because this relation between spatial and temporal is very important to keep in mind because you think of well we have these different resolutions of the sensor so let's try to just use the best the finest resolution you have but that becomes a trade-off because to you to cover such final resolution your your speed of how often you can actually get this it's sort of inversely proportional you have more coarser satellites are going to orbit the earth more frequently so here's just a visualization of how for example for the sentino 2 that's a 10 meter per per pixel in average type of mapping how it takes how the orbit uh it's formed essentially for you to remap the entire world takes you about about five days or so and then you start to have already issues of um you have this data maybe coming from different sensors you want to be able to work with them and the good thing about uh geospatial data essentially is that usually comes with metadata so this means you know where was the location that was taken the time some features such as the orientation angle of your sensors and so on but then you can have issues as well different types of projections of how this data is coming so if you just try out of the box to try to align things one is represented one type of world representation the other data is represented in another way and then how do you do project things to the same space and do this conflation such that you can do this type of multimodal reasoning becomes another tricky thing to do so that's what I wanted to cover in terms of motivation hopefully you find all those applications exciting and you're curious to know how the hell you handle all these different problems so we're going to start talking now about uh these more nuances of what you have with your data how you the relevance of these different types of uh sensors and uh then in a few minutes start to get our hands diry with actually loading some of those SE that's what I had to start and now I'm GNA let you guys before thank you thank you s so let's talk about a little bit in detail um AR observation data some of the examples that fipe show are very striking examples there are all these different problems we can solve with Earth observation data and when you go to for example uh us gs's data hubs to get say Sentinel data one question you might have is well what is the data am I looking at is this raw measurement is this process somehow because if you're interested in uncertainty you probably want to know those processing steps the parameters Etc and you may want to understand how uh what the variation might be so before going into uh so I would like to talk about the details of this and I want to start with really the basics of um Earth observations um just to give you an idea of some of the key Concepts so one common terminology we use is this broom versus push broom and this this basically is an animation from um from NASA that I would like to quickly show so when we have a vit broom sensor you will see that the sensor is not just getting a pixel at a location as it moves it's also doing a lateral scan and all of those images are being Mosaic together to create this final data product the algorithm behind processing Vis chome sensors is is uh is a little bit more complicated but the equipment itself is the sensors themselves are easier to use and when we think about push broom and you can really think about a broom here you will see that there is no lateral scanning anymore we basically get slices of images as the satellite moves so Vis broom so not only you're moving forward but you're also doing lateral scans and that does impact data processing versus push broom so those are two important things to think about and in the final animation that fipe showed you have seen how these satellites are moving so as they're moving what we are basically uh doing is uh for those of you who are very new to Earth observations something that we've been doing for uh at least a decade now on our phones which is uh creating panoramas right when these when functionality first came out you will take a picture of a scene and then you will slowly move it very much similar to this continuous data collection from a satellite to Mosaic everything to put everything together to have a global view so there are a couple of Concepts to talk about here the first one is the swap and again this is from this animation so what is the SWAT SWAT is basically this area that as you're moving that you can visualize with the satellite so that's what we refer to as a SWAT and this is particular to lanat a SWAT is about 115 Miles by 1710 miles and we put these swats together in the end to have a global image so how long does it take well you've seen this in the figure approximately for lat 8 we have have 16 days so in 16 days we can create an image that is also quite different than some of the image problems you work in for example with web page data um I'm seeing a lot of you taking pictures of this which is great uh when you take a picture of this it's the same snapshot it's not like one half of this board is from two days ago and the other half is from today right but when we look at global data we have this issue we don't have a synchronous snapshot we can't because you are obtaining this data in swats so in a 16 16-day revisit time you're going to have this you know higher than two week latency in certain parts of your data so these are two important things to about talk about this a lot and if you're new it's just a good to have an idea of how to visualize this and what this means and we talk about global observations again remembering that the global observation does have this concept of revisit time F also touched on excellent concept he was talking about an active sensor for example some of the uh synthetic apperture radar sensors that we use versus passive sensors um for example with last set data set we work a lot with passive sensors so what do we really mean by this again I'm not going to be talking about any of the physical equations but I want to give you a working knowledge of what we mean here so when we have a satellite and we are we are Imaging the surface with passive sensing we are recording electromagnetic radiation what does this mean we have we have sunlight we have reflection and then we record Its Reflection so this is this is really the principle behind uh behind what we do with passive sensing but of course there are problems with passive sensing again going back to the analogy of you taking pictures of this Imaging this uh imagine that you're a little bit far away like the satellite is far away so how far are these satellites only 408 miles away about 750 kilom so they're pretty high up there so the distance is quite high and then you can have occlusion problems such as clouds clouds might get in the way so cloud cover is a big problem with this type of Imaging because if you have clouds in the middle there is going to be dissipation and you're not going to be able to see under those clouds and another very important concept here is Terrain so how do you do terrain Corrections the reflection is easy the equations these equations are actually much easier to solve if there is absolutely no elevation change to the world especially if it was flat it would have been a very easy to image Planet it'll make our lives much easier um I can see some of you uh really hoping that that was the case because it does create a lot of headaches for those of us who process data day in and day out but we have we have curvature and we we need to have these terrain Corrections and this is another important part that we have to keep in mind the terrain correction we have to we have to take into account what we call the digital elevation model how is the elevation the terrain is changing because it changes the signal that we measure it changes the travel time of this signal it it impacts how much it's going to be dissipated refracted so this is quite important to understand and one last thing one simple another simple point I want to make is Earth observation doesn't only mean satellites we have many different modalities of Earth observation and they all have their places so I want to talk about this a little bit um we have drums this is example this is an example of a maybe a quacopter hexacopter you name it these are very useful then you need millimeter level data for example Precision agriculture if you want to understand how plants are growing over time if you want to measure their greenness or a plant disease or if you're out in the field doing geological surveys and looking at tiny fractures you need this type of information you are not going to be able to capture this from selles and we have large area surveys U there are great examples of this the one that comes to mind is Gio the global aerial Observatory of Arizona State University this is basically a dornier plane that has a lot of sensors satellite gr sensors attached to it so they do flights just like a regular airplane um and they collect data but when we compare UAV versus um the plane there's already a resolution difference and there's also this response time that's very important for example if I need to image this area with a drone I can fly it whenever I want and image it if I need a satellite image of this conference center right now while if the satellite is not passing over this conference center right now that's just not going to happen so there's also this concept of being able to get the data when and where we need it and the last one is satellites this gives us global data which is very important to solve these large scale problems of course there's less frequent revisits and there's no response time as I said you don't get to control um I I can basically be capricious about when I need my data I'll have my data when the satellite is flying over it so another thing I want to talk about here and this is really mostly for reference um we are going to be sharing these slides mine will be available fully I'm not going to talk on behalf of orid National Lab but I know that there they're going to be coming later at some point um so the world reference system so I was talking about how satellites follow a trajectory one thing we need to know is okay for a given location say Seattle for land at eight satellites when is it going to be flying over at this particular location well there is this uh referencing System since we're going to talk about programming in a little bit if you're thinking about this as a 2d Matrix you might think this as at what time it at which X and Y is the satellite going to measure so we have this row um row and path indices so the path index basically defines in the longitudinal Direction where the satellite is going to be whereas the row defines latitudinally where the satellite is going to be so this gives us a data frame with which we can collect satellite data and with which we can analyze the data so understanding uh row and path is very important and there's a unified system for it it's called wrs or the worldwide reference system so here's a closeup of these the different colors obviously indicate that the variables changing so path is going in this direction and row is going in this direction so if you need to know what patent row index a specific place on Earth is you can find it it's kind of like being able to say well Seattle corresponds to this row and this column of this two-dimensional Matrix we know we have the structure for our data set so today I'm going to talk a little bit about landat um because I built the Hands-On example around landat but the concepts are very interchangeable but since we're talking about lat I also want to mention that when you look at different data spans for example if you need data from the 90s versus 2000s or 2010s probably the sensors themselves will be different we are kind of used toess with with our phones then you look at the pictures that you taken those Kodak cameras in the 90s the resolutions are different the images are different if we kind of see that with the sensors too the resolutions change type of data that we can collect change and and there is this span of data that's being collected with the lanat series so right now um we have lanat 8 and lanat 9 is going to be coming online too soon hopefully but right now the latest and greatest is lanet 2013 lanet 9 is out I haven't worked with lanet 9 data yet but currently we work a lot with lat 8 data which has been um which has been in orbit for over a decade now so this is going to be my only text overloaded slide but I think it's important so let's let's let's dissect this because when we do our Hands-On section this is the type of text that we going to be bombarded with and this really just kind of looks like cling on so let's see what what these different denominations mean you will always when when you get a lat image you will always get this l so that just means that this is lat data so this L never changes and in the second position you will get information about the sensor type so what kind of you can think of this as what kind of camera for example is being used what kind of sensor is being used to collect this image it can be C for oi tis and I will talk about what these sensors mean in the next slide o for oi only t for TRS so I don't need to read this one by one but the second position is basically the type of sensor what does this mean if you need Infrareds you probably need specific types of sensors so you should be looking for those sensor codes and the other one is the satellite mission number in the previous slide I talked about these different generations of lanap are do you need lat 8 data or are you requiring historical data where you need say lat four or lat five sorry lat five so you can enter that or you can see that in this mission number and the L's here they're generally overlooked but they are very very very very important I Cann highlight these L's enough so processing correction level one point I made in my initial slides is terrain is important you have to correct for terrain so that you're looking at a quote unquote correct image so what we always want to see in this lll slot is L1 TP terrain and precision it's Precision corrected and there's terrain correction and also the data provider is confident about this correction because they run correction algorithms and just like any algorithms there might be things like overflows there might be non-con convergences they might have issues when they're processing this if they don't run to any of those issues and they're confident about the correction you will get l1p this is the highest quality data that you can get if you get to L1 GT you can think of this well there's there's terrain correction that's being done but then there might be some problematic areas to that image so globally overall it's not at the l1p level uh but it is still terrain corrected L1 GS means that terrain correction was attempted so Corrections were attempted for this image U but they were not done they could not they could not be corrected uh that can be due to many issues with the signal itself um but but one thing you probably don't want to do is if you're working on this research project or your company's working on this project basing a lot of deductions on L1 GS data U because the corrections can actually make a big difference so the highest quality data that you should be looking for is l1p and what we see here with PS and rs these are the path and row so they change with geographic location as I said Seattle has a specific pattern row index so for the same satellite everything it just basically gives you the spatial subset where is my data coming from so for different areas you will get a different coding to this and the capital year signature is the acquisition year so when is this data collected followed by when is this data processed you might actually see data from the same row same path same collection year at different processing year then generally if the same data has a different processing Tim stamp um just the general rule of thumb is its quality is probably higher because it might have been reanalyzed and Rec corrected to a higher tier and also we have the collection categories RT is real time which means that you can get satellite data captured over an area of interest in real time like today and then we have tier one and tier two that add some time lack to this data that's being collected Ed we will see this in our Hands-On example but I just want to give you a reference kind of like a look back of where to go and you can find this also in US gs's manual about lat all these lat satellites like any great piece of electronics they come with handbooks and manuals that you can read if you care to read them so you can find this information there as well so we talked a little bit about sensors so let's talk about these sensors uh oi is operational land imager so it produces data products at different resolutions so it has a 15 meter panomatic image and 30 m multispectral so multispectral um Philip has some great examples of this he was talking about RGB how these channels these RGB channels can have different types of information like infrared in his example so this is multispectral basically collecting data from different segments of the spectrum so we have this type of image and global view every 16 years as you now know they collect data on a swap so every 16 years we can produce this imagery TRS is thermal infrared so if you're collecting thermal observations from the surface of the Earth temperature information it is a it's at a lower resolution it's at a 100 meter spatial resolution and again the SWAT size is is the same actually the SWAT size in all these is the same with the exception of etm plus and just a small picture here this is the Cub detector array so this is a part of the ti RS sensor uh this is just to say that all these different uh projects have very specific sensors that that continues to correct collect data etm plus this stands for enhanced thematic mapper again it's a 50 meter panomatic 30m multispectral so we have the 30 meter resolution and also it enhances tis in that it gives us 60 M thermal data so this is a newer age sensor where we can collect this type of higher resolution thermal information at a smaller swap one of the sensors that you have seen sensor types I'll quickly go back is TM so TM is basically succeeded by em etm plus so this is a new rage one and the last one is the multispectral scanner um so you can see that this is a rectangular box so it's not Square to resolution the pixels so this is an 68 M by 83 M resolution and can get global view 16 to 18 days depending on the type of data that's being collected and just to give you an idea like what does a 30 m resolution satellite image look like what is but this 15 meter and again I stole another image from USGS because they explain this in a great way so when you look at a ballpark so when you look at a third 15 meter you're really inside the player area right 30 meter you have all the bases covered and 100 meter you're looking at the field so just to give you a sense of the resolution the highest level that we can collect right now is with with the landset is 15 met so you see that this is still a pretty impressive resolution given that this statea is being collected about 750 kilometers away from this point and globally so that's all for the quick introduction um for Lancet that's not good maybe that was a sign for me to stop talking or keep talking or maybe yeah either we'll try both um so with this being said I'm going to jump on to the Hands-On section where we will uh walk through this uh callab notebook just to give you an idea of well how can you utilize this data that is publicly made available um to us so you can find the link for it in our um web page and as I said if you want to follow this along all you will need is an AWS account if you don't have an AWS account don't worry I will still talk about um when you start an AWS account how to use it to get to this data um as I promised the batteries are included in that the the libraries that we will use are here um they are not a part of Google cab these are not standard python packages that cab ships with so we will quickly install these um one of them is the pack client and other one is raster iio um these are these are a very frequently used packages and as I mentioned this notbook will be available for your reference anytime you visit the conference web page so if this is your first time running this notebook and if you do want to follow along you can go ahead and run this in a callab notebook it'll install these packages for you and one thing to note in the rro iio is actually has an S3 module S3 is in an awss bucket uh so here is the great thing um these remotely data products there are just terabytes and terabytes of data flowing in every single day so it's really hard to keep it on your drive and you don't have to if you need data from a specific location specific time you can just make a quick paying to the S3 bucket that has this data set at the time and location you need it and you can just get it from the cloud to analyze it so you don't even have to store this data so for those of you are following along um this might take a minute or so to install these pack packages so let's quickly talk about what's happening here so lanat is a USGS product uh and they are they have this in a server so what we are doing here is basically connecting to the c um server and getting different types of collections so when we look at collections one thing you will see is L2 L3 L1 um yes sure that's a great idea better can we see in the back no sort of let's see yeah okay great yeah thank you for that so we have these different levels and as I mentioned these different levels also give us an idea of Thea quality right so this is this is important to think about so one thing I'm going to do is again um for us to not staring at the the wheel of death just spinning and spinning you're going to get a small spatial subset of the data as opposed to getting data globally and this is also a good moment for probably if unless you're looking at a global problem if you need data over an area um you can subset this on the server side what do I mean by this you have Sentinel 8 data globally available um every 16 days but let's say that you need only data over the convention center for some reason or Seattle right you can only ask the server to give you the data for that location and nothing else so you're not going to be moving terabytes of data in and out that already helps quite a bit so this is one way to do that where we can define a geometry with the coordinates you can think of a bounding box where you have minimum latitude minimum latitude maximum latitude maximum longitude so if you're basically defining the four corners of that rectangle here so this is somewhat of a mystery for now what are these latitudes longitudes correspond to um and and we'll do a little quiz when we look at the images to see if you can determine where in us we are looking at this is a place in us but this is a bounding box basically um these are the bounding latitudes and longitudes for it so now we're going to make a query yes it's a good question so um for for the recording I'll repeat the question the question is like is there any problems with projections and mosaicing when you use U when you use something rectangular because that's a great Point uh what we're Imaging is not is not rectangular so what we're basically using is Mosaic image that's being served so if we are not actually grabbing swats necessarily that intersect and then Mosaic on the Fly we already working with the data product itself and getting that subset that's a great question so after we defined as bounding box again when you visit this or revisit it I think this is the part that you will be modifying the most so how do you make a query how do you get satellite images because this is us asking the server this is my criteria I want lat imagery that intersects this geometry within the founding box in this time period so um basically I want one year's worth of data all the way up to our little our session today on the 17th of June 2024 so this is the times time uh swap I'm looking for another great thing here that you can do is you can set a threshold you can say well I don't want any noisy images I want cloud cover 95% what that means is in any scene at most 5% of the pixels are Cloud the rests doesn't have any cloud cover so that you can actually see the surface you can be more strict with this you can um you can try to up this to 100 I don't know if you will get a lot of pixels but 95 is a good rule of thumb just to get started just make sure that just to ensure that you you always get some data we going to be looking for level two data so again this is data that we have some corrections on so this is my search criteria and I'm asking the lanat server to give me data from this time interval within this geometry with all these parameters that I ask it so when we run this sure oh so this is an attribute of the data set itself when the data is processed this is already for that Row for that area this is already identified so for that row and path ID so in that small area that our satellite covers um the processing step done by the data provider already reports what is the percent cloud cover so what we do is in this in this area we are only taking in the areas that have um at most 5% bu yes yes yeah you can definitely reduce the tolerance here um but especially with lanat data you're just going to start seeing a lot more blobs on your image that just bleed over to all the features that you're trying to capture so did this criteria over that box that I specified that satisfies all this criteria I was able to find 105 L set scenes so just to give you an idea of what do these scenes contain well they contain some geospatial metadata so it it gives you the type so we know what the type is the properties like what are the bands that are collected the geometry the extent uh the bounding box that goes with this extent which USGS collection it's in ETC so this being um one thing I also want to do here is just to show what what is inside this assets parameter which contains data the satellite data itself and I would like to direct your attention to these all the way to the QA so what I highlighted here so again Philipa was talking about this right not every image is RGB that is true even though we have blue green and red we have others we have n so we have near infrared we have the swe Bands U collected by different sensors and also we have qas so qas are very important to look at so this is the stands for quality assurance so it tells us whether or not these pixels that we're looking at is there quality assurance is there a lot of noise there a lot of uncertainty and we can get this information um for the data set that we're getting so with this in mind I'm just going to start printing some direct links to data in the S3 bucket that satisfies our criteria you see that we are getting data from the O tis sensor for 2023 and now you know that this corresponds 24033 corresponds to row and path index so where is this data set those two combined gives us an actual location and here is why I put you to the torture of talking about this long overloaded slide it's it's it's the data description right so when we look at this you see that l so lat data so this is lat 8 question um all of the nice pieces that are B file names stored inable metadata as well orary that's a good question I always go to the file name um I'm I'm unsure it must reside I mean it should be in the metadata but I always par it in the file name as I'm reading them yeah so again coming back to this so we have lanet U eight data it's the row indexes U when the data is collected and when the data is processed so with a lot of data sets we mostly get one or two week lag between acquisition and processing so you see that this data has been processed five days within five days it was a ired um and we see information such as B4 so this is this is the fourth band um that has an IR information so these are all the different lat scenes that be that we gra yes the the um processing correction level on these is l2p yes you didn't talk about that one so um so this is level two data which which means that the terbin corrections are either being done or attempted this is even higher than the L1 TP no this is this is the second tier not the first tier so so far what we were able to do is you were um and for none of these you actually you don't need an AWS account so this is just metadata so far we are just getting information about the data itself but this is basically as far as you can get with AWS where you can get the address S three addresses of where the data lives and an AWS account account helps after this stage where you want to read this in Python is a data frame so for this I'm going to use dra iio this package B of three is again highly recommended um just like any programming language there are at least three four packages that do the same thing and people who develop them will always claim that that is their packages are right way to do it um I find B three to be useful when you're working with S3 buckets especially for landat data I highly recommend it um and I'm not involved with people who develop b three at all so this is my unbiased opinion if you're wondering so one thing I'll I'll mention here and this is really the logistics uh sometimes that's a little combersome when you're first getting started doing something like this when you create an AWS account you have an access key and a secret access key you can think of this as like your username and secret password token uh you can get that from the AWS site the easiest thing to do is to sort in a CSV somewhere and not hardcode it onto your notebook that's always ill advised um so that you can just read it because you will need this information to um to get your credentials so I have got this information and saved this in the CSV file and all I'm going to do and you can use this um here just going to authenticate an AWS session using my credentials and I'm going to start reading some files and this function is uh from a tutorial from spatial uh spatial data science uh there's a there's a GitHub repository on spatial data science and this is one of those functions but I think this is very useful to talk about some of the things we can do so the main function we're going to use is called Rio so for all of the St locations we're going to be read it from the cloud and we're going to be represented this python as a data frame so what are some of the things we can do one of the things we can do is projections so Philipe mentioned projections are very important when you're working with geospatial data um so here we are going to be projecting everything to 4326 for those of you are in GIS this is the bread and butter projection this is wgs84 projection and this is the code for that projection so whatever projection data comes in it's first transformed into this specific single projection and this is very important to work with the same unified projection for all of the data sets of course when we change the projection the boundaries of our data set will change so we have to transform those two so that we can start getting our uh data projected with the correct with inside the correct correct window because then we use this window from the to only get the data where we where we need it inside the bounding box we decide so Define this function another function I'm going to Define so that after this we can take a look and see what these will look like um in remote sensing we work a lot with indices so sometimes an attribute we're looking for may not be very visible in the red band or the green band or the blue band but when you combine these three bands bands to create another band another index features might be more visible so this is exactly the case ndbi is one of these this is a greenness index so for if you're working on agriculture if you're working on Plant Pathology um Forest management you might probably are working with ndvi data and this is how it's derived it's uh it's basically this equation is the difference between near infrared and the Red Band divided by the summation of the two and this gives us a much clear picture of the greenness of um of the scene that we're looking at so this is an example of an index so basically I'm just going to use these functions uh to start reading the data calculate this ndvi index and plot these ndvi indices so now we are getting this greenness index and these are all the scenes that conform to the bounding box I created um I guess this is the big question where was the bounding box in us are we able to see it I mean I know a couple of you s Lou exactly I know doesn't help working for St Louis University this was a very underwhelming reveal um but yeah so this is St Louis you can see the you can see the Mississippi River um you see that a water body's Reflections is so much different especially when you're using a greenness index um you can really see areas where greenness is much higher like open green spaces uh you can see that there's a big disparity here so this is just a quick example of querying lanat data getting this on your computer without having to download data in Python so that you can start analyzing it and calculating an index so this being said um we want to take a half an hour break after this um so think we might be a bit early so we can maybe go over a little bit more I can go over like other types of resources that are available but first we can probably take a t right okay then we can do a quick Q&A if there's any questions about this code um but when we go on our coffee break which um after maybe after you cover one more I believe they schedu for 3M so we at 2:30 right now okay we are early but during the coffee break what we plan to do is if you have very specific questions to this code or you know if you want to bounce ideas for specific things you might try to do with lset I'll be here and one of my students Don will be here so that we'll be walking around answering your questions if you want to ask them yes just going to propose if we're Hadad of schedule maybe we can just move on and then we could be half an hour early yeah yeah yeah we can keep going out with sure there are a few things that I I had listed to show more at the end of other resources like we are showing here one example of data surch and libraries but I can spend a little bit time now covering other things that you guys can look for like how do you visualize this data what other sources of data they out there and so on so that's why we we can use this time now Y and we are overprepared so even if we early we might not be able to promise leaving early we we do have I think we have like at least seven hours of material for this Jo so this code I went to the website I wasn't able to access this one or I see the link is there a it's it's in the schedule okay so it's it's it's an embedded Link in the schedule but we we'll double check if that's an issue yeah second question is um so so it seems like when I just Google the L set it's a free database what's the difference between that and let's say mapbox or maxr how do they like if it's free why do companies buy so um for one um one of the things they have like for example maxar um they they might be serving indices derived from the land set data so that's process procing B processing we did is very simple ndvi right but there are a lot more processing you can do to create an actual data product something more actionable like when you look at the STA set you have an idea of do and don't and that requires a lot of compute power so that's already secondary data based off of land set and another thing that companies like that do they have what we call uh bu satellites they're much smaller satellites uh they orbit at different um basically depending on the mission um they create much higher resol solution imagery for example maxar is one of them Planet Labs is one of them they do have sub submeter satellite data um they they produce that with very specialized special types of satellites so they charge data premiums for that there's actually a big ASX this is freely available data but here is the ASX USGS so the US government collects this data this they make it publicly available which is wonderful but then uh this data is served somewhere and the the AWS scheme that's called is request or pays so when you request this publicly available data I mean at some point this lives in a server somewhere right so somebody is paying for something um as long as you don't exceed the limits I see it's free but for example if you're continuously streaming landside data in analysis at some point you will start paying AWS charges if you go over a limit because even the freelance data is served as a part of the requester pays um schema for payment can I ask one more question so I'm I'm familiar with maxar we get their data so you know when they come in they come at 30 uh you know uh and so they do some enhancement to get it down to 15 meter um what's the best resolution from landset that we can get so it depends on the bands we do have 15 meter resolution data as well but it it really depends on the type of sensor that you're looking at so again there are a couple of things we will cover this I think both of us are going to cover this um when we get to the machine learning aspect after the Break um there's a whole family of methods called super resolution methods where you can take I mean this already exists in um image Corrections like you can take a very blurred picture of with your phone but an algorithm can really Crispen everything right so with satellite data we have a lot of terrain information for for example we might have on theground lar data where that gives us centimeter level terrain information we might use auxiliary data like surface types for reflectance Corrections Etc we can just take the raw data and use super resolution CNN to create a much higher resolution version of that data but at that point that's a data product not the originally sensed data yeah I think that's what maxar is doing thank you appreciate before I move to this slide deck you guys have any other questions related to the code here or anything yeah one question so in maxar and S I remember reading in the regulations that you are not allowed to in paint or modify the images is it the case with lense so in terms of the application not work easier so I think it's application dependent but on the legislation side I'm not I'm not quite sure what the Restriction that L set is again I can talk about my sphere when you're doing academic research R data products are publicly available um like there aren really a lot of restrictions but that's yeah that's all I can answer to do you do you know well I don't have a direct answer to your question but I think a relevant comment in Touching both questions is how things work with the the other side of things of like a instead of using this open so muchat lower resolution in a sense I say so muchat lower because when you talk about 10 meter per pixel sometimes to people like from more like climate domain or someone they say oh this is super high resolution while doing the type of work like I I Illustrated at the beginning about building footprint extraction that you want to look at very specific structure if you see an image that the resolution is worse than uh 70 c meters per pixel will say that image is bad so it all depends kind of the application and so on and there we work a lot of imagery from maxer so or three type of data so to say like a half a meter per pixel but then you rely on agreements so it can be agreements maybe from who is sponsoring your your research that it has something place with maxer or maybe things Cascade of some sort of governmental agreements and that also touches on how you have the cover for this date in certain areas or not you can get into some messy situations of like so much thankfully I'm down the pyramid enough that I don't get to see too much of how things work but you can imagine you have multiple events of Interest taking place at the same time who gets the highest voice to say like ask your satellite to look at this place right now versus looking at the other place so it becomes again that trade-off of the avability versus the quality of the data what you can do not so these types of open data can be very relevant in particular if you're thinking about covering areas as large as the entire world and somewhat frequently versus if line sporadic Acquisitions that is very high resolution so yeah I wondering is this pretty tied to AWS or do you know if there's like go cloud and aure yeah so there are many ways to get this data set um so this is what I consider is a the native way where the data provider has this agreement with AWS um so you don't find yourself working around somebody else's API they built on top of this but there are for example if you just want to be able to download scenes without writing code one place I highly recommend um which happens to be my old ex employer too but ezri uh they have this open system called The Living Atlas U so L set imagery you can just look at times there's actually they have a great tool called lat Explorer which is a web tool you can search locations like with a click and to click UI on a web page so you can download scenes that way um another one is the Google Earth engine um they also they also have availability on the status set um I'll just talk louder so they Google art engine is another place you can use and Google Earth engines API has specific apis to get Sentinel data um so that's that's place to look at any other questions about this notebook yes format is the tips the D yes everything that's a great question everything is geote uh because in addition to the pixel information the metadata contains things like projections bounding boxes so it is it's all in geti format it's a good question any other questions great and if more questions come up as I said we're going to have an unstructured Q&A during the coffee break I'll be happy to answer those questions as they come and the question about the other sources apart onws is thank you perfect PL for what I want to jump on to right now is essentially it's showing a little bit of the other resources that you have for example the Google Earth engine that gives you a lot of uh ways of downloading Thea process in the DAT and they also have some collab notebooks out there that you can play with so let me jump to those slides here which I believe are maybe one second just to localize which deck I had this here might be different when you're looking one thing that came to my mind about your question like one detriment you might have using like going outside of AWS is like for example this happens with the Google Earth engine this being massive data and if they're serving it for you their archive span is generally shorter than what AG us has the AWS so if you're looking at very historic data that might be harder in hard to get like I have many notebooks that broke now because they are referencing data CL like 2010s Etc that impact cost it all too is it like like I know you mentioned if you it see threshold data being pulled in start paying free that compound if using something other than AWS I'm not I'm not quite sure i' I haven't been using blert engine for a while um but it might yeah I don't know what their data structure is D structure is yeah I haven't either because again I work with other types of data mostly but I would guess it's a pretty high threshold because many people do research using Google Earth engine to essentially allows you not only you have a large large gallery of data sets there that you have index set not that all of these data sets were generated by Google or anything like that but it's essentially gives you a way of searching through those databases and then you have these Python apis and so on and also they have some pretty cool different types of applications of plugging in what are computer vision types of tasks on top of these images and so on so that's one of the resources I would encourage as well you guys to play with if you're more curious about and then there are other resources both around data downloading processing and also the coding part so Aron has mentioned in the tutorial how the rerio is one of this libraries very widely used for your loading your both your tiffs and also you can play I believe with the rest also with Vector data right yeah so that's one of the things I'm going to touch a bit more uh when I mention about the examples of this machine learning tasks is that when we think mostly let's say like those benchmarks for computer vision you're essentially thinking about your input are rers or pixels and your outputs are pixels like segmentation Maps or something like that when you are dealing with these geospatial domain applications at the end people interested in using your predictions they want to see maps they want to see things like a localizing space and use their tools like what is a a 32 or an open two that essentially you load them representations that are polygons those are vectors so you have this xray step of how do you convert from the predictions of pixels to polygons that like are geolocated and so on so these types of libraries they useful for that but still one of them there is another one goo that uh if anyone has experience probably we can go like the whole session just talking about g dog because it can be a pain like me when I three years ago started going from more just more General computer vision started to play spatial data that was my main challenge because it's that kind of library that you have so many things buil on top of it that allows you to do so much stuff but it has so terrible documentation and lack of like a tutorials or anything like that it's getting better there are like some uh wikis or medium posts and so on that you can find around it so I mention here that you if you start to play more you inevitably kind of need to work with ch allows you to do great things of like this restur ization and loading the datas and so on but can be a bit painful so just hear the the this caveat about that I don't know if you have any other comments about that's one point yeah the doc on that is interpretation of somebody's interpretation so and then more related so in this next part of the talk right after the part of the tour about after the coffee break I'll cover a little bit more of like uh this image classfication ation segmentation especially self-supervised learning Parts I'm going to talk quite a bit about what has been going on in this domain because it's where we start to have more of the unique aspects of the data characteristics of gpal how you can leverage that for training better models and so on and then there are some very cool repositories like the torch sh that is one repository you can go to that also has some very nice uh getting started kind of SES with uh Jupiter notebooks I believe in collab as well that goes both from this uh data loading part and also using some models out there whether is forication segmentation and so on so Binding Together this aspect of like this data loading and also conventional pie torch uh kind of uh functions so it's you can imagine it's almost like a a wrapper or or as they say supposed to be like similar to what you have torch Vision where you can get many of those implementation architectures and other things for your spatial data and then there is another super cool repository uh that's this satellite image deep learning this has grown so much over the years was like a uh one of these unsung heroes that you see sometimes in Twitter or someone his name Robin Cole that he started putting together these things and then people started to become as a community so it's sort of a volunteer based kind of thing that you have many um uh lists of what are the data seces you have or some of the data sets label data sets for the different types of tasks or the different types of models with checkpoints available and so on so this can be very cool if you want to search for like data sets models so on so forth and another thing uh terms of data resources we mentioned about L set some other things to mention is that you have this aspect animation how hard it can be to get the Side Resolution images but many of providers like maxar and planet and Umbra they have what they call the open data programs in particular for data associated with uh for example disasters scenarios like uh natural disaster so one they make data uh imagery available there sometimes with some pointers locations of where the things happen or not so this can be one interesting way to start playing with this high resolution data of course the availability will be smaller proportionally to what you have like in This Global corage insets and so on but that's a very nice initiative that there is out there there are other types of uh uh index platform so to say or searchable platforms that you can find data one that I would highly encourage is this one by the itle E uh grss I'm going to talk more about that at the end of the our tutorial about uh Community overall around Jo spatial data remote sensing but this uh EOD platform the thir observation database is very nice for you to search for by the type of sensor and also search by the type of task maybe the labels that you're interested in so on so that's another very nice search the stack index as well is so much similar to what you have from the the AWS just just want to keep track of time uh so these are other platforms that you can find a lot of data same for the particularly the NASA missions you have the Earth data search platform it's like huge the galleries that you can have a DAT like Centinal kind of data or harmonized lens settin centino some lighter data as well so there's plenty of data you can get out there sometimes that it's hard to find a unified place to go for them but that's one of the things that in particular the Earth observation database is an attempt to try to do that to try to start having a a more Community Based way of people they want to contribute due to that is easy to to reach out and put a data set there and so on and then what I have in the other side here uh it's around how you can get and visualize uh other types of data so the qjs platform is one that is particularly useful in terms of Open Source free platforms for visualizing this large data like the Tiff files that can be covering like a very large areas you can imch you're talking about images that can be like easily 30,000 by 30,000 pixels and multiple channels you're not going to be able to just load that in a Jupiter notebook or so so you have these platforms that build upon things like the gll that I mentioned but in a much more friendly way for you to operate and you can visualize you can do some Transformations there in your your data both the geotiff type of files that you have for the images and also Vector data so you can have uh you have the open street map which is a pretty cool uh platform initiative that they they call like the free Wiki of the world is essentially volunteer based where people can go there and they can uh for example uh say that this building here in this location that has been mapped it's high school or it's used for industry purposes or start adding any type of attributes that can be relevant for other people on the world it can be researchers or or other analysts to make use of this information so you see a lot of inputs in particular showing up for places when you have some events of Interest whether are disasters or something that people more actively start to label things in the open street map so you can think about a very large gallery of even label data you can think of but from a perspective of like noisy label data but stuff that often times is pretty useful to you to get started in some tasks in some areas when you're dealing with your spatial data and then uh I also find the ganda's library another pretty cool source for to play with Vector data for example we had in the in the the handson part ofation about defining your coordinates you could imagine you can download some open sources you have of maps where you have the delineations of where is each City like whether St Louis or Seattle or some other levels of uh boundaries and then from that geop penas allows you to load this sort of what we call shape file these Vector files and do operations as the name said like as a data frame as a panda St so you have a lot of operations that build on top of dependence data frames and also offer you ways of visualizing this data doing spatial queries and joints and merges and that has a pretty good documentation I would say I've been quite enjoying playing with gendis so that's another thing I would highlight and there are many other sort of open projects where it's like related to uh constructing databases and manipulating databases for like for example postes and other things under this initiative called the OS uh go so it's open source projects that is also very relevant research another thing that I will mention already is there are plenty of cool data sets and challenges in this domain that come often in a year year basis some of these challenges have as well uh monetary prices and they're usually touching in topics that are very interest from the research Community they are kind of open challenges whether it's more from the application side like for example you're talking about flood mapping or you're talking about uh finer grain characterization of buildings or you can think about like how do you do the fusion of different types of data like the S and Optical that we talk so much about so to mention just two you have the data Fusion Series of contexts that's being organized for multiple years already by the pess um image analysis um technical committee and you also have the space net initiative that is across different institutions uh has changed over the years but you have consistently data providers people from Academia people like me like from morea National Labs and so on where we work in different tasks that have range for instance at the beginning for building segmentation or Road Network or that kind of illustration is shown about uh data acquired over this location same location but different view angles and uh things also related like multi- temporal analysis so these are some of the things if you get mares to play around you can either use this data because this data remain available or keep an eye are participating maybe the next the next few years so those are some of the things i' like I'm using the time to cover about resources uh if you have any questions around that we can use this next uh eight minutes or anything that we talked about so far then we're going to hit three we have the coffee break and then after that we're gonna hit more about what we call this common machine learning tasks for remote sensing data and hit quite a bit in like what we have in terms of those Foundation models right now for remot and data because not surprising W any one because most application domains that you have of AI right now people are develop Foundation models and the definition could be quite loose but it's very interesting the types of data sets the types of self squ learning techniques that pop up compared to what you have for the broader computer vision so that's the the seed and plenty for after the the coffee break and if you have any questions for this next five eight minutes otherwise you can use the time to just keep walk to the go ahead um would you say that geotiff is kind of becoming the the primary standard for representing all different formats of uh Earth observation data or is there one that's recommended as kind of the best my answer comes from just my experience that has been with mostly like a RGB infrared ads and and so on that has been always du that I deal with but I'm going to Def defer the question to to that maybe has experience I was just shaking my head I mean jtip is very widely used um you also have to think about the data so with r data you're talking about pixels I don't know why I'm reading over I um we talking about pixels that's okay so that's a good question um I think is very common when you have pixels in other words you have centroids and a uniform geometry around those centroids pixels uh but we have a lot of different remote sensing data for example when we have when we collect SAR data uh this that comes in a format that is known as an Lis so that's a that's basically the SAR format because instead of just getting pixel data now we're looking at uh Point scatters so it really depends on the type of remote right so it really depends on the type of data that you're looking at um but for so you're talking for raster Indy geot tip is GE geot tip in my opinion is the most interoperable one um especially in your open source I do agree like the pixels jti is is where it's at um there's also a big story about processing these for example if we're working with raw data like there are these Legacy platforms that people in different like for example if we're working a lot with spectral data spectral Corrections Etc um there is a type called Mr Sid does anybody know Mr said yeah you know yeah so it it really depends on like also what kind of processing you're going to do on that data because once you go into that format it unlocks and locks so typ of analysis you do they have like their own JPI but I think part interoperability for Pixel data uh geot for Vector data shape files is really the um um the main type so when you basically have irregular geometries like County polygons so you're looking at shape shape files um and for Point scatters the Las type is is basically the main one I've heard of the shape file used it that's like is that a Google standard the KML kmz or is that different so that's an excellent question um I wish I had a slide on this that's an excellent question it's actually U not a Google standard Google doesn't set the standard for geospatial the standard is called ogc so geotes shape files all of these are ogc compliant and ogc stands for open geospatial Consortium so this is a global Consortium that basically defines what open source or open geospatial data formats are um so shape file is a part is ogc compliant geot TI um goj pegs these are all ogc so if you want to get like a whole list of I will look for ogc compliant data types now I've also heard of a tool called shapely is that related to shape file processing no I think shapely is a python package for reading Vector data I don't know the history but I think they named it because shape files are the common Vector data types inom Source but they're not like directly they don't like shapely isn't designed to process shape files well one of the most I mean the most common Vector type if you're not using AR is shape fil so probably yeah there's a question about I can speak to shapely it's actually the python of GEOS which is libraries which are perform B plus L has almost nothing to do with SH files at all it just lets you do um plan geometry operations rapidly thanks thank you thank you I'm not sure if it's like related to this but for example when you see a like field from a top view there are like certain places there are like the teres which like distest the like a view like a shape of the field do you ever come this kind of stuff and if you do how do you remove those teres from the field to like make the processing Smo like make your DM Smo something I think is talking in the terrain correction that you were talking about right um so there might be a couple of things so if are you if you're talking about correcting the raw data because of the terrain so that's a Terrain correction like we were discussing in the lat data um so that's basically you need to have a demm a digital elevation model and you have to use that model at what whatever resolution it comes in to correct the remotely sense data for terrain because when we talk about Reflections obviously the idealized conditions is flat surface reflection which never happen so we need to use that in that terrain correction if you're talking about different curvatures from different parts of the Earth Earth being combined into one is that the question so that's that gets it the process of maybe Oro mosaicing are you going to talk about Ortho like what I'm asking like say for example you only have a d and that you don't have AE fens IM in case you have on access to the DM how do you get rid of those you know like is specific because this those kind of stuff actually dist yourm like if you want to process something for a specific thing they come come come in front of you to do so like how do you deal with those particular St like you use any particular process to get rid of those or like Sy I'm not I'm not quite sure about the work for I understand your qu I mean your question is like how do you correct the DM itself right before the process maybe flip as an answer I generally resort to the high resolution DMS coming from NGA um so these are high resolution Corrections are already done by the NGA so that's I think the gold standard when you look at EMS uh and they also are working on a unified height model as well that you can use if you have like very big disparities in um the areas that you're looking at but other than correcting the DM itself I that's not the kind of thing I do no and un fortunately me neither I know that I we have some folks working in groups close to mine in the OK National Lab that work together agencies as or mentioned to develop those DMS but I do not have the expertise on that unfortunately sorry um so coming at some of the stuff from a computer vision background kind of moving a little more into geospatial sometimes it's hard you get this feeling like you don't know what you don't know right I don't know if you guys had any recommendations for like you know at least with deep wearing you know there a lot of like loops and kind of survey you know textbooks and stuff like that for to get started with um do you guys have any kind of recommendations for any like you know online courses or anything like that that uh you maybe you found helpful or you find helpful to send out people that's a great question uh they're well deserved some of the things that I can mention is like these last three years like when I first moveed to this domain versus what there is now nowaday there's more resources now which is a good thing unfortunately there're somewhat decentralized which makes kind of hard to give an answer to that like for example I believe that the torch Geo that I mentioned is a good Ting the wrong computer it's one that is good uh because it has this uh examples like a collab things and so on and they also have I believe some videos explaining a few things similarly for this satellite image deep learning they also have like a Blog kind of version that there are some videos explaining uh good things but uh I wish I could point you to one specific website or a book but I do not have one that I would mention except for these decentralized resources like these ones so the short answer is there is one I mean so F first and foremost coming from computer science right when I think of Earth observation and computer science I mean it's not a fair comparison right these are very different things but computer science I think is like very computation and Method heavy and much lighter on the data like you can get pictures of cats and dogs video feeds it just works it should whereas what we work on is very data heavy in that for very different types of data that you get the methodology set shrinks or expands quite a bit because it's very much Mission driven so that's why it's as fipe said it's really hard to say like if you read this if you go through this that's it for remote sensing because people who work on deformations they gener work on things like SAR if you're if you're looking at images and things like lset or the Copernicus mission is basically a whole other thing on its own that we didn't even touch today but does similar things to lanet so it really depends on what you want to do um the as you might have seen there's a gallery section to our web page that is now currently empty um so that's the great that's a great feedback um so I will add a link there is a spatial data science open Muk it basically just says python code Snippets data so that you'll learn how to like read shape files where you find remote sensing data um they don't take teach AWS a ton they just give you small geot test for you to download but to just to get your hands dirty more with a broader span um we will just add that link as like an outgoing resource so if you visit our web page of the week or something end of the week it'll be it'll be there just to add to sec I'm also a computer science student I'm also moving to the special cell the thing that Mo help to me is I I'm usually working on soil data like I'm like for example H slope mod when I heard it it's like I don't know what is this so I go to the paper see it and then as your com sign it's much easier for you to do the coding up one is as a com don't look at the KGI it's a crab the best thing is go for scripting if you use a Gras Gras scripting is the best thing and again the documentation is really really really bad so you need to figure out how to do it this is this is the struggle for a comp sign so best is go through the paper go through the Gras scripting documentation scripting documentation those are the best sources for ask so um I don't want this disturb the discussion I think this is good but just for everybody so this is we we we're taking a pause it started actually four minutes ago so we are kind of on a break right now so we're just going to be having this open discussion um so if yeah if if you don't have questions you know we you're not going to be missing anything we'll come in back at 3:30 but yeah so I think that spatial data science moo covers very specific a lot of landat examples um and I think it's also like the abstraction level if you you really don't need to know how projections work you can take the projection engine of restro at face value assume that it works the same way you assume that mpai indices are always going to be right when you read an index uh right um so it also depends on on the abstraction level that that you want to work in maybe one one question you mentioned already that satellite things things are not always and it's interesting and I have one personal use case that I would like ask your experience is there some some modalities can help detecting yeah like for example was talking about the nvi like this is widely used to assess like the health status for example regation right and often times I see for example I do a lot of work so for that are more like GE Spa analysts that they look at the images and help us for example labeling data and sometimes our Focus has been more like for example bu infrastructure but sometimes you want to separate for them to able to see the trees better they Riz sometimes more on the near infrared information than just the RGB because it becomes very evident that green so I make note of that so um as far as I remember so the list always goes but for example in terms of the indices there are right now there are 68 different types of data there's actually a journal paper just came out about that so we'll add that so that you can see like basically a whole La list of all the indices that you can ccate and what they're used it's yeah it's it's out there so yeah depending on what you're Capt that's great how can access I don't yeah so it's all going to be in the gallery portion Al for this tutorial so if you revisit by at the end of week maybe early next week once we're back yeah it'll all poate at the bottom of the web page and one thing I was wondering about um you sort of touched on this in your introduction so um is author rectification I'm wondering I'm I'm actually more familiar with the SAR side the opal side um and there pretty substantial difference but um in the um that that needs to be oror rectified I'm wondering how much does that happen with all people when you're assuming if you're looking roughly down you're G to have much less of an issue with that so that still has to be done because when we think about the swats locations along the SWAT are adjacent but locations across the SWAT AR they're actually collected when the satellite is crossing over a different well when you look at a second it's actually traveling a different trajectory so that's the okay and are the files you're showing being downloaded those already rectified yes okay okay sure thank you I said a logistical question this this talk is being recorded yes what is the link to that going to be available afterwards yes so it's all going to be in gallery link to the recording Post in a gallery yes is that is that cdpr website or is that something you guys so this this tutorial page you can so then you on our actually we've been we've been recording this whole time I should probably pause recording like where there's the schedule it has the link to this so I I'll show you what what we mean um so so we have this web page um cvpr oh we not seeing it it's always awkward point something there let's see so this okay so this is the web page for our tutorial we created a specifically develop of no so here this is already embedded in the cvpr program but if you want to take a picture picture just in case and this talk will be if you scroll all the way down we have this Gallery upcoming section so everything I mentioned be at the bottom here thank you so much it also goes back and a question that might be a little silly for you oh that's fine please please go ahead resarch um for M I worked for Min Corporation and a lot of the work is basically a lot of work what we' seen so far I the problem I'm working on is Sensor Fusion C with radar andal data and have a couple I guess just general implementation questions that I was hoping maybe I could get yeah but now feel free to make me some we'll see have more than we see yeah I think that works there you go all right so we GNA resume with our tutorial now talking a little bit first about what we're calling here the common machine learning computer vision tasks for Y data things such as image classification catic segmentation object detection and so on and I will not spend too much time talking about convolution little networks or Transformers or all these kind of tools that we can say that have been co-opted by the broader computer vision and used sort of in the same way for these problems I would rather spend more time on the extra strategies that are you start to equip those what convolutional networks or Transformers or your pre-training objectives when we start talking about self-supervised learning and so on that leverage them the challenges that we have with this type of data and Leverage the data characteristics that can make them your the quality of your algorithms better for instance one of the things I'm going to talk about is like the object detection part when you think about object detection you're thinking maybe like this frontal view you have your bounding box with the coordinates the X and the Y AIS and then you look at things from above if you have something in the diagonal your bounding box aligned to the X and Y axis is going to be terrible it's going to be a huge boun box that's not very well attached to your object so you need to take care of the rotation rotation aware kind of bount boxes so that's one example of extra Nuance that you need to develop for this type of uh all problems for Earth observation dat yeah there you go what I want to mention so the classification semantic segmentation tasks in particular a good chunk of the methodologies they are the same that you would would have from broader computer vision there are the nuances that you need to prepare your data data loader sometimes who have more than three bands and so on and then I want to touch in these additional challenges related for instance to the spatial resolution part that a lot of times it implies in we were talking a little bit over the break of doing what we call domain adaptation kind of strategies uh that's one of the the things that people try to do to address that over the years the other challenges that you have Associated is like how you have more severe imbalances so what I have Illustrated in this figure in the middle from from one paper from supr a few years ago is how you have this imbalance between the foreground and background objects that can be much more severe than what you have in the usual uh what we call Natural images regular computer vision data sets so you can have a much wider area covered by background with very tiny foreground objects of Interest or also sign significant variation in the sizes and the shapes of your objects of Interest themselves like we think about cars on these images you can barely see this resolution there or then you compare that to airplanes or through warehouses so you might have objects that can be very significant variation in terms of scale and then you can have these other variations like you have the same area uh with image acquired in different times of the year there's no coverage going to throw off your models completely if they have not been exposed to that so these are some challenges that you have whether you need to collect more data for each of those scenarios or start to developing some tricks for handle with that last but not least I will touch more at the end on that but this aspect again that your final product deliveries sometimes they are not your pixels they are rather your vectors and the quality of the vectors then you this step going from pixels to vectors becomes an additional problem so some of the tricks that I find uh interesting from this literature like segmentation type of problems and image classification and so on they include how you can have for instance in the left side one work called this rvsa that is essentially you're equipping a Transformer that has this some Transformers essentially to increase the efficiency you have these strategies of doing Windows right to focus your attention of the multi-ad step that you have in your blocks these windows then when you look at overhead image you have again the aspect of the rotation so there's this additional clever trick that when you're are learning this uh scalers to Define your window you actually add another regression scaler for the rotation such that you can have Windows within this uh Transformer block that are looking at different configurations within your network for this aspect that I mention about the vectorization part or going from pixels to to polygons two problems that are interesting here is one looking at the building segmentation kind of problem if you try to just go uh and vectorize what your predictions are you see that if you look at that pinkish left example you have a lot of jaish uh like little blocks kind of thing because they're essentially your pixels so ideally you would like to have better ways of handl that and one of the ways that you can have is you can start to have auxiliary losses so these guys for example what they use is what they call this idea of frame field learning so it's essentially to learn like the orientations of the boundary around your your object at each uh pixel each region that that will allow you then to do a better job at your vectorization step so might along these lines you have another problem that's sort of a classic um maping the physical environment that is the road mapping or Road Network extraction that one of the big issues you have is discontinuities you can imagine maybe you have a tree growing a little bit over the the road and then when you deploy your segmentation model you're not picking that as a part of the road so you have a disconnection there when you start to think about root planning that can become a problem so you can try to have for example other type of loss function that people use here where it's essentially to learn uh the orientation between different key points that you have uh composed in your objects if you're familiar with po estimation you might have seen this kind of idea before that you have key points there for example you have the human Poe and then you have this issue that maybe you have occlusion for one key point or another and having this orientation like this point is pointing in direction of this other one can be helpful then for you to have a next postprocessing step that uses this information to reconstruct better your structure futes and then going to the what I mentioned about the rotated object detection some of the tricks then they start to be like when you look at this conventional rcnn types of families you have the reg of proposals you can try to generate proposals that are of different rotations but then you can end up with a ton of proposals that cascading being more computationally complex you need to have more issues with like no maximum suppressions and so on so what you can start to do instead of is starting to regress offsets or different variations for your Bounty box one illustration is this oriented rcnn where is essentially the idea that you have uh you can imagine that you have at first your regular Vision proposal kind of a block that will suggest coordinates of bounding boxes horizontal bonding boxes that look suitable for evaluation maybe find an object there but then in in addition to just like these X and y's of your rectangle you you learn certain deviations is extra regression parameter of how much you should have of a rotation or maybe or how much you should have of a an offset that allows you to compute an angle for this kind for this bunny box so these are very interesting tricks I would call that they work pretty well then to enable uh how to you go from conventional uh computer vision approaches and make them work much better for object detection kind of problems and this is very relevant when you think for example you you're trying to do object tracking you like many of the tools that you're going to try to do for that is like to extract the features from the the your object of one frame and then compare the next one if you have a poorly attached bounding box you have a lot of surrounding noise background that that's going to give you pretty bad features so when you think about this kind of cascading issues it's very important that you do better with this sort of nuances of your data another problem that I feel like it doesn't show up as much in the broader computer Visions change detection uh looking from the perspective for example of U Disaster Response I think is one very good example from application perspective but you have a lot of data sets I mention you just one here That's a classic the xv2 that has images uh from different types of natural disasters like floods and earthquakes and volcanoes and so on and essentially your problem consists of off you have uh usually an input that you gives you a pre and a post event images and you're trying to estimate the levels of damage to you some at some point it's G to come back meanwhile I talk louder so so your test then consistent in estimating these damage levels whether it's to build infrastructure or maybe change in the land it can vary and I say usually you have pre and post event images because that can become a problem when you're looking them an application perspective itive an event happen today and then I get some images maybe tomorrow or so for the Post event and heck I don't have pre-event image in my gallery I cannot do anything about it so there's many works that you have seen that the results are better if you have this pre and post event uh imagery available but developing models that only look at post will be like a sort of holy gray of approach too so they tend to perform quite a bit and overall for this change detection problems you you can have many variations of what are using uh convolutional networks or Transformers and so on but the overall the structure I would say of your architecture looks similar under this idea of siames models so you can have here different variations uh there are two very nice papers that are four or five years ago but still remains uh pretty up to date in this core ideas that you could imagine that you're just trying to have a comparison of like a individual estimates of your L cover or your buildy Maps like you first look at separate model for pre event then you look for a separate model for the Post event and then I compare difference of the outputs that's the most sort of naive approach that not going to work as well you can think of a strategy B that you're essentially just tesing a single model trying to do the change detection directly or you can start to have the S structures that start to look at more uh your difference in your features being extracted so imagine that you're looking at either L cover changes or Bey footprint changes you can imagine that you will have one model trained for this task for for a single image you can deploy this model for both the preos event image and then you can start to have another module like a decoder had that looks at these different features and it's tested just to learn them these damage Levels by looking at these intermediate features so that's essentially what we're showing here in this strategies in the left and then extra tweaks that you can compare is whether you're concatenating these intermediate features or you're doing the difference between them has been some uh results showing that the difference is better but sometimes it can vary a bit so there isn't really a consensus but overall it's minor differences for a methodology that works pretty well uh in in practice for you to to enable this kind of change detection approaches another thing I want to hit about this uh common machine learning tasks there are a lot of data sets out there so we have cover between the before the the break about some of the places where you can find those data sets there's this very nice paper from 2023 from Michael Schmid and some others uh researchers discussing uh the data set for deep learning Earth observation that they give you they review over 400 uh papers they have a supplementary material full list of them they categorize in these categories you can see that prevails a lot these far problems that I mentioned about image classification object detection citic segmentation then you have also problems like change detection super resolution problem and so on and uh the thing then is that you can need to keep in mind that there are some biases in this date as any data unfortunately and the biases can be here in different ways one is that you have much more for for satellite than AEL and drone multiple it brings back the topic that is worth a reminder that or made that Earth observations just s light but there is this bias overall um in the data collections as well similarly for the type of uh sensors that you use most of them end up in RGB and the avability of collection of multiple times it's it's rare to have data sets that have a multiple T which can be very useful me pause recording I did resum you go back to this and um you also have geog Geographic diversity uh Geographic uh biases in the sense that you can imagine that stakeholders or the companies operating these data sets they're located more developed places and some events of Interest tend to happen more in some other place some in some specific places so you can have under representation of coverage in in certain areas I mention about this uh before the break how the Earth observation database can be one good repository for you to search for this label data sets and also some according to sensor type just taking a break because I saw people taking some pictures and um the other aspect of Earth observation data is that can be uh uh challenge but can be uh potential for doing great things there's a ton of volume out there and already archives and the ton of volume and increasing levels being collected can think about hundreds of terabytes of day uh of data and you can deal like images that can be super bigger as a single file itself like when you mention about the Side Resolution images that we deal with often you're talking about images that are 30,000 pixel by 30,000 pixels can have four or more channels so that requires these extra twos of how do you visualize them how do you process them but all in all that's when you start to think about self-supervised learning kind of practices that we see now being Ena pre-training for foundation models there's great potential for that here here's just another illustration for just the nazer science data archive what is the projection that we're going to see as New Missions are added over the next years we're going to be having a lot of petabytes that can be used for developing models and applications so with that I want to start talking about this idea of foundation Model S observation no surprise that this has been like a very active topic now where is in computer vision conferences or remote sensing journals and so on because you have these challenges of uh many of these models that we have for the different applications they are task specific you have one model for this task one model for this other t T and you're still dealing often times with the same type of data so ideally you should be able to be leverage at least part of this model instead of retraining everything from scratch again they with developing models for each test also requires a lot of label data label data can be pretty hard with these very large images and objects that are super tiny so it's a very challenging effort to label this type of data so I believe probably most of you are familiar with the idea of foundation models if not is this concept that you have pre-training of models using self-supervised learning so you're not requiring labels using very vast galleries of data with the idea that this gives you like backbones or encoders or representations that you can then easy use to more easily uh adapt to a wide variety of Downstream tasks you can imagine for the type of test we're talking about you have a model that is pre training and then you can have different decoder heads more easily adapted for whether is VI F fre mapping land cover classification change analysis object detection and so on and then ideally what you would like to have is not only these different tasks but also this aspect of different modalities being able to reason across Optical data across SAR across later and so on that is the the dream so to say when we think about Foundation models for Earth observation so there has been a growing list here's just a few it's a difficult attempt to list all of the things have been developed over the years and I'm going to cover a few of them that I've consider particularly interesting in terms of the different strategies that they are using so this is a slide from Professor hammed Hal Mohammad from Clark University that has been doing a lot of work on this as well and we been working together he provided his least for us to to build upon there go so when we look at the self-supervised learning strategies uh across computer vision I think it's fair to say that this four main types of U or better saying these pre-training tasks used for General computer vision because supervised learning first goes against the idea of self supervised but the pre-training idea you can have either supervised learning for different tasks or you can have contrastive learning whether is with just images and sub augmentation or contrastive with like language and text language and images or the idea of like mask Cod that you have the mask image modeling as a pre- task far off Earth observation you see analogous things popping up you have uh big data sets being put out of there for self-supervised learning with uh over large areas and uh sorry for supervised learning with multiple labels for different tasks over large areas you have tasks such as doing similar that you how you can do test text and image you can think about doing location and image that's one work I'm going to talk about and the contrastive learning the image modeling parts and when we look at this table that I show you in this slide before there's a lot of work been doing particularly Master modeling so and then a few of them also using quive learning some recently ones combining these things together uh and I'm going to touch in Cross a few of them they find of particular interest because of the different nuances of what they are leveraging the first one is the contrastive learning itself that is super cool in the remote sense in Earth observation data that you don't need to rely on synthetic augmentations only like when you think about things uh the the more conventional types of contess learning computer vision you do like some uh blurring or you do some rotation and flipping or colorization and so on so forth for observation data you actually have data most of the times geolocated timestamps and other characteristics there in your metadata that you can form pairs of data over different time stamps different sensors different view angles all over the same location so you have this sort of natural augmentation that you can use for contrasted learning and learn reach features out of it so you have data sets like the functional functional method of the world and then you have one of the first Works doing this sort of contrave learning for uh Earth observation data that the additional trick that they put in place as well it's not only that they do the contrastive learning over the image acquired over different time stamps but also this idea of predicting the location of that image because they they use essentially images from I think was from Wikipedia that they use in that paper that you have like the longitude and latitude of where the image is acquired so you can have an additional type of uh pre-text pre-training task that you can use for pre in your model and you can have different variations of what kind of uh invariance or constraints you want to impose if you want to do like seasonality being like a you want to construct such that the representation across different seasons they should be similar or they should be different or what kind of variations you want to have in place it's another way that you can get creative here another very cool work is uh the set clip like folks uh probably plenty of the folks there today there Vision uh Workshop taking place right now it is very interesting is that the idea is the same intuition of what you had for clip for doing this contrastive learning between text captions for an image and learning uh this multimodality in this way you can do the same for having a location encoder instead so the geoc coordinates of your image and paired with your actual image and from that start to learning EMB bettered representations characteristic of a certain location so you can think about uh pretty interesting tasks that you can enable with them I think this visualization that they have in their paper is pretty cool how you can look at the embedded space and do like a tsne type of analysis and see how you start to see some U proximities for example of certain types of climate zones or biomes being closer together this eded space versus in other which so you have a way of enriching your purely image representation the actual knowledge that you get from the location where that image is and I'll will touch a bit on that again when we talk about generative AI for this type of data it'll be like a five minutes I'll just give you some some examples of Co stuff being done on that for mask modeling there have been plenty of Works doing that whether it's for just RGB images or for example how the NASA IBM uh Foundation model called prtv that they have been approached this for uh multiple bands so they have this data set called the harmonized lens set ino2 that you end up having six bands and then you can play with different variations how many bands you're using and what they did is to then essentially expand your regular VI architecture to be like 3D so you have 3D patching Bings you have 3D positioning codings and then you pursue this kind of mask modeling that in their case demonstrated across Downstream tasks that included uh cloud imputation CL Gap imputation so it's essentially we showed a bunch of examples how you have cloud coverage becoming an issue you can imagine you have maybe acquisition over multiple time stamps and you can try to infill that area covered by Cloud things such as crop segmentation or flood mapping wide fire mapping that are very suitable tasks for this kind of a image resolution that we have on sentino 2 and L set and then there are some extra tricks that have been you can think about when when you do Mas image modeling for this type of data that you can try to think of how do I play when I have Acquisitions over multiple timestamps or different bands do I mask everything that single patch that location across all of them or do I maybe mask some randomly independently random for one Tim stamp and independently randomly for other Tim stamp in a similar way for different bands and then they for example the set M paper set m incoder paper shown that they saw some improvements in the model quality if you do this kind of independent masking instead of the patch across all your bands and all temporal Acquisitions being always mask so that's one of the flexibilities you have with this kind of data we we have here the the part of like how do you embed in your representations information about for example the time of acquisition so in the same way that you have this position en coders in your Transformer modules to denote what is the position of your token if your patch when you do the split of the image into patches you have this position encoding with like a senoidal function and so on you can encode other informations in analogous ways for example the time of the month of the year that the image was collected or which band is associated with that token another very interesting one I a little bit over the break is this aspect of like how you can deal with this difference in resolution the differences that you have one pixel meaning half a meter in one imagery or meaning 10 meters in the other so this K paper what they do is again playing with this positional encoding but then infusing that information like what is the resolution of your pixel and with that you start to have more robustness that allows you to have models that perform better better with this variations across spatial resolution from the supervised pre-training part there's a recent paper was actually in cvpr last year that was called the satell pre-rain is a pretty big data set with Sentinal 2 a nape images so high Ras in a sense and lower R imag and that's accompanied by a lot of different types of labels you have labels according to the different uh for example you have storage tanks or you have uh power plants or you have maybe some attributions about the type of structure that you have there what is the usage of it or information about the the land itself or some other types of um features that essentially allow you to formulate different types of tasks so you can imagine a pre-training where you have a single encoding module but different decoder heads for each of those tasks under this assumption that that's going to lead you to learning richer internal representations that later on you can replace some of these decoder has to enable other fun strategies and then we go back to the conversation about MTI modality right again multi modality doesn't necessarily mean image and text you can mean things like how you handle the different resolutions you can handle like the passive sensor versus the SS uh and also the different view angles and so on and so forth because ideally what you would like to have is this type of uh model that you would be able to consume data from different data modalities maybe just a few of them available at hand but in the end of the day you would be able to enable all these Downstream tasks using whatever combination of data you have available at hand because you know that intrinsically you have all this data geolocated there is information that you can try to bind and learn share representations for this thing and then here for the self supervised learning with multimodel data you can have strategies that are very analogous to what you had before like just a simple uh pairing of patches of the different sensor types that you know are covering the same location the con contrasted learning on top of that you can try to do some combinations of things like you can try to think about like how the OM set this for instance that you have a pre-training of of modality specific encoders using a mask modeling kind of strategy and at the same time then you have us then a modality Fusion module that learns a joint representation by doing a contrastive learning across the representations coming from different encoding modules and another work then for example how the Presto work does you can start to think again of this different type of permutations that you can do in the type of masking that you can do whether you do like a for pixel alone or you do across a whole band or different timestamps so you or the different sensors so you can get creative then with these different permutations of the masking that you can do and whether you're patching the representation or just playing with each P you need it another work that is very interesting to look at is the climax work uh that was done uh was published in icml last year so that's more on climate data uh like temperature and many other features I find interesting mentioning here because the strategies they're still useful in a similar way and the challenges are similar in the sense that you have variations in the spatial resolution of your data you have different types of channels with different types of U uh modalities of your data so essentially what they have in place here it's kind of small to see but is again this sort of strategy of first model modality specific encodings in that sense the modality specific tokenization that they do and then start to leverage uh this idea of cross attention mechanisms to enable this mix of your your features com on the different modalities such that you aggregate these variables these tokens together and then you have after that a regular encoder Transformer so to say to learn this share representation AC course the different modalities uh the US set is also similar way essentially the trick that they do here that's very interesting is having this one um the idea that if you're deal with data with different resolutions you can try to think about doing more patches for the high resolution ones and fewer patches for the the the lower resolution ones such as they kind of cover in the same area each overall and then you have additional embeding that tells you what is the band or is the red the green the blue the infrared and so on so that you can have the separate tokenization first but then you have like a pooling mechanism that you create a single representation for your patch that then leverages the information across this m and finally another work that I got to it few months ago or weeks ago through archive and then I saw I was glad to see that it was actually it's actually part of the the conference this year I Friday they're going to be presenting that is dis Sky sense it's there is a lot here to going on in the sense of using contrastive learning across the different modalities that they have the aspect that they have different encoders for different like low R High R and SAR imagery but the one thing that I found very interesting is that sort of along the lines what I mentioned about the set clip of learning this type of uh geolocated representation the way that they do it here is that they do a clustering mechanism in your embedded space that learns different prototypes for each location that you have in the world like we split the world in a grid I think they did like 4 496 regions you do this clustering in the feature space for each of these regions and that gives you a prototype that for each of those locations that they show um in the downstream tasks that enable better representation learning for downam STS afterwards now the one thing that's uh worth highlighting is that I mention how you have many of this uh strategies similar to what you see in the broader computer vision being used here being effective but there is the the aspect of uh Computing availability and resources availability and where we are in terms of model scaling so this is one of the things we are playing now at Oak I can tell because we have the superc computer that are from tier supercomputer that has believe around 6,000 nodes each with ag GPU so you can do plenty cool stuff there and but this availability of uh large high performance Computing resources like in House of most of these uh Foundation models do being done by what language or computer vision has been taking place it's not really the case for most of the research and observation it's mostly conducted by like a we or smaller companies or mostly Academia or National Labs and so on so it's still this step of doing analysis of model scaling it's something still to take place there has been more progress now pushing a little bit more to closer to one billion uh parameters in these vs or VAR variations but when we look at the broader computer vision have things about in 20s and probably more vision of parameters and when you look at language models is even further that you're talking about hundreds of billions and and and that you need to be careful like that you don't want to burn resources just for the sake of it but it has been shown that uh the scaling part enabling enables quite a lot of improvements in terms of your Downstream capabilities and also this sort of emerging abilities right things that you cannot extrapolate from model like smaller models but rather you just find out that they show up on S scale to a certain level things such as how you see language models being able to do translation or doing like those calculations and and here is just a quick messy overview that I put in place in terms of some large data sets that are considered relevant for pre-training so you have data sets for more like a optical imagery such as the foundation function mapping of the world that has about 300,000 samples you have the Minon a data set has about 1 million images with L cover labels and then you have uh I mentioned about the SAT spe train that has many of labels across different tasks you do have as well some more multimodel pre trining data sets a lot of them are with sentino one and two because it's essentially you have uh one of the easiest Pathways to start dealing with Optical and s data covering similar locations from this U pair of the constellation and the big Earth net as well both it's original version also the multimodel one paing again sen 2 and one is you can do a lot of things with them and then things are somewhat all over the place in terms of evaluation so there is a consistency in the sense that tasks that matter are T like classification CTIC segmentation and so on and here I say test that matter for more like this kind of optical data satellite data kind of thing that uh it's what a vast majority of the works do but when you start to think about multiple modalities and other types of imagery that are necessarily Optical there are other types of tests that can be relevant whether it's like uh assessing some vegetation indexes or other types of parameters so there has been some more uh works now trying to push for more standardization in the types of test that you should be looking at and adaptation criterias I can mention the GE bench paper is a very interesting one to take a look at uh in that regard and how they are mentioning construction of data sets these aspects of Licensing what should be your criteria for what are you're doing a fine tuni with maybe some limited budget to make it more even and these aspects that both in the construction and for chaining data sets and also for evaluation the different kind of things you want to consider but again I I point to this paper about the data sets for of observation there are no data like more data that they talk about how you want to uh data sets covering different seasonalities different types of data different types of Downstream tasks and so on to give one example of what we've been trying to do for creating this pre-training data sets is this aspect that you can leverage what we call the sort of auxiliary anary informations you have data sets that give you what are the climate SS you have data sets that give you what is the population density in certain locations what are the biomes so all of these can be relevant and useful for you to create ass set that are more diverse in capturing different types of uh overall appearances or scenarios like Urban Rural and so on so that's essentially to as the message of Don't Look only at images there's when you talking about GE spatial data there's plenty of other auxiliary data sets that can give you very relevant information that you should be looking for and then finally I want to touch a little bit the next five minutes or so is some progress has been taking place in the idea of generative AI for Earth observation data there are things that have been ready or happening for years particular using Gans at the beginning things such as how can you try to synthesize realistic for in satellite imagery from L cover Maps your conditioning inputs and more recently doing that as well is in the future models but conversation that most oftenly comes when I go talking with people in this geospatial domain GIS and so on is this concern about the Deep fakes right you can think about deep fakes in particular the three types of characterization here three categories one is the idea of s synthesis like when you can have for example you condition on a certain caption that tell you what you want to have in the scene or maybe you can actually condition also in our geospatial location and you can have other types of alterations things such as you have a certain layout that you want to create an image that looks realistic according to it and then you can have a pairing of the layout with things such as uh whether the city has been flooded or after an earthquake and so on or you can think about uh other types of manipulation like maybe you want this original upper left image to actually have a green area around it instead of the just a buil environment or you want to split that big building into three so this is actually another work that right now has been presenting I believe at the the workshop the Earth Vision Workshop by Sir Kumar Nathan Jacobs and others so very interesting work that essentially what they are doing is using this aspect of diffusion models and also the set clip that I mentioned before so the set clip offers you a way of having an embedded space that encodes information about your geolocation and then you can have a pre-trained diffusion model like what you have here in this Yellow Block here that you can try to further condition it in image you didn't see before under this aspect this a strategy that's called a control net where if you're not familiar the overall intuition is you have a pre-train model like this Yellow Block imagine you replicate this block you keep one uh version of this block Frozen and that other version of the block you're actually fine-tuning for your new data type and then you pass the features of this new block into the Frozen one and that starts to have guidance for your synthesis to consider this additional uh conditioning modality so essentially that's how they do this idea of conditioning on the location coming from set clip and also the conditioning coming from this sort of vector maps that you can have so essentially how you can try to synthesize images condition and text and and the this spatial layout the other interesting work is called diffusion set that is I found it particularly interesting that in addition to the strategy of the conditioning is how they embed other types of metadata information so I mentioned about how some works like the SC does this U positioning coding that becomes aware of your spirital resolution so you have an additional type of a uh coming with each token that Express what is special resolution and here they go beyond that and do like for the geolocation they do that for the cloud cover they do that for the time of the year and other types of metadata that you consider relevant For the synthesis of an image so that's a pretty I find it a pretty interesting way of trying to aggregate this information to condition not images just like what you have in broader computer vision but also absorb the sort of metadata and criteria that you know is relevant for this type of data that we deal with could spend a lot of time talking about this but just to highlight some of the potential usages and risks Associated to that is like you have many of this work showed up with the motivation of things such as like you can remove Cloud C yes go ahead it's a very good question this works uh I mention they are technically more the RGB and uh unfortunately how most of the things unfold is we have more RGB works but it tends to be that once you have RGB you start to have the further developments okay let's try SAR let's try the the other near infrared and so on so the ones that I am aware they have been mostly RGB uh there was before this other one that was using Gans at the time that they did play with SAR if I remember correctly the quality for SAR was a little bit worse uh but um yeah to this the short answer yes most of them have been RGB but I would in Vision soon you have the natural development to started the blar and so just mention a few of the things to keep in mind about that like so there's this potential uses for like cloud cover removal or doing super resolution in more in better ways or creating synthetic label data sets like if you have it's one of the challenges like if you think in a conventional way of creating like for L cover or other types of features you have the image and then you manually what you have the image if you can flip things around that you have a conditioning semantic layout and you condition you synthesize an image that obeys it you already have what is the label Associated to that that you can do training of models about that but that can be pretty problematic with like some of the concerns that people have is like how you can imagine uh that part is trying to synthesize some images s ating some changes to infrastructure or other types of events dump that into social media and start to explain a few things or even further when we think about how we scrape data for pre-training models you can imagine like a at extreme that you could try to intentionally make big public data sets out there or try to poison some volunteer based data sets with some adulterations that maybe are not very evident and that can Cascade in your pre-training models that can be very problematic so this overall aspect when you generate your data you Pro you having the development of good practices of like how do you make watermarks and so on documentation how is dat is coming from like you think about a super resolution kind of you need to know where is the methodology being used there to see what's what actually can use for a downstream test or not and um leveraging the nuances of the date like how I mention there's this creative ways of at least incorporating metadata and so on those are important things to consider with that I am done with this part of the covering the self-supervised learning and so on we can take maybe a few one or two questions before we jump into the the other part of the to beautifully Don thank you so much such good you um I don't have a question as a as a comment and you can maybe elaborate on it I work in private sector so you know like we have to always use non-commercial uh commercial ones and with this field is little bit challenging so for example when we work in the open field of like regular images the transferability of the models to this field is very challenging and then we have the problem of like cleared data and uncleared data so there is lots of review and work being done that is not being accessible and then on the top of that one some of the models like Street clip and building on the top of it like pigeon and Spade for example by Nvidia it's all non-commercial use so anything we cannot we cannot on the top of them because of that one unfortunately yeah yeah I I can imagine how CH must be like I work more we sort of in between could say between what is Academia what is private sector as a National Lab we're doing applied science but we are doing nonprofit and we have access a few times like data for example the high resolution ones but then what you can publish this data becomes a problem and also what you can use for training this data can become a problem right like um a mention how we ID for This research of enabling the reason across the different modalities because if you have a lot of gallery of pre-training and low image low resolution one that can think hopefully enable you to need less data label data for the high resolution ones so that's one active F research that everyone thinks that would be so great to have that I I think it's good that we are finally moving this direction more self-supervised learning to enable that but I don't have fortunately a solution for that yet but I got I can tell I guess is that people are aware trying to work on that so yeah and like sometimes like spade for by Invidia is for synthetic data generation it's a it's a powerful beautiful model but it's for non-commercial so for example if you want to do a downstream Tas like semantic segmentation or something like that on it you cannot yeah that that would be definitely an interesting thing to to discuss more like this whole licensing aspects can be very problematic one of the things that we're not covering here but overall it becomes a huge problem thank you again I'm just going to chime in one thing you didn't mention um was the ontology like some sort of a standardized ontology for geospacial representations which I think you're gonna tou a bit on that right okay the next part right now thank you for the plug and then you can feel free to chat me further as well like all right I'll pass the mic now possible of your presentation I'm sorry will a copy of your presentation be right just could you rep question uh the question was a copy would be available so for myself yes and I I think yeah from my end of some of these slides I as a National Lab of course I'm not showing any classified information here or anything it's just that you have separate rounds of approvals one for presenting and another one like for putting out there the copy for anyone to download and stuff so there might be a little bit of a delay from my a few slides just because of that but the likelihood that I would not but s is low I am expecting to put their soon so all of them all the materials uh they're going to be at the website in the gallery part there just that some things might take a little bit longer yeah and that's a good reminder so all the materials that we're going to provide after this either supplements or goes beyond what we discussed is going to be on our web page for this tutorial at the very bottom reception so um f was right your question was actually a great plug a great segment what we're going to talk about um so I work a lot on what is known as spatially explicit machine learning um and this is not the kind of machine learning task that is often discussed necessarily at cbpr or neps because this isn't necessarily just neural networks so how do we represent spatial information how do we represent things like proximity being along moving along the same direction um being in the same region so how do we represent spatial information like neighborhoods for example explicitly at the algorithm level rather than just trying to learn it after the fact with a neural network neural networks are great with learning but uh we can represent spatial relationships we can create for example uh spatial clusters or we can make spatial predictions given set of accidents happening where is the next accident going to happen or where is going to be the next hotspot of accidents spatial prediction or spatial clustering what where are going to be the next uh what are the next climate regions going to look like so these are inherently spatial problems and we can create a spatial representation for more um succinct and U parsimonious learning problem so this is what I will talk about and this is something that I actively work on in my lab um since we've seen great examples of supervised learning semi-supervised learning by Philipe already I thought it will be appropriate for him to talk about unsupervised learning so I'm going to focus on unsupervised learning in particular specially expit on supervised learning so as far as I can tell everybody has some background and definitely a lot of interest in machine learning so in that Pursuit you might have seen something similar to this even though it's not in this graphical format this is something I created uh this is from Mitchell's artificial intelligence book I think that's one of the foundation books when you study computer science and specialize in artificial intelligence where his point is there is this experience there is this thing we want to learn where are the Clusters or um what is the value of this variable we're trying to predict or where are the bodies that we're trying to extract from an image where artifacts um there's some feature engineering you can think of this as rearranging your bands calculating indices Auto rectification so basically getting your data ready and then there's some learning process that's being done and then there is some interpretation of this and of course this counters the the other way of doing it which is just using human interpretation and knowledge basis so creating these ontologies and especially in spatial decision- making this was really the field to be in back in the 70s where if you're an environmental problem there was a huge repository of next things to check next things to do uh as a matter of fact people who created the term machine learning back in 19 56 at this small Workshop in darkmouth uh they were working explicitly on these types of ontologies but of course how do we move kind of marry these two worlds incorporate information about what we already know but still do this in a more unbiased automated way so then we talk about spatial representations how can we take what we know about spatial relationships represent this in a numerical way so that we can use some of the existing methods that Philipe discussed so how can we marry these two worlds for spatial problems we can do this with spatially explicit machine learning um I will put this paper um I like the title because it's a pun and I like puns it's a it's a paper called is spatial special uh by Michael goodchild he's a professor of geography at UC Santa Barbara uh he makes a big discussion about how spatial data is special and that it requires this specific numerical representation so after we talk about this learning problem on supervised learning just a quick breather so uh reminder we talking about clustering so we don't really have samples to learn things from so you don't really have this predetermined example of what the answer should be like we don't know the answer so we need to partition the data somehow without seeing any examples so we have to describe the data somehow rather than in supervised learning we need to create a function approximation and in the spatial Sciences domain spatial clustering so if you look at different Journal papers in this problem uh the problem is called regionalization defining spatially continuous clusters of your data think about climate zones think about your plant hardness zones if you if you have a green thumb um those never have little Islands in them they are not pixelated these are contiguous regions so this is what we expect so how can we impose contiguity this is the regionalization problem and this is what I will talk about as a part of my active on ongoing research so one way of doing that kind of like your question with ontology is a graph-based representation and graphs are a very natural way to represent spatial data and spatial relationships as a matter of fact graphs are also very widely used on neural networks we have graph neural networks for spatial problems they're an optic so I want to quickly talk about what is a spatial representation what is a spatial graph representation what does it mean so in every graph we have some vertices um and if there is some relationship between vertices they can be connected with these edges so this is basically the um the simplest way of thinking about a graph and of course uh we can think about this when we um when we're working with spatial data of course there's some um graph bottlenecks to use in graphs for example the number of edges explodes very quickly in other words let's say that you have 1 million pixels and you want to represent chain neighbors with graph edges so you're going to have billions of edges now so this is going to be a very large data representation so being able to represent this effectively is important that's not good think I got some know what happened there so just to show you an example so this is what we mean with the regionalization problem where we have some underlying data that we want to create these spatially contiguous clusters what you're looking at here is uh the map of Uruguay and we have temperature data we just want to create similar temperature regions but so this is much different than what we what you might be used to with K means or TSN all right so when we are creating clusters spatial proximity is important we have to create regions they cannot be pixelated all over the place so when we talk about graph based represent ations going back to your question we can have data so these are the county these are the um Municipal districts in Uruguay uh different polygons and I have some values observed in them so this already is a hard problem to represent because first and foremost um one great thing with CNN's that brings people like me and Filipe here who work on earth science on Earth observations is CNN's are inherently spatial whether we like them or not because you have a convolution you have a filter that spatially rolls over the image now that doesn't really work when you have Vector data with irregular sizes areas um there are some regions in Ade that are so big that you can slide a window over and you can still be in the same data point whereas there are some very small ones where the same slide can give you tens of neighbors so how do you resolve that you can unify this with a graph where you can represent every spatial location with a Vertex the red points there and if there is a conceptualization of neighborhoods for example if you consider um a neighbor that shares an edge to be a neighbor right next door you can connect them with lines so that's how you create this graph so this is a simple graph representation this is what we call a queen quanity graph representation of the data and going to this representation again what you're looking at on your left it Chang is a learning problem so when we think about clustering what we're talking about if you think about our Loyd algorithm we start with some core points and then we move those core points until elements that are attached to those cores are as different as possible but when you have a graph data set this changes the problem a little bit this representation now we are actually in what we call a pruning game in other words we need to cut some edges off this graph so that we create these subgraphs that are very that that consist of very similar elements so this is this changes the problem so one of the um very important mathematical tools here are trees also spanning trees that we use a lot in spatial unsupervised learning we can take a very dense graph where we have to parse through all these graph edges and reduce it with a spanning tree you can think of this as connecting the dots you basically only keep edges between the two most similar elements and you come up with this tree and one mathematical property of this is you don't have any um closed loops and you Traverse all of the graph elements and now we're looking at our clustering problem becomes finding these optimal edges to remove so that we can Define these sub trees that are graph so this is the clustering problem but as you see the spatial twist is much makes this problem much different so coming back to the question of on ologies representations graph graphs are very powerful ways of representing spatial data because they allow us to solve spatial problems so this is the end result once we remove those edges from the trees and we can basically play around with different parameters like number of clusters this gives us these clusters that are all spatially contiguous the results for CH means is much different than this it's color are just all over the place because there is no concept of contiguity so this is what we call the tree based regionalization basically represent every spatial unit with points depending on the neighborhood conceptualization connect them and then remove edges uh create a tree then remove edges between the similar elements until we come up with sub trees that represent different clusters and very similar to the L's algorithm and we do that by looking at standard deviations we basically want to minimize the deviation within a cluster and maximize it between different clusters so that's how we pick which edges to remove so we did this uh to create temperature regions in contiguous United States um the accumulated data at the county level so this is 2 meter um 2 m resolution simulated temperature from a global climate model like the ones that is used in the climax model that fipe mentioned uh this is for um the march of 2022 V just aggregated temperatures darker color means hotter and what I want to do is I want to create temperature regions purely data driven I'm not going to use any climatological information other than the data but I want to acknowledge the fact that the regions need to be spatially contiguous so this is the CES result as you see there are patches everywhere but the data there's already some spatial autocorrelation that already shows you this north south stratification right but we can't really resolve areas especially Colorado is very hard right you have the mountains effect of the mountains you have very big differences so it creates a lot of patchiness in this area where the temperature shows a lot of variation so we canot resolve this with a non-spatial method but when we use the method I just mentioned this is something that we developed where we using trees you see that the result is contiguous now we are able to Define regions when we connect the data this way you can also think about defining uh probabilities and this is a recent paper we published um basically clustering problem is an optimization problem and when you are imposing constraints on an optimization problem that means that your search space is much narrow in other words your probably missing better clusters by imposing that they need to be contiguous they need to be neighbors right you might probably be able to create more homogeneous regions if you don't have to impose that they need to be regions or contiguous so we assess the impact of this constraint by clustering probability so the lighter colors means that low cluster probability in other words that spatial unit can flip over clusters very very easily if the data was perturbed and no surprise at all those those ones generally exist at the peripheries of the Clusters where it already has a border and it's easy to switch and we can see the impact of it'll comeb back we can we can see the impact of the spatial constraint on the result so this is what we mean with fuz regionalization where we can create these uh probabilistic estimates so um this is just an example of a type of spatial machine learning regionalization problem but I want to talk briefly about this um I'm glad that onology came up at a great time U but this is not what I call an ontology this is just a numerical representation of spatial information um so spatial ml methods are being developed in parallel to a lot of non-spatial mod methods like cnns deep Neal networks being applied to spatial data um but going back to the ontology one keyword I would um I would leave you with is the knowledge graphs which is uh I would say one of the biggest undertakings in geography since GIS um there are multiple Labs working on this where um the idea is to connect spatial information with very large graphs like this so that you can query things like um even something non-spatial like an event like the news using geographical commands and information so this is a huge um undertaking in terms of representation so if there are not questions about what we discussed we can move on to our application segments question a quick question so part of the um the pre-based regionalization is to be continuous that's the constraint yes and so if we look at the map of the United States let's say you know we're like um where it's in let's pick up an area in California let's say in the middle there that temperature matches for example Michigan is that lines going to keep continuing there or how how are we dealing with that that so that's why they use the um some of some of deviations what we basically look at is so when we when we create a cluster here everything that's connected with the white line they're in the same group so we assess how much variability is there in that group compared to all the other groups so if you connect say san berino county in California to an arbor you're probably going to create this like huge lanky tree that crosses half of the country right the the SSD there the deviation is going to be very high which which is going to result in a lowquality cluster because there's a lot of variation with clustering our whole goal is to reduce variation by creating homogeneous groups in the data so this is how that will be learned by the model awesome the second quick question is like one part of is also you could say oh I want two clusters I want 10in clusters yes um so I mean can we let the model pick how many clusters like would that be har instead of dictating how many yeah so the part of the work we do here we actually use something nons spatial we just use um the AIC between different clusters like we basically track the quality of the Clusters um actually we use something very similar to like the elbow curves I'm sure you've seen those in clustering model so you can use things like silhouette index because after the Clusters are defined um I not mean to say AIC this is for supervised learning I meant Mutual information so we look at Mutual information in other words we look at like how different different groups are and how similar how s similar the groups are um so we we we can simply quantify that and test that for different number of clusters and basically pick the pick the best one great work thank Youk here I have a what they already open that slide was the one that I had I'm just going started I'm just GNA run run off because just GNA take a both fast like five minutes next yeah so from my end of this one one the final things I would like to mention is looking more about the pipeline when we do this analysis of bir observation data to do mapping such as sometimes we get requests for mapping entire regions or maybe the entire countries how do we go about that because training your model is just one part of it right so using as example this part of building footprint mapping that's something we do this context of like population mapping for instance when we think about our PIP plan there are different components that we need to consider right you get first request like can you cover area X starts with do you have data for that does that data fit in water your model can handle in terms of like the spatial resolution your cloud coverage and so on once you have that okay cool let's deploy our model see what we have already in house that is suitable for that and then you see that maybe this new area has some combination of like a some desert or some mountains that maybe undergone some snow coverage that the model has never been exposed to so you're like okay I need to do some uh extra fine-tuning so you need to go about doing more annotation about that specific area so we try to operate them for this type of problems like The Bu fprint model that we already have in place under this sort of active learning approach of looking where you have a model deploy the quality what you get and when you have to do a fine tune you have some diversification criteria whether you look at the geolocation or some of the embeddings that you can get for your pre-training model and then trying to facilitate as well the work of the folks doing the The annotation because this can be very intense if you're looking at a let's say a then urban area try to label all the buildings so instead of what we can try to put in place our ideas of like here's the current model output highlight to me which ones are wrong and fix those instead of reannotating the whole thing so those are some considerations to think when you think about deployment pipelines for this type of data that important to keep in mind the other part to highlight is again our final product are those Vector Maps so when you look at the first step of vectorization you see this kind of very irregular shap shapes that when you start to talk with people like that they want to make use of this data they one of the it becomes a major trustworthiness issue because they look at it and say this doesn't look like Beauties so you can have maybe the even like let so a better uh intersection over Union a pixel level with that version but that's much less appreciated or less trustworthy than when you have some corrections of your vector shapes and so filtering your false negatives that are super small kind of thing for your final product quality and then it touches the aspect that those Imes are huge you cannot simply take for granted that you can put a big batch into your GPU and process everything in parallel you rather need to think about how you do the tiling so you split your big image into blocks suppose like 1,25 by 1025 pixels you need to be careful that you have some overlaping between those when you're doing the stride because otherwise you create artificial boundaries that can become like false negatives in your protection modules or you can create discontinuities or your structure that actually don't exist and then you need some uh tools for learning this kind of synchronization this management across maybe different gpus or different machines different nodes for quite a few years we were working for example spark as one of these tools more recently we're now playing with dusk that's one tools that I would point you to if you're interested in doing particular more thinking about inference side of model mod that you want to deploy over large volumes of data and coordinate this process across different machines dusk has some cool stuff including some uh Jupiter notebooks collab things with f AR so you can easily get up to speed with that instead playing with it that's one of the things that I wanted to show here and then the other example I just want to mention a bit is how you think about problems like uh for example if you're trying to do damage assessment regardless of what kind of damage it is or what what area it is you can have you start a problem like you have maybe very large areas that you want to deploy the model or over there how do you start to look at why exactly is more likely that we saw damages for both labeling data and the plan your model so there are resources out there how I mentioned for example there is this United Nations seces of final data products but also you have um resources such as the live UA website that gives you some points of where some events are happening or you can you can rely on map sometimes even of news Outlast or anything that gives you some spatial information where you should be looking at so again it's not only about the images and also when you define the problem like how do you do about creating guidelines for The annotation because it can be very subjective sometimes when you're talking about levels of damage like uh one of the things we try to build upon is of course things that are already out there for example the material got from FEMA but then these guidelines they do not consider that you only have image from above you might have a guideline saying like well you have this level of damage if you have a crack in the facet I cannot see the facet so you need to do some conversions and you need to interact with the folks doing The annotation because you can think like ah maybe I'm GNA add like another field for them to put their level of uncertainty if you start putting too many choices it becomes more stressful to work and more like a VAR is much more to the point that you you have less reliability in your label data than and less efficiency in the labeling process and this part of efficiency of delivering process then also comes into place St trying to leverage what you already have in hand like in our case we can think about leveraging your building uh footprint mapping models that you already have so you don't need to retrain something from scratch just for the damage assessment you'd rather use this model for the pre and post event imagery look at the internal representations and just train a decoder had to assign damage levels to this building Footprints and for an annotation perspective as well this is beneficial because you're just asking the folks what the damage level that you think uh this structure has undergone instead of having to draw the whole polygons of each individual structure and then finally here bing one SL from uh KY reer so he's a he was a keynote speaker of another event that we organized at the walk the the inter application of computer vision the beginning of the year she was talking about from the perspective of folks like an American Red Cross that she tries to do this this uh approach of bringing these techniques for usage for awareness for guiding their work on the on on the ground and then there are plenty of China is related to like he she said like a you cannot expect folks to run their Jupiter notebook to get some inputs they need to have like a maps and data formats and stuff that they in the ways that they already have the the softwares or research that already used to work with because they are already Under Pressure you do not put like more things that are complicated for them to use and you cannot put things that cost a lot for them to use either especially if you're thinking about humanitarian uh type of uh uh efforts and also like again this aspect of like a you need to have ways of quantifying uncertainty you have to have ways of describing what the model is doing at least in a way to have the trustworthiness and then being able to use because you cannot expect people to be AI experts to be able to use those tools so I already covered those bases of the resources so only one quick thing I want to mention before or tou is another application other resources for you guys to take a look at is in particular the community that you have around remote sensing so the GSS has plenty of journals uh that you can look at if you want to publish stuff on that and if you want to get like good material to learn more about it so you have theg RS that more longer journals you have letters you have special topics strongly encourage you to look at that and many other events that we have been doing uh there particular the image analysis and data Fusion we have by a bad coincidence was today as well the Earth Vision Workshop the some collaborators have been organizing for multiple years around cvpr I think we cover different bases more like a giving the background such that maybe next year or so you guys are still interested in that doing more PL stuff and maybe you want to publish or get more into the techniques go to Earth Vision keep an eye on that for the next vprs also in lclr and the Wy bmvc we are having presence where us or some collaborators so there's plenty of cool stuff for you guys to look in terms of Ito e things and also there's the ACMC spatial and the sprs or other types of resources that I encourage you to look at to get more this

