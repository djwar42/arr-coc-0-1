---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=b_M_QNKAhTQ"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:02.828Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=b_M_QNKAhTQ

4650e8a5-7f2d-46f8-bd3b-345f8c814cbc

2025-10-28 https://www.youtube.com/watch?v=b_M_QNKAhTQ

2a30fb89-1080-4464-a4e4-1433f12d560e

https://www.youtube.com/watch?v=b_M_QNKAhTQ

b_M_QNKAhTQ

## ComputerVisionFoundation Videos

okayy good morning um so thank you for uh being here so early in the morning on the first day apologies for that not our decision uh and welcome to the first joint egocentric Vision Worship uh which is held in conjunction with cbpr 2024 as you know and it will be a full day uh event uh today and so thank you um on behalf of all the workshop organizers that you see here in the slides uh and thank thanks to them for uh um the organization of uh of this event um so I'm just say I'm just gonna say a few words and then we can we can start with the program so this is the first joint entric Vision Workshop we called it ego but I think an issue we need to address is have we met before so this is the first ense Vision Workshop but I mean many familiar faces and probably already know each other some of us know each other very well um so I I think we need to spend just a few words on on what's the history behind this first edition of the worship because there are some past events that are very related to this worship and these are the most related ones there are maybe others older than this which are not in this slide uh but for sure um some worship series that are very related to this worship are like the Epic worship series and ego for both the human body hands and activities from egocentric and multiview cameras and uh ARA tutorials and P worship um and everything starts with the the ego sport which was established um last August last year and uh these is a diverse set of people volunteers that contributed to the field and the idea is that this board tries to bring together different people and different uh communities subcommunities who have been working on egocentric Vision in the past and uh the result is these new series of workshops and the idea is to unify previous efforts so that people can get together in the same room and talk about the same things more easily uh and so uh the eist worship brings together uh a diverse set of challenges so as previous workship we base a lot the the whole program on challenges because it is a good way to get together people who are actually working on the specific uh on the specific problems that we have and so specifically uh the hollow assis Community ego Ford and ego X Ford the ARA community and the Epic community and these are the main uh uh the main communities which are in this initiative um this is the um a very brief summary of the program so we have Keynotes and we have challenges we have um then we have a poster session and we have a distinguished paper Awards uh you can find our program also online uh so very briefly we have uh four different Keynotes at the times that you see there so the first one is going to be in a couple of minutes uh by Jim Ray and um um and these are briefly the times uh but I mean you can find all the information online um and then we have uh four different slots for the challenges where the results of the challenges in the different um in the different uh data sets will be will be announced and these are the times that you see here uh in the program and then uh we have a poster session this is during the break uh couple of information about that is um this is uh going to be in another building in uh the arch building so you have to to reach that building it's in 4E uh it should be the same place where you can get your posters if you printed through the the their service and we're going to have um 18 extended obstructs so this this is ongoing or already published work that of people who submitted their work to the the worship and they were selected to present there are poster numbers which have been assigned so if you're going to present please stick to your poster number and use the appropriate poster boards and also we have been instructed that we should be uh able to use those boards only at those times so please remove the posters after the poster session uh and we also have 17 papers who have been invited from the main conference from cvpr so these are cvpr works and the idea is that uh this will be um we hope helpful for you to get a preview of what cool works are going to be presented later uh in the in the conference and you also see online the presentation details for cvpr so you can make a list of things that anyway you want to attend uh during the main uh cbpr uh conference uh and this is same time uh 10:20 to uh 11:20 um all right we also have a lunch break of course and a coffee break in the afternoon and uh right after the lunch break we're going to have uh the egoist 2022 and 2023 distinguished paper Awards uh where we will give Awards uh to some papers that uh have been selected by the egoist board and um which have been found to be uh particularly impactful despite being published in the last couple of years so I invite you to attend and to and to and to see the list which is anyway already online on our website if you want to have a look at it and uh and finally we're going to conclude the workshop uh at about 5:30 pm. when we finish everything uh all right so we're now ready for uh the first um first keot choke and Sant is going to um share the session but give us a moment for this adap I worksh so today we are very fortunate to have Jim who is a Founder professor of computer science and Industrial and enterprise system systems engineering at University of Illinois Arana champag previously he was a professor in the school of interactive Computing at the Georgia Tech Institute of Technology where he co-directed the health for Center analytics and informatics in this keynote Jim will be speaking on egocentric view of social AI please join me in welcoming Jim it's a rather large file so is this on by the way testing should be on doesn't seem to be tting or should I just be loud when I touch this it doesn't make a sound so yes true or is there no and now it's on okay and let me just check whether the whether the audio on these slides play yes just in case there's a problem yeah that's why I was thinking so hard I was like maybe don't not trying to find out yeah if you feel confident but is is working yeah is working yeah that's why I was thinking so hard I was like is it is the mic picking I'm not sure you guys hear in the room I can just mic the I can just mic the laptop it's not a big deal you know okay but is it if you can yeah let's try one more time the WR that's why I was thinking so hard I was like where does from okay let's just do thate it with the mic on ah right that's the typical zoom problem um is this fine or do you need I don't need this presenter notes that work great awesome but I'm back to this again I think on Zoom is it still a problem can you uh do your magic again I think every time I go back into the thing it it's just an Italian yeah all right well thank you very much for the invitation to be here um and delighted to share with you some of the work we're doing at Georgia Tech and now at Illinois uh on understanding social interactions actually I guess I'm five minutes early is it okay to start or should I hang out awkwardly for five more minutes up here okay better for me all right so um I wanted to just set the stage uh for a second uh by giving you something to take a look at you know Jack Ed over in accy he committed suicide yeah hi listen I got to get those photos from the re toucher by tomorrow morning okay I'm leaving you H squee I can't hear what okay you too thanks L see you on guys Z K I'm leaving you K Keys here are my keys here's my American Express card here's my Bloomingdale's credit card here's my checkbook I've taken $2,000 out of our savings account because that's what I had in the bank when we first got married was this some kind of job here's the cleaning here's the laundry ticket you can pick them both up on Saturday you you have to you tell me what's matter I'll pay the rent I pay the bill and I paid the phone bill so or you really pick your time St well I'm sorry that I was late but I was Missy making a living all right come on okay we stop now so that's everything all right so even if you didn't watch this classic 70s movie from the US um as a human familiar with social interactions you can already make a lot of deductions about this very short clip you can think about what is the relation between the two the two actors in this case Ted and Joanna you can understand how they view each other get some clues about that you can understand uh what's happening with their relationship you can make predictions about the future what are their goals what are they likely to do next so the building blocks for these kind of inferences of course are the kind of the meat and potatoes of multimodal analysis that we've seen over the past few decades in our community so understanding the visual component of that interaction gestures facial expressions and so on understanding the the dialogue who said what to whom but for humans those low-level details are just the building blocks and all the important stuff actually lives in the interactions between the individuals and their relationships over time and understanding those interactions and relationships is really the core of what we're considering to be social Ai and this is essentially kind of the Grand Challenge for our field for the next several decades is to build AI models that can really kind of get to the core of those most basic human interactions and understand them in fine grain detail uh on the basis of data like you just saw and you know I don't have to tell you different different examples and applications this might enable in virtual reality in Social agents and so on okay so my goal today is to sort of give you a snapshot of where we might where we are as a field with respect of this Challenge and along the way just showcase some work that we've been doing so in order to understand social interaction the first thing you need is some kind of conceptual model how do we describe this process of interacting computationally and the starting point in this case is just two individuals and then the basic communication channels they the words that they say the sounds they make and then other their non-verbal cues combined and in order to communicate the most important thing is that you have some set of shared beliefs things that you have in common that are the substrate for the interaction and these can exist at many levels so at the lowest level there's the interaction itself how do you take turns how do you coordinate so that you're speaking appropriately at the right times all this is done unconsciously by humans all all the time throughout our day then there's higher level things like our social relationships how do we relate to others what's the nature of those relationships and then the interaction takes place in a specific situated context like in this case in an apartment in an example that you saw or at a ball game or what have you and that context informs as well the interaction and lastly there's some set of beliefs about the world some set of facts we have in common and then we use that in part to communicate now one of the most important things that happens during communication is actually establishing these shared belief leaps you figure out does the person you're talking to understand you do they get what you're saying you rephrase you kind of figure out what's going on if there's a problem and the and that is in turn driven by your ability to make inferences about what your social partner believes so your social partner has private beliefs that are in their brain you can't access them directly but you can make inferences about them okay and so this process of inferring what your social partner believes about the world is a critical part of communication and something which is just beginning to be studied computationally so if you then think about going back to that clip what it would mean to analyze that clip there's a bunch of things that we need to make progress on so obviously we need to make progress on the low level modalities particularly the visual ones I think speech is in relatively good shape uh but the non-verbal communication Still Remains I think is a challenge for our field um going beyond that we have to talk about how people are attending to social cues in their environment and then ultimately build this theory of mind which is your ability to make these inferences about your social partner I was referring to earlier so these are some key challenges that I'll that I'll that I'll touch on today now in the process of looking at this I just want to emphasize due to our Workshop here um the importance of doing this from egocentric perspective okay so this is the perspective that humans actually have when they're interacting and one of our basic tenants that got us into this area a long time ago is the importance of capturing that interaction from the perspective of those that are participating directly and you can see a lot of reasons for doing this scientifically and also just pragmatically and we still believe this is the the best way to sort of build agents that can understand interactions to have this perspective so I just want to walk you through a couple of basic problems just to kind of set the stage what are the challenges that arise in solving uh these various tasks and the and the first problem the one that we looked at first is this basic idea of eye contact so this basic interaction of looking at someone's eyes it's a visual perception task it's it's a building block for social interaction it's extremely important in communication and so early on we built a data set that let us tackle this problem and in this data set there's a child in this case I I'll talk about why it's a child in a little bit uh sitting across the table from an examiner an adult who's wearing a pair of glasses with a camera right here on the bridge of the nose and you can see from the setup that whenever the child looks at the examiner they're looking right into the camera that's in between the examiner's eyes so then you can frame ey contact detection as just a binary classification task and in that setup it's actually quite easy to set and here you can see the performance this is a child that we collected data from in Atlanta at Georgia in a lab that we have they had there um you can see green whenever the child makes eye contact with the examiner The Examiner is wearing the camera here of course and then you'll see in a second this is a boy with autism at our partner site in Wild Cornell Medical Center um and again he doesn't make it as much eye contact but we when he does we were able to detect it with this method all right and so one of the key proper properties of this method is the accuracy and this is a really important aspect particularly when we want to think about using these types of computational models to support development developmental psychologists behavioral psychologists that want to use these tools to treat humans to deal with people and their various U you know uh uh challenges and conditions that may arise in terms of social interaction with autism being kind of kind of economical example uh for these folks to use these methods as tools they have to be accurate and so one of the things we've really pushing on is understanding how accurate are these models and then having the ability to validate them and so here you can just see evidence that shows that this eye contact detection model that I just introduced is in fact working as well as humans are at this at this particular task so each yellow dot is a human expert Raider of eye contact in video being compared to their fellow humans so the spread of those yellow dots is kind of the uncertainty or the ambiguity that humans have in assessing the social behavior and humans of course are the gold standard for all of this work and they are also you know not perfect in some sense in assessing or agreeing what's going on and the little green diamond is our algorithm and it's right in the middle of the humans so if you took one of the humans away replaced it with AI then the accuracy and rating that behavior would not change and that's our kind of definition of human level performance okay so from that perspective the research program here is actually pretty simple we've been doing it now for you know for for for more than eight years um which is simply you know pick and pick out and and identify all these important constructs like eye contact collect appropriate data sets and and and appr appropriate populations build these AI models and then validate that they're working as well as people are in solving these tasks and if you complete all those you know constructs you can then really build this foundation for social a all right so of course I contact is just a tiny example of some of the you know visual behaviors of interest and the visual behaviors themselves are only a small part of the overall space let's talk about some of the others so more generally we're thinking about individual shown on the right here interacting with with their peers in some environment and then ey contact in that case is just are any of my social Partners looking at me but there's many other social gaze behaviors that are very important for example we'd also like to understand where our partners themselves are looking who are they attending to what parts of the environment are they attending to that information helps us understand what they're what information they're receiving it helps us understand this perspective taking what what they might believe about the world okay so this is a basic challenge to understand the attention of others and that's framed uh as you can see in this example so there's a video clip and at the top row we focus on the woman uh you put the green box around her head it means she's the target of analysis and then we identify where she's looking in every frame and so the Green Dot shows where we think she's looking and that includes the case we can't tell where she's looking because she's looking somewhere off off the screen and in that case you just see the green box with no no Dot and then we can do the same analysis for the man in in the row below repeat this for all the people in the scene to understand their patterns of visual attention and so I'll just present some as at the moment unpublished work that's our latest uh effort along these lines and use this to illustrate this sort of task so here's the definition of the problems so you have an image you got a bounding box for the head your output is some heat map that shows where you think the person is looking including whether you can see them in see it in the frame or not and the ground truth data for this of course is always humans clicking in the image to to know where they think the person is looking okay and so there's been a long history of work in this area I won't recap all of it but this is an early paper with the two-stream architecture we introduced an architecture a few years ago it was much more complicated and also worked over time we introduced a data set that's still still the kind of the the standard data set for video-based uh analysis of this task and then recently things have gotten really complicated so I'll just highlight for you some of the uh properties of some recent models they're fusing all kinds of different channels depth body pose focusing on the eye Etc all in an attempt to drive the accuracy higher for this task and these architectures have gotten super complicated okay and not only that but then adding all these separate modules requires them to be fused and this Fusion step is also challenging and if you don't get it right then you don't get the benefit of all these additional parameters you've added to your model which in many cases are a massive number of extra parameters now this is interesting because this is in contrast to the progress we see in other parts of computer vision where that now there now exists these very large mod models with many parameters but a single model which is capable of solving multiple tasks at the same time okay and so in this case as you can see dinov V2 and some of the standard things that it's able to do if we go back and look at the cues that are that are thought to be useful in predicting attention we can notice an interesting fact that these cues also take the form of dense prediction tasks in the image predicting at every location something which is happening which is relevant to the task at hand so this raises kind of an obvious question could we laborate these exist Foundation models and actually solve this task of visual attention estimation now you're probably used to you know these Foundation models doing everything so when I when I say that you may just say oh yes you know sounds reasonable but the thing I want to remind you is that the people that built these models they didn't care about social detention okay they didn't care about people I mean in the context of their models um you know and they really had no idea about this as a task so it is a question actually whether or not these these models and their representations contain sufficient information for us to understand and visual attention Okay having spent This Much Time on it you can imagine that the answer to this is yes okay so this is a new architecture this is the work of my student Fiona who's I think somewhere in the back of the room right there um and and the name is entirely hers as well as everything else um and so the way this thing works is that it allows you to take this model this dyo V2 model you just freeze it you don't touch it in any way and you utilize the representation that it produces and then with some additional Downstream layers you modify the representation to extract the information information that it contains that is relevant for this visual attention estimation task okay so this is in some ways a new approach to tackling these social Q related problems not by collecting massive data sets and training complicated architectures but rather taking the existing massive representations that already exist and then asking the question how can they be repurposed to solve these tasks all right so this is doing really well compared to all the previous work and I'll just show you some examples that's a mistake right there so these are all examples on the testing set for our data set we collected earlier I'll just show you briefly some out of sample examples these are just some YouTube clips that we downloaded around the model on um of course Works uh less well on these as as the some of the others but still works pretty well okay so I think this is an interesting finding and I think it has a couple of implications but the m one that I want to focus on is this hypothesis okay that you can see on the slide it's possible that these models that we already have our hands on so to speak already contain extremely rich generalizable representations of the people and their behavior they're just buried in the weights somewhere and the question is how much much information is there and if it's there across all the modalities of interest if you think about all the models that are out there currently this could be a really interesting path to tackling these problems now one of the consequences of this hypothesis is that we have to think really hard about validation so it's in this setting we can imagine collecting validation data sets that are completely different from the data sets that we used to train the models and that's actually super important that'd be totally different and and that covered it all by the training sets the models are using and how to collect those data sets actually particularly in ways that would validate the utility of these models for conditions like autism and other types of Developmental and mental health conditions this is actually a non-trivial challenge that we started to work on so we're collaborating with a variety of psychologists with data sets around human social behavior and we're going to be building this validation data set to understand the performance of these models in a more kind of uh comprehensive and detailed way all right so now I'm going to uh accelerate a little bit and just get you through some of the uh other things that we're working towards so we've been talking about prettyy low-level attention related behaviors and uh in particular there's a developmental trajectory that's relevant here which is moving from sort of pairwise interactions one person looking at another to understanding interaction in this broader context and there's this notion of what's called Joint attention which is that you're interacting around some object of interest in the typical scenario a child is playing with a toy and there's an adult there that they're interacting with in some social setting um and this is known to be really important developmentally in in in representing kind of a milestone of a child's Social Development uh this happens at an age when children don't have very much verbal language they mostly can't speak but they can already communicate effectively with non-verbal cues so developmentally this is a really important window in time and we've been of building tools to an analyze these scenarios in our in our in our goal of understanding and supporting autism so you're probably all familiar with autism as a developmental condition I just want to highlight with this one finding here why doing attention analysis is so important so this is showing uh very famous study from a number of years ago and what the study shows is that if you take children at intake so when you first encounter them and you just assess their non-verbal communication skills and then you follow them for four years okay so a long time developmentally with kids that are 18 months four years is a long time you follow them and you and you track their language skill their ability to communicate with language and these are all children with autism by the way whose Communication in general is going to be impaired by their condition but what you can show is that the sample of kids with autism can be divided into two groups and the group that has high non-verbal communication skills and they're really young does much better than the group that has very few and so non-verbal communication through these types of findings has been shown to be really important for predicting outcomes and understanding outcomes in autism and so building better tools for analyzing non-verbal communication with computer vision for example can be a really useful tool in this field to help understand children's Behavior and this is an example of what that might look like so this is the same kind of data sets I showed you earlier these these these scenarios are designed so children have to interact socially in the context of a toy so this is a classic joint attention type of task and you can see the model is ability this is an older model by the way but but you still get the idea this model's ability to infer where the child is looking and this allows us to build a picture or a portrait of their of their non-verbal skill at an important window in time but turning this into an actual measurement and model of joint attention is still an open problem and there really still are not extremely good uh methods for doing that so in general this idea of joint attention interacting in the context of an object is super important developmentally super important for social communication and really there aren't any satisfactory models right now I would say a bunch of different work's been done but but nothing that sort of is really exactly what you want but I think given the progress and tools this is a good time to really mount an attack on this task of joint attention all right so I've been talking a lot about visual attention and I just want to uh shift gears briefly and bring in some other aspects of communication so so of course there's the language itself what we say with our words and we'll get into how that might be used in a little bit but it's also important to understand where people are listening as well as understanding where they're looking and of course understanding where people are listening is much more challenging because there's no eye movements to focus in on the tell where attention is being allocated it's entirely covert um but it is a crucial task and humans have the ability in in many cases to solve it by leveraging multiple modalities taken together and so the first piece of work on this uh that we did uh this is actually I think the first piece of work on this task and particularly in this entric context was done again by Fiona last year in collaboration with a group at meta um and you can see our paper from last year's cvpr that shows the results on that and this year we're going to be presenting um this conference an update to this work and this is about enlarging this idea of attending in this case we're just looking at where the where the egocentric uh user the camera wearer this or the sensor wearer in this work we're understanding where that person is listening now we're kind of extending this work to include the entire scene so the the combination of my attention and the attention of my peers to each other and to me so this is a more comprehensive analysis of these conversational interactions from from from these different perspectives and so this is a work that uh when w she W she are you in the room yeah right back there in the back W she's going to be presenting this on Friday so I encourage you to come and see her poster I think she's also got a poster in the poster session in this Workshop later in the afternoon so you can get a preview on Friday and then any unanswered here today rather I mean not not here but in the other building today any unanswered questions you can find when she again on Friday okay all right so let's just quickly and I'm not goingon to get into the details of this um in part uh because I don't want to uh I want to leave something to cover in the session so this is the basic idea I think this tells the the key story so we're going to go from this egocentric perspective of what am I looking at listening to understanding exocentric perspective what are my social Partners looking at and listening to put them all together and now we have a comprehensive characterization of the pattern of interactions among a set of people of course if all these people are wearing some some wearable sensor rig then we'll be able to get each of their perspectives from different uh points of view and that would be an amazing uh characterization of this pattern of interaction and this is really pushing heavily into both the egocentric perspective and the combination of these modalities so it gets Beyond this the classic speech based analysis which misses all the important information that's contained in the visual channels in these cases so this is an architecture it looks very complicated um for solving this task given the video and a multi Channel audio so this is a microphone array that produces multiple audio channels and I encourage you to find weni and get her to explain to you what's going on in this architecture all right so let me just give you a quick uh demo of how it works so in this video what you'll see is the entric perspective uh from one of the cam one of the sensor wearers and then you'll you'll see and hear the interaction and then we'll show you as this uh is evolving in time how the graph is being inferred and how it's being compared to the ground truth get graph that was produced by analyzing this data set just give you that one more time you can hear it better actually we we okay so that's the audio and video and this is the result of the analysis all right so that's a pretty complicated actually characterization of interaction but it's operating at the level that uh is necessary if you want to truly understand how people are interacting and what information they're receiving from their partners and from their environment so now I want to just uh uh basically uh give you a little bit more of a preview of some things to come so we've been building up kind of Step by Step uh these different building blocks for understanding these interactions and we've been building towards this ultimate goal which is really a kind of multi-party multimodal analysis people interacting in a variety of contexts talking in a variety of complicated patterns utilizing all the different modalities of communication Visual and and auditory to understand not just you know who says what to whom but what their goals are what they're trying to accomplish what their relationships are and and there's still no single model that is capable of doing this but I think we're making steady progress step by step towards such a model so this is kind of at the heart of the question is how to get out the Dynamics of this interaction using all these modalities and I just want to Now preview a couple of data sets that have become available recently that give us an angle on this so the first data set is one that you're probably all very familiar with ego 4D but ego 4D was actually a very complicated data set with many different constituent components and so one part of it that I want to highlight here is the social Benchmark within ego 4D which is a subset of the data that was specifically focused on understanding social interactions and so uh here are some of the statistics again this is just the subset of e4d that contains people interacting with each other in a variety of complicated environments and and very naturalistic settings as well and you can see the different sites where the data was collected um and notably it was not just sites in the US this is one issue that we face in general in this area is kind of a western bias in a lot of these data sets and so we're you know steadily moving towards uh improving upon that um variety of complex sensors um you know head headworn recorders uh ey tracking glasses and so on are present in this data set and then a variety of different tasks have been uh framed and and and worked on and this is just a sample of what the data looks like again this is the collection at Georgia Tech yeah that's why I was thinking so hard I was like hm so you can see the four videos all time locked together time synced and then a variety of complicated behaviors and patterns of interaction that are captured on this data set yeah if you feel confident and so we've been working on this steadily in the ensuing years since youo 4D came out and one of the most basic things we're trying to understand is the behaviors in these games that are being played and these are social deduction games like Mafia one night werewolf you're trying to guess what your partner's roles are and deceive each other essentially to win the game and so the first piece of work we did was looking at persuasion strategies understanding how people are trying to uh fool their peers through what they say and do Viller you I and I forgot so this guy is trying unsuccessfully to lie to his his his uh friends who don't believe him um so there you go so that's a failed attempt to persuade um so this data set contains uh a rich set of ano examples of persuasion strategies you can see them here um and it provides a benchmark and and a series of tasks for understanding persuasion strategies in the context of these egocentric uh kind of face to-face interactions including data from the ego 40 data set and also data from YouTube as well so kind of enriched by additional videos and then uh this week we'll be presenting a new uh work along these lines so this is an oral that that uh that sangman has sangman are you in the room no I think he's not here yet so he's going to be presenting uh on Thursday a talk about this new kind of augmented data set that includes additional tasks and you can just get a sense of some of them here so there's a task around understanding who you're speaking to so identifying the speaking Target of the utterance uh Co pronoun co- reference resolution it's a basic task on understanding natural language but it's one that benefits tremendously from having access to visual information so a lot of this is about really really doing a better job of taking the visual Channel and utilizing it to understand the language itself which for adult interaction is super important actually so there's a number of really challenging tasks here that are all brand new and a very large data set that lets you look at them one of the key challenges is to align these modalities so take the utterance which has been transcribed so you understand what someone is saying and then figure out who exactly is the person in the video that was saying that at what time were they saying it and so that dense alignment between the visual information and the and the natural language is really important and quite challenging actually in this egocentric uh perspective and there's a bunch of Novel Baseline models that you compete against and this is a baseline architecture that I'll Skip and the main point I want to make is that the accuracy is very low okay so please go work on this data set you can certainly improve these numbers and then you can publish that in some other meeting all right so uh I'll start wrapping things up a little bit so just want to thank our sponsors over the years uh for supporting this work I want to just acknowledge the amazing team of of collaborators both computer scientists and psychologists we had the pleasure of working with and many different institutions um and I just just want to conclude by saying that there's a you know a Grand Challenge before us to understand human social interaction from this multimodal perspective I think we're right at the at the cost of being able to make progress on this problem in ways that will surprise everyone um and a key aspect of it is for vision and and auditory and NLP modalities to come together which is a topic this community is very excited about and progress along these lines would be very helpful not just in the standard kind of VR and AR settings you're probably thinking about but also in helping children with ad conditions to be you know more effectively treated and hopefully have a more more effect more successful Liv so there's many things and many motivations that can be behind this work uh with that I'll conclude thank you thank you so much Jim for an interesting talk so we have time for two or three questions so does anyone have any questions to us anyone so yeah understand relationship but I also feel like there examp the drama it's more like or people trying to trans their more of than just yeah so I mean the example that I led with of course obviously you know this is a highly charged emotional scene in a movie where it's kind of the key scene in the movie where this you know you watch it for an hour and then it leads up to this you know this kind of development so uh is probably much more enriched emotionally uh than most of your daily life interactions hopefully you know um but I think it's still you know the pattern is still there that to really understand something like that you have to understand how to people relate to each other what they think about each other and then interpret all the cues right so the the objects in the scene played a role in communicating the intentions of the actors so it's not just the words it's also what they're doing with their faces and their hands and to me that's the key challenge but I one way to answer your question is yes you know the challenge is before us it hasn't been solved yet um and there's plenty of work to do to get there question yeah it's a good question so so in gaze we know from from example for example from eye tracking work that the temperal information is really important um when you have access to IM movements at really high temporal sampling rates you can do a lot just in the temporal domain without doing anything else um the temporal dimension in in the type of attention we can get from video is also important uh and it's not model nearly as well as the spatial Dimensions so there's a few papers like like our earlier paper that does this but in general it's pretty open so I think it's important and I think it's pretty open right now how to bring time into that picture effectively you had one more question so I think yeah so for the more complicated scenarios with these multiperson games at what point does the does the speech or the language just of take over compared to the NC yeah it's a good question so the question is in these more complex you know interactions among adults in the context of the game or There are rules if you just look at the dial alone the NLP is that enough you really need the video and I think the paper that we'll present this week has has significant evidence that you do need the video but it is you know there's a sense of which it plays a supporting role however um you know the thing about it is that uh in real life interactions the speech can be very complicated in particular people say very little they communicate a lot and say very little the transcripts if you've ever seen them of you know of of interactions tend to be really opaque if you just have the transcript alone so I think it's an open question as to how much information vision is going to provide but the need for vision I think is is pretty clearly established yeah thank you so much for amazing let's thank him again now we will hear from the holist challenges which will be online okay so we have couple of minutes where we try to um check if everything is working working with the audio so we'll start in a couple of minutes um okay can you hear me yes yes let me check if I can get the audio um to stream because maybe it's just this one can you can can can you try to to speak and we see if yeah I'm speaking now how about now okay nothing changed just a moment or should I uh increase my mic volume right okay so how do we get audio from the from the computer so I Tred to connect this I don't know if there is any other extra Stu that I need to take you cannot hear me at all or it's just no so we can we can hear you but only through my laptop and we will like this so that other people can okay I got it you have your device manager can oh um maybe I think it's this might be this one yeah try that yeah try that one that just popped up is e em yeah exactly okay can you try again okay how about now I don't know if you are yeah um okay so much okay okay so just give me a moment because I still need to get a chance to uh download the video that you send me yeah please just go ahead just disconnect and uh check it judge we should I thought it was a judge okay thank you good point I'll try to fix that and I I just want to make sure the video from my side also whether you can hear the sound or not um let me do you hear anything from the video uh no we no not from the video we hear you but not from the video okay if you cannot hear me then it's still fine I can just explain uh by uh prob okay so I believe you can see my screen uh yes just let me check is it on the okay so it works okay so just give me a moment I download all the videos that you sent so then we can proceed smoothly perfect do do you think you can play those videos back from your end because that would make things easier yeah I can try uh let me try quickly that um for example but what I'm not sure is like whether we can U Place sound and the connection is good or not so let me try could you hear any sound no but I think there is an option when you share your screen there is an option forward the AUD so you should stop sharing then share again okay share again and there should be an option uh saying share s yes that should be today I would like to present our work on so let's let's try to do this way mhm okay this sounds great for me so then in that case we ready to go all right thank you for your patience yeah thank you so much for Shing up this you're you're sharing the the presenter view uh you should switch the view seeing the not that's very interesting okay now that's great thank you oh okay thank you okay so hi my name is tangan I'm currently a PhD student at CBG eth JY and today I will present about I I will present about our Hol Hol assist challenges um so first we uh I will introduce like our data set first to get more context after then um we will talk about what challenges we have and then um you know like the the participants they will present about their own um on their result um so yeah so like to implement interactive AI assistant in AR and BR um I think there are many challenges so when you think about the um mixed liity applications um one of the most popular application is AR remot assistant so it's basically uh Ecentric communication so the the user for to the somewhere far away you know remote person and they get help through the entric view and then the the person behind the uh you know like this mixed device they they explain what we need to do and the other popular application in U mixity uh uh this is uh there there is a app called AI guide and this is basically designers design you know like a warm up step by step uh for the some procedure or task and then they give some guidance through this mixed lity so meaning that there is some 3D um arrow that what we need to interact with or like or what will be the next step in this like procedure task however these two will require a tons of effort uh to to Implement like for example AR remote assistant they we always need like a person behind and a guide also designers need to design step by step so actually yeah then what we should do to implement this interactive AI assistant so we do the challenge is L of Lear word Interac data set uh with a full spectrum of per perceptual sensors so therefore uh that's why we collect this Holo assist data set um this is captured by 350 unique inter uh instructor performer pairs offers 169 hours of data with eight modalities and 2,243 sessions um so as you can see we have many other modality so we can see modality 1 by one uh here for example we have RGB and head p uh we also have deps image and then you can also see Point Cloud uh in the scene uh we can we also have like a 3d hand pose and then this is a projected hand pose uh we also have eye you can see orange Ray here audio uh we also have audio and conversation between U people and we also have IMU uh like for example Accel matter and mag matter uh lastly we also have um after text summary uh which is annotated by annotators uh after they watch the entire video so what makes our data set be more unique so we basically always have more and instructor so we are require two at least two persons to um you know recruit this data and whatever does from this performance side the instructor can watch they what they are doing through this uh laptop like a screen and then intervene whenever they need some help so for example uh in this example the performer trying to assemble the um Nintendo switch grip and in instructor watching uh this that scene in Le time uh through the screen and then instructor can intervene in Le time so when they make a mistake then instructor say okay so that this not the correct way you need to uh assemble your like a right side of the the the Nintendo switch uh from top to down for example so here here is been more detail example so this is um manipulating um a vehicle example and then the the user the performer trying to do some like action to fix this like state for example and then since this is correct action uh the instructor just say okay okay now we need to go to the next step um which is like a slide the shifter arm onto the mounting Peg uh Etc and then by following this instruction um the user the the par the the the performer do some you know like a manipulation but they here like for example they did mistake so at that time um the the instructor intervene to the situation and then just uh comment what they need to do like they said oh it's right below your hand is that wrong piece of metal and then finally the performer uh do the the correct action uh and then it goes like up for to complete the task so there are many different object categories uh for example oops for example there are small object that we can easily like a manipulate like that and this object usually change their object post quite like frequently for example cameras Nintendo switch uh copy small copy machine or Ram graphic card uh we also have medium size object we can move it but once it when you manipulate uh it usually stay in that locations uh like copy machine or like SM printers uh we also have big object usually they are big in some location and then it's like quite like a large like a Furnitures or some like a big printer big copy machine also we have like a layer object uh which we frequently uh we only can see in the like a laboratory or some like a factory so so here's some like example of our annotations uh we have hor Grand action uh usually takes like a 30 to 40 second uh in his barrier gold driven actions and we also have fine Grand action uh which is one to two one to around like a three second of each section and then this is very you know like a fine grain like a like we we basically otate very very like a very small chunk uh of the action whenever they to they move and then also we alate a conversation between the uh between the like a instructor and performer um so we can see some like a data statistics here uh we try to uh collect the uh different different object like a bit like evenly as much as possible and then we can see the like a um totalization length per activity for example and this utility card usually takes like the most uh takes a lot of time like us like 30 to 40 minute to assemble that's why we have the most uh lengthy um most lengthy here and then we also have like some like a skill level and like every here um what we maybe need to see more like closely is um this like a course action for example we it usually last like between 30 to 6 seconds the most and for the fine Grand action we have so many uh F Grand action between one to two second uh we also have like um a verb noun for f Grand and Course Grand action here here uh let's see some video example I think it's it be makes you understand bit more clear so basically now the the instructor I explain what they they need to do and then now performer try to follow the instruction and performer load the paper yeah since up the performer did I guess something wrong so the instructor tried to give more information what they need to I yeah and then like the instuctor provides more tips uh how to properly uh load the paper here and there's outut the you have to pull as well and like one oh yeah sorry I'm not sure you can clearly hear the sound or not but you know there are like a many interaction between the participant and the instructor to complete a task together so um in this ego based uh challenge we have uh two uh challenges F first one is f Grand action recognition and the second one is mistake detection we also have intervention type prediction 3d hand post forecasting but we um we didn't cover in this Workshop but we we are looking forward uh to cover this challenges in the future as well so um B Grand action recognition so given a short clip our participants need to clash by F Grand action label which usually last like one to two second and in this example you can see there's like a b Grand action label they put like copy in back P so this like basic weate this using like verb and noun uh and the participant need to clarify them correctly and there's also M detection um so they uh they need to take the features from the uh start of the beginning of the course gr action until the end of the current action clim and as you can see here like there's like input and then you can see whether it's a mistake or correction uh in this test they need to change the the like a printer like ink so whenever they you know touch the like a the paper part it it is like a onate as mistake and then now it's going the correct way and there's another u b interpration type prediction we want to know which interpration type for example like a confir action or correct uh mistake or followup from the from the from the instructors and given on an input of like Windows of 5 second before the intervention and there is another one um for 3d hand proc casting basically given three seconds of input here for example for your information uh green green Hand p is ground and red is forecasting and then forecasting is uh like this so we we we want to know the continuous 3D handp for the next 0.5 and one and 1.5 second but this is very challenging so this is a a um failure example and you can see it's not easy to get like a correct hand PS for the future but we believe this will be um you know like a uh possible applications for how we can actually guide in in s space so we also want to you know do this like Benchmark bench like Pro promote this Benchmark more um in addition uh to the data set and paper we also share more resources for example we also uh share this like capture code quote PSI uh you can if you go to this link this is the approach from the PSI team in Microsoft and we collaborate to build this application to capture the H assist data set and then now these application are open so you can more um easily uh capture the all the data stream from H using this app there's like a hor capture app and H capture server and then exporter as well uh we also share hand ey gaze projection code and with this code you can also get this like 2D projected hand pose and as well as ey gaze uh through using this code so yeah if you see uh this projected and posst and I guess is it's quite interesting uh how usually um the the user focus on when they do some manipulative task so yeah from now on uh let's share the our challenging resar so first like a f gr action recognition so um we actually left uni uni b z uh congratulation you you got like a first length uh in this F Grand extion uh congratulation for like the autm and um so they basically improve a lot like for like a top one accuracy like about like 30% and for top five around 8% um and then like also like for the like a verb and noun accuracies as well and for the mistake detection um also congratulation for the uh Michel antono and Giani you uh you got the first ranked in in this mistake detection benchmark and um this team improved around 15% for like F1 over score which is quite a lot of increasement compared to our Baseline which is around 35 so yeah I um so each team will present their own result so yeah let's starting uh let's start from uh find Grand action recognition hello my name is Aron I am a student at the University of ban ital and today I would like to present our work on action recognition task using Hol assis data set Hol assist is a Noel large scale egocentric data set where to people collaboratively complete physical tasks it includes more than 100 hours of undated videos and approximately 2,000 fine gra actions it also includes uh seven modalities but we used only RGB videos in our model in this action recognition task we need to perform multiclass classification of the fine grained actions across different activities here I would like to introduce our pipeline uh first of all we worked with dreamed find grain Clips independently and given that such Clips can be very short the duration can be even less than 1 second the original frame rate of 30 FPS was preserved during the video decoding process so our framework is similar to the framework from TSN paper or temporal segment Network where both temporal and spatial sampling methods are employed to be more precise uh we uniformly sample frames from uh predefined segments in the final model we used 16 such segments and after this we stuck at and crop the sampled frames and then input the data into 2D convolutional neural network it could be any uh Tod CNN such as reset or Inception however such uh CNN is augumented with the fusion model that I will discuss in the next slides finally we train our model using cross entropy laws to predict class logus okay um the main difficulty of video understanding lies in how to fuse the temporal and spatial Dimensions accurately and efficiently to address this problem the gate shift fuse or simply GSF Network architecture has been developed the GSF is efficient special temporal feature ex structor that can be integrated into any 2D CNN structure with almost no additional parameters on the right side of uh the slide there are two examples of inception and reset where the GSF module is inserted and the experiments show that this particular locations of GSF give the best result uh actually the GSF is a generalization of a popular modu called TSM or temporal shift module which performs temporal modeling by moving the features along temporal Dimension GSF also Blends features among the temporal Dimension using shift operations however in the TSM the feature flow is fixed by design it means that um features are forwarded from one block to the next block uh without data depending decision GSF replaces the fixed Channel split with learnable blocks that I will show in the next slide here uh here I will explain in the high level how the architecture of GSF Works after the convolutional block which is blue here we split channels into two groups and apply grouped spatial gating for each of them the gating block learns a special weight maps for each group that determine which regions are shifted in time and which regions are not shifted and it's done by separating 2D convolution output into group gated features and residual features group gated features then go through time shifts to encode temporal information uh and finally we would like to merge shifted features with residual features uh the easiest way would be to make just a summation but uh Fusion by summation may be not optimal and instead GSF performs weighted averaging on the two featur groups to sum up GSF employs adaptive blocks that can be learned based on the data okay um in order to train the network we used batch size of 32 to best fit our resources the stasic gradient descent Optimizer with momentum and multi learning rate scheduler where the base learning rate is set to 0.01 which decreases by half at the Box 5 10 13 and 15 with a total number of pox of 15 also we tried different backbones Fusion methods and pretrain weights the best result was achieved by using Inception V3 backbone plus GSF Fusion module and something something pretrained weights the validation learning curves on the right show that GSF outperforms other Fusion methods um for the inference we conducted temporal and spatial sampling simultaneously and repeat the process 10 times so the predicted logies from each of the 10 sampled Clips are averaged to obtain the final video clip prediction as you can see in this table our approach achieves better accuracy than the times forer Baseline and notably that it was achieved with fewer parameters yes this is the end of my presentation and thank you for your attention hello yeah so thank you so much for your recorded presentation uh we uh due to the uh time manner we will Mark the Q two Q&A uh session in the end of the next presentation so I will uh go to this mistake detection uh presentation by Michelle um yeah I will play here hello everyone I am M manuto from the University of katania today I pleased to present the solution developed by me and the unic team for the mistake detection challenge of all assist smart glasses have recently gained significant popularity with various assisting product available on the market capable of providing assistance to the user through augmented reality in order to provide the timely assistance wearable device should be able to identify moments in which the user make mistakes or is confused and require help if such instance are properly detected the IE system can proactively support user by offering contextual information or suggestion on how to best carry out the task at end our hypothesis is that mistake manifests through unusual behavior patterns displayed by the person wearing the camera who may experience disorientation while performing the given task we aim to develop a gaze behavior model specifically a gaze prediction model as we can see in this video from Hol assist after closing the paper tray and before making a mistake the user exhibits some unusual and disoriented gaze Behavior The Gaze pattern return to normal as soon as the mistake ends in the example we just saw we can observe approximately 9 second of unusual and disoriented Gaz Behavior occurring before the student mistakenly grabs the incorrect tray from the printer in our proposed setup at each time time step t a model takes as input a video observed to a Time step t and a relative to the Gaze trajectory where each element of the trajectory is a 2d gaze fixation the model then return a score called s indicating whether a mistake is occurring at the current time step T in this context High s score indicate the occurrence of a mistake while low s score indicate a correct action therefore we can view the mistake detection problem as a classification task where each time step T is classified as mistake if s is above a given threshold this is our proposed architecture the model input consist of T observed RGB frames and the 2D gaze trajectory the first half of the trajectory highlighted in Orange is utilized in one of the two gaze f strategy following a peak finding operation the latter part of the trajectory is predicted the Gaze completion model takes as input the video and the first half of the input gaze trajectory predicting a gaze trajectory aligned with the remaining part of T the goal of this module is to predict where the user is looking in the video conditioned on the initial trajectory conditioning helps reduce uncertainty in gaze prediction and provide a prior on user Behavior our gaze completion model Builds on the state-ofthe-art paper in the eye of Transformer Global local correlation for entric gaze estimation incorporating a novel trajectory conditional module this module integrates the partial input trajectory into model computation and includes an additional trajectory consistency term during training number analysis we use the probabilistic approach to determine How likely is that a given ground through a fixation detected by the device match the predicted it map this involve evaluating the probability that the actual he fixation occur at specific coordinates within the predicted it map the likud is calculated based on X and Y coordinates of each fixation Point by examining these coordinates within the cont text of the predicted map we can estimate the probability that the ground through fixation Point match the prediction to assist the accuracy of the entire predicted trajectory we sum the likelihood of each point along the trajectory over a certain time frame this cumulative score give us an overall measure of how well the predicted map correspond to the actual trajectory of the a fixation over specified period this method allow us to quantitatively evaluate the performance of our predictive model in a detailed and systematic way here we present the result obtained for the mistake detection challenge H assist provide three distinct sets training validation and test we trained our model using only the training set and selected best model and threshold based on the performance of the validation set the table display the result obtained on the test set for the challenge it's evident that the task is highly challenging as our proposed approach tends to frequently misclassify some mistake as correct therefore there is humle room for improvement here we present some qualitative example on the left we have a no mistake example that was correctly classified note how the pred Ed gaze in blue follow the ground through one in green on the right we have a mistake that was correctly classified as mistake note how the predicted gaze drastically change when a mistake occur thank you for your attention if you have any question please reach us for more information please scan the QR code hello everyone yeah so thank you so much for the presentations um so I believe um there there are um the two participants in in this like Zoom room so please ask maybe one or two questions uh for this presentations so any questions so maybe in the interest of time since we're running a little bit late uh we can uh we can go to the uh I mean we can finish here but maybe since the winners are in the room we can uh take a picture of together I think that'd be nice um and and we try to fit you in the screen do it work does it work uh sure like how how to make the picture together like this um do you have do you have a screen with all the winners in your slides uh no not really so so maybe yeah okay uh so maybe maybe I'll uh stay stay there I'll zoom in you cannot see but it will work trust me uh I'll Zoom you and maybe all the winners if they can come to the here in front and we take a picture some else change yeah I wanted also to to get the team or or or do you think it's uh I think I think this is GNA work uh John okay uh can't okay okay so I think the others are online so let's do maybe let's ask them to turn their cameras on the other winners so that we try to okay okay and uh Mela should be there as well okay not sure if we can get something a bit more I I think yeah I think everyone is here yeah I'm sure we have all that control in the end I feel like before was better ah so like can sh by sharing screen let's keep let's let's keep this let's keep this as it and maybe yeah okay so uh thank you um is there anything else or this concludes the session I guess yeah yeah yeah that's from our side okay so thank you very much and now the next part the next thing that we have in the program is the poster session uh so this will be in the other building so in Arch uh 4E which is one block away uh so um we should all head there and be back here for the next talk which I think is uh 1120 okay so let's be back here at 11:20 for the next keynote so see you later e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e okay hello everybody um okay this is like on off but anyway uh we'll start in a couple minutes we could not anticipate that going to the poster session would take something like 10 minutes because there's still a poster session is ending now and people are moving here because it's another building so we'll start in a couple of minutes thank you e e e e e e uh okay so di if you if we can check um we do an audio check if you can hello hey does this F work yeah I think this is loud enough okay brilliant all right so I think we're ready to start the session uh so let's welcome uh Dean laros who our next keynote speaker uh unfortunately she could not make it so she's not here but she will deliver a talk remotely um and uh so Dian works at neighor Labs um and she's very well known for her work on geometric understanding and we're very excited to hear um her talk so you can you can start thank you so much thank you very much for the introduction and thanks again for the invitation as you said I couldn't be uh at cvpr in person this year sorry about that but it's still my pleasure to to join today and uh present remotely and uh and be able to share some of the recent work that I've been involved in where we've looked at the 3D aspect of egocentric Vision so as you're all extremely aware in this room uh in standard video sequences the scene is observed from an external point of view and for egocentric videos um they offer a first person view which means that they're taken from the perspective of the actor and egocentric Vision has many very concrete applications to illustrate this I'll refer to the excellent igv paper from kiter and many other colleagues from hers who've been working on this topic for four years now and uh their article provides very concrete and very convincing examples of potential future applications of egocentric vision for each they list the relevant computer vision tasks and what we observe is that many of the scarious components required to reason in 3D additionally I'd like to argue today that many of the other tasks would also benefit from a Frey understanding of the overall video so that's that's what we are going to talk about today uh yet egocentric videos they are quite challenging when uh when it comes to understanding and what happens in U in 3D so most of the experimental results I'll show in this presentation will be obtained on sequences from the Epic kitchen data set and in this presentation I will cover three pieces of work that all relate to the 3D understanding of egocentric beings the first one I will focus on objects that are moving in egocentric videos and this joint work with Vadim and Andrea and we've presented uh this work at the 3dv conference a few years back but first let me clarify the task we would like to tackle here in this work we wanted to discover and to visualize all 3D objects that move at least at some point in a video sequence and on the left you see a short clip from the appic data set and on the right you see the kind of output we would like to obtain so as you can see we have removed the background fixed the Viewpoint and this way we're able to visualize all the objects that are moved at least once during the sequence the overall goal is to be able to reason about forone objects in a way that is also consistent with the geometry of the scene one way to frame the problem I just described in the previous slide is to say that we would like to perform forground object segmentation yet if you look at what's been done in literature typical approaches for background substruction they make strong assumption about the scene in general the camera is fixed and objects are well defined and um the method typically applied to sequences like uh the one we just um we just seen now our work also relates to the task of motion segmentation and here the goal is to segment moving objects from the background generally the camera is either fixed or follows the object in a steady way now if we look at this video sequence of an neocentric video of a person cooking you can easily assess why none of these families of approaches can be applied in our case let's dive into challenges posed by uh egocentric videos so for this video from epic kitens we can see that first the person is constantly moving over the wall kitchen so this results in a very large variety of viewpoints and the objects are not always visible as a second problem uh we see that the actor because it actively interacts with some objects it includes part of the scenes in most of the frames and finally this kind of videos contain many objects that rarely move so in the key frames that are extracted here for this slide you can see that some of the objects have not moved for seven minutes they will move later in the video so with our definition they're considered as foreground so they need to be segmented out but for the first half of the video they have not moved at all so this gives an overview of of the challenges we'll have to deal with now let's forget about the task we want to solve for a second and let's review the popular Nerf family of approach based on neural rendering nerve has been initially proposed for synthesizing novel views and departing from traditional approaches for this task it does not explicitly model the geometry of the scene like boxel or matches would do but instead it implicitly model the geometry of a scene by means of a neural network and for this scene Nerf would for instance allow to render the scene from any Viewpoint even anden viewpoints this neural network takes as inut a 3D location and the 2D viewing Direction and it is trained to predict an emitted color and a volume density then for a given Ray from a particular frame we can sample the volume at different positions along the ray the neural network uh provides the color and the density at all those different samples then we can apply volume rendering along the ray um and this will predict the color of the corresponding pixel now this predicted color should correspond to the grr color of that pixel and this is the information that is used for train but Nerf has initially been designed for static SC only and Nerf W is a method that came a bit later and is an extension of Nerf that handles unconstrained photo collections it is designed to deal with photometric variation due to time of day or weather conditions and also it's designed to handle moving objects and occluders the thing is it does not fit what we need here because it is designed to ignore trans objects not to capture and segment them more recent methods have been proposed since then with the exact same goal but they are not applicable to the task we want to solve here for the exact same reason because they're meant to ignore the foreground so now let's go back to a problem of Interest the scene of a neocentric video we've said that it's a bit too complex to be captured by Nerf because of the moving pieces but what if we start making assumptions about the different components of the scene first we could model the background using a fairly standard version of Nerf such a model would capture things that stay static during the video such as the table the floor and objects that never move then we could try to create a second model indexed with time to capture the objects that move during the sequence for example here the CTIC board will move at some point and the knife moves a lot when it is in use and then we can model the part of the actor that is visible in the video such as the hands and the arms and this can be modeled separately from the objects because um they have unique properties that I will discuss later and this is how we build the proposed neural diff um it's a neural architecture that combines these three different Nerf like models into a single representation of the SC but each model needs to be designed carefully for the two models that are capturing moving components such as the second model for forground objects and the third model the one for the actor we use time as an additional input this allows both models to render views not only depending on the spatial location but also the temporal Dimension and this is very important for both models because the two of them combin are supposed to capture anything that changes over the W video then we model uncertainty with a score that enables both models to capture anything that is not well expressed by the static model one way to properly separate the dynamic part into the foreground object that we're really interested in this is this is the goal of of this U of the task we're trying to solve if you remember and the actor is by using an inductive bias because we know that the objects are mostly anchored to the word coordinate system but the body of the actor is attached to their head hopefully and therefore move with the camera so to capture these two different types of motion we simply Express the actor model in the camera coordinate system and this detail actually proves crucial if you want to be able to separate the two kinds of motion now let's have a look at how this model Works in practice on the top left corner you see the ground CH given only the Viewpoint Associated to each frame from the video we see that neural def successfully reconstructs the full video sequence and it does this by estimating and combining the Reconstruction for each of the three model components that I described just before this component respectively corresponds to the background the foreground and the actor now because Nerf is initially a rendering technique it means that we can render the exact same scene but from a fixed Viewpoint and this is the kind of results we get when we do the same rendering but we choose the Viewpoint this allows to better observe how the different objects come and go in the video so I've showed you the Reconstruction task but what we're really interested in in this work is U the task of foreground object segmentation so so let's go back to this task that we're really interested in a gr Tru annotations for forr objects are shown in red on top of the left frame and then we see the result from Nerf W the Baseline introduced before and we see that it fails to capture the plate and the actor on this example for a model but which doesn't have the actor component just um a background and a foreground model we see that the entire lower body is missing and we see that the hands are segmented but with low confidence now if we include the actor model so all three components we see that the full body is captured correctly with high confidence and we see that objects are also segmented better so this conclude the first part where we have used a three stream neural rerer together with some inductive biases to segment foreground objects in egocentric video sequences let's move to the second part in this second part we will look at an approach that tries to learn representations that capture both semantic and geometric information and this is Joint work with Vadim U and Andrea and we've presented this work at the 3dv conference two years ago if we look at these recent neural rendering methods such as neural death that I presented in the first part um we've seen that 3D construction can be cast effectively as an unsupervised learning problem the problem is that such methods they're typically optimized per scene and because of this they capture information about the appearance and the geometry but they're not able to capture any semantic they're not doing any semantic understanding on the other hand self-supervised methods they're great at learning powerful and invariant representations from sets of images if we take the example the dino descriptor we can look at local attention Maps produced by Dino but this is typical for all these self-supervised Des scriptures typically they correspond to coherent semantic features and in this work we wanted to combine the best of both words the 3D understanding that is captured by Nerf leg models but also the centic information that is brought up by strong 2D features and this is how we proposed neural feature Fusion fields or nef in short and we show that 3D reconstruction can be integrated with cementing understanding at the Image level let's look at how we can do this integration uh for this we start with a collection of images then we reconstruct the 3D scene by modeling it with a neural Radiance field right then for each image we extract semantic features such as a dino one and then we fuse those Fe semantic features into the geometry of the theme using a 3D reconstruction task but why would we need semantic features when we want to reason at the scene level in entric videos well I'm going to give you one example given a patch in an input image for example this orange patch that you see here then we can segment the corresponding object not only on the input image but also directly in 3D as shown in this point Cloud let's go back to the way Nerf is built we use the RGB value of a pixel in an image as ground TR color then we sample 3D points along a rate that corresponds to this pixel these 3D points they are fed into a neural network which will produce colors and densities for each of them then we apply render volume rendering on this rate and this allows to render a color the model is trained so the rendered color matches the crown TR let's now see how we can extend this Nerf like model first we can generalize rendering by also reconstructing features at not only colors but on top of that we formulate this task as a distillation task the student is the neural renderer and the teacher is the self-supervised model which produces the features and then we train the neural renderer to also predict the features of the teachers in our case Doo so the model is optimized by jointly minimizing the image and the feature reconstru struction loss unlike typical features distillation in this particular case the student and the teacher work in different domains respectively 2D and 3D let's look at an example application of that model we'll forget about egocentric videos just for a little while and we will consider static scenes the static scene we've model with a van Vania Nerf and we show that we can precisely localize semantic objects from just a small input patch given a query patch our model is going to predict and render the features of this patch the features of this patch are then averaged to obtain a query feature Vector we can then compute the distance between this feature Vector this query vector and the features of every pixel in an image we visualize this distant maps for the feature of the Baseline to the teacher Dino and for those obtain with our method Nerf and we observe that the student has surpasses the teacher and that example and and features have been enriched by this 3D information that was available let's look at those an supervised segmentation results in motion the input image on the left is divided into a foreground part and the background part note that concurent paper presented a similar contribution a few months later at murps and that this idea of distilling 2D features in a 3D model has been heavily studied Sy them considering all kinds of features Doo Dino V2 clip features some as well and different kind of 3D representation such as Nerf like here but also the more recent Goan splitting we have seen how this works for static SS but our formulation is generic because we combine it we can combine it with any other nerve variant so now let's go back to our goal that is segmenting objects in egocentric videos remember we have a few challenges to deal with we have objects that move frequently and we know that the actor heavily udes the scene so how do we deal with such challenges well we simply build nef on top of the neural model that I presented in the first part of this talk neural diff can deal with all those challenges thanks to the way it models the scene with the three separate components that I presented before one for the background one for the foreground and one for the actor as for neural diff at time is an additional input so the model can deal with the dynamic nature of the scene and each component predicts its specific colors but also features um and all these components uh are going to be trained jointly so all the parts of the model are run together this has many potential applications and I'll illustrate just the one of scene editing given a query so the query is this object that I cover in green with the mask here on the counter we would like to localize this object in 3D thanks to um the 3D representation using distances in the feature space that we've just learned and once the object is localized we can render the video sequence without rendering the object in each frame as the the object originally appeared at different locations for different time steps we can assume that the background has been observed at some point in the video which means that um the frames are correctly rendered under the object even after removal this is because this knowledge is implicitly obtained in the radiance field that separately model the background and the foreground to summarize we have proposed nef a method for the distillation of 2D features into a 3D representation and we've combined the approach with neural def in order to specifically deal with egocentric videos the last part of this presentation is quite a large effort in involving many great people that I mentioned here in this slide from Oxford Bristol University of Michigan and this paper has been presented at new last year as um quite often in computer vision data set have fueled progress by attracting interest from researchers but also making new task or new models possible and uh this last piece of work Builds on the large effort of diad and colleagues who build the appication data set that has allowed for this progress so um what I want to present today is um the Epic field data set where we have reconstructed the geometry of 671 videos from the Epic kchen data set and we provide camera pose for almost all frames in those videos this means that there's now 3D information for 19 million frames covering 45 different kitens um as you can guess egocentric videos are not the easiest ones to reconstruct in 3D because structure for more methods they of they often fail on such complex videos so why exactly was this reconstruction challenging well first stand reconstruction pipelines they often assume that the SC is static and these messes are not robust to Dynamic objects and in these videos we've seen that there are many Dynamic objects including hands that occlude the scene in almost all the frames on top of that videos are several minutes long so that's a lot of frames to hand uh we've seen that the distribution of the Viewpoint is heavily uneven because the video alternates between on the one hand small motion for instance when washing something in the sink but also fast motions that happen in between um this small portion period for instance when somebody is grabbing something in the fridge and then going somewhere else so we address all these challenges by applying what I'm going to call an intelligent sampling mechanism more precisely we've applied um this sampling to each video in order to reduce the number of frames and to balance the Viewpoint distribution we then use structure for motion to reconstruct the scene from the filter frames and finally we registered the remaining frames to this reconstruction here you can see the result of the intelligent sampling algorithm uh after this algorith after this sampling uh the video is only composed of a subset of the frames the ones that are most useful for construction the goal was to then sample the video while keeping enough overlap between the frames to allow to allow for a proper reconstruction and for this we estimated theography between frames by matching sift features and thanks to this we removed everything that overlapped too much this resulted in discarding about 80% of the frames without discarding the critical frames uh bioc critical I mean the ones that happen during quick transition between places what happens if you don't use the sampling well in many cases the algorithm just fails and no geometry is estimated when it does work the the reconstructions are not as good and for one of the example where the Reconstruction does not fail without sampling you see that without this intelligent sampling let's call it like this you can reconstruct uh a lot less of of the scene with this sampling method you can reconstruct a lot more of the scene now that I've described uh this data set which is epic extended with this 3D information this allows to Define new tasks in neocentric videos so among the hundreds of videos we have reconstructed we have selected 50 videos to build a benchmark and this Benchmark is composed of three different tasks the first task is the one of dynamic New View syntheses so given a subset of video frames as input the goal is to predict missing frames and this Benchmark task is is composed in a way that uh it considers three levels of difficulty the second task is the one of unsupervised Dynamic object segmentation the goal here is to identify which regions in each frame correspond to Dynamic objects we can distinguish the dynamic objects that the actor is currently interacting with to a second part of um the dynamic part which are the semistatic objects which are all the other ones that are composing the foreground the third task is the one of semisupervised video object segmentation here given the mask for one or more objects in a reference frame the goal is to propagate these masks to subsequent frames on this Benchmark task we've compared a few naive 2D and 3D baselines but we hope that the community will come up with exciting new solution for this task note that although we have buil explicitly our Benchmark on top of Epic kitchens our geometry estimation method can also be used on other entric data sets and here are two reconstructions that we've obtained on the EO 40 data set before concluding this talk I'd like to encourage all EP kit users and all Epic Kitchen Challenge participants to using the additional annotations provided by epic fields and to do though I've picked a couple of recent works that have used these 3D annotations on very or very similar ones um for their task so this will allow me to give you two more examples on top the three Benchmark tasks that I described in the paper where geometry is relevant as a first example I'd like to mention the work of Lorenzo madian colleagues where they automatically extract affordances from the appicon data set using a 3D map of the environment the 3D map is not exactly the One released by epic Fields but it's it's similar and spirit and this 3D information allows them to provide pixel level Precision for the affordance location and they've released the EPF data set which adds multi-label and special affordance annotations to Epic kitens as a second example I'd like to mention the out of sight not out of mind paper from kerian colleagues where they use the 2D object mask annotations from visor and the pipeline from epic fields to localize cameras in 3D this allows them to leave do annotations in 3D and to track objects across the scene also in 3D even when they're out of sight to sum up we've released the Epic Fai data set which enhances epic kitens with 3D annotations and a companion Benchmark with three tasks I really hope you'll find these annotations useful and uh and you'll try them for some of your egocentric video understanding tasks this is the end of the presentation let me just add that the people I mentioned here on the top slide are the ones who made all those contribution possible and I'm very lucky to have worked with them and now I'd like to to take any question if we still have time for that of course so I just turn on the microphone so you probably could not uh hear the Applause but people clap their hands uh so uh are there any questions okay I I got a question is a bit technical but um I I saw your results of reconstruction of some uh ego Ford ego Ford sces and since part of the data set is also paired with um um with 3D SCS of the environments it's just I think it's a a limited number of of of environments did you have a chance to check if your 3D reconstruction matches that uh kind of pred ground truth maybe it's not a ground Tru but it should be accurate No that's a great question and this is one way we could have evaluated the reconstructions um but as I mentioned we really worked on top of Epic kitens and we just tried on a couple of samples of Epic on EO 4D just to show feasibility but this is not where we did most of the study and most of the work but it could be interesting to uh to compare what is obtained with the Reconstruction and what is predicted with these scans that available with the data set I know that Diego for the uh data set has been also extended um so yeah that's uh that's also an Avenue to be considered but not something we've done so far any other question Dima so yeah inspired by by all these things how can we convince 3D people that Pixel Perfect 3D analysis isn't really needed um because I I do think you know this is this is part of the problem right that that they're really wanting Nerf perfect pixel level stuff which I think is it really needed for video understanding you think that's a very good but very tough question right it depends what you want to use your reconstruction for of course you need a reconstruction to be precise enough you if you want to start locating things in 3D and be able to track them right because otherwise the Reconstruction algorithms are are are not as Rel as relent if you have some of these uh these issues uh but if when it comes to um this reconstruction maybe something sparse would be enough for many of the tasks uh and this is true that it does not directly translate into perfect um New View synthesis as a Nerf Community or otion splatting Community is used to uh but these tools are still useful I think for uh modeling the scene and um and starting quering um the environment to understand what's going on so um this there's a bridge to Gap there uh for both communities to better understand what is needed in each side but I think your your question is is really smart and I don't have a um yeah a better answer than that other question okay so let's thank uh thean again and um thank you very much thank you so much and and then we go to the next session which is the Epic kchen challenges uh session I guess you're going to keep using this okay that's great okay no wor it's not work it said stop working now's going to share the slides our slides second okay a so today Jacob and I will be talking about how the last year was for the Epic kitchen challenges so to begin with if people still don't know what epics is so epic is a egocentric data set of 100 hours of egocentric videos where we have 20K unique narrations and 90k action segments along with 20K million 20 million frames then we have the Epic triology in Epic triology we have visor which was very well explained by Dian right now so I'll skip over it then we have epic sounds where what we have done is we have just given audio to the annotators and annotated what it sounds like so we have audio annotations for the actions that are happening then we have epic Fields uh which was uh done here so we have 2D uh we have converted 2D video to the 3D scene and we have accurate 3D mapping of where where the kitchen is how the things look like and now a quick flashback of how the Epic kitchens challenges have turned out in the past years so the first round we had was in cvpr 2019 Long Beach we see all the happy faces here of people who were winners in the challenges then in 2020 we unfortunately went virtual but still uh we were able to have winners in the challenges there and these are the people that we have for 2021 we have multiple winners uh we have winners in five challenges and these are the people that were winner in 2021 then in 2022 we came to New Orleans we have multiple challenges we have challenges from six uh six CH winners here and we are we have multiple people who can we can see virtually as well as in person and then in 2023 last year we met in Vancouver we have around seven challenges where people were winners and now we will talk about the challenges that we had this year so we had nine ongoing challenges out of which uh three didn't receive any valid responses the rest have valid uh responses and then we have one new challenge which Jacob will introduce to us now so yeah the challenge this year audio based interaction detection so for those who aren't familiar if you're given an untrimmed audio in Epic sounds the goal of this challenge is to detect the action boundaries or the uh sound boundaries and classify all of the proposed regions as well so if you're familiar with visual action detection this is like the audio counterpart um so for those who aren't familiar or might not remember from last year we had the Epic sounds training splits which matched the Epic kitchens ones um and we also then went a bit further and divided the test set into two challenge specific subsets so last year we had the recognition subset where we released the uh available time stamps for the actions uh but not the labels and then we also withh howed sort of a half detection test split as well which we didn't release any information or so there's no time stamps there's also No Labels you're just given the video IDs of what we'd like you to evaluate on so why is audio based interaction detection challenging so in Epic sounds we have 78367 categorized annotations from over 100 hours of audio spanning 44 different classes with unique sound signatures and then all of these classes are also variable length they're potentially overlapping uh with an average length of 4.9 seconds so so we're expecting your methods to be able to distinguish between all these sounds s of distinguish between the overlaps and know what all of those sounds are so it's it's quite a difficult challenge so as this was the debut year for the audio based challenge we're just going to have a look at some of the numbers and what we got so we had 27 submissions from nine different users and then from the Baseline that we released to the top one method we have an improvement of 7.47 map uh over the Baseline this year so if any of you are interested in this challenge then you're welcome to participate next year and see if we can push these numbers even higher uh so congratulations to the team we placed first for that as well so going on now we will present the awards and the certificates um and as well uh we have a select few uh methods to present their work so we'll start with the virtual rewards um so first of all congratulations to the team at Deep glint who placed second in the action recognition track as well as the action detection track then we have the team from the Department of electrical and electronic engineering at the Hong Kong poly Technic University who placed first in the multi-instance retrieval track uh congratulations to the intelligence perception and IM image understanding Lab at jidan University for their third place award in the multi- instance retrieval track as well and then congratulations on the school of artificial intelligence aidian University who placed first in the semi-supervised video object segmentation track and then congratulations again to jidan University for their second place award in the semi-supervised video object segmentation track then congratulations to the third place team in the semi supervised video segmentation track uh for the intelligence perception intelligent perception and Computing International joint Research Center at jidan University uh for the first place award uh for the school of artificial intelligence at judian University for the audio-based interaction recognition track and the third place award for the audio-based interaction recognition track uh from the intelligent perception and image understanding Lab at judian University and congratulations to the third place a war the school of artificial intelligence at judian University for the audio-based inter action detection track and now we'll go to the inperson certificate so for this we'd like to ask the teams to come up and collect their awards from sidan and if they could wait at the side as well and we'll take photos after all the teams have been called up so to start with congratulations to the team at the King Abdullah University of Science and Technology or CST for their first placed Awards in the action recognition track the action detection track and the audio based detection track and congratulations to the team at the University of Oxford and the University of Bristol and Czech Technical University in Prague for their second place award in the audio based interaction recognition track audio based interaction detection track and action detection track in third place as well and then our final inperson award we have the first place domain adaptation for Action recognition track from Shanghai AI Laboratory um as well as second place multi-instance retrieval and third place action recognition okay so now we have our invited speakers for um the challenges as well um so this will be a 7-minute talk with a 3 minute Q&A at the end and we'd first like to begin with the team from C for their rank one action detection sound detection and action recognition method can you please connect to every joined over zo strong resum or so that you can share your screen there do you have the link no I don't have the L okay what says your link e e e e e e e e e e e e e e okay sorry about that so sorry for that so another issue is that since we train the model the verb task and noun task separately we then need to construct the action labels so we select the top 10 noun classes and top 10 verb classes for each time step and then we calculate their product to view the action probability and classes actually when we evaluating the action labels on the noun and verb task we observe uh 3 to 5% performance drop so this reflects that there is conflicts between the n task and verb tasks maybe joint training will help it but uh we didn't have enough time to explore so in the end we surpassed the uh our last year submission by a very large margin from 2252 to 3197 this builds a solid Baseline for future works and also our strong results does not only come from the Innovative detection methods but also come from a very good detection code base we use actually we use the the open Tad this is a recently released a large scale Tad code base and supports over 40 methods and nine Ted data sets so it's very easy to mitigate to a new data set and can achieve stronger performance so therefore we use this codebase to uh build our uh method also also on this newly Rel released audio based interaction detection data set and thanks to the organizer they have already provide a very good feature which is auditory slowfast and we use this uh auditory slowfast with our uh code base and our method we boost the baseline from 7.35% to 14.82% so this will also set a solid Baseline for future works we also notice that this in this uh task the audio feature is much more important than the visual features although we extract intern video two feat features which is very strong but it performs much um worse than a simple auditory slow fast so apart from the pre pre-extracted features we also want to introduce our cvpr cvpr work that is doing inin learning so previous all the T experiments Le from video understanding experiments they work on the pre- extract features however in this recent work we managed to perform end to end uh training with 1 billion parameters over 1,000 frames so we find that actually by doing end to training the performance can be greatly improved so we can surpass the best features by a large margin and on some data sets we can even double the performance the average M performance so that is very important means in training and also even this backbone is already fine t on the short video clips let's say applications we still can uh find uh that performance uh gains by doing in training on long form with understanding this also uh indicates that the uh Upstream recognition task is not very consistent to the downstream detection task and maybe end training on the downstream task can further help the performance so our code base and technical report will be released in our code base and if you have any question you can ask or communicate with me our poster is hosted in the June 20th in the afternoon so if you have time please have a look thank you so much so we have a few minutes for questions are there any questions yes congratulations on the award I was wondering if you tried the audio recognition task uh we don't we don't have enough time sorry time like you it and it didn't work yeah yeah okay and um or the anticipation task is that something you've looked into at all uh we we didn't also have yeah we didn't work on that sorry yeah yes please that's a very good question that actually Mamba was recently just re introduced and there's a paper called video Mamba Su they explore the Mamba architecture in the long we don't understanding because Mamba suits the long form sequence modeling and they basically proves that uh vanilla mamba or Vision Mamba they if you apply it on the on the feature temp long on the pre-extracted feature it has really similar performance to the standard self attention or others but if you you share share parameters in the SSM they can have uh larger improvements I think the Mamba the the strong performance of Mamba comes from the one they have good architectures and also they need less hyper parameters to tune and also they don't very large data to find T to learn the inductive bias I I think from the perspective data efficiency mama has more Advantage yeah one last question yeah the so meance yeah that's a good question actually uh we find that even uh uh actually for example basically mambai has the half parameter of the hybrid caal block but we if we also app if we if you increase the r parameter of Mamba to the same amount you can have uh worse performance so each Ro is the best performance we tuned yeah let's thank our speaker once again thank you for it's another team from F University for their e e e about 14 challenges application later is the present to be can you please use the slh SL is it okay now yes a sorry okay so you can start when is this one um hello hello this is not sorry we l yeah they trying thank you thanks for the introduction and my name is Chan from fan University and today I'd like to introduce our solution to the application challenge okay so this is a collaborated with e b guen and other teachers from Shanghai laboratory okay so our general idea is to design a very powerful egocentric video language model which we called ego video to uh to solve a series of tasks and this is achieved via Progressive training and we will just uh introduce this Progressive training strategy so here this is a general the first the first way to do uh video language pre-training as sorry can I like this okay so the first dat we do video language PR training in general domain which is the also an in video2 model and I'll dig a little bit detail into this model so it contains three steps and I just show step one and step two here because uh we didn't use the model for step three so in terms of step one it is about distillation training from image experts as you can see we train a a video encoder and with mask modeling and the feature representation distilled is from we distill this from an image expert for example interv and also you can do the feature-wise uh similarity feature wise L2 similarity here so this about distilling the the rich Knowledge from the image expert to to serve as an initialization of the video encoder so this is about step one and step two we do the multimodel contrastive learning it's a so we have a video encoder then we have a bird large model as a tax encoder and audio encoder so we have these uh three modalities data and we can do the constuctive laws like uh between video and text and video and audio and audio and text something like this so this is about multimodel contrasting learning so in the first stage we the data we use is a combination of multiple action recognition data sets including connect 400 and 600 700 and activ Ne Etc and then we only use the videos and discard their action labels because what we do is distillation training we don't have we don't use the their action labels okay so for the second step we use hybrid combination of uh image TX and video TX data sets including lion and web vid and inter and so on okay so this is about for training on uh General domain so the second stage we do the post training on egocentric domains because you know we need to shift the uh the domain from General to entric so as we can um tackle the uh egocentric videos in applications right so here we choose uh a lot of uh entric data sets like ego 4D and a subset of uh entric videos from how to 100 million because you know for instructional videos some some clips they are also egocentric but most of them are exocentric and we choose uh some of them to merge into the entire Ecentric video data sense and also some some videos that are generated by algorithms from recent works like video recap and lavila okay so we have this very large amount of Ecentric tax and pairs TA V and tax Pairs and we can fine tune the our text encoder and video encoder on this data set so experience here is more frames for example 16 frames are better always better than four frames but also it takes longer time to to to train a model okay so um also more entric data leads to better uh generalized performance okay so in terms of stage three uh we fine-tune this uh the model we have for from stage two on different tasks like action recognition and domain adaptation and multi- instance retrieval but that domain adaptation we only find through the model on the source domains because we don't have access to the Target domain the label of in Target domain so as you can see for Action recognition and domain adaptation we just use the cross Ute laws and for multi- instance retrieval it's very similar to The contrastive Learning so we just find to like the stage two okay as you can see our model achieves better performance than the the previously uh previous model lavila and Avan which are both developed by VL from UT Hostin which is which were also very good works and also all domain adaptation I think because we collected a very large data from uh you know e 4D and how to this covers a wide range of activities so I think suddenly sorry okay okay so I think this model is better at domain adaptation here so I think we can know okay so I think this model Now is better um at domain adaptation because the data can cover more domains here okay so for M instance roval our model also achieve very good results and also for zero shot and fine tune yeah and here are two experiences for the first one is video language pre-training on web skill image or video text payers learns very general good features and the second one is our post training on a collection of entric videos also demonstrate very good transferability okay so uh to conclude this work our work views ego video a very strong video language model with Progressive training strategy to solve a series of action understanding task on application and it's a bit pity because we didn't try uh other task like Professor has mentioned we think we will try in the future okay so the the code is also available from here which will update it soon than any questions ad generalization mod yeah so our focus is about creating a very large scale Ecentric data to try to cover the you know no longer L to adaptation going be able to solve is about colleting yeah I think it depends and for example if the The Source domain and Target domain they have very large gap I think there's still a very uh large work to be done to design you know very good model to solve this problem but if you think if the domain is not that perhaps some nowadays the vision Foundation models they can also cover this domain so perhaps it is not very technical thing I mean tring to uh we've been try to refining it nowadays and to be honest we annotated some of them because nowadays there are no uh publicly available ego axle classifier from the best of my knowledge and we've been trying to solve this problem it's even though it sounds really easy but in practice and when you do this you face a lot of problems for example how do you define ego and there are a lot of eego like uh videos in how to it's a photos that only contains hands but it's not recorded by in first person view so we've been trying to you know how to um the D this and we're trying working to to to improve this data okay thank you thank you so much any other question then we would like to thank our speaker thank you thank you so much it's now Oxford and Bristol for their second place ranking in sound recogition sound protection you like to change your click here yes click here and then select yes we see yourview on So You happen oh HDM you a yeah hello my name is Jason and I like to have a presentation of the presentation title is Tim a time interval machine for audio visual action recognition my name is Jason and this is a joint work with Jacob Angelas Andrew and Deon oh this gives us sample this shows a sample clip of the Epic kitchens and then as you know epic kitchens has a distinct annotations in the visual and audio with the both in temporally and semantically and but the question is like how to use this annotations so the current approach in this action recognition is that we have this like a start time interval with start time and time we're interested in and we want to recognize the action so the current recognition approach is we have this visual we have a visual we cop the video and a corresponding audio and we somehow you use this put this this two modality to the model and then recognize the action but actually context helps like if you look at this example this person is a washed potato and then they take the knife and they start cutting the potato if you know what's happening after this like wash potato wash tomato and then then according to Tomato it's really helpful to recognize the current action which is was tomato and there are actually current approaches which uses the neighboring Clips to recognize the current action however there there's not always we have this like certain like start and end time of this neighbor clipse to recognize the current current action what we usually have is the kind of un trim video and we only have the certain like time stamp of the start and end time and then we want to recognize the action so current approaches don't use this like UNM video quite well to recognize to use the context and then to recognize the action so therefore we are kind of interested in Tim which is the acronym of time interval machine so we have if we have this untrim video and then a corresponding audio and then we want to see that say that like the for rest of the slides we use the blue as a video and then red as audio and if we have a started end time you're interested in and then we also have like a modality so we can have a question that what's happening with this in started end time in the visual site or what's happening between start end time audio SL if we have this started end time we create this encoded query with tagging with the modality and then put this to the our model called time time interv machine along with this unre video with a video and audio to and this time inter machine is a transform encoder and then get the visual action or auditory action the IDE is quite simple so it's like if we have an untr video we want to know what's happening between 3 second to 5 Second in the visual modality we take this a three and five number and then create the encoded query and then put this query to the out model and then guess the corresponding action we can do this in the different modality we can do this in the audio modality say we want we want to figure out what's happening between 0 to 5 Second in the audio audio modality we simp simply took this like two numbers zero and five and make a query and then ask him what's happening between here and then we can we take this get this corresponding action can do this and then also the cool thing about our model is that we can simp really simply deal deal with the overlapping action so if we want to know what's happening in the audio between 11 to 14 second also 13 to 19 second we simply just take this like two numbers and then ask the Tim what's happening between them and then get we get corresponding action so I'm going to present the full picture of this Tim if we have an UNT video and the corresponding audio we have this freeze visual encoder a video encoder and corresponding audio encoder and then we put this to the GV and GA which is just a simple linear linear linear layer and then we get this visual features and then audio features we have to note that we are these visual features and audio features like cover certain time of time of time interval and then we have visual visual time query and audio time query so we have a question of like what's happening between 0 to 4 second or what's having audio between 1 to 3 second we simply put this numbers to the time MLP in time interval mop and then get this like feature which encodes this four 0 to four or this time interval and then we put this query with a classification to CLS token and then we also I just said it but like like there's a features also covers certain kind of time ststem so we needs to have a concept called feature time interval so we we put also these time interval to the time interval MP and then T get the features and then attack this to the corresponding video features and audio features and of course we also put this introduce this learnable modality token so we put the learnable video token to the visual modality features and then learnable audio token to the audio features we put all together to the audio visual Transformer and then from the corresponding sales token we have the classification head and then we get the corresponding action both in visual side or audio side and this also shows like repres shows like the how this like time interval MP but it's really easy it's like like we have this feature time intervals and also the quy time intervals which is like two numbers start and end time you put this two number to the this MLP which is a three layer three linear layer and a Rel in between and then we have this corresponding feature time en coding and en query time en coding and then we put this feature time encoding to tap this to the Fe the features and then query time encoding is used as like query or question to the model that what's happening between these time interval so this is like a approach for recognition but we have to adapt Tim for detection so we introduced two different methods one is regression head so for regression head we have this like a CS to CSS token and then also the fre timeing coding and then theity token and instead of using only the Cs head we also introduce a regression head which is like regresses the action the time interval when the action is happening so if this is Rec cry as a 0 to 4 second it should it regresses to the nearest act time interval where action is happening and also we construct this qu query pyramid which is has like have a cover covers from the short time to long time and we put this all to the Tim and then get the corresponding like actions and also this like intervals but during training we assume that the queries which has a IOU with the ground sh ground show is more than 0.6 second we deem as a POS positive query but rest of them are like negative query and we use only the green ones for our detection and so these are results we are we had we had a fourth place in the action recognition and then third place in the action detection we want to emphasize that like we use only the public data set and then we also didn't use any like self-supervised train method and also we got second places in some recation check this is a regular paper we release the code and then if you want to take a look please come to the Thursday PM session thank you so much Jason are there any questions for Jon okay so then we'll continue with the session with our fourth and final virtual talk okay can you hear me now and see my slide so slide is not full screen uh if you can please go into the presentation mode so I can start now yes you can start okay hello everyone I'm W and we are very excited to be here to present our solution for the multi instance retrieval challenge specifically we propose a noal loss function called symmetric multi- instance loss which offers a more precise learning objective for this challenge first we come to the previous training framework sorry second uh this TR in this training framework we have the uh video input and the corresponding narrations so we also have the pawise hard labels after calcul the similarity Matrix and we need a loss to constraint the similar similarity Matrix and the Heart labels so the most common loss here is the max margin loss of which the learning objective is sp minus SN larger than gamma the SP and SN are the similarity between the positive and netive Pi respectively and the gamma is the the margin uh however this challenge differs from the classical visual taxt retrieval tasks by providing a relevancy metrics that can act as a set of soft labels for video tax paers so in 2022 the EO WP proposed a new training framework here we still have the uh video clips but we no longer use a corresponding narration instead we find all the narrations at where the relevancy value is larger than 0.1 so we search all the relevancy metrics and get all the hard symol candidates of this corresponding video and we random select one for example the open fridge and add it to the soft label Matrix as as the similarity Matrix so the CP or the relevancy value for positive Pi here could be any value between zero to one so for example it could be 0.7 0.5 and so on so with this training framework we can easily transfer the max marging loss to Adaptive Max Markin loss so here the the market in the max marging loss is the gamma and in the Adaptive Max margin loss it's the cig multiplied by gamma so it's it also regarded as CP by gamma the CP is the relevancy value between the positive Pi however it will raise a problem the problem is the soft label Matrix will not look like look like this but instead the actual soft labels will look like this the c ik or the relevancy values between the negative pi will not always equal to zero and in the bottom of this slide shows the actual implementation of this loss function so consider a special case where the relevancy value of negative pi is even higher than the positive Pier that's that is to see the negative Pier is more positive than the positive Pier so in this special case the amm loss Will optimize the model to the exact opposite direction so how to Sol this problem we ask three questions here the first one is the relevance value for the negative pi the C is not always zero we have a very simple solution we modify the margin from CP by gamma to CP minus C by gamma but there is another problem as our mentioned the relevance value for the negative PE would be even higher than that for positive PE so the CP minus CN is less than zero and in this situation we switch the position of the positive and negative PE that is to say the positive Pier is now the negative pier and the negative Pier is now the positive Pier so you can see that our loss function become two parts one part is for R is larger than zero and the other part is for R is less than zero and how about r equal to zero we set our relaxation Factor here so since the ukian distance between the positive and negative per is less than gamma when R equals to zero we will not optimize it otherwise it will become the dominant loss so here GES the uh complete form of our symmetric multi similarity loss here and together with a flip and add trick it's a very simple trick just flip the video frames during inference and as them together and the emble strategy we TR six different models and calculate the main value of the similarity Matrix and the with all these Solutions we get the best performance compared to the previous SOA in 2023 we improve the performance by 5.34 on average map and 3.49 on average ndcg and compared to the amm loss our loss improves the vit large model by 2.3% on average map and 2.8% on average n dcg and our code is now available on GitHub and we will release more fascinating thing on this repository thank you for your attention thank you so much for the thank you so much for the talk uh do we have any questions than okay okay so thank you uh so this concludes our epic kitchen challenges session now we will meet at 1:40 after lunch for the ego based Awards so let's see who w w the awards and let's happen then see you later e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e oh e one yeah I realized I to get a than come hard we'll wait a few minutes we're definitely waiting for our photographer we not starting before he is here you point e e e right um welcome back thanks to the speakers and the session chairs for the more for an excellent morning session we're starting the afternoon two sessions which as the program States the first one will involve three things the egoist distinguished paper Awards which I'll tell you a little bit more about I know many of you are here for that specifically then we have the combined keynote from Michael Frank Frank and Bria which will be introduced by tar and then we have the area um challenges and data sets before the next coffee break right what is this new adventure we thought about looking back a little bit it's not a lot but a little bit at papers that have transformed the field in 2022 2023 so these are peer-reviewed papers at conferences or journals that were accepted between January 2022 and December 2023 and we're looking for original and Innovative contributions to this field the field of egocentric vision and particularly we're interested in papers that offered contributions Beyond just writing in the paper in the form of code models and data and indeed this had been a major factor in the impact these papers had had on the field so this was an invitation for people to nominate their papers each paper was submitted along with a prologue that describes Why by the authors so it's a self-nominated prologue on why this paper should be a deserving of the award um it also had the list of reviews um that the paper had been accepted based on we received oh I think online we do say we did receive around 30 submissions that all were deserving and passed all the setup here um and papers have been selected based on either being chosen by more than 50% of the board that voted for the awards the non-conflicted board or at least three three out of seven people so it's a combination of either and the award will be given in alphabetical order so there is no particular order of these Awards I'm giving them completely alphabetically each set of winners will be given a two minutes speech it was labeled as Oscars without the tears so no tears will be allowed but they're supposed to reflect on how um the data was collected how the impact was achieved so we'll be going through the 2 20 minutes will be the award deserving AES as long as their two minute wisdom speeches okay and this is my brilliant arts and crafts project with the certificates which will help the the uh Antonino kindly agreed to take the photos so this is the person you should be smiling U for the photo and again in no particular order the first distinguished paper award in our first round of ego Awards goes to assembly 101 a large scale multi viw video data set for understanding procedural activities thank you very much for the word um so assembly 101 was a collaboration between meta and Angela's team at s National University of Singapore and we both had different interests so meta's team wanted to have some a new data set entc data set captured with quest Quest two and capturing hand object interactions and our motivation we were working on procedural activity understanding our motivation was having a new data set that's not from the kitchen domain and that also has um entric recordings so we started this project during covid time uh we recorded this data set I think you through like three different time zones I was back then in Germany and Angela team was in Singapore and the other meta team was in the US it was uh yeah not very easy but at the end we managed to finish the data collection annotation and everything and 101 um afterwards like after the publication got uh interest in many I Direction many um parts proced for procedural activity recognition it's been used and they later also published that work on assembly called assembly hands where we were trying we try to provide ground Tru for this kind of interactions and recently I also see more and more papers about um mistake detection in procedural activities this was also introduced in this paper uh so for me personally um assembly 101 is also very important because after this project I um got a job at this um group gr at meta and we started a new team for Action recognition and right now I'm actually doing online action recognition developing models for smart glasses uh for online action recognition and my first data set uh the first data set I used for this project was SM 101 um yeah I think that's all thank you very much the second paper again in no particular order these are alphabetical goes to Ego body POS estimation via egoe head POS estimation okay uh thank you for the Amazing Word I'm very excited and honored to be here today so our journey uh starts with a goal of estimating forward human motion you more accessible setting using eego Center which requires minimal sensors and the most challenging part for this problem is a lack of large scale pair data sets with both Ecentric radio and and full body motion so uh capturing such a data set is not very practical since motion capture usually requires specific Hardware um so in this uh in our project the key is to use headp as intermediate representation so that we don't rely on such pair data set with both Ecentric video and for human motion and another critical component is the condition diffusion model and uh I think the conditional the diffusion model really Advanced a lot of research Fields including ours which enables us to generate highly diverse and uh very Dynamic human motions from head poses and I think our paper opens some new exciting opportunities for example um maybe we can integrate more we do dve data with ego ego to further improve the motion estimation performance and second I'm also excited about uh I'm excited to see whether we can use uh this approach to extract motion from Ecentric radio and use the extracting motions to further uh improve the human uh Behavior Analysis to infer actions and intentions and finally I'd like to thanks uh thank my advisers for their support and uh uh positive feedback during uh the research project and their encouragement gave me the confidence to address the changing task because at the beginning I really don't believe this will work um I also want to thank the um authors of previous work and uh for Fruitful discussions about the Ecentric post estimation topic and this discussions really deepen my understanding of entric post estimation and thanks again for the award and we are very excited to push uh continue pushing the boundaries in Eco Centric region thank you great the next is ego Ford around the world in hours of egocentric video hi everyone uh thank you so much on behalf of all the 84 authors of this paper for this recognition which is really awesome so if you don't know about ego 4D um in a few words about the motivation the aim was to collect everyday unscripted activity in as many places as possible and as As Natural settings as possible and at the end of the day this meant about 3,600 Plus hours of this such content from almost a thousand different individuals in 74 cities around the world and the kind of the who the team behind this came from 14 different institutions as I said 84 authors and it really brought together PhD students faculty researchers Engineers from meta as well and this was a process so thinking of Journey it was was about a two-year effort that started Brewing within 2019 towards the end um maybe summer fall and then launched this data set for the first time in 2021 so I was reflecting back on what were some of the challenges along the way to get to this output um one you know we were dealing collectively with a very large group of scientific collaborators um with lots of ideas lots of energy and also lots of logistics which included engaging with these 900 plus camera Wares or participants um we had challenges and interests in ensuring privacy and ethics standards so that this would protect the subjects involved but also ensure that for the research Community it's a kind of um full-fledged resource that we can really rely on and continue to use and then you know um heard it in assembly 1012 there was Co along the way so if you notice that twoyear span now we had people in the wild doing unscripted activities also during covid lockdowns and trying to capture that so for me personally this effort has really been a highlight my career so far um why you know the willingness of these World experts in video to come together to do this long-term focused effort um and two it's just been so thrilling to see the growth and participation on these challenges baselines getting better and better um as you'll see later today you know now the fourth instance of the challenge and I think it's something like 85 teams who are participating now and also to see the adoption by Sister communities like robotics language um bringing new ideas that didn't exist at the start of this effort but now can um be enabled in new ways so finally big thanks to everyone who created e4d some of whom are here in the room um and thanks again for this recognition great we have three out of three female Representatives I like this the trend is good we're making some balance um great the fourth award goes to Ecentric video language pre-training um so thank you very much uh since Kevin is absent from the cvpr confence due to his personal issue um so I want to say hello I'm I'm not working in this area and today he sent me the note to deliver the speech on behalf of him so um I'm very proud that ego VP has made it to the dlish paper list at e along such amazing works so uh ego VP is my first research project during uh my PhD and when Professor Mike uh introduced the data set E 40 to me I was super excited and dive into exploring the massive Ecentric videos um I have spent a lot of time to uh explore the EOS Centric videos and I'd like to talk about how it came to be so at first I found that uh they didn't really help with the third person task such as uh msrv and this made me constantly think about uh what what kind of task ego 40 could help with but then it uh I tried to uh I tried it with ego uh ego Centric videos such as uh epit by chance and it showed significant Improvement and this made me believe in the uh importance of entric video training and uh I'm very glad to see that ego VP is now receiving uh the community's attention and its model and findings are making a difference to the e Centric community and finally I'd like to thank my collaborators uh including Alex Matia Michael Bernard De le and Mike and many others thank you very much next is ego humans an egocentric 3D multi-human benchmark right this some of them are in the room M uh ly aush Chris Richard uh this was my first data set project and uh also the hardest project I worked during my PhD uh it took us 8 months to collect the data and about 1 month to get the methods to work but it was a learning experience going all the way from cameras to having a model which works on your of data and uh there were questions which uh I would not even imagine you have to worry about when you're working a data set project like how many memory cards are they prepared for tomorrow what batteries how many charges do we have app uh trying to make sure that GoPros don't heat up in Seattle Summer that was a big issue we run into but uh I think this was the biggest learning experience and now it's a whole thing on its own there has spin of data sets and the code is being used to uh more data sets in the future but really appreciate thank you so much for the excellent and the next a distinguished paper award goes to Ego vlp V2 egocentric video language pre-training with Fusion in the backbone I uh thanks so much for giving the award to e V2 uh this was my first internship during PhD uh I was in meta during the summer of 2022 and and I started the work with the aim uh to build good summarization uh systems for long form entric videos uh but then we saw that for building summarization systems as the summarization data set at that time was very small scale we needed very good pre-trained weights and from there I started to look into ego 4D the previous papers like ovp and lavila and from there we build the pre-training uh system so the whole uh thing took almost 8 to 9 months to work uh to complete the pre-training then to transfer the pre-train ways to multiple tasks and benchmarks but the overall experience like it was a like multi- Institute collaborative effort and I learned a lot and I think uh uh this was the first big big scale project in my PhD so I would like to thank all my collaborators I think some of them is already here elel and ping Chong and my advisor for supporting next epic sounds a large scale data set of actions that sound uh so yeah um thank you very much for the award so kind of before epic sounds everyone kind of knew audio and visual have very strong correlations but there's a bit of too strong of assumption because people kind of assume this was perfectly aligned um but when we kind of thought about it there's a few issues we use different semantics to describe sound and sometimes we use different time stamps to to describe actions uh for the same thing in visuals so this is kind of where epic sounds was born and yeah we went through a a years worth of work um annotating 100 hours worth of audio equally as many hours typo correcting and listening to sounds that are really indistinguishable you'd be surprised that an egg sounds like a ceramic Bowl um when you're just listening to it through headphones um and yeah it was it was a really big effort um from all of us so thanks to our co-authors vanelis and Andrew who aren't here as well um thank you to Raja and his team for helping us collect all the annotations and sort of getting all those temporal time STS for us to go through and yeah thank you to the audio Community as well the E cental Community for really responding well to the data set and sort of taken up through the challenges and given us um a lot of feedback and helping us to understand the data even more um anything yeah and and yeah um thank you very much next Holo assist an egocentric human interaction data set for interaction AI assistance in the real world um so I'm actually covering in for shin who is the first author she couldn't make it today and I think T is um on call here um but uh Hollow assist has been a a labor of love for all of us involved um you know when we started like two and a half years ago um there were very few truly multimodal data sets out there uh for long form video understanding and um uh you know we were all working with the Hollens uh and we realized that there was a lot of other inputs that we could use Beyond vision and language to uh really inference on tasks related to um long form videos um so we started from there and then um the second idea that we had was to actually um have sort of this interactive agent that uh that can look at what we see and basically help us along the way for stepbystep guidance or U sort of this open-ended questions of the task around the task so um you know if if you look at the data set we have both actual recognition but also other kinds of Novel uh metrics around mistake detection uh intervention prediction and so on which uh actually try to model the user intent um uh Beyond just the task at hand um so there's already a lot of Engagement from the community thank you uh we love love that excitement goes on and uh a huge shout out to everybody involved in Microsoft research Neil josi and his team and also uh in Microsoft mixed reality Mark qualif and all of usol thank you two more next learning video representations from large language modeles um hi everyone uh it's very uh happy to be here um to win this award and then thank uh the committee for uh selecting lailla as one of the outstanding Awards um this project was done during my internship uh at meta and then it was mentored by uh my two amazing uh mentors uh isan and Rohit uh who help me really help me a lot during in the project because uh I think I actually had a lot of like back and forth kind of things during the project development um so basically the project was trying to learn a very good uh video representation from uh large scale video and then like before we mostly like have these kind of uh uh uh like single label video data set and then we realized like it is very uh impossible to scale to like even large scale because we know like these U video or like action particularly they have a very complex combination of these nouns verbs and then even more complex like kind of descriptions and then we realized that um natural language might be a way to go and then uh it's very uh fortunate uh to to have uh you go Ford data set who actually have very uh very very nice kind of natural description like curated by the entire community like uh like three thousands of videos with the kind of uh human annotated uh uh like natural description a natural language description so it serves as our starting point and then we uh also figure out that we can actually leverage the large language model to help us create even more uh uh captions uh which actually we find that it is very it's it's becoming more and more popular in recent kind of computer vision community actually um so we have this U uh caption model like like actually moves Beyond these kind of entric videos but also like for these uh like more generic kind of video community so it's very nice to see the impact of the lavila so we actually have like over 400 stars on GitHub and then I heard that uh like the the model itself is also like used in some internal kind of product things um no more details yeah so and then after this project I'm still working on like kind of building more scalable and then more foundational kind of video language models and um we we also figure out some more efficient way to train lailla so if anyone is in the community like Academia and don't have these kind of four or even more like GPU clusters we we also have a kind of lighter weight uh solution to train kind of lavilla or like even model like can compete with the laa using just hpus like in Academia resources um so yeah so like I think I I say too much um yeah still I'm very happy like because I'm too happy so I maybe speak too much um yeah so thank you everyone yeah and uh the last selected award in no order goes to sound space is to a simulation platform for visual acoustic learning uh thank you for the award uh hello everyone my name is Chen Chen I recently obtained my pH degree from UT working with professor chrisen grman and right now I'm a post at stford working with f and is and Del so it's a great honor to receive this award uh for our work on S spaces too and I want to briefly reflect on this journey and uh the impact this project has had sound spaces to became as an ambitious idea to bridge the gap between audio and visual perception for Ecentric Vision in the embody setting where a robot can see and hear the world as humans do in enabling a more immersive and interactive AI experience capturing uh scalable and massive EOS Centric audio video streams uh can be challenged and we decided to take a simulation Jour approach where we can simulate both video and audio given the meas of the environment and configuration of the space in a scalable manner we incorporate an audio propagation engine into habitat which is a video simulation uh platform and build a continuous uh configurable and generalizable audio visual simulator that produces realistic audio visual rendering on the Fly developing some spaces too was a challenge but also rewarding which required the collective effort from both researchers at UT aen but also researchers from reality Labs at meta the impact of this work has been profound it has been uh the most widely used audio video simulation platforms for Ecentric audio rendering transforming how researchers approach multimodal a tasks seeing the community adopt and build up on work has been incredibly fulfilling some spaces has enabled new research directions facilitated advances in navigation models and even cross over to the AUD committee impacting uh challenges I for example at aast I want to thank my collaborators and the supportive research Community this recognition has been a collective effort and we are excited to see how some spaces do will continue to inspire and drive future Innovations thank you with that I would like to WR particularly thank all the members of the EOS board who took a lot of time to review I also would like Tok thank and congratulate all the people who submitted but were not selected in the top 10 they were an exceptional set of papers that we had to look into uh great progress and I will end by thanking um the U mat ARA team for sponsoring the monetary award which hopefully will allow the authors to enjoy a little bit more beyond my handcrafted Artcraft drawing certificates that that you would receive thank you very much and next we're going into the third third keynote the third Keynote with to sh e it's good Let introduce you all right um it's my pleasure to introduce Michael Frank and PR along is our speakers for the session uh Michael is a professor at Stanford University where he heads the Stanford language and cognition lab which builds computational models to study children's language acquisition part of which involves how this relates to their visual perception and briia is an incoming assistant professor at UC San Diego leading the brand new visual learning lab um she led a lot of the work on the vision to language acquisition link while she was a post talk at Michael's group uh I interacted with Michael very briefly over email back in 2022 we supposed to schedule a meeting uh to talk about some connections between my research and his own group's work uh but I was two months from graduating and understandably things just fell through fell off the radar so I'm very happy to hear hear about the research picked that up and um see what our two Fields can learn from each other VI thank you very much for the invitation and the kind introduction I we're very happy to be here today to connect with this community and to share some of our most recent findings so my goal today is to do some scene setting and tell the story and then turn things over to Bria to tell us about uh the recent work that we've been doing so as an outsider to the computer vision field the pace of progress in the fast the past few years has just been incredibly exciting and the same is true for natural language processing where the rise of Transformer language models has just fueled these incredible advances my question is how these tools can tell us about the human mind and how they can provide insights into how children learn comparing AI to humans is exciting but there's a fundamental problem which is that AI uses vastly more data and often vastly different data so starting from the perspective of the human mind here's a picture of my 5-year-old son Jonah he's just learned the game Candyland and he's playing with two of his buddies what does he have in common with modern AI models critically both he and state-ofthe-art language models are zero shot Learners they can learn new tests like quick overview of the rules but there's a critical disanalogy between these systems by our best estimates Jonah at age five and other five-year olds they've received between tens of millions and at most hundreds of millions of words of training input in contrast llms show zero shot learning at hundreds of billions trillions or even tens of trillions of data tens of trillions of words of data so there's this data gap of between three and six orders of magnitude that we need to explain between Learners and models and these same arguments of of course are playing out in computer vision where people have been asking whether we need better algorithms to reach human levels of performance or whether human level data will be sufficient to get human level performance even with current approaches so turning to the cognitive science literature we can ask what explanations are on offer for the gap between human scale intelligence roughly in the order of tens to hundreds of millions of words and llm scale intelligence at hundreds of billions and although I'm framing this in terms of words I think we can think about this more generically in terms of data and so these arguments really apply in the vision domain we'll show you that in just a moment one approach in cognitive science that's been very popular over the years has been nativism in which the key argument is that humans come evolved or pre-trained with representations of the world including of Concepts like an agent or an object and this pre-training via Evolution leads to very rapid learning so maybe they make better use of the data they have a second possibility is what's referred to as constructivism here the emphasis is on children's active in social learning they get the world curricular IED for them both by the social actors around them and by the examples they themselves generate through their exploration and uh their movement in the world a third possibility is simply that the kind of data that kids get is fundamentally different and the grounded real world data they receive ceive is actually much richer than what you get when you boil that down into a set of decontextualized tokens and of course when you're looking at a graph like this you might importantly ask how much of this difference in order of magnitude is simply due to evaluation or comparison issues after all multimodal Transformer models also speak many languages and can recall many facts about the world that's not so true of my 5-year-old what I want to argue today is that there's a strategy that can help us address these kinds of questions the strategy is that you take your algorithm of choice you train it on children's training data and you evaluate on children's learning outcomes in other words you try to create an apples apples comparison between models and human Learners in order to understand whether the efficiency of algorithms is sufficient to learn what children learn on the amount of CH data that children get and then of course we can do the hard science of uh using llms or other models as cognitive models and comparing to other kinds of models for example those with greater pre-training of some sort or greater ability to explore the world and learn actively or from social others so with this kind of strategy in hand we can then be a bit more specific about how we Define what we mean by a data Gap now I've been giving you a sort of general idea with respect to zero shot learning which I think is a particularly exciting phenomenon but we can also Define data D gaps for specific tasks for example visual categorization or F shot learning or for uh grammar learning and language and so forth and the basic schema that we want to Define is that we need some kind of human compatible metric so our Benchmark needs to be something where we can get a human Topline then we want to assess what kind of data humans actually have access to and Define some age level of performance whether we want to match a 5-year-old maybe that's a low bar or whether we want to match an adult human and of course calibrate to the data quantity of that particular age and then we can create scaling curves and Define a data Gap with respect to those scaling curves so this is the kind of approach that we're seeking to follow in this kind of form and in order to enable that my lab aims to build open data resources that help us characterize children's and their performance make those available to the community uh both for developmental scientists to study learning and also for uh folks like you to use as benchmarks some of our resources concern children's language input this is both multimodal video as well as Corpus data we also archive data about children's in the moment processing with eye tracking and their learning outcomes based on large samples of kids from around the world so in uh the next couple minutes I want to tell you the story of our journey into looking at egocentric video as a particularly important kind of training data which is of course why I'm here and hopefully why we can tell you a little bit about some of the work here so the first data set that we created was called seam this was an early data set that we gathered using our first attempt to add an entric camera it was a small lightweight Matchbox siiz camera with an aftermarket fisheye lens we gathered data from three children about 470 hours two hours a week from six months to 30 months gathered now quite a while ago starting more than 10 years ago and the quality here was relatively low it's 640 by 480 uh and the audio especially is low quality now this data collection we've heard a couple people say well this data took me eight months and I I respect that but this data collection was quite onerous and for that reason the only people who participated were the children of Developmental psychologists um but we were very proud of this data set because we were able to share it openly through data.org which is a video data sharing site specifically for developmental data developmental data are more sensitive they're gathered in the homes of families and the activities that are represented there include not just meal times and play times but also diaper changes breastfeeding a variety of intimate activities so we need more protections on these data but we want to make them as open as possible to researchers so if researchers at universities are interested in accessing these data they can sign an Institutional agreement that uh requires that they not re-identify or reshare the data and if folks are working in uh non-institutional um industry research Labs that are interested in accessing the data they can collaborate with folks uh in universities to gain access through an authorized investigator so if you have trouble with access to this data set please let me know because it's our intent to make this open to the community and you can see uh you know many of you are our specialists in entric video so you know a little bit about this but you can see that the child's perspective is quite different from the adult perspective as noted by Pioneers in this field like Linda Smith Karen Adolf chenu and a variety of others who have used this method from whom we learned how to apply it our first steps here taken by Bria were into characterizing the social information that kids had access to and our idea was to use off-the-shelf computer vision to get a sense in this larger Corpus of videos what kids were seeing so we used pose detection via open pose results were substantially worse than in non-egocentric video in part because of the complex crops and angles that the models uh encountered but still better than previous systems that we' used and in this data set we found robustly something that other investigators had noted namely that there were far more hands in the field of view of kids at all ages and for all of three of the kids than faces suggesting again following previous research that hands are really a uniquely valuable learning signal and you can get a very nice sense of this here uh in these frames which have been blurred to show the uh General kind of characteristics of the human field of view which is fixated on it a single point with high resolution so you can really see how hands are demonstrating pointing showing and so forth really could provide some interesting signals we had less success with seam however when we looked at understanding category learn the videos were really too low resolution for segmentation models at least at the time so we used mask our CNN and uh we really got very poor performance uh even after fine-tuning we only did well on a small number of object categories the video angles often obscur as what kids were acting interacting with and as has been recognized by a variety of work in this domain in the natural home setting infants see some objects a tremendous amount and then there's a long tail and dealing with that long tail was very very challenging given the nature of our data set so we weren't able to pursue this Direction with those data however the growth of contrastive learning unsupervised learning methods in computer vision was really really an important step forward for us and so our collaborators Chun shuwang and Dan yans started to train a variety of different contrastive learning algorithms on seam data their interest in this study was in doing neural predictivity so they were using both uh fmri recordings and also MAAC electrophysiology data to try to predict the kinds of representations that primates learn in their visual system based on these contrastive learning algorithms a variety of different algorithms including Sim clear local aggregation and others and chunu found fairly good performance for these algorithms both static image and video based algorithms on Sean but they called this really a glass half full glass half empty finding because the uh the unsupervised approaches didn't actually beat either unsupervised approaches on imag net or supervised approaches and so there was really more to do here so uh there was a kind of memorable meeting where Dan came to me and said you know I know this was a lot of work for you to get this first data set but could you get a lot more data so bria's gonna tell you a bit about that multi-year project now thanks for having us here today so here in the second part of the talk I'm going to tell you a bit about the Baby View Camera and the data set and what we've been able to do with it so far so we sent out to create a better higher resolution camera for use with kids we collaborated with a product design firm in San Francisco called daylight design to construct this device which is based on the ultra lightweight GoPro herob bones which you can see attached here to a child safety helmet with Bo of my children nicely demonstrating for us these are relatively easy to build and easy to deploy there's a 3D printed Mount and the components it's around $400 each and our team can build these in around 10 minutes importantly with the GoPro herob bones we can now capture both video data but also accelerometer and gyroscope data to identify the child's self motion so now we build entire bouquets of Baby Views that we send out to families around the US hopefully other places in the future and we ask parents to record their children's everyday experiences in the home and so our goal here is to record an entire child's year worth of data that would be 4,000 hours of data for contact sayam with around 400 hours so this would be tenfold in size so of course this is quite ambitious and we are still working on it but I want to give you a sense of what these videos look like and in fact um here we go this is a video from my living room this is my daughter doing a dinosaur puzzle in the living room and you can notice right away um that you can see her hands because it's a much different field of view we've rotated the camera so it has an effective 100 degree uh field of view in the vertical mention and now you can see not only who she's seeing but also what she's doing with her own hands and what objects she's interacting with so this is one small snippet of a video from the data set currently we have 28 families we're going to be having our first release on data BR in November 2024 so right now there is a parental embargo on the data so parents can excise portions of the data set that are sensitive afterwards um so it's not yet available but will be in November so we have 28 families that have contributed to this first release um you can see the cumulative hours of video on the y-axis with the age and months during recording on the xais different families are recording different amounts of data but some of our star families have already recorded you know upwards of 60 hours of data and are still recording to this day so we are continuing data collection right now through next September and perhaps into the future pending funding we've been recording not only in the home but also in a different type of environment that is a preschool environment where we're starting to capture Children's Learning environments Beyond these early years in the home with their caregivers these are peer-to-peer interactions among three four and 5y olds in a preschool environment this particular environment is a monor Like Preschool where kids roam around relatively unstructured and so we get a lot of variation and a lot of different kinds of activities happening and while we're still analyzing these videos we've already started to look at the language data in them we've seen quite a bit of variability just in what happens in this one preschool so we've looked for example at the grammatical complexity of the sentences that adults and other children and the children wearing the camera are saying and during different activities either indoors or outdoors and we see quite a bit of variation in that grammatical complexity as well as variation in the amount of speech that's happening and presumably in their visual environments as well so just in this first pass we see quite a bit of variation in language across these different activity contexts this higher resolution video captured in both the home and the preschool environment has some real advantages as you might anticipate so we turn to Poe detections going back to our original work and we wanted to Benchmark a suite of pose detection models to see if we could get some that might actually accurately detect the poses in these videos so we have the Coco validation Set uh the different models off the shelf on the x-axis and then we annotated around 300 frames ourselves of every POS key point in a number of frames and you can see that there's quite a gap in terms of how these models are doing on these Ecentric developmental data points as well as uh versus the Coco validation set we do see that our largest models um are doing pretty well so these higher resolution videos and the bigger and better models are allowing us to have improved pose detection for all key points not just faces and hands so here is another video from the new set again uh in my living room since we're still under the parental embargo here's my daughter getting a book to bring to story time and you can see the pose detections overlaid um on both her own hands as well as myself and my son these higher resolution videos also have advantages in the other domain we've examined that is in object segmentations so this we've evaluated less systematically but just to show you a few example frames this is the same mascara CNN model this is not fine tuned just off the shelf these are higher confidence segmentations and you can see see while there are quite you know a number of Mis categorizations those dupl are not scissors um it's still getting a pretty good sense of the boundaries of the different objects in the scene so we're excited to see how these higher resolution videos are enabling us to use all sorts of different off-the-shelf models to extract descriptives from these data which will be useful just for characterizing what's happening across Development Across individuals um and across different contexts next we turned to seeing what kind of models we might be able to train on these data what kinds of models can actually learn from children's egocentric visual experience so we asks um how well do self-supervised Vision models perform as they learn from increasing amounts of Developmental entric video data um so this work is led by Stefan stof who's here today you can say hi if you'd like to ask him any questions um so what we're going to do is train on uh 1 5 10 25 50 100% of the data set and we're going to be training actually on a combined data set of both SE Cam and the baby view home recordings so we're trying to get as much data as we have on this first pass and we're going to be um you know collecting more data in the future but this is to see when we have as much data as we do what can these fishion models actually learn what we're going to be doing is training different versions of dinov V2 which is a self-supervised Transformer and one of in particular we're going to be using this model because we can evaluate it on a number of different Downstream tasks so we can evaluate it on um both imag net classification using a linear probe but also depth estimation as well as semantic segmentation so we're going to ask how well do different uh Vision models trained on increasing amounts of the data set these are going to be randomly sampled frames from the data set how well can they perform on these Downstream tasks so next I'm going to show you the results from these three different domains so here on the Y we're going to have the top one accuracy as well as the top five accuracy for image net categorization the Red X is the random industrialization of the model and in blue we're going to have increasing amounts of the egocentric video data and so in general what we see in both top one and top five accuracy is that as you have increasing amounts of data of course you're getting better at image net recognition but there's still quite a big gap to imag net pre-trained dyo V2 would require quite a bit of data unrealistic amounts of data to get up to that human performance you might also note that little pink dot that is actually ego 4D pre-training um and even when we're increasing our data set size by quite a bit by training on ego 4D we're still quite a bit far away from ceiling so the suggest that learning from entric videos when we're taking these random frames from the videos at least is really challenging we're still quite far away from achieving human level performance we also examined depth estimation as well as semantic segmentation here the relationship is a little bit less clear for scaling as you add more data you don't necessarily get that much better at either of those tasks so we're still quite far away from being able to use this kind of data to train a model to human level accuracy but you might remember from the first part of the talk when Mike said that we want to be evaluating on children's learning outcome in fact how should we be evaluating models in the first place should we even be evaluating them on image net object category recognition if we're giving them videos from the first two years of a child's life so in fact we very much agree and so that's why in the last project I'm going to tell you about today I'm going to tell you about our work trying to evaluate models like children in order to do this you need to have large developmental data sets of learning outcomes which is non-trivial at best I'm going to show you an example trial from our picture matching task which I'm going to show you the data from today here are my kids yet again now a bit more grown up she hears the word duck ches the correct image here's the word milkshake it's gonna ask me for some guidance not quite sure what that is still gets it right you'll notice there's some distractors on here here the word is KN she gets distracted by that other body part but eventually gets it right so here on all of these trials we have distracted that vary in the similarity to the Target word the target visual concept and so we can actually start to model not only the actual Target Choice how well children are at knowing what a knee or a milkshake is but when they choose distractors what distractors they choose so this was a task we use in a test we've called visual vocabulary we have um quite a few uh children here we have about, 1600 kids and now we're going to be able to use these data to calculate how model human dissimilarity changes relative to model training so on the x-axis we're going to be using checkpoints from open clip which is a multimodal model and we're going to be plotting model human to similarity on the y- axis so closer to the bottom is going to be more similar to humans and we're going to do this for different age groups of kids so we have three to 10 year olds um around 200 adults as well and what we find is that really early on in training you actually see that the model is more similar to the youngest kids so it looks more like the four and the seven year olds than it the adults the 25-year-olds that turquoise line and as training goes along you see that the model becomes more similar in its response patterns to adults so there is at least some similarity between how the model is learning visual representations for these common visual concepts and how kids are learning representations how they're getting refined over childhood this is just one of many ways that I think we should be evaluating these multimodal models and in fact it's one task in a new project we been calling Dev bench which is a multimodal developmental Benchmark for language learning so visual vocab was one task here are the others which I won't go into in detail but are now up in our PN archive so we have a pretty large age range um there's infants in this Benchmark as well all the way up through adults there is ey tracking task there is the task I just showed you um there are similarity judgments and we think that this is really important for evaluating models like children so that we're not necessarily evaluating them like adults or against a gold standard that's not realistic for them to achieve based on children's training data and so in general we think that by examining children's sta training data constructing models that are learning from this data and then systematically evaluating them on children's learning outcomes is a really important way to try to understand the data gap between human Learners and machines and with that I'd like to thank you for your time uh highlight all the wonderful folks who been involved in this work and we'll take any questions now we do have time for a couple of questions so great presentation thank you very much for coming and sharing you with us I'm curious about this this reluctance in the child making the decision very quickly because that's nothing that's not what models do right our models are very decisive do we know whether this is an a crucial part of Developmental learning and if so what insights can you offer to us on what You' learn from this time of information sure yeah thanks for that question um so in this particular video I only showed you responses where the child was correct those were pretty easy trials um but you can look systematically at Children's error patterns over development um which we have done and are incorporated into that dis arity metric um and children they go from kind of choosing distractors randomly to choosing more related distractors over development um and so the behavioral paper that comes from that task we see a really long developmental trajectory in this like all the way through age eight and nine children are really taking quite a long time to have very precise representations of visual concepts um but you can get graded probabilities from these models so you can see that they're more confused you know between a knee and an elbow than between a knee and a milkshake so I think that that metric is actually doing a nice job of picking out um model to human similarity that maybe having a better knowledge of what objective the children are trying thank you for that so so I think I take your question to be uh how does this uh what's the interaction between understanding uh kind of sample efficiency and algorithms versus objective I I think I'm being kind of classic sloppy cognitive scientist and uh lumping in algorithm and loss function in the same thing but but in fact these are two different things and um I guess I would say that that um you know our collaborator Dan yans has really kind actually hammered into me that he he thinks our current generation of loss functions is insufficient for describing human learning or even maybe primate learning that the kinds of predictions that organisms want to make about the world are very different than the kinds of predictions that say Dino or Sim clear making other question about um Bri you had the slide about how the training of an open clip model resemble seveny olds as training progresses was it purely based on just um accuracy and a benchmark or do you think there's specific quotes about what a seven-year-old has learned versus what a one-year-old has learned that is also captured um so this is I kind of went over this quite quickly but this is not just accuracy this is actually modeling the uh Choice patterns so the probability that a child is choosing the Target and all the distractors relative to the models um probability of choosing those so it isn't just pure accuracy in fact that isn't a great uh you don't see really nice correlations across development when you just look at accuracy Rel yeah that that's a really nice question the age ranges here are a bit different so um the video that we have from the home environments are younger kids and then these are older kids who are doing um the vocabulary task but I think absolutely you're right that eventually we really want to know how to say frequency of haptic interaction or visual experience or auditory experience relate to the learning outcomes that children are seeing it would be nice to have um you know this within child but that's a slightly larger project maybe just add one thing to that answer uh that kids are really really good at out of distribution very early so if you get a one-year-old and they know the word spoon they can identify any spoon or you know butterfly any butterfly it's they're not limited to the only butterfly they've seen and that's fundamentally different and really kind of part of this story that we're interested in investigating exactly there's some works that do that not not from us that are very nice actually yeah maybe let's do one last one dat collection I'm Dev more babies hate hats I don't know if you've played with the baby but they love to rip things off their head yeah yeah yeah so so so um we actually find that this helmet although it looks big and clunky is actually much comfier than the headband type mounts that we've used you need something rigid enough to maintain the view angle um or else you end up look with floor cam or ceiling cam uh right probably some people in this room have tried this sort of thing so you need some kind of rigidity and if you have rigidity um you have uh you know real tension on the head and that's what the baby's hate so the soft helmet has been by far our winner in terms of uptake but really you know uh our recruitment here is in some sense limited by the fact that each family requires specific in uh attention so we check in with every family every week and see how things are working with them uh do they want to delete you know when their child was naked on the video and all and so forth so that's that's the bottleneck for us is simply the the kind of um we call it the hair and feeding without of time um do we do we have time for one more pick one I e e e e e e e e e bingo all right back over here okay thank you everybody should be okay um I'm going to be as brief as possible M F's got a one minute Mark for me in t- minor 7even minutes so let's see if I don't get the hook to take me off stage um I'm just going to give a very very high level intro because there's a lot of good ease coming from the team showing really technical results and uh various other sort of like interesting data sets that have been produced in the course of last Oro project area um and I'm going to title it basically Bridging the Gap to always on contextual AI which is really what the heart of Project Ara is all about and to start with I'll take us right to the top uh which is to remind us of the Paradigm that we're all in which is the first wave of human oriented or human Centric Computing and what we mean by that is this um actually not so long ago Alan choring and a lot of pioneers in computer science uh had no idea what a two-dimensional graphical user interface was they didn't even have a mouse let alone network computing and printers and the internet and all that they didn't have any mobile phones there was no mobile Computing in fact computation was something that humans did and the dream was to essentially mimic um if not one day go beyond what humans can do quite naturally um now between then and just a a few short years later Doug B came along and showed what was at that time called the mother of all demos and so from the point in sort of like the 1940s late 1940s to 1968 the explosion of the first Great Wave of human Orient Computing had begun that meant that in the first demo for the first time there was a windowing system with a graphic user interface that a user would use to control a computer that would allow them to essentially amplify their intelligence and it included not just mouse and a keyboard and a screen but also network computing and so on and we forget about this because you know it seems so long ago but actually if you look at that date in this date and think that everything in between everything that basically leads up from the very first kind of personal computer which was called the xerx alto that put up that demo into sort of a real machine you could buy all the way through to the smartphone that's in your pocket actually ear in 2005 that's it that's the history of human oriented computing and yes it introduced the Paradigm we're used to but surely there's a lot more to be done and uh touring of course you know before there were these graphic user interfaces was thinking about computation and the interaction Paradigm very differently they were also thinking simultaneously about artificial intelligence and Robotics and also not just machine intelligence but animal intelligence and they were thinking about all these things together not as separate areas of study and one particular point I think that stands out is Tan's quote from a conference he was giving a seminar where he says what we really want is a machine that can learn from experience you know we don't want have to sit down and program everything into it sort of thing and if you think about between then and now with your phone in your pocket especially if any of you got chat GPT in an app you can certainly say that AI is here and it has learned on all of the human experience that has gone into essentially web-based data sets internet scale AI is here and and it's here in the form that you can chat with far surpassing what originally would have been sort of the choring test chatting with a human and wondering is it a human now of course you can fool it especially if you're an expert but you know plenty of studies show that chat gp4 and things in its class are well past the threshold of being out to trick most humans who are not experts in the field for many minutes if not indefinitely some people really think of this thing as you know conscious aware of its surroundings of course the thing is is that something's missing it can't possibly be aware of its surroundings unless you tell it about your surroundings and even though we're at the cusp of multimodal AI today where you might show it a bit of a video clip it has no concept of your lived experience unless you tell it about it so you know The Human Experience goes Way Beyond text and even Beyond you constantly telling it about your experience what we really need to do is give AI your real life context now that's starting to happen with a new wave of AI around essentially wearables plus AI this is where something like meta smart glasses uh start to be able to provide you know in situe available wherever you are access to things Beyond just what you're talking about in text to in this case video clips and do sort of you know visual QA on it and that's good stuff so this is the beginning of a paradigm of wearables and AI towards what we'll call contextual AI okay to get there to go beyond where the current products are we need a lot of egocentric Vision research take place and that's why we're sitting here today in this particular Workshop because this is the beginning of this Paradigm and uh it's actually fun to have a look at the history of this of course people might know about the Hol lens we like don't like to think about holl lens as being historical it's actually quite a modern device ARA came after the first holens but way before both of these there was Mike land at Sussex and with his Partners doing work on egocentric Vision in particular through the lens of cognitive uh you know science and understanding how humans see the world and what you can see here is one of his own made devices that allowed him to sort of be able to get in the wild eye capture which was very novel uh back then and then one of the very first studies with a backpack and an eye track and an egocentric outward facing camera to be able to get and start to understand where do humans look when doing tasks and how do their eyes move and potentially why they were trying to uncover the very first uh sort of like science of in the wild egocentric research now ARA is a step very much from that point in the direction of but it must be at scale and it must be available to many many more people and that's essentially what we've been doing in Project area so if you do go along to project area.com you'll find yourself lots of access to data sets and be able to click on buttons to be able to find if you can get one of these devices that are free for Academia and Industry partners and essentially start to capture for the first time at scale in a device that doesn't weigh you down that can last multiple hours egocentric computer vision data sets that include not just the outward face and camera and eye tracking but also spatial microphone arrays and the ability to process that into a 3D SpaceTime coordinate system that allows us to essentially anchor all the information that's coming from the P the wearer in a consistent frame of reference which I think we'll see later on has a lot of advantages so why are we doing all of this well to get past that first Great Wave of human or Computing the 2D screen go Way Beyond it to what we call contextual Ai and essentially that is the combination of context both the physical and the digital with that AI system in fact we'll say that we're not entirely sure that chat gb4 isn't already sufficient especially in an agentic framework what we're absolutely sure of is that if you don't get the context from your wearable devices to it then certainly it's not going to be that useful to you and that brings us to the elephant in the room which is the AI today is defined by essentially solving for the problem of data and compute data both for training time sure you've got of data it could be much bigger than your individual data set could be internet scale and we need to get there from an Ecentric data set perspective but much bigger scales than we currently are and at the same time that data is going to look very different from the data that's available to you at inference time because it has to come from an actual wearable device so whilst you may have trained on massive scale simulated data sets at some point you must cross the chasm into the real world wearable where you have things like motion blur you have things like battery life that have to be traded off against the resolutions and the rates that you can run and the same is true with compute I might have tens of thousands of A1 100s that can go to work on those massive data sets but at some point the model inference is on devices that have 10 times less battery than your mobile phoners today and will actually have considerable less compute than your mobile phone will tomorrow that just isn't the area for thermal dissipation in weable the size of glasses compared to a mobile phone with that MF saying I got to stop so I shall welcome to the workshop for ARA in the egocentric Vision workshop and I'll hand over to Ed thank you very much Richard just while I get set up here with the the laptop have a bit of a show of hands of of who is working either with project devices or project set at the moment from the room a unmuted as of now here we go hi there online uh okay so as Richard introduced project ARA's goal is really to accelerate machine perception and AI research for future AR glasses or really any type of AI wearable that needs to be able to understand your context from The Real World but rather than sort of talk through what the project AR program is little bit about now I thought it'd be good to introduce one of our research Partners who can talk about it from their perspective so let's take a look I'm PhD student in computer science at H I work on computer we are on the dri for exampes take but you didn't notice that there was a there model paying attention to the object front it's very important for the natur use case of prediction now with the help of a we can video understanding model that can help using accidents M of is the you can map the scene where the t is looking at whether they paying attention to the cars there in front of them on looking at the mirrors while taking time also it gives you the point CLS can detect there is another that is passing by which not be visible in the and also gives you audio so information so that's all the sensors can work together very easy we can just the and upad through the Serv information out of the box and that has significantly in the speed of our every year millions of people die out of accident if we can add for this we can save okay so that's just one example of how one of our researchers that we're partnered with is using our Project Ara to accelerate their research but really there are two parts to the Project Ara program one is the kit that we provide to external researchers like yourselves to be able to conduct independent research and we have a number of tools that I'll talk through what that involves and the second are these resources these open science tools models data sets to be able to use and benefit from Project area regardless of whether you have access to the to the glasses or not and just to take you through some of the updates that we've had over the last six months I want to just talk a little bit about both of these sections in so first of all I mentioned about the project area research kit had a number of different um tools to be able to use the glasses we allow people to be able to record sequences using the glasses to be able to process those sequences using what we refer to as machine perception services like calculating a trajectory or understanding where i gaze is this is a service a cloudbased service that we provide to be able to uplevel people's research challenges and work on different types of goals we have uh tools to be able to use these glasses for prototyping so you can stream data and then process that data in real time and then a number of different tools to be able to access and consume that data accordingly so a little bit of a glimpse over some of the updates for these internal tools that we've improved over the last uh the last half uh one of which is this new tool that they use for actually uh working with data from the device this is called RS Studio this allows you to log into your your device to be able to access the recordings uh from your glasses to be able to select those recordings pull them onto your computer iterate and kick Kickstart machine perception services so you can calculate things like trajectory um really really really very simple um for the purpose of time I'm going Tove forward a little bit here we have a new machine perception Service uh for wrist and palm positioning uh within sequences so you right click a sequence you kick off the the wrist and palm machine perception service and then you can then get that output um delivered to your computer and we have a new really exciting tool called time sync or tick sync this is essentially allows you to take multiple different glasses that are capturing simultaneously in the same space and you can actually align those very accurately um to within a couple of micros so just to talk a little bit about how that works is essentially we have one set of glasses that acts as a hotspot or a server that then the other gasses can can speak to and it really is as simple as as uh clicking from your computer to initiate a recording when you have multiple devices set up and we really do all that hard work uh for you so that's a little bit of a a quick start overview over the new features that we've had with the research kit now moving on to the open science resources um a couple of days ago we launched the uh data set Explorer this is essentially a One-Stop shop uh enabling you to explore Project Ara data sets we've really been influenced heavily by what the team have been doing with the ego 4D program who have a similar Explorer we're really building that out allowing more people to be able to understand the contents of data sets visualize them prior to downloading and then downloading them all via the the browser we also have improved our tools be allow you to take the uh essentially the default um format that we produced with project area called vrs which in fact is open sourced and then convert that to any format that you want for any of your Downstream tasks and uh finally we have things like ego blow as well which really produces uh the best-in-class tool to be able to offis skate data that you've captured using egocentric data um so that you can adhere to the various different uh restrictions and requirements around the world for capturing data responsibly and finally I do just want to give this little teaser about uh the support we have for nurse Studio that now allows you to take data directly from Project Ara and Kickstart Nerfs using that and this really is just the beginning of our work towards enabling this kind of reconstructions using R data so final slide from me I will just say you know we really want to work with Partners here in the audience today we want you to enable to you to capture data using project di the data is yours we're interested in learning how people work with these devices what people can learn and we provide a number of sets of community guidelines to help capture that data responsibly now with that I'm now going to introduce Chris she uh to talk through some of the uh um data sets and challenges over the last last year over to you Chris she let me help get up with there then sh first yeah yeah all right difficulties oh can we see yeah and here okay great uh hi everyone sorry for our technical difficulties uh my name is Chris and I'll be talking a little bit about uh one of the Reconstruction uh one of the challenges we had from 2023 to 2024 called AR synthetic environments reconstruction challenge so just to motivate this challenge uh we'll take a quick little look at some AR and VR applications as shown here so in order to enable such applications including gaming virtual tele presence augmented spaces Etc a fundamental piece of technology that's required is the ability to understand the geometry of indoor spaces so to that end we introduced the ARA synthetic environments data set now what is this data set this is a large synthetic data set of 100,000 unique procedurally generated indoor spaces to our knowledge this is currently uh the largest data set of this kind uh so it sets a new precedent in terms of scale for indoor synthetic data sets uh furthermore we've provided a bunch of annotations to enable research for egocentric machine perception tasks and uh if you uh check out the QR code more details can be found on the data set website so what is in this data set we'll just take a very bird's eye view here um quite literally uh as mentioned uh the data set itself contains 100,000 unique multi-room scenes that have been procedurally generated and so on the top uh right here you can see a few 10 10 of these scenes from a bird side point of view each of these scenes have been populated with 3D CAD models coming from the Amazon Berkeley data set of which there's about 8,000 unique meshes now for each of these uh unique scenes we simulated an agent walking around these scenes uh and then simulated um sensor data from an ARA uh rig so what you're seeing in that middle there with that shf is eight scenes eight of these trajectories uh you're seeing the rendered simulated RGB trajectory of uh AB scen so you can kind of see uh the indoor structure some of the objects Etc we also provide semi-dense Point clouds which is a product of our ARA machine perception Services um not shown here but I'll show in the next slide or next couple of slides and importantly we also uh provide ground truth annotations for scene reconstruction uh in what's called or what we're calling a structured scene language uh and that as you can see at the very bottom uh shows the architectural ground Troth layout for these scenes so what is um this structured scene language that I just mentioned um so this is kind of an interesting point here which is that we're providing these uh ground truth annotations in a language that kind of looks like CAD uh and this format contains all the geometric parameters that are necessary to fully recover the ground truth geometry uh but it's very compact it's only in the order of bytes and it's very very simple so this example here represents an entire scene of approximately eight walls a few doors and a couple of Windows uh most previous previous methods typically directly estimate these geometric parameters and this format suggests that it's possible to use methods that directly produce text based representations um so by releasing this ground format we're kind of encouraging researchers uh to also investigate uh prediction of text based representations for geometries and to pair with the release of this data set we also released a challenge the ASC reconstruction challenge um and this is characterized by the following statement which is given in AR trajectory generate a structured language description of the main architectural layout elements of the entire scene consisting of walls doors and windows so your input can be for example an rgp trajectory that we've released or on the bottom here bottom left a um a semi- point clouds from AR machine perception services in this case it's overlaid over the RGB images and the goal is to produce a text text based representation of the scene as shown on the right here and just note that the geometry and the text themselves are equivalent we hosted this uh Challenge on the ev. website and the winner of this challenge will present their solution in just 10 minutes or so cool um so I'll just briefly digress a bit to discuss our own research into this problem also using this data set so we investigated a novel method called scen scripts and this scene script is a relatively simple architecture it's just a 3D CNN and a standard trans former and given that AR trajectory um this time it's not a gif but you imagine you're walking through this scene uh we use Aras machine perception services to obtain a semi-dense point plug as we saw in the uh past slide and we run it through the scene script Network and this network autor regressively predicts tokens that represent the structured scene language that you see here in the third column and we found some encouraging results not just on our own synthetic validation set but also on Real World scenes including offices and home environments so on the bottom here we're seeing some uh Sim to real generalization uh of the predicted structure of architectural layout and furthermore we're able to easily extend this representation of the structured seeing language to include 3D bounding boxes and also course 3D object uh reconstructions as shown on the very very bottom right here you can see some couches and some chairs and tables reconstructed coely and to find out more please check out uh the website uh using the QR code and just to leave you with uh something kind of fun here is an example of uh that Sim to real generalization I was mentioning this model was trained purely in simulation and this is running in real time on a uh real world apartment um and so you can see the architectural layout some Co object reconstructions bounding boxes of arbitrary objects stores Windows Etc and so and this is viewed through uh an app we built on the quest 3 headset cool uh so this is SC script uh and the as reconstruction Challenge and next I'll pass it to shiin who will discuss the ARA digital twin challenge just while we're waiting um is there any questions for Chris on either the data set or the the research just while we're waiting for for the zoom to to come on online we have some questions at the end and Sh as well we're really excited we'll be able to come up and present his methods for winning uh both the scene script challenge that you're about to or pardon me the Reconstruction challenge that you just heard from as well as the object challenge all right than chis yeah uh everyone my name is shin and uh I'm going to talk about a digital training object detection challenges today so um first of all uh let me give you a recap of what we have released in the last year the ad data set so ad ad data set is the I mean probably the most most comprehensive and high high quality cont of the real world this data set we have ever released so the motivation of this data set is like we just want to give you everything in 3D the conscious of everything in 3D that includes the object includes the human include the scene as well so we want to see how many AR and AI applications that we can help with and also inspire you to come up with new applications as well right so let me show you uh an example from of the hundreds of sequences that we released uh in the last year right so this sequence basically um in the left most column that shows the RO sensor data of Ares captured in the real space and the rest of the cols are the BR associated with this Ro sequence um the third row you basically see the 3D oriented b bs that means not only the object shap but also the most of object shap will all to you and uh next is the box of every object and also the pixel accurate 2D segmentation of everything in addition to that you also see this hand the human hand also has a segmentation that is because we also track the human as well in scene uh we also provide you with the D app because we reconstructed the server and uh in addition to all of them we also give you a syac data which is one to one match the Sens data that is in the left most column SE all right um then how did we make it um we basically reconstruct every object in real space um every object so um what do I mean by it is uh we basically push the geometry accuracy to the submillimeter accur accuracy and we also reconstruct the complete set of material of every object and we also push the uh similarity between the real object and the synthetic rendering to the most um here is an example that you see one of the photo is a real capture photo and the other one is simulated one in addition to it and uh we trck everything as well so that includes not only the static object like the Furnitures that not moving in the SC but also the moving object that is interacted with the human and we also the human p as well um also associated with all of the machine perception capability given by the Ares like ey tracking hand tracking um then what we did is we basically model the entire scene as well so the geometry of the walls and the ceilings and we model every Life Source in a space to match the real board and then we put them together that gives you the this beautiful rendering of the apartment this is a rendering not a real capture uh sequence that you can see the photo realism is basically at every corner of this apartment um let me also give idea about what is the look like for this data set so we cover two spaces uh one apartment and one face and we release in toal 300 sequences for you to use um in total we have like 400 objects and we have a couple as well so it's kind of 700 users that has used and this is still going up which is very exciting and thanks for using dat set as well okay then next let me talk about what's new in the ed2 and right now as well so the first thing is like as just covered his is like we have already unboarded our data set in the data set explorer that means you don't need to go over this tutorial to download this data set to get idea now you basically just go to this website and you can visualize the sequencies just by one click and uh so don't hesitate and check it out and if you like it download it uh the second thing which is more more exciting is we are releasing all of the 3D models 400 of them at the same quality that we have seen in previous slides to you for free um yeah and me show example so this is one of the small objects that we show in the apartment and uh we release all of them I think previously we have some license issue that we are solving right now we are cing it we're almost there so just looking forward to it all right um let me talk about a bit more about challenges so we host a challenges which is the one one of the first wave our challenges in the last year so our challeng is about three object detction challenges just one of the Ed cases of our so in order to do this challenge we basically apply the technolog into a new space which is not apartment not office but a shopping mini mini shoping more um that means we have the same quality no matter for tracking or reconstruction for dis new SP um this is through the uh the challenges that because is a shots 3D object tracking problem that is we select additional 20 objects um those are example of these objects they have some textualized object as the text object as well syc objects all kind of difficulties and uh we 80 sequences in this space and uh um participant had these sequences with 5 Seconds of the brunch that of each sequences and the job is to um is to predict the 3 po of this AR for the remaining of the sequences so here I show you one example the 3D money box appeared at the first 5 Second and that disappear then par is required to predict for remaining of them all right um so um how we evaluate it basic because we Rec we reconstruct everything so that we use the surface distance instead of instead of conventional like I 3 matrics so that it's more accurate and more fit into the AR applications and then we calculate the average Precision over all of the sequences and result 12% uh which is very exciting but also indicates the challenges we have here right the new sensor modality from the evil sship and also the nature of the difficulty of the 3D object um poess Nation um all right so that's all from my side but I want to welcome our winner um shes uh to uh on the stage right so he is a winner for our both challenges and he will explain to you how he conquer these two challenges thank you here e e e uh now my solution is divided into two parts one is wall detection and the second stage is the windows and the door detection so this is just an overview of how the wall detection Works uh just describing this in more details now so uh first of all I taking the B Cloud then I take the E trajectory and I convert that into a top down 2D heat map so I'm not sure if this is like clearly on the on the right or not uh yeah yeah okay yeah I'm not sure if it's clearly visible here but like what you can see is like this like a top down Fe map and where you see like the white lines are mostly the places which have a higher density and the black ones are the where the places where we don't have a huge density of the point Cloud points so what you can see from here is basically the higher density points B would most likely be walls because just how the walls are structured we have like a much more higher density more points in those directions and that's what our object detector basically captures so once I have this two top down heat map I basically train a simple object detction model you model and what I'm able to get is the walls as objects now this gives us the walls in like top down 2D but we want the 3D structure of the walls the the H walls so for that again I use technique I take the point cloud and instead of compressing this along this Z axis as we doing previously so previously it was like a top down view now what I do is I take the point cloud and I compress it along the X and the y axis so what that gives us is basically this histogram and this histogram represents the number of points for every height level at like different high levels so uh you assume that there will be two peaks there will be one for the ground because you have like more density of points along the ground as well as you have another like high peak at the seiling level as well so the difference between these Peaks uh the gives us the height for the walls so this concludes the height and the like a 3D structure for the walls once we have that then I go for the doors and the windows so this is our two Street architecture here uh first of all I take in all the camera images r images and I train a segmentation model on top of this so this is basically to generate two these segmentations of walls doors windows and a background class once you have this uh segmentation done uh I essentially do something like a recasting where I taken the camera parameters the intrinsics and extrinsics and use that to essentially recast the segmented points or segmented image onto the balls so what you see here is basically like this is like a 3D Point Cloud which is extracted out of the segmented images so all the points which belong to like a door or window they will be projected onto the walls and you will essentially get like a 3D Point Cloud which can either contain points from the door or the calls uh sorry the windows uh once we have this what you have is basically like a 3D Point cloud of all the windows the doors in the seed uh but now what we can do is like we just take all of them and just cluster all of them together to get separate windows and lo points window and object because at the end we want like a 3D bounding box from the point Cloud so that's what I did initially but uh that did not really work out uh just because there was issues like the segmentation is not really perfect there's like noise segmentation sometimes and also there's issues like like there are mirrors in the scene and sometimes uh the reflection of doors or Windows can be seen in those mirrors and that's how you basically like just have like false positives like false positive Windows false positive doors so this not really working out so that's kind I think the context I think which we already discussed to as well the context is very important and the segmentation model was like not even taking the context clear was doing a segmentation on separately on each of the images so how can we solve this so for this basically I generated another data set based on the segmentation model which I have so once you have segmentation done and the casting done you have basically these to the window 3D Point Cloud now I basically I take this 3D Point cloud and I take each of the balls separately to generate a 2D image for each of the balls once you have the 2D images for each of the walls I train an object detector on top of it to get the final refined bounding boxes so how this works is basically taking in for every uh for every image or every wall you take in all the points which belong to that wall TR object detector on top of that and since we already have the ground bounding boxes for the windows and doors we can just simple object detector again to get the final defin boxes and these will not suffer on the problem of noise segmentation and also like it not have the problems of mirrors because like it it's able to figure out like what should be the shape of each of the uh windows or the doors so this basically covers the solution for the first challenge uh I'm skipping over a lot of implementation details in terms of time but if you have any questions you can take it later uh now coming to the digit doing competition so the problem statement was basically to predict the bounding boxes for an object interest and like this is like for initial 5 Seconds we have like the bounding Bo three bounding boxes for the same object we don't know what exactly that object but but we can have extract that information from the Bing boxes and we have the camera of the the video as well so the solution is basically again two architecture uh there's a separate thing running for the first 5 Seconds when we have the actual Bing boxes and then something runs for the next for theing team uh coming for the uh the first part so basically what I do is I take all the RGB images initially for the first 5 Seconds uh and I take all the bounding box anotations and and these are 2D bounding box anotations and I basically crop out all the objects of interest in there so once you have the images for the object of Interest I pass it I pass all of them through the image captioning model which generates separate captions for the same object and these can be sometime inconsistent because if you see this one like it's very hard hard to tell whether it's like a mobile phone or it's like a video game it can be multiple things but what we're expecting is that we have like some more consistent classes and like we can basically get some consistency just using like large language model so basically what I do is I take all the captions generated uh basically from the L langage model to extract a refined caption and once you have this refined caption essentially use it for the Post generation so I take in the testing images I use the Define caption and and I use it for a prom based segmentation model so basically this an extension of segment model with an language extension which can take the language prom and I get the 2D bounding boxes for the uh object once you have the 2D bounding boxes I Bic take the dep from the anot 3 position so the Assumption which I'm making here is basically the overall distance of the object from the camera does not change significantly but in case it does uh we can what what we can do is simply just pay a model and use and just project that along the direction of the object to get the boxes and yeah this again it was a very short description uh what we can like improve upon this is basically by daging the entire train data set so I was not using the entire TR data set just because of computational resources but if we can have like if we can have that we can probably improve the mod performance uh also the W detection in the first challenge it can uh po be improved by also leveraging the RGB data set and the dep prediction model it will potentially help both the challenges as well uh these are some of some challenges which I personally face while working on the competition so again the L of computational resources so if we have that then we can potentially have improved participation as well and also I think we do not have access to the evaluation script for the first challenge so if we have that we can potentially do like local validation so yeah that's it from my side and yeah I'll have the code available for this on my get it's not available right now but I'll make it available and again thank you for having me yeah thank you everyone Richard give me a chance to to present you with with a gift in addition to the to the first prizes there it's really a tremendous first step towards this and really really grateful for the time well good we're now going to just listen to a couple more presentations about new data set that we could expect uh which hopefully will be of notes to everybody in the ground here starting off with a new data set called pot 3D e good afternoon everyone I'm paj from meta reality labs and today I want to talk about H PD uh brand new Benchmark for hand object tracking so it's very exciting to see the large scale video recordings that have been created using all the different variable headsets can Ure diversity of objects in these recordings we are faced with a scaling problem excuse me just want try and figure out while screen sharing is do it from my screen okay sorry about that so I was talking about the sheer diversity of objects in these recordings and it poses scaling problem for us like how do we go about indexing and passing all the objects that are visible in these videos and our eventual goal over here is to develop an always on personalized contextual AI agent so one solution is to only focus on objects with which we interact with our hands for example if you're looking at a view of a cluttered desk we really only care about the objects which our hands are touching such as the laptop the mouse or the phone if you're walking down a grocery aisle we only care about the objects which we pick up with our hands maybe put it in our cart or put it back on the Shelf we really don't need to pass the rest of the objects in the scene hands are like a spotlight on the objects we care about so development of an always on personalized contextual AI agent likely requires a robust understanding of hand object interactions unfortunately the majority of the existing data sets Focus either on hands or on objects independently we strive to address this dir of data sets by providing annotations for 3d hand an object poses involing involving daily use items captured bya multiv viw egocentric cameras we introduced the hot 3D data set which contains 1 million plus multiv view frames of highly accurate hand and object annotations recordings from the AR device will contain RGB and two monochrome entric views of the scene the MPS pipeline also provides us with a slam trajectory of the headset a semi-dense point Cloud reconstruction of the scene and also the ey gaze trajectory of the participant with the best of our knowledge this is the first time a data set is bringing hands objects and IG together into a single Consolidated data set so to give you an idea of scale of the data set as I already mentioned we are providing 1 million plus annotated multiview frame sets it contains 3.7 million recorded images across all the camera screens spanning 830 minutes of recording the data set contains 19 participants interacting with 33 daily use objects with a cumulative object trajectory exceeding 30 18 km so how did we go about collecting this data set we used a motion capture system called optit trap where we place these 3 mm IR markers on the back of our hands and on the objects we built a custom rip you can see it on the left to mount op track I cameras to track these markers within a spatial area of 2 and 1/2 by 2 and 1/2 M and within this rake we recreated three scenes from daily living for example a kitchen scene with a kitchen island and a shelf with produce items placed on it a workroom scene with a computer table and chair and also whiteboard to write on and a e e e e e e e e e e e e e e e e e e e e e e e e e e and then I I need to unmute the video too so they see me yeah if you want yeah then here this is the usual problem this is what it's sharing it's sharing then I do but what is recording is recording this VI so let me share let me actually yeah you should probably stop sharing and then select display settings but then it Shar that's a problem um okay so let me yeah so we can share start sharing and then well you should you should stop sharing well I don't think I have anything out here yeah if you start the presentation now then then you should be able to select the screen you want to share here and then I do and then and that's what muted so the yeah but we still we still can wait coulees you don't see the pointer I see the pointer but then it's TR to use the pointer function there Point yeah yeah that's better because do can also see that I just say hello yeah I'll be the chair of your session oh perfect so good control me the time I have know check how much time so just five minutes before after okay yes I I can show so it's it's 30 minutes including questions okay so I'll try so when do you want me to let you know maybe seven minutes before so I have two minutes to wrap up five minutes because I yeah I know I hopefully okay I'll show you the five minutes but I'll show you seven minutes Sly there is an understanding okay so this point already by just got to basically be actually you for this so yeah it's still it's: right now but we'll wa a few minutes more for the everyone uh to make sure to be back to the room so yeah please wait for three or four minutes and also please be seated uh for people who standing there actually the object the AR both I'll begin five minutes or something yeah right so yeah let's resume our last sessions so I'm takum Magi from aist and it's a great pleasure to introduce uh Professor Fernando La t for the last K sessions and he's a associate research professor at CMU robotics Institute and he directing the human sensing laboratory and his research interest are in the field computer machine learnings and in that particular applications human houseal augment reality virtual reality and M that focuses on the data not the models and yeah today he in prod talk about eent vision for human sensing so yeah yeah thank you very much for the kind introduction and thank you to the and thank you to the organizers for uh letting me speak in this very exciting workshop and and also in front of this very distinguished audience today the agenda for today I briefly talk about some activities that we do in the human sensing laboratory uh to try to understand human behavior from sensors and then we are going to Deep dive a little bit in how we can use entric vision for face and body avatars and other activities that we are going in the area of entric vition for human let me let me start a little bit about talking the human sens laboratory I started this 16 years ago to try to model subtle human behavior and this was in part inspired by the work that John godman did John godman is a psychologist from the University of Washington and since the 80s he has invited more than 3,000 couples to his lab his lab is called the L lab he invites the couples to the labs and ask them to start an ongoing topic of disagreement between the couple one likes the dog the other doesn't like the dog and then after 45 minutes seeing the interaction he's able to predict with between 85 or 95% if this couple is going to get divorced in 15 years from now you might wonder how does young Gman does he looks through the video and he encodes every conceivable emotion that the couple shows in what is called a specific AA this specific recording have five positive emotion Joy validation humor and 15 negative emotions like content criticis Stone way he he tries to encode all these all these emotions and then LS map into this label if the uh if the couple is going to get divorced in 15 years from now but in order to that you need someone with expertise of your Gman and you need the manual manual labeling of uh of doing all these encoding of femo so the my my big dream is always to develop machine learning algorithms to be able to predict all this subtle human behavior from sensors and by the way if you want your marriage to survive two tips from junga first show five times more positive emotion than negative one second never shown contempts to your partner along these lines we did work like depression assessment so just we if I'm depress I'm going to an interview with the clinician the clinician ranks made the Press state after asking several questions how are you feeling today do you have suicidal thoughts and they come out with what is called the hamton but this depends on what you tell the clinician and the expertise of the clinician not on how you behave so our goal here was to provide some quantitative measures of behavior from a from depression from audiovisual information we record the you know the interviews between clinician patients we record the face we record the body and we were able to pray with around 70% of accuracy the patient was depressional only from audiovisual information again this is better than chance there is a sigma and recently another thing that we are very interested in this human sensing uh laboratory is try to monitor the well-being of individuals my parents are in Spain and I live away in the US so I would like to have systems that can monitor the wellbe of them but I've been working in computer vision systems for more than 20 years I don't want to have cameras so what one of the things that we ask ourselves is can we use the Wi-Fi signal Wi-Fi signal that in this room to try to detect humans in the scene and that's what we did what you see here is how we are able to detect Den po just from the reflections of the Wi-Fi sign now this is a very conted scenario we have here three emitters and three receivers this is from your router the router that you have a home and then we uh we have people walking around and from the reflections of this wi-fi signal for the amplitude and phase we were able to H you know we were able to detect these des and we posst this as a supervis anyway so in these two works the key point was to table to recover really subtle human behavior like in the case of depression or marriage outcomes or in the case of wey where the signal is almost noise and that's what we have to um to to recover and that's the algorith that we do now he this an ego egocentric Vision workship so right now the the challenge is you know how are we going to use ego Vision information for what is likely to be the fourth revolution in human computer in my last time I was lucky enough of see at least three Revolutions in the area of human computer in the 80s the computers in the 90s the cloud computing and the interaction in the computers in the go web 2007 we have the introduction of iPhones allow interaction from everyone to everywhere and now we are in the verge of the introduction of augmented reality and virtual reality with a projects like ARA or projects like West from meta Vision Prof no so in many of these AR applications or V applications we are going to require subtle egocentric Vision we need to recover really satle signas from humans and I'm going to show you some examples the first one is entric vision for face avatars this work done at meta while I was at meta and here the right now I'm I'm transmitting this uh this talk via Zoom it's not the same I I cannot interact the same as the audience I have here that the audience I have asum and by the way say hi to everybody in Zoom okay but here I can see your face I can see if you laugh at one of my joke I see if you have attention to some of my slides is not possible so the future of Communications in the metaverse might be something like that where what we are going to have is we are going to have an immersive headset that has three cameras two pointing towards the eyes one pointing towards the mouth and from these three cameras what we have to do is to extract the information and map it to an avatar that looks like you and behaves like you and again the trick is in this subtle human behavior that we have to stct and this video probably you SE before but uh this is how it looks like here in the top Corner we have two people talking and they see the other person face in the v it's much more immersive and that's why in order to that in the Pittsburgh reality Labs uh the first thing that we did is we captured a we captur your face and your body with capture systems that have many cameras more than 100 cameras uh this is how a data capture looks like we have kind of 50 mm sens lenses and we can capture four level details that's uh you know that's a lot of cameras very high resolution and from that what we did is first we build 3D mod AR and BR is all about 3D perception so what we do is we have the the the the multi multi settings we do photomic stereo the Tak on key points then we try to fit a common mesh this mesh is going to be common across all the faces we do some refinement and at the end of the day what we have is we have a mesh 3D mesh and we have a text and from that we do what the machine learning person does nowadays is we we learn deep learning models with an Al encoder in this case a varation out enod we have is we have the mesh we have the texture and what we do is we find this variational coder that has two parts one the state of this ver encoder encode something about the expression and the other latent space is encode something about the Viewpoint now we have this is a person specific model basically I collect data of you and I have a gen a person specific model of you this is a decoder that giv one latent space of uh expression of viewpoint I can synthesize but the big question is okay so I have a generative model that a person specific generative model that can reconstruct me but the question is how I'm going to solve the correspondence in Vision you know that always the main problem is to solve correspondence in this case of settings for entric vision also the problem is the correspondence how do I find the correspondence between this faal expression and my because if I know this mapping then it's a regression problem like many other things in computer B but I need to solve the correspondence and because I have the headset I don't know so we do this in a self-supervised man and the idea is we are going to have many videos of someone that we have captured with this VR headset and I know the decoder of this person this is frozen this has been learned offline and then what I want to do is the following I want learn an encoder such that when I decod this uh the expression and the pose of this Avatar and I render this in the Viewpoint of the camera I can reconstruct these uh views that's what I wanted here there is one other issue that we have especially with egocentric vision is uh we have uh we have here infrared cameras and here my model has been collected with regular RGB cameras so there is is a domain transfer that we have to learn so basically given a set of images that I collected and a person a specific model I want to learn an encoder and a domain transfer such that these two things align and this is what shows in this video that we show here that we are able to um you know to align this uh this 3D model that I have person specific model with the video stream that I have from my uh V headset um now this is the the problem here is this is only for one subject is person specific so what we want to do is person independent we want to generalize to many users and even for one user you can see there is a lot of variability people change there is variability in the background there is variability in the appearance so the other thing that we did is and supervis expression alignment across many subjects there so we have really accurate correspondence for one subject and then now what we do is we learn across many different subjects so how we do that so for every subject that we have we have the correspondence with what I explained before between the V images and my 3D avatar and then what I'm going to do is I'm going to learn this from many independent subjects now what I want to do is to have a backbone that is common across all the subjects this backbone what has to do is marginalize everything that is not related to expression this back on it's going to marginalize background it's going to marginalize lighting it's going to marginalize appearance because this is not relevant to the expression that I want to transer so that's what we do and the other thing is here we have the mean of the 3D shape that also helps to do this normalization the other trick that we did is uh we did 3D augmentation to not only to the with neural networks to don't not suffer from over feating is always critical to do augmentation in this case it work much better when we do 3D alation and this video shows something very interesting that is when we learn across several subjects we are able to align subtle again and always the trick is subtle subtle facial expression across users okay you just record different video sets talk whatever you want and then we align all the expression across users and that's interesting and some of the STS are very sub remember that the bar for communication is very very hard we have to have communication systems that transfer every satle expression because in every satle expression we can have a meaningful social sign so that's um that's the that's the key uh and this is an example of an unseen subject and how we transfer you can see in the mouth how we transfer the expression from this personage I can skip this and this is J of course um and yeah you probably have seen something in this before but we can add uh we can add texture to the to this one now let me talk a little bit about bodies we we can apply similar technology to bodies so we have Ecentric video and the problem is the same you know from only one monopol Ecentric VI we want to transfer the body motion to an avat now we have a problem this is the problem we have many problems otherwise we will not be here but you know one of the major problems that that we are going to have and this is common to any egocentric uh egocentric you know algorithm first we have a strong perspective distortions so we have fishy lenses White Field of views that creates a lot of distortion in this particular case we have self occlusions especially in the lower body um there is as we all know ambiguity between 2D to 3D is a nonlinear mapping recover 3D always ambiguous but but and this is where the pro area has make a good contribution also to the to the community we don't have data with these people that's one of the major issues that we have with entric uh entric views and for the42 also what that we don't have enough training data and in this particular case because we don't have enough enough training data what we did is we created a new synthetic data set of humans and in this case we just had 23 male 24 females around 400,000 frames and we try to cover variability around SK skin tone clothing and different type ofes surprisingly these uh synthetic data work pretty well in realistic scenar and this is always the domain Gap that we have between synthetic and real data is always hard to do but uh solve but in the case of humans we have very realistic 3D models from graphic engines and and this make it work well in terms of architecture there was only a couple of ideas that I think are interesting so we find a mapping from the image to a heat map this is the heat map and then we find a mapping from the heat maps to three why do we break this into these two stages well for two reasons the first one is we have a lot of data to predict heat Maps but we don't have much data between 2D and 3D and the second thing is the data of 2D to 3D can be easily easily generated with synthetic data um and the other thing that is interesting but this is pretty common nowadays is that we do some UM multitask learning so rather than predict from the heat Ms the XYZ position we also predict the rotations and try to reconstruct the heat Ms so this backbone you know is common to these three tasks and provides task and provides much more robustness to the to the method and then these are some results in synthetic experiments not surprisingly we do better than uh what was state of there a couple of days years ago uh and then we just do an ablation study than just having these uh three different um three different channels of multitask learning help quite a bit from 122 millim to 50 so that just validate the multitask learning it's it's a good idea especially when you don't have much training dat uh we also test this in real data which is Moab 2 and uh and yeah in this case again it perform around 20% better than state of the air for indoors and around 30 some 32% better in outs okay and this a little bit the video that shows how how the system works uh this is with real data and in this case we map this entric view to the to the 3D stick figures yeah this is by far is not you know it's not good enough for for any are applications and there are much better results nowadays but um but yeah back in time was the state of there now what other applications we are working in the area of entric vision for human interaction one and this a collaboration also with meta with the team of shuga we are very interested in data free online L what what is the problem here we have V headsets and a glasses too that one of the key things are going to be a are going we are going to need to design new ways of interacting with the hands we have amazing hand tracking that now can you know can can track the hands even when we have self occlusions and we have the hands touching to each other the problem is that many times when you ship these models uh to a user the user wants to add new gestures imagine I want to add a new gesture which is we I have a model that has been learn with around 20 uh 20 uh you know 20 gestures and I need to what what here's the Cav I cannot use my training dat for two reasons it's very big and there are privacy issues so what I have to do is to adapt my model somehow to this new gesture without touching my initial training how can I do this online learning without having what is called catastrophic forgetting I don't want to forget the U the the gestures or the the gestures that I have learned so that's another uh you know that's another issue that we're trying to solve and and this the previous paper is under the review and then this one also got under the review and here what we want to do is with AED reality glasses one big opportunity that we are going to have we are going to have cameras looking at people and one of the things that I want to know is you know do I have an engaging conversation with this person I would like to have my cameras my glasses have cameras looking all you and try to come up at the end of the day you know which is the slide of the topic that you were more interested just by analyzing your behavior and in this case what we did is we invite a couple of people that they don't know each other and ask them to talk about topic for instant C they start talking during 50 minutes and at the end they self-report what is the engagement that they have and our question is from all these self-report engagements can we predict this report self-report engagements from Behavior information that I can observe from the classes and in this case we use both we use Behavior but we use also text with large language models and spoiler alert yes we were able we are able to predit engagement just by looking listening at least the engagement that is you know that is uh related to the self report and this is how the system work we everything faes bodies and also a big component is the large large model surprising surpris the Tex we are able to right have a lot of knowledge about um and now if if I want to give a message here also to keep is many of the entric problems that we have for AAL reality and virtual reality the main problem the main bottleneck it's not necessarily the algorithms as much as I like is the data if you think about the when I talk about the project or Wi-Fi depression of face and body 80% of the effort was on collecting the data what's the problem collecting the data you know it's not doesn't bring you much Glory nobody wants to collect data it's not the fun part if I tell you now how many people wants to data collector with data collection with me next year any volunteers no you think there is more fun to do but it's the most critical part it's the most critical so it's not only also about collecting data it's about labeling and when we're talking about labeling Behavior behavior is extremely hard to label things like depression you give the videos of interaction between patient and clients to two different professionals to different FS you know it's it's very hard to be consistent in Behavior or in the example that I put when you have the V headset cover in your face try to infer what is your real 3D faal expression again it's not not a trivial thing no um yeah so and when we do collection I mean in this case we are interested in research but I spend two and a half years in my life in in production settings and in production settings data is much more critical and so you have to make sure that uh my my wor nightmare is that hey we ship a product and then it doesn't work for a sub population I'm going to have real product you know or when I have a an an algorithm and I say how do I know where my algorithm fails you know things like that that are critical and has to do also with the data and how we process dat so I just want to send the message and I know some of the organizers here are doing a great effort towards this direction so yeah uh great um yes just the placent reality for all of us that we want to do machine learning is remember that 80% of our time is really the data 20% is the fan of the algorithm but that's uh that's what it is um and just to to conclude and then we can leave it a littleit for questions H you know egocentric vision is critical for many human sensing applications in arvr I just show some applications in avatars for VR wearable glasses just to recognition of human understanding uh for many of these applications trying to stack what is the subtle signal is is critical and uh three important three important problems on this uh you know egocentric vision is going to be data data so let's be ready for that that's it so I think I finish on time oh sorry and I forgot to say thank you to all my collaborators because I didn't do actually most of this work so so let me thank everybody this uh oh yeah it's here so I want to thank everybody this work some parts was done where I was in reality labs and also some other part where I was at CMU and many people contributed to this work and yeah now we can leave a great talk and we'll have next our question so yeah we give any kind of questions I wouldn't expect less from you yeah so we're star to get used to sort of like visual entc um at least in this group um what about sort of out there next Generation sensors that we don't have yeah I'll give you an example what about chemical sensing from you know from a human you know they might be sweating a little bit there might be chemicals in that swear might might tell quite a lot about the prediction about what happens next have you had some thoughts about what the Next Generation sensing could yes I mean in terms of Next Generation sensing definitely for for instance for a reality glasses my favorite application by far by far and I'm sorry because I'm a video person but it's the audio you know when you have a microphone array and then you can uh in noisy environment you can focus in someone amplify the voice of this person and the noise no so in terms of something that can happen in the next two three years it's it's really definitely the audio uh and uh then in terms of more long-term type of uh sensors I mean we have from H H sensors we have these other sensors you know there are many sensors that we can attach now the question is how do we develop for this technology to really a scale we need to make sure that we have enough po consumption you know H privacy issues that are going to come especially from the video site you know so these are two things that really need to happen you know to make sure that that this technology is going to be push forward so another simple one will be kind of galvanic skin respon you can attach this very easily to the glasses this measure the conductivity of the skin with this you it can be a good predictor for instance of stress you know that is very useful no one one of the things I always thought about how can I use these elemented reality glasses is um uh I I you know is to know yourself one of the things that the best thing that you can do in your life is to know yourself can these AAL reality glasses help you to know yourself for instance yeah if you accumulate all the times that you interact with someone and you see their stret so or the times that you love in an interaction and you know you can collect a lot of data that can help you to know yourself so things like that there about the other I did not think too much but things like alanic esin response can be done you know pretty pretty reasonably nowadays other questions yeah Rel to the first yeah for instance in the first study when we were doing depression detection how do you detect if somebody is depressed when somebody is depressed they tend to have more Gaye aversion they don't look at the eyes that often you know to build trust with someone and it depends cross know but in the US let's say in western civilization to trust to build trust with someone you have to look at the I 80% of the time that's these are the studies no now when you are depressed you don't do that you you look much less now when you are depressed rather than having an honest smile that you smile with the you know with the eyes you you dump the smile you do like this but it's not an Hest SM you Dum the SM so these things are super super subtle and then if you think you want to recover these things from face Behavior the head is moving all over the place so you are recovering a very subtle signal behavioral signal when the head is all over the place so you really need to the couple very well the rigid and nonrigid motion to be able to do these things no so this with respect to FAL Behavior now with in the interesting thing when we did this study of Engagement is that the large language models actually understood a lot of nanes of language to be able to predict that the results that we got are super interesting I don't have a good explanation for that you know um but but you know in the in the scale that we did experiments which is of the order of 30 40 people that was super interesting that the new senses of in a conversation whether you are engaging or not with someone part of the language was be able to be captured by this life Lang so and by the way I love the r when I have the left language model and I can talk to it that's a great feature and I think we can have one more question so don't miss out this great opportun uh then I want to ask about like tradeoff between like sensor quality and the like trading model because especially in production mod production I think it's rare to have ideal sensor perfect sensor and you have to balance between thatle use 2D sensor to estim 3D and yeah how to balance those because if the sensor is quite like too cheap or like kind a quality I think that the total effort will be too high so any tips for my advice is if you don't want to get the Press don't talk to the productive so they are going to give you a budget which is like what can I do with this you know so they're going to tell you two things first to ship a product we need to develop the hardware two years beforeand so two years beforeand you have to tell them what you need and then they are going to give you I don't know a 10% of the CPU budget to run your out and you're going to say I mean what I want to do here no so uh but these are the constraints of real real production scenarios no what I do believe is many of these things we can sense in in low resolution poor sensors if we have a really good training data that has captured in very high quality sensors so if we have a very strong prior that has capture with high quality sensors then we can do inference in low quality sensors but we need this high quality prior data the compatibility between that high quality and Lal will be kind of important it's important but once you capture so once you capture high resolution you can always render things in very poor resolution you know so so the question is to have a good prior in a in really high um you know high quality and then render or or synthesize this in low quality which is the thing that you are going toate yeah thank you for the talk and give some Applause again e e hold the th so maybe people can also see okay yeah thanks everyone uh and welcome to this last session where we'll talk about the ego 4D and ego XO 4D challenges so this year um like we continued to host our ego 4D challenges this was the fourth edition of the ego 4D challenges we had 14 tracks this time um and it included two new tracks um this year we also introduced two new teaser challenges based on our new data set ego XO 4D and these teaser challenges covered body pose and hand pose and for each of these challenges we had uh cash prizes for top three winners so for today's program here is our high level schedule like I'll first go over uh the overall synthesis for our ego 4D challenge uh what the trends look like uh give a bit of overview of different methods uh then Chris and Gabriel will go over ego EXO 4D uh Challenge learnings from that we have two Spotlight presentation one from the ego video team um uh covering ego 4D and some of the ego XO 4D challenge work and then Hammer team which will talk about the hand pose and then finally we'll have a small prize ceremony for all the winners uh from different challenges so this is uh basically like in cvpr 2023 we had 14 tracks uh for these challenges this year we introduced two new challenges goal step and EG schema and removed one of the challenge but all other challenges remain the same since we have so many challenges I'll go over each of them quickly uh to give you sense of the diversity of these challenges and what are different aspects of egocentric video understanding that these challenges cover so first set of challenges are about episodic memory these challenges include understanding what's happened in the past for a user so we have visual query 2D and 3D which we provide a crop of an object which is referred to as visual query and this object needs to be localized both in 2D and 3D uh in in the the given video we also have a track for natural language queries where given a video we pose a query in natural language and we need to respond to that query based on this video in natural language form related challenges called moment queries where again the query is in a natural language form but we have to localize these moments that are asked in the query temporarily in the video so we call these moment queries we also have this General object detection challenge called ego tracks this is designed for egocentric uh object understanding like these tracks are really long uh an object can uper in like really very long videos can appear disappear and reappear so this is a pretty challenging uh task among the two new challenges we have this goal Step Challenge where again we provide a language description um and then you have to localize in this untrimmed video the start an end time uh for for this particular procedural step and finally we also have EG schema as part of ego 4D this is a benchmark which is about uh understanding very long form video and so you're given a pretty long video clip and you're also asked a question with multiple choice answers and you have to provide answers for those so this covers our episodic memory we also have hands and objects which is about understanding how hands and objects are interacting uh we have some audiovisual kind of challenges which is about understanding speakers who is speaking what are they saying how when can we actually transcribe those uh we have social where the objective is to understand when a person is looking at someone or talking to someone and finally we have this set of challenges about forecasting uh which is about given a video and a moment just predict what is the next action that the user will perform um in this Challenge and longterm is same thing but you have to predict a series of CH actions that the user will perform so this is a list of all the challenges that we have um now I'll go over some trends that we observed this year so as I said this is the fourth edition of our Challenge and we have seen the ego 4D challenges growing very strongly over year over year and we started with very small number of teams last year we had a pretty strong participation but we are happy that this year we have you know managed to increase the participation even further this year we observed 37% growth in the number of teams participating and 43% in the number of methods and these are different because a same team can have different methods appearing in different challenges so that's why we separate the teams and the methods but overall we see a pretty strong Improvement or gain uh in uh continued interest in this challenge in terms of per Benchmark uh episodic memory is the most popular but forecasting which has been historically one of the hardest uh challenge to participate uh uh obser we observed significant growth in forecasting which is really great uh this is a pretty hard problem uh but we're seeing strong interest in that and AV and social retain consistent interest now we further break it down uh we see that this goal step challenge which we introduced new this year had a really strong interest like 23 24 teams participated in goal Step Challenge so it's something that is gathering a lot of interest in the community uh regarding participation but as I mentioned LTA which is one of the forecasting uh challenge also garnered a lot of interest um this year and so we saw highest growth in in terms of like the ratio like looking at me short-term anticipation and long-term anticipation and as I mentioned the new challenges gained a lot of interest so one thing that we observed last year was it was in getting very difficult to beat baselines and previous year winners because these challenges have been running for a while and the methods are already at pretty good performance level and so we continue to see this like we saw a bit higher like 71% of the methods outperformed previous year winners but a significant amount of this was also because of new challenges which had like new baselines here so if we if you only look at this number for challenges which have been running for a while uh about 60% of the challenges people were able to like beat previous year winners so we see a lot of participation but it's not always the case that the baselines or the winning performance improves every year just because uh the methods are already starting to get pretty good at it and we had pretty good number of uh reports which were submitted after the challenge conclusion so this has been always a highlight for me like ego 4D has strong interest from Academic Teams and more than 80% of the participation uh even this year has come from Academic Teams which is great like uh this is a pretty huge data set but still it's great to see that Academic Teams are able to work uh and make use of this um data set finally in terms of the progress for each Benchmark this year we observed um like goal step which was a new challenge but again like within the first iteration we saw 85% Improvement in the Baseline performance which is really great and St is another very challenging task and but we saw pretty significant jump um this year and similarly notable gains in moments and nlq so next I'll go over three method highlights um which were sort of like standing out this year first is I want to highlight this method called ego video which uh essentially what it did is it trained like a new backbone starting from intern video to but collected like a large number of uh egocentric uh video text pairs uh to create this new backbone and then they combine these backbones with well established methods for other challenges like individual challenges uh to sort of like win in a very large number of like challenges this year so uh they won in like they have rank one in three and rank two in two of the challenges uh based on this improved uh backbone that they have we also have this method called uh car for uh goal step which uh interestingly what it did is because goal steps um within a video can be similar like because the same step can appear several times these are cyclical videos and so instead of like treating them indep dependently what they did was they grouped them into similar goal steps and first try to predict all these different steps and then enforce some sort of like a soft prior on top of that to localize uh the exact goal step that you're looking so it was an interesting approach trying to really uh like uh understand the problem structure and approach it in that way finally we have the EG schema challenge uh it's a pretty uh popular Benchmark right now and big teams like from Industries are also using this to Benchmark their method but we have uh the hqa team uh which got rank one in the schema Challenge and their approach was to do like clip level captioning so they take like very short clips 4 second clips and generate clip level caption and then create a summary from those captions and then use some sort of Chain of Thought uh approach to answer the McQ that that that we care about so these were the highlights of the methods and next I'll hand it over to Chris to go over the ego x4d um synthesis okay so let's talk about the E x4d teaser challenge synthesis summary right so the Benchmark that we introduced is this ego POS Benchmark um the input is going to be egocentric video and then the output is supposed to be the 3D joint positions of the camera Weare and for both the body and hands for each time step and the convention that we use for the skeleton is Ms Coco so 17 3D body joints and 21 3D joints for the hint um the data set that we collected uh for body pose and for hand pose this is the largest manually annotated ground truth a data set that exists uh we currently have for hands 38 th000 3D joints that are labeled and then for the body 376,000 uh 3D joints that have been labeled manually all right so for the teaser challenge um we had a lot of participation we had 12 unique teams uh eight were for the hand pose Challenge and four were for the body pose Challenge and we had a very active leaderboard and you can see that there are many many uh sub conditions for both hand and body pose estimation okay so let's talk about the ego xl40 hand pose challenge next okay so for hand pose we're going to be given RGB egocentric video frame and then we're going to estimate the 3D Joint locations for the hands that are partially visible in the ego View and like I mentioned earlier the hands are parameterized as 2 joints per H so here's an example of the entric video or the image and then the 3D skeleton so a little bit of motivation for this part of the data set um many of the existing handos data sets are recorded in constrained environments so usually in some kind of lab or capture system so we wanted to to break out of that and really get data from in the wild and then in uh some of the data sets it's also the case that the hand motions are quite simple so they're just gra grasping or holding an object so we wanted to go beyond that right so here's um some samples from our hose data set uh we have multiple views so we have one ego view that's the one on the very left and then we have four to six EXO views depending on the setup of the recording and we recorded data over many diverse tasks and environments so here's an example of bike repair and the covid test cooking and music okay and let's see I mentioned these numbers before already so in total we have over 4 million 3d hand key points that have been annotated I'm not sharing the screen okay okay sorry about that okay so for this challenge the metrics that we use are MP JP and P MP JP so MP JP is measuring the per joint um estimation error of the location and then the PA here stands for proceses Align so uh we're comparing everything minus the the RO rotation and translational component of the hand and the Baseline method that we use is Potter and we train this and evaluate it on the Benchmark as our Baseline method okay on to the winners so for first place we have pcie ego hand pose uh this had a performance of the P MP MP jpe as 8.49 in second place is hen 3D for hand mesh recovery uh this one had a score of 9.3 and then the third place team is the ego video team um and I think is this death knight is the yeah yeah okay is the ego ego video team uh 10.2 so overall we're seeing a 23% reduction error observed by our primary metric okay so some observations so out of the different methods that were submitted Transformer based methods uh were were the strongest methods um we know that uh from other problems as well that en ensembling techniques tend to boost performance uh we saw both parametric and N non-parametric methods uh achieving strong performance and then some of the challenging um hard cases is in the data set were things like occlusion and truncation when they're very extreme um and also the the risk location and hand orientation being uh Ambi uh ambiguous okay next we'll move on to the body pose challenge um hello okay so my my name is Gab I'm one of the organizers for the EG poost challenge this year and I'm going to summarize as just as Crystal and so the task at hand in our case was body post estimation so as he pretty much got the near rough idea is you have this camera post through time and you have this entric view for this case a fisheye video of someone playing basketball and what you're trying to get out of this is the 17 key points of the person within the video or as we call it within this challenge the camera we so um The Challenge itself well not overlapping with how Chris already pretty much described everything in the data set that we had is this is a challenging task because well the cameras where the camera where his body is rather rarely within frame right and um participants had no access to any exocentric views or seen information even though of course ego EXO was annotated through these exocentric views um the video on the left is an example of a method attempting to do this kind of reconstruction and you can see how compared to the ground truth as this person's playing basketball looking away it's pretty hard and it gets like drifted away through time so our primary metric are also MP JP which is pretty much the mean per joint positional error so you get the E cium distance between the predicted point with a particular ID and then you average it out over the entire body so in our case uh we also distinguished physical activities from procedural ones since the skeletons of the generality of them are pretty much particular When comparing how you play football for example and when you're doing a covid test on or playing the violin so even though those are not primary metrics whenever I show you the leaderboard you will also be able to distinguish how different methods did on a particular set of tasks and in our case well you can see mjp is cm and you can see exactly how thises a 16 cm error looks when somebody's playing football so our Baseline for this challenge was only taking the was only taking the camera poses through time you get them through a linear embedding to a higher dimensional State you put a propioception Transformer encoder three layer eight headed one and then you get a linear stabilizer to Output a 51 Dimension Vector at the end which resides into to a 17 by3 uh answer and then you just assign key points in a particular order and turn it that way so um our Podium uh on third place we have our levelwise cross attention V so uh in nutal they got a heads and a three layer Co Transformer in parallel in parallel with another 17 heads 32 layer Transformer which they called fine and they cross a tend a weighted block at the very end from each one's out in our second place which will be one of our spotlights later it's called a conditional human motion diffusion model so given a encoder decoder style Transformer with six layers and with two networks of size 550 they cross attend with conditioning and the rotary positional embeddings so um something notable from this place is that the camera poses for these people were actually conditionals for the diffusion model in which what they are iterating through our PL parameters and you will see more about it later the only input for this model was also the 30 HZ slam post and their output is what the S smpl parameters I said once they went through the diffusion process uh very particular thing to notice here is that these participants didn't even train on our challenges train set and they only transferred from aass right very nice and our Podium winner is called multiscale Model Fusion so they implemented multiple Transformer linear layers robot layers and MLP module even after the stabilizer that I showed you in our Baseline and they played around with the amount of layers within several transformers getting to 12 different models that run in parallel and then they assemble the differences at the very end to get improved accuracy uh here are our challenges results so as you can see SGU has one with an mpgp of 50 1932 the second was from Berkeley which got 17 time 19 stj got 1809 and our Baseline model started at 1851 uh the mpj PVE which is the velocity within different key points and Frames also got smaller as methods got higher in the podium which is a nice touch so sorry our conclusion here Transformer based methods were definitely the most popular type diffusion modes uh are definitely not to be ruled out and the general parameterization of inputs that you're going to learn about it later proved a great transfer learning from a mass learn priors embling fusing and using different kind of models in parallel with multiscale methods definitely boost performance and in our case we got a 177% reduction of the main mpgp metric so yeah I think next we will do the spotlight talks we have two one from the ego video team uh which one across several tracks and the other is actually from the handp uh Team um apologies for the confusion there but we'll have the handpost team also present one of those spot like so can I invite video um uh team member hello thanks for the introduction uh my name is Shan from fan University and so today I'm going to introduce our team ego video Solution on the first uh ego FD and ego fold ego axle folding challenge okay so with first uh we're I'm going to introduce ego video a powerful visual Foundation model for egocentric videos and some of you have may have heard it in the morning when in the application challenge I'm going to introduce it again in a very brief Manner and then I will go through the ego 40 Challenge and ego axle 40 challenges so I'm going to cover uh these tasks so I won't dig into the details of each ch chenge and I'm going to I'm only going to give you a very general idea of this uh the solution so if you can you want to uh the details you can refer to our code and uh report which will be published soon okay so first give you a general idea of our strong Foundation model how it is trained and it is applied um the eagle video model is goes through a two- stage training so the first stage is about video language pre trining in a general domain as you as you know the general uh video domains it covers a lot of videos more far more than entric videos so it can provid very good General feature representations um to be specific the first step we train a video encoder which is which contains one billion parameters from a distillation training strategy from image experts such as uh interv or uh Eva clip something like this okay so we use data sets including kakis 400 600 700 and AC and Etc and we dropped the action labels in this stage because what we're doing is the feature distillation we only do uh can see of it self-supervised training okay so after this uh distillation training we conduct a multimodel contrasted learning stage so we have this pre-trained video encoder and we add another uh tax encoder which is a bird large model and all and also video encoder so we we collect uh uh video text audio pair data sets and we can do the alignment between uh video TX video audio and maybe audio TXS and also we do the the training um simultaneous with with image Text data set such as line okay so after the the the video is trained on a general domain we shift to The entric Domain so we do the video language post training in entric domain uh so in this stage we collect a series of Ecentric video data sets such as ago 4D and uh some of the how to videos which is sourced from YouTube and you know instruction videos all also contain some entric activities but you know most of them are also are still exos exocentric ones and also a little bit videos from uh recent approaches that are generated automatically such as video recap and Laila okay so here two Keynotes are that more frames for example 16 frames are generally better than eight frames or four frames uh and also it cost uh more training strps more training resources and also more entric data leads to better performance generalization okay so we use this video we use this video model to extract uh video features for like Downstream tasks and first I'm going to introduce the inq and ghost St model which is based on the ground inq which was a very powerful model and so they're both the grounding hacks so we do uh so we solve the task in a very similar manner we extract the video features and the text features and we do multimodel multikill the interaction like a tension here and output the the score and you can see from the results that um you know the inq is very challenging because many team participated in this Challenge and so we assemble more models like uh uh grounded and a grounded bqa which was also very good work and also the conclusion is we have much stronger feature and you can you can get really better results so in terms of the uh moment query we choose the Baseline models the action sensitive learning one it is basically trained on uh localization loss action sensitive contrastive loss and a classification loss and also we replaced the basic 3D scene and we our e video feature and you can see the results that the um besides e video features we have also uh implemented some features like uh uh intern video and some other features and we try to merge them together and leads to better results and in terms of the long-term anticipation we try to solve it with a large language model because uh large language models are you know better modeling the long-term uh long-term relationship in the videos like can so from the the ego schema challenge right uh the basic idea is to sorry continue from here so the basic idea is to uh use use the ego video model to infer the current actions and we feed them to the large language model to prompt it to you know generate the next possible actions uh in a few steps okay so you can see that LM can also the fact the anticipation results and as the E video is uh strong enough to predict the actions of the scene uh videos so better predict actions leads to better results okay so for the short-term anticipation it is more close to a a detection task so we adopt like a 2d backbone together with a 3D backbone and a 3D backbone here is our AO video model fuse the features and you know optimize the the the classification and the the the bound and box loss okay so the results can be seen that you video features indeed is better uh in terms of the VQ 2D and 3D model we adopt also a very strong Baseline model VQ lock and do a very slight modifications over VQ lock so we conduct two stage training strategy uh the initial VQ lock only contains the stage two training and we found that the full training you know the to unfreeze the uh Vision encoder uh in a first stage training least to better results okay so for to tackle this 3D localization we just used the model in 2D and to unprojected to to project it to 3D using the depths and the the camera poses uh which is also a similar uh idea from Eagle Lock And as you can see so unfreeze the vision backbone leads to better performance and we can also achieve very good results by training free method to tackle VQ 3D there might be a little problem with this slides so the general idea for body and hand cles is that we do the multiscale fusion like from we alter the the the depths of the Transformer or the pulling size for the uh pulling attention Transformer so we can get multikill features and we try to merge them together for better performance okay so even the performance not that high okay so so this is a big I big picture of our ego video to solve a series of challenges in this task and you know task specific model example might improve the performance for each Challenge and you can refer to our GitHub which will be updated soon and also report so uh one takeaway thing is that that uh we've been doing some uh research on unpaired ego Axel videos because you know ego Axel video is collected in a very time synchronized matter time synchronized manner because uh you need to uh record the videos uh with one eagle camera and lots of Axel axle cameras from uh static places which is very hard to collect this data so you go Axel 4D is really a very good data data and to we don't have that uh human resources and computations so we we can try to uh unleash to try to relieve this uh situation we try to focus on unpair ego a videos to to build a pseudo pairs uh for example we I build a eagle folding videos with uh exocentric videos from how to 100 million so we can do a lot of alignment stuff here and also another work called ego EXO learn it's also focused on unnon asynchronized ego and EXO videos which is a I'll give you a very general idea so imagine you're a kid so you watch your parents or the world to do things so there's always an Exel view demonstrator here and so you try to imitate the the op the action from the demonstrator so we have an ego view follower so this this this step you can achieve uh very uh uh unpaired data in a very in a different environment and a different time so like this we are working on unpaired E EXO videos and if you're interested you can try you go and visit our poster session listed here okay so thank you we have time for one or questions if there any questions for the speakers so one question is um you mentioned like the impact of creating this ecocentric video text pairs like were there any studies which uh you did to actually measure the in that impact about just creating this entric only sort of data set for feature learning or what if you just do the combined data set how much performance gaps do you see for some of these tasks okay we indeed did some uh a good question we did some studies on the impact of different combinations of entric but we didn't uh WR because uh because we think that's uh that's going to be U we need to refine some of them so we will release them after our refinements thank you next SPL speaker is Dan she'll talk about the hand solution from the team e e e e so I'm here to presenting our work reconstructing hands in 3 with Transformers and we also submit um uh use our method in the hand post estimation Challenge and I'm presenting the work um um and also representing George um me Ilia Andre David and jandra so um P has been used in a lot of um human daily activities we use our hands in communication in crafting in gardening playing music instruments and also um in cooking and to understand hand activities well we first need to understand human hands and we have done previous work that to understand hands in 2D so our previous work we can find where the hand is and also we can find the object the hand is interacting in the image and later in our work um towards the reacher understanding of hands as at scale we upgrade this system to also um predict the segment for the hand and the object in hand and also we predict the hand grasp for the hand and we also predict whether the inhand object is a tool or not and if the in hand object is a tool we also um predict the second object which is a mated by the by the tool in hand and if you're interested in this um system please um check our method um in the paper below and um so far um our per has been focusing on 2D but hence is actually in 3D hence moving in 3D so um in our uh paper in our Meed Hammer we focused on understand um s hand and uh we try to reconstruct the uh hand mash from uh hand crop image so um in hammer we have two uh ingredients that's quite important to make the me really work on even in the wild image the first part is about the big model so um in Hammer we use um The viit Edge backbone which is much larger sorry much larger than the previous method um Frank cap which use resent 50 so the motel size is increased significantly the second part is about the data part um so in our training data we combined 10 existing data set we combine all of them together as our training data and uh this is um four times larger than Frank M cap and plus we use um the in the wild images from uh existing data and we believe that it can also be improved to like uh inrupt more in the wild data in the training yeah so previous we have seen um results on um third person view and this video shows our result on uh EV e exo4 video and it does quite well yeah we have more um examples on our web page so if you are interested uh feel free to check more examples and check our paper so um when we evaluate our me hammer actually we find existing um 3D Benchmark is quite saturated which means that um method the the the performance of different method they're quite similar and cannot be distinguished um so to evaluate um uh s hand reconstruction method and especially their performance on in the wild image and e e uh Centric image we um collected uh this um uh hint data set uh which is show for hand interactions data set and uh in hint we use um frames from uh new days of hands video data set and uh frames from applications frames from Eco 4D we use frames um from this U video data set um as our image uh samples and then we do the 2D key points annotations for them and uh since those frames are they come from uh videos they capture more natural hand uh and object interactions and also in hint we've um focused on in the wild image because you can see all the videos are not like uh captured in in the lab setting and also um we have like images from ego Epic and E 4D so we have a lot of um entric uh view image so yeah and in terms of the result on hint um we see that um compared with um the previous state of the art our method is um around uh three times better on the metric of uh 05 PCA and here it shows the result on new days frames and uh it has the same trend on um the application visor frames and E 40 frames and also in in hint we also annotate whether the key points is included or not which means whether the the hand key point is included by other objects um which is which happens a lot when hand is um in interaction so we also uh evaluate um for Visible joints and also for occluded joints and uh you can see the results um are around three times better on the 05 PC metric yes and here for the ego ex40 challenge um it annotate 3D key points using um multile views because it capture multile views it can um get the 3D key Point cations and also from um uh e exo4 it provides provides a baseline method called pter which um can predict 3D key points and uh for the results um the hand post uh Benchmark uh the challenge the primary metric is p and PJP which is on the left side and uh yeah we show first show the result of Potter yeah this is a baseline um for the uh hand post estimation Challenge and then um we provide our Hammer results which is just the out of box without any fine-tuning so you can see um here in the primary M primary Matrix the PA and pgp our Hammer is already outperformed the Baseline poter and uh later we try to make the method to like better on the uh Benchmark so we do then do an example of um hammer and Potter which is um we um get the result from both methods and then we do the uh alignment to get the result from the assemble of um both methods the last one is um our um challenge submission so we also tried a little bit of fine tuning hammer on the uh the uh 3D key points annotations provided in EO x4d and we see that um when we combine um when we do the assemble of the results from uh the three models which is Hammer Potter and plus um hammer fine tuned we get the best performance on the primary metric so here are um more results of uh our method on E ex40 images and it does very well when the hand are um interacting with objects and under heavy occlusions and here are um more examples um more extreme lightening and also hand in gloves and also hand of different color color tones here and our method is not perfect it's still have um some failure cases for example sometimes the RIS orientation as shown in the first row is not correct and um also um when the fingers the hands are under heavy exclusion as shown in the second and third row only like uh a little bit of the hand are shown in the image the method also cannot um reconstruct the hand very well and L row shows that um because um our method uh depend on the hand bounding box as in as input so um if the ground responding box annotation is not correct or of centered then our message will definitely not work here yeah this is our um me hammer and thanks for your attention I think we have time for one question doign it gets worse why because the main better abely non align got worse yeah I think um but the difference between those two Matrix is like a a procross alignment so the primary Matrix after alignment and the other matric on the on the right side is not uh after alignment so I think this is a a common issue um between like for method for parametric um uh method of parametric models which regress manual or simple compared with the other method that directly reconstruct three key points so there is a like Orient like hand risk orientation prediction so it's very hard ration and then you again get better yeah e e addition so great okay so for the final part um before we conclude this session like this is the price ceremony that we always do for ego 4D and now for ego XO 4D so we will be announcing like winners by each Challenge and so whoever is present here from the team to represent please come uh to the uh Podium here to get your certificate and also would love to click a picture um so yeah we'll uh start with the ego video team which won the first place um in the VQ 2D uh Challenge and also the second place in VQ 3D challenge so maybe you can go there okay next we have the first place winner for VQ 3D the IV team do we have a representative from here yeah that would be yeah okay for nlq the second place team is iar do we have a representative here from iar no so the first place is ego video um here they go again lots of PR and certificates second yeah second place for mq again e video so first place for mq is long form video understanding team okay so goal step third place is I learn um second place is ego video first place is caror team um okay for schema third place is lifelong memory n second place is spans ego AI team do we have a representative here okay first place I learn um we don't have a representative here so looking at me second place is USB L team first place is Lenova research so stda third place is snu team thank you yes congratulations second place is team Zario from Kat and first place is ego video LTA we have the second place H AI Pui team and first place is ego video okay so moving on to Ego XO 4D winners um third place for body pose we have the sjtu team they're not here second place is the UC ego team do we have a representative here first place is ego video third place for handp ego video second place is the hammer team and first place is Lenovo uh we don't have it them here so yeah before we conclude we really want to thank eval AI uh folks who always support us in the background like they just do so much of debugging with us every time we host new challenges like there's always like technical issues so really thanks and then the cbdf uh foundation for hosting ego 4D so yeah any feedback challenges definitely email us and thanks everyone uh for for being here and participating in our in our Challenge and workshop

