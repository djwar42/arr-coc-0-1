---
sourceFile: "23568   Multimodal Algorithmic Reasoning Workshop"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:05.254Z"
---

# 23568   Multimodal Algorithmic Reasoning Workshop

f45a2074-cd97-402b-934c-dd9141970f81

23568   Multimodal Algorithmic Reasoning Workshop

bfaf7f4a-8fbd-4e88-97c2-0844b017930f

https://www.youtube.com/watch?v=LooLbLs3O_Y

LooLbLs3O_Y

## ComputerVisionFoundation Videos

um Mr organizing team I would like to welcome you all uh to this uh workshop on multimodal algorithmic reasoning and uh so the goal of this Workshop is to uh is to understand the recent progress in AI from the perspective of of say slightly theoretical side for example using neural algorithmic reasoning or from the architectural point of view using like multimodal large language models and as well as interpret the these this progress in terms of how humans understand or how human intelligence works for example using the cognitive models of intelligence and so the idea of this Workshop is to um bring this these different perspectives um and kind of uh try to understand in a holistic manner where uh we are and what are the steps ahead uh towards finding the missing rungs in the ladder of intelligence and uh so we have a a pretty packed program today and we have five Keynotes 30 minutes each with three oral sessions uh three oral papers selected from the from the bunch of papers we received and so all these Keynotes are um segregated into these different themes like neural algorithmic reasoning um large language models and uh the cognitive models of intelligence and as this is the same for the oral papers as well and we have uh some best paper Awards and we were also conducting a smart 101 challenge as part of this Workshop so so we'll have awards for that um and then there will be a poster session that will be in the arch building and that's from 11:45 to 12:15 um so without further delay uh I since we have a very packed program um I'll start with the keynote so it's my pleasure to welcome um pet chich for this first keynote um so Peter has been doing a lot of amazing work in the area of neural almic reasoning he is a staff research scientist at Google Deep Mind and uh is also affiliated with the University of Cambridge and uh maybe if you uh if you have worked with geometric deep learning for example graph neural networks his name is uh is well known um he is an inventor of graph graph attention networks as well as graph deep Infomax um and he has been recently doing a lot of amazing work um in this algorithmic reasoning set of things with graph neural networks so I like to welcome him for the first keynote and uh give him an Applause for starting stuff thank you you need you need to allow me to unmute myself just let me know when you've done that and I'll share my slid in the time okay and you'll need to unshare your screen so I can share mine cool okay and right you should let me know if this works on your side Perfect all right thank you everyone for coming over in the morning listen to this first s that I am giving today on reate Unity is my pleasure to be here this is actually my very first cvpr I'm having a fantastic time so far so what we're gonna be talking about today you what are the kinds of arguments I'm going to be making we're gonna be talking about classical computation and when I say classical computation I want you to think about the kinds of things you might typically encounter in an undergraduate course in computer science so things like sorting searching pathf finding clever ways to organize information efficiently and so on and so forth of course because we are in a deep learning context we are going to talk also about how to take that computation and capture it with neural networks and further on thanks to the theme of the workshop today we will also talk about multimodality so how can we represent this data in various forms and have those forms interacting with each other to hopefully achieve something that our base models are on their their own unimodally not capable of doing so hopefully this will be of interest to Broad areas of this audience I promise you there will be also popular uh computer vision language models uh references throughout this talk as well so it should be interesting to the broader cvpr Community now before I dive into the contents of the Talk itself I'd like to just take a moment to reflect on how we got exactly where we are I've actually you know I I've searched uh for the string neuro algorithmic reasoning for prior works just to make sure that my claim here is correct and I actually haven't been able to find an earlier mention of the words neural algorithmic reasoning in research before October 2020 when uh I co-authored this excel in paper so really this was my attempt at organizing the various bits of work that were happening in the area at the time broadly trying to fit computations into neural networks and uh this was the first time we used this term when nobody else was using it sometime later we wrote up this position paper on neural algorithmic reasoning I would say a lot of the claims that we make in this paper are no longer correct but I think in spirit it is still something that's uh very useful as a as a guiding as a Guiding Light and yeah it just kind of it started from there and I've been just blown away by how many different Works have then taken this idea this concept and even squeeze it in their titles and then these papers landed in top tier conferences these are just a handful of references really if after this talk you want to read a bit more about what people have been doing over the years and how it has gradually accelerated as well so this is 2023 and 2024 you can see once again your algorithmic reasoning features in a lot of titles and people are generally doing some really interesting stuff with this idea and this now led us to both a full-on tutorial that was at the log conference a few years back and also now we have full-fledged algorithmic reasoning workshops and I just like to start by thanking the organizers for giving me the honor to present one of the Keynotes uh at this at this Workshop I think it's it's a really fitting way to see that this term that I use to sort of organize information has landed uh pretty much uh in the spotlight now now that that acknowledgement is out of the way I want to tell you a little bit about where I'm coming from because the word reasoning as you probably all know very well is a super loaded term like if you ask a hundred different people what does reasoning mean to you they will probably tell you a hundred different things so I felt like it would be very useful for me to just be straight with audience at the very beginning and tell you what it is that I want when I say reasoning what is the thing that I want to achieve okay and what I want is robust reasoning in your own LS now let's make very clear what I mean by this or specifically what does it mean operationally for my team uh at Google deepmind so we we've actually had many months of debates over this internally as well like what is it actually that we want to achieve if we're working on reasoning effort this is roughly what we've arrived at and I'm very keen to discuss any of these points with you later on after the talk as well I'll be sticking around for the whole day so for us reasoning means a robust procedure for solving instances of problems okay this is really the fundamentals of what it is the word robust is the important one so notice I haven't said anything about accuracy people sometimes when they want reasoning they want 100% accuracy I don't think it's necessary to have 100% accuracy to have reasoning humans we are I believe a good example of reasoning systems and we are not always accurate as you might know right in fact one very good example of how human reasoning is not always accurate is the fact that it took me twice as long to find the conference venue yesterday then Google said I would need because I was stuck in a loop somewhere right so human reasoning is often approximate doesn't always get us where we want to be in the optimal way but I would still count it as reasoning and also uh you might then say depending on which Camp you come from that reasoning must involve explicit symbols and you must explicitly explain every single step of what you're doing in a way that humans can interpret I also am not explicitly requiring this I think it's useful for interpretability if you have access to explicit symbols but if I have a really potent robust system that does all of its reasoning inside High dimensional spaces I will also allow this as reasoning some people might disagree with this but this is the operational definition we're going for but the key important thing so it's not accuracy it's not being symbolic it needs to behave consistently or at least predictably across all problem instances that we care about and what this really means is is we care about outof distribution generalization this is really the key mission that my team is trying to attack and maybe a good way to put it in layman's terms is if you want to build a model that truly captures and understands the concept of multiplication for example and you'll see why I'm giving multiplication as an example in a second it should work equally well on all instances of multiplication problems or at least tell you roughly how much it's going to be wrong if it's an instance that it doesn't uh that it cannot deal with right and you'll see later on in this talk we have even simpler things than reason than multiplication that these models will tend to struggle at but there's a very specific reason I'm starting out with multiplication you might have seen papers and results like these before you know nowadays the frontier models of use and deep learning are these decoder only Transformers and they have hundreds of billions of parameters Argo they compute hundreds of billions of multiplications just to generate a single token but if I ask them to multiply three by three digigit numbers they cannot do this correctly so here you have the accuracy of prompting a language model to multiply two numbers of different sizes this here is with Gemini Ultra which is one of Google deep min's Frontier models and you can see already 3x3 multiplication becomes really really painfully bad about 37% accuracy things obviously get worse as you increase the sizes and just in case you were wondering is this something specifically bad about Gemini no it's not there are equally many papers that show the same effects happening with gp4 so this is not something that's like dependent on one specific LM architecture versus versus another it's a problem all of these Frontier models have okay and another thing that I think is quite important and will be important later on when we discuss certain possible solutions to these problems is we don't just want to solve it in a way that theoretically allows reasoning and that's great we also want to build something practical so any solution that we explore should in principle be easily scalable to hundreds of billions of parameters so that one day it can actually land in some of these Frontier systems uh effectively right so this is what the aim for us is and I hope it sets a good framework for the remainder of this talk now you might ask why bother with this right like multiplication or why B without of distribution I often talk to people who are representatives of the scaling camp and they will typically tell me one of two things the first one is just gather more data out of distribution is not a problem if you have the right data for the problem right however I think we've known this for a while now maybe these papers are not that familiar uh nowadays but they have been quite influential and have been spotlights at top conferences previously that give this very simple geometric Arguments for why no matter how good data you build you cannot really generalize outside of that training data unless you do something really special to your architecture or your problem this paper from K Lou and others from MIT that was published directly in 2021 gives in my opinion the most beautiful oneline argument for this and that is if you take a railu MLP which is a standard Universal approximator Network it's a piecewise linear function so when you escape the support of the trading set it has to become linear right so generalization is impossible if your target function is not linear because eventually you're going to hit that linear regime and what they've done in this paper is not just that observation arguably that observation is quite obvious in retrospect they've proved that the convergence rate to this linear regime is super super fast so the moment you escape your training set a little bit and you can see this in a few examples in different dimensions the model very quickly goes to predicting a linear function and at that point you cannot extract even if you wanted to of course this result only holds for Value MLPs nowadays we use different activation functions but there are extensions that take this argument in the realm of causality or category Theory and I invite you to read these if you're interested to know more the second thing that uh the scaling Camp might say and I will have a very concrete answer to this later in the talk is just let your model call a tool or write code we know how to multiply with calculators let your model call a calculator right well okay sure this does fix the problem at least in theory and research in this space was done for many years in the neuros symbolic Community works like tool formers have brought it back into the spotlight the general idea is you use your neural network to predict inputs for a calculator or an algorithm or a code interpreter and then that predicts the outputs you care about this will not have outof distribution issues by definition but arguably I think it's admitting the feed before even starting I'm not saying we shouldn't use tools by the way I think tools are going to be a really important part of generally intelligence systems of the future I'm just saying we shouldn't just stick stick tools onto what we have right now and call it a day I think we need to get better at our base models as well because if you just stick a tool on top of it you're acknowledging your network cannot do this and you're committing yourself fully to any errors that the tool might propagate from what the model is predicting and also we'll mention later doing it in this way introduces a lot of funky bottlenecks which makes it Britt hard to compose and might require a lot of effort so my conclusion from all of this is very likely we need to do something about the base model architecture to capture computation better it's not going to be enough to gather more data it's not going to be enough to stick tools on top of it now once hopefully we agree that this can be a good Avenue to push our research in let's think a bit about how can we even evaluate whether a model can learn to generalize out of distributions so let's think about what we need to support this kind of evaluation we need some tasks for which we can generate outputs reliably so it must give us correct answers even when we escape some reasonable problem sizes it must be so efficiently so we can generate data on the fly or pregenerate useful data sets and it must be done for any distribution of Interest which is valid right I argue that these three points by definition I've just described to you A polinomial Time algorithm like it's attractable procedure that allows you to generate outputs for any input you care about right unless it's a problem for which you can easily scale human uh labeling which might exist but it's certainly not as easy as this so we basically what we need is algorithms poal time algorithms and when I say that I mean the data of the form that you can see under the slide here for insertion sort for example this is a standard polinomial time algorithm for sorting a sequence of numbers and you can see gradually it starts with a list 52 431 and step by step it inserts one element by at a time in its correct place and at the end you'll end up with a sorted list one two three four five right and we have taken inspiration from this way back in 2020 and we worked on it for two years to try to create a unified Benchmark which allows you to battle test your models out of distribution over a lot of polinomial time algorithms and what we've developed is based on this uh textbook which I believe should be familiar to many of you in the audience it's the standard textbook for undergraduate degree courses in computer science uh the clrs textbook as we as we call it by the first uh by the initials of the last names of its authors Corman lerson orenstein and specifically we've read through this entire book which found that there's about 90 algorithms of interest in there we've picked out a careful selection of 30 spanning a variety of possible reasoning skills you might want your model to exhibit spanning sorting searching pathf finding dynamic programming string matching and even geometric algorithms so there's a lot of different skills that you can learn there and we have released this uh publicly as well as uh in the form of a companion paper published at icml one thing that's really important is because it relies on polinomial time algorithms it's not just a single static data set it's a data data set and Baseline generator so you can easily add new tasks to this change the input distributions for training and test do whatever you want the library will take care of both sampling the data batching the data creating models for you and launching training runs so you can really kind of just launch off a new task or modify a tiny bit on the processor Network architecture and get results immediately this is the reason why building this Benchmark to two years there was a lot of things we needed to put together even though in principle the ideas were quite simple so you can our companion paper if you want to know more and also we have a fully open source implementation at GitHub that you can use to start generating this data today if you want to play with it and what have we done with these models we were interested in you know training generalist models that are capable of reasoning and as you can see uh from these representations one very nice unifying uh structure that can be used to reason about all these problems is the graph so graphs are basically a very nice way to reason about discrete uh data in model and what we wanted to build is a single model capable of executing all of these different algorithms so think about a model that can do sorting path finding convex whole finding all in the same core architecture of course the problem we have is that data has different shapes so we need to have separate encoders and decoders but we could share the central part of the computation and it turns out when you train a model like this uh you get something uh which is actually quite generic and Powerful it's a work that we published at log a few years ago and here you can see the performances of our single multitask trained model on executing on four times bigger inputs so out of distribution against single task Specialists trained on each individual task and for all intents and purposes on some tasks is better on some tasks it's worse but on average we basically have this one generalist that's roughly equally potent as individual Specialists so it's a pretty good result uh for the capability of building such systems obviously we have more to go to make these curves go up but it in principle it generalizes to four times larger graphs now nowadays obviously people still work a lot with gnn's but there's a lot of interest happening in the language space and we felt it would be a great idea to try to build clrs 30 into the llm age and by the way many reasoning papers on language models build their own synthetic algorithmic data sets usually for addition or multiplication or things like that so modifying our Benchmark to suit language models also unifies a lot of these previous papers so it's another kind of useful uh addition so we created a language version of clrs which we call clrs text and it's still in the same GitHub you just need to add a little bit of extra path to reach the folder where the text files are or text file generators this was a great collaboration with Sean Tom and AI from Maryland and uh this is basically what the data will look like so now you have textual versions of those graph traces I showed you before the model is prompted with the part in green and it has to predict the trace in blue which shows you how gradually the array gets sorted as it does insertion sort and it's evaluated on the final part in red after the pipe character so you get you say that an output is correct if you have an exact string match that gives you correctness on the final thing you produce now another great part about this Benchmark is that remember how in the graph version we need to have a separate encoder and decoder for every modality now that everything is in text we can have just one llm trained on everything so let's see how well that does if it actually doesn't really do that well so these are some of the results there's 30 algorithms in sarus text you can see the full plots in our paper but these are some of the results we get for various both fuse shot prompted state-of-the-art models and fine-tuned smaller models so first thing that you can observe the curving green is Gemini 1.5 flash which is a really fast Frontier level model distilled from Gemini 1.5 Pro we've two shot prompted it on these things so it's aware of the formatting and you can see generally doesn't perform that well for most of these algorithms there is a huge gap that you can unlock if you fine tune on some of these sequences so even though these models have been exposed to a wealth of algorithmic traces on the internet they clearly haven't learned to prioritize that information and they cannot really robustly do it additionally even in the realm of fine-tuned models so here we use the Gemma 2 billion model which is a relatively small model but you can efficiently fine tune it on these sequences you can see that even in the realm of fine tune models you can have huge up takes this is like 60% improvements for certain sizes if you add some small tricks like better position embeddings position embeddings are an easy way to lose out of distribution performance if you do it badly here just by adding randomized position and Bings we get a much better performance on some of these algorithms and that's the gap between the blue and the orange model but another thing you might notice the red dots you might be able to see on the slide are the sizes of the problem we trained on and you can see the moment we escape the largest size that the model is trained on you have this sharp drop of about 60% basically to zero accuracy right so there's a very big weakness out of distribution in these models that you can see on all the algorithms I've just highlighted one but it happens everywhere right so this is a big problem because at a first glance it might seem quite surprising gnn's easily generalize to four times bigger inputs llns cannot generalize to plus two larger inputs very easily why is this and we actually believe that the culprit is the auto regressive nature of large language model specifically the fact that all paths that matter must cross through through the final token which predicts the next token for next token prediction and what does this mean this means that uh you can construct inputs for which these representations gradually collapse uh I'm not sure why my images are not loading okay hopefully hopefully they will load okay this is a bit unfortunate because uh it's supposed to show you uh the things that we can do uh on these things okay well hopefully hopefully the yeah there we go the image is here so first of all you can easily construct a sequence of inputs for a language task where you make all the tokens the same and the last one is different and you make you can gradually as you increase the size of the problem the representations of that final token will start to collapse and eventually it will become smaller the difference between two different sequences the embeddings of the last token will become smaller machine Precision which means the model will be unable to reason about those two things differently aside from effects of random sampling and the second one this is very nicely explained as a picture because you have this causal masking which allows a token only to send into the future tokens at the earlier part of the sequence have way more paths to the final token bottleneck than the ones later in the sequence and you can easily end up with over squashing of information where the earlier tokens are in a much better position than the later tokens and this is an inherent bias of the computational graph of the system and by the way because of this they provably struggle with rudimentary tasks like counting and copying and this is These Bars here are just the errors so how big is the error when you ask the model to just sum one plus one plus one plus one plus one of a certain number of ones the errors just grow and adding Chain of Thought does not help and if you look at the error analysis it's not just that the model slightly gets it wrong it illustrates the model clearly has no idea what it's doing this is a distribution of answers that Gemini 1.5 giv gives when prompted to do these various counting tasks it tends to really like the number 100 it will just say 100 when the number is big enough and not really try to do anything else right so clearly it hasn't learned the algorithm for counting and by the way you know you may say these kinds of problems are easily solvable with tools but good luck using a tool if you can't even copy your input into the tool right now what's the culprit this is not something that's inherently bad about Transformers it's only about the fact that their decoders are trained with this causal mask objective which exposes a bottleneck in the final token and repetitions make things worse so if you add some simple tricks like inserting commas between repeated tokens as you might do in these Finance data this actually improves a little bit the representational differences and if you want to know more about it we have this paper on Transformers need glasses which elaborates on this further but arguably this is just a small mitigation strategy it's not a Panacea ideally you want communication between tokens to be unrestricted but this is harmful to scale we have causal masks to allow us to train these models easily at scale so what instead we had a non autor regressive model that we plug together with the autoagressive model in a multimodal fashion and that leads us to the transar architecture so the way it works and we actually published this at the workshop here today it will be presented We have basically a GNN that's trained on this graph data as we described before and can generalize to four times bigger graphs and we let our llm cross attend to the embeddings of the GNN to extract that information that comes from the nonaggressive pipeline and hopefully this makes things easier and this is what it looks like and I promise there would be a famous Vision language model mentioned here so you have your Transformer part that works on the tokens you have your GNN part that works on the graph and then you because these two have very compatible representations you can just do standard Flamingo style cross attention to map the embeddings of the nodes of the graph into the embeddings of the tokens of the Transformer and just by doing this one simple trick you get huge improvements on outof distribution reasoning on clrs text so some of these improvements for different categories of algorithms are well over 40% or even 60% I believe in language model literature when you have an improvement of 20 or more percent you tend to call it emerging capabilities I believe something like that yeah if you want to know more about transar this is a paper it was a great work from Wilfred as the first author and many others from my team at Deep Mind unfortunately Wilford could not be here with us today because of Visa issues but I will be presenting I've brought the poster today and if sticking around for the poster session I'd be very happy to talk to you more about this so one closing remark I want to make is that transar is mainly a proof of concept it shows that if you include this multimodal information with non-auto regression it helps you bring benefits in reasoning tasks however you cannot easily use such a model out of the box if you don't have a graph that you can use as a second modality and we hope our study motivates some future Research into fixing this problem one very simple promising Avenue we're currently exploring is distillation right once you train a multimodal Transformer can you then distill its Logics into a unimodal Transformer which only looks at the sequences we explore various trade-offs of distillation versus next token prediction and we're actually able to train a competitive unimodal distilled model at 0.25 loss factor which actually beats the Baseline which is the the model in blue that's the base Transformer in outof distribution reasoning it doesn't work everywhere it doesn't work everywhere with equal performance but we at least have this preliminary result to show that it is in principle possible so in conclusion reasoning out of distribution I think is going to be really important especially if we want to do science with language models or to do any kind of robust reasoning and I believe in the computer vision Community you come across these kinds of problems all the time for self-driving and similar problems right and our Frontier base models are fundamentally bottlenecked the very computation graph of the Transformer exposes a bottleneck in the last token which which means information is not treated equally and can very easily get lost if you want to read more about it the paper from federo barbero and others is a great reference one way out of this kundum there's many possible ways out but this one is particularly okay this am I have I Okay cool so one possible way out of this conundrum there's many possible ways but the one that we have explored in this work is to embrace non autoaggression and one very easy way to introduce non autoaggression without breaking the scal ability of the Transformer is to embrace multimodality train a nonauthor aggressive model on the non-author aggressive data and then let the Transformer use its embeddings this leads us to trans Nar from Wilford bony and others which we will be presenting at the poster session later today and lastly if you want to try other ideas or improve the non-author aggressive idea and have a good Benchmark to evaluate a of distribution language capabilities with access to the graph modality as well we now have that as well we have the clrs Tex Benchmark that is publicly available on Hub and we're very happy to help you use it if you want to try it out for yourselves this is great work from Lissa Mara Sean MC Bor bar and many others at Deep m in Maryland that's my last slide thank you very much for listening and happy to take questions we have time for uh two or three questions so there is no microphone that we can bring over there so you probably need to speak louder sorry excellent talk say if you could com on the complexity of Transformers have quadratic complex in terms of input for many algorithms you really would be one not only subar only yeah so maybe maybe you have to repeat I'll repeat the question so the question if I understood correctly is about commenting on the computational complexity of the algorithms and how it relates to what the Transformer is doing this is a very important Point uh basically many algorithms as mentioned when especially when you paralyze the computations are not just sub quadratic but sublinear so a lot of the computations that the Transformer is doing might be redundant however even though there's lots of redundancy we've shown here so the tasks that I showed earlier like counting and copying is literally pluck out one character out of a whole sequence not even copy the whole thing very clearly sublinear however the model struggles even with that so it's I would say at this point it's less about computation complexity and more about allowing the information to travel through the transformer in the most effective way but yes uh especially when we talk about generalizing over longer sequences you're probably going to also want some recurrence in there currently the Transformers have a fixed number of layers follow the base architecture as before this will obviously break eventually but on the sizes we have it wouldn't but it's another thing to think about once we sort out this initial problem yeah there's a question there so this is a great question I'll just repeat it to talk about how to interpret what these models are doing Beyond just uh uh Beyond just looking at out of distribution performance so I'll try to quickly sweep through to find another paper I want to share here but one thing I can say is one of the things we do in the transn AR paper which I'll talk about during the poster session is we can also look at not just at the accuracy but also at the shape score for example in the par score so is the model capable of following the format of the problem and is it capable of predicting the arrays of the right size so one thing we discovered there it's very rudimentary but it's already a starting point the ways in which these Baseline Transformers fail often is that out of distribution they fail to even predict the correct shape of the array because they're not used to something of that particular size and having randomized position embeddings or just non-auto regressive training helps with that so that's one thing we discovered another paper which is really beautiful that you can do especially with the GNN is to analyze how the representation in the high dimensional space move as you solve the problem and this paper here from Vladimir Miran latent space representations of NS basically puts the high dimensional embeddings of these GNN into PCA space so two-dimensional threedimensional and he has these really beautiful plots on how those embeddings move through time as the model learns to solve things and actually looking at them this way helped them to fix many of the architectural problems with the system so those are two things that immediately come toiz yeah another question in the back yeah right that's a great question so uh I would say both are problems that are kind of tangential so here you can see just to highlight it again if you add randomized position and Bings it can give you a lot of boost the main problem is that the position and Bets at least the popular ones are always the same in every position which means that if you give the model a bigger array a test time it will see combinations of position embeddings it never saw and it can easily overfit to the sizes you're giving it randomized position Tings are a simple way to break this problem by just randomizing the positions that you give to the system as long as they're in sorted order uh that's a really easy thing to do nowadays there's a lot more great suggestions like fire embeddings and recently Abacus embeddings which are even more fine-tuned to like figuring out the different positions of operands and stuff like that I think position and Bs are going to be a really important area for research here and they're arguably the easiest way you can see gains make a marginal change in the code run off your same training script see a big Improvement in the curve so at least in the short term that's going to be the easiest non-auto regression is already a bit harder right and how does it compar to over squashing I would say they're two complimentary things so position embeddings kind of are there to help you uh hopefully if you do that right generalize better to different sizes from a positional perspective so you can easily index into different places over squashing is a problem that just means that things that are earlier in your array are going to have a much easier time propagating information at things that are later couple that with the fact that a model learns a recency bias just because the last few characters are always most important for next token prediction in many cases the tokens in the middle are very painfully underst staffed and this results often in the u-shaped performance you see in retrieval right so I would say you can fix PES but you still haven't fixed the over squashing problem for that you might need to well one direct thing you can do is change the computation graph structure of these models yeah um so unfortunately we are out of time so maybe if you have more questions feel free to meet our lat here yeah um so thank you so much P again let's uh have a I can't hear you by the way e y Dr Bing can you hear me yeah I can hear you now thank you through this microphone laptop yeah you can talk through that but she will hear this okay uh let's start uh ladies and gentlemen uh it is my great honor to introduce Dr charlesa an assistant professor um in computer science and electric engineering at Stanford University and a co-founder of physical uh intelligence uh she has uh her research interest is um uh her research focuses on uh enabling robots and other agents uh to develop broadly in uh intelligent behaviors to learning and interaction and she has made significant contributions in area such as a vision based robotic manipulation uh meta learning for visual learning and scaling robot learning to Bo data sets um so today we are talking about AI reasoning and M model models and how will this affect robotics so let's hear that uh here from Dr char in her keynote on robotic U reasoning with vision language models and uh yeah so let's uh start but unfortunately Dr fin cannot be here uh in person today uh but let's uh welcome her uh virtually yeah let's get her give her a warm welcome yeah great um thank you for the introduction hopefully the audio and video is all good um really sad that I wasn't able to make it in person I unfortunately caught Co late last week and cancelled my flights uh so that I wouldn't get everyone at cvpr sick um cool is the audio all good cool okay um so I'll get started uh I'm planning to talk about how robots can leverage Foundation models for high planning and reasoning and oh a noob can't hear it I think it's on the Seattle side I'm not muted for let me know if I should say anything to test y up um can you can you speak CH see is this better you can you uh yes yes um so should I start or is can people hear is it is it okay okay yeah you can start thanks okay um wait really really sad I wasn't able to make it in person I um yeah got Co late last week and so I had to cancel my trip to Seattle uh there should be some time for questions at the end if uh if anyone has questions um yeah and I'll get started so I'm really interested in whether we can leverage Foundation models for high level planning and reasoning in robots and specifically I think this is really interesting because any sort of scenario where a robot might need to be accomplishing something over long time Horizon it's important to be planning and reasoning at a high level rather than just making small adjustments to the robots actions now in some work a couple years ago we looked at leveraging language models to help robots plan as an example uh we might prompt a language model saying I spilled my drink can you help with the hope that the language model can help provide a list of potential um things that the robot can do in order to help and so an ideal planner would Leverage The Language model to uh kind of break this down into step-by-step things to find a sponge pick up the sponge um come to the person put down the sponge um uh potentially also help them actually wipe up The Spill and so forth now unfortunately if you sorry naively ask uh language models uh this sort of prompt um they'll suggest things like do you want me to find a cleaner you could try using a vacuum cleaner um or fla will actually say I'm sorry I didn't mean to spill it which isn't exactly uh the kind of useful planning or reasoning that we want from these models and so naively um applying foundation models for high Lev planning and reasoning um can sometimes be helpful but it's uh challenging to get them to be most effective uh for integrating with robots because they need to have a lot of understanding of the environment and of the robots capabilities um and so what I'm going to talk about today is how we can leverage Foundation models and specifically Vision language models with robots for better higher level reasoning we'll actually look at this both in the context of legged Locomotion as well as manipulation cool um so let's start with some Locomotion examples and as like a kind of to set things up we're going to prompt the vision language model with a language prompt to kind of give it some information about what the problem is um generally that is controlling a robot at a high level and so forth and we're also going to give it images from the robot's entric Viewpoint and so these are images taken from an RGB camera mounted on the robot and then from there the goal is for the VM to Output a skill uh one of a few different options that it has to walk climb crawl turn left turn right or move backwards as well as a magnitude small medium or large for that skill um so this is like a fairly simple setup um and it has like some structured output of of what it can do at a high level and so it should be in some ways fairly straightforward for a vision language model to look at an image and predict a skill um but unfortunately we find that uh naively prompting a vision language model to Output the skill at each time step uh can struggle and in particular the vision language model can misinterpret the scene or can misinterpret what a skill might do or the robot's own capabilities and it's also really easy for the robot to get stuck where the vision language model either isn't sure what to do or it just repeatedly provides the same uh high level action without making any progress and so um in this project we kind of investigated and and tried uh investigated kind of what blms uh do in different circumstances and specifically how we can improve upon the naive application of VMS to this problem and there were two really key ingredients that we found to be uh really helpful for actually enabling robots to reason about the situation um and the first was to incorporate history so instead of only um only looking at the current image of uh the robot actually incorporating all a number of previous images and previous highle actions that was taken and by by feeding this history into the context the robot should be able to identify if it's stuck and try a new strategy um if it is indeed stuck and the second thing that we found to be helpful um is uh in some ways perhaps it's somewhat intuitive uh that that it might be helpful because it's somewhat similar in a Chain of Thought reasoning um but we found that is quite helpful for the vision language model to actually not just output the like what skill it should take at the current time step but it should also output actually the skills that it should take multiple steps into the future and we found that uh this actually substantially improved performance and helped the robot foresee if there was going to be a potential failure later down the line um in its uh in in its future uh and so we refer to kind of the combination of these two things with the VM as VM predictive control in the sense that this is actually quite analogous to model predictive control where we're going to be planning multiple time steps into the future um except at a very high level in terms of high level actions the robot should take um and then at each time step we'll also be kind of replanning um to plan out a new sequence of actions um and only taking the the first skill in that sequence um and executing that on the robot um a little bit more specifically uh here's some examples of um kind of an excerpt from The Prompt that we use um the first was regarding history so um we prompt the the vum saying that the beginning of your response should use the state of the robot including its current position and orientation to reason through what progress the robot has made with your previous plan and so this is kind of prompting it to think about um both the current information as well as uh the previous information um that's in the history and the context and then um with regard to thinking about multi-step plans we prompt the VM to describe a multi-step plan using the available actions specified above for completing the task and give reasons that the plan might fail you should reconsider your previous multi-step plan of actions to take keeping in mind the history of what actions you have selected and the progress in the scene now um if we actually kind of for this uh example and we actually look at the output of the BLM we see that it's able to reason through um and realize that the robot is facing the same piece of furniture um and indicating that it should find a different approach uh and then it chooses an approach that kind of instead turns to the left um walks and crawls uh rather than simply trying to move forward into the piece of furniture again cool um so we're going to evaluate um whether we can use Vision language models uh as a high level planner for legged robots and uh the the kind of setup that we're going to be looking at is um again we're going to have like a low-level uh essentially language conditioned controller and a high level um the high level policy will be either um VM with history and with a multi-step plan or without one of those two things or simply a random highle planner that randomly selects a skill to use we designed um five different settings for the robot to try to tackle and the goal in all of them is to try to reach this red chw toy uh for uh the robot dog and I should really emphasize that these are well first uh if you look at this from kind of the third person point of view it looks somewhat simple for the robot to navigate the situation but the robot only receives egocentric images from the robot's camera as input uh and it has to use that uh that's the only information that it gets and the second thing here is that it really needs to combine both really low-level skills like crawling and climbing over objects uh as well as high level reasoning over what it's tried before in order to successfully succeeded the situation because there's lots of obstacles um including obstacles where it's required to crawl under something or it's required to climb over something in order to succeed cool um um so let's look at a few examples so this is uh one of the first examples uh that we tried and uh if you use a vision language model without history the robot will um or the vision language model will keep on trying the same sort of approach over and over again where it's trying to go forward it does actually it does actually eventually turn left um but then get stuck in a situation where it looks somewhat clear but the right uh should ER of the robot is stuck on the leg of this couch and in contrast by incorporating history um the robot is able to very quickly uh it it tries to go under the couch realizes that that's not successful backs out and then eventually tries a new strategy to successfully get to the chew toy um here's another example where uh the gap between these two things is too narrow for the robot to go through and so um it looks like it might be able to go through but it's struggling and without history it doesn't realize that uh it's not going to be able to go through the Gap and in contrast uh by um by reasoning about the history of what is tried before the robot is able to navigate to the the true toy successfully now looking at some outdoor examples uh this is another one without history um in this case the robot is on this kind of somewhat unstable step it's really it's a little bit challenging to see but there's um this unstable piece of uh of wood step right here um and the it's not actually something that the robot is able to walk over uh using a walking skill it kind of looks like it might be able to walk over it but it can't um and by actually using history and then trying a new approach of climbing over it rather than walking over it it's able to successfully uh kind of Traverse the situation rather than getting stuck um cool a couple other scenarios um this is a scenario where um the left side is showing using history but not using multi-step planning uh and it's not able to figure out a plan that eventually gets to the chew toy and it eventually walks um kind of out of the scene uh and then in contrast here we're able to figure out a plan that uh that is able to kind of navigate under the bench um and uh to the toy and lastly one other example again the robot only sees these egocentric images uh that are right here uh it's really easy to get stuck in the bushes uh and kind of repeatedly try a few different strategies and without multi-step planning it's able it's not able to successfully uh kind of navigate the situation um but by doing so it's able to uh navigate the situation more quickly uh here on the right so those are some qualitative examples of the kind of scenarios that we put our robot through uh quantitatively we compare these different methods and not using history is shown in dark blue not using multi-step planning is shown in the salmon color and kind of the full um approach is shown in this turquoise color and we find that overall we see a 20% or larger Improvement across these metrics this is looking at the average time to completion the median time to completion and the successful and we find that really both history and multi planning are really important for actually effectively using Vision language models for these tasks um especially uh yeah really both of them are really important um if if you're going to pick one of them uh using history is uh is especially important and leads to higher performance than only using multi-step planning cool and then one other thing that we tried is to see if we could actually improve the robot further by giving it in context examples and the idea here is that if the robot has seen the situation before uh or or seen similar situations before and it has it understands the effects of what it skills do in different scenarios then those kinds of examples of uh whether it succeeded or not at applying a skill in different scenarios should be really helpful for the robot when trying to solve the task again or trying to solve a different task in a new scenario and we found that uh indeed actually if you include these in context examples the robot can actually sort of do in context reasoning and and learning about what it's effect that it skills have on the world and we see uh kind of 20% Improvement in performance with these examples and a 50% reduction in time to completion and in both of these two uh pretty challenging scenarios that the robot had never seen before it was able to reverse them with 100% success rate with the in context examples cool um so the takeways here are that um history and multi-step planning really greatly improve the adaptability of the robot to different scenarios and this in turn improves the robots resilience and performance and you can sort of think of it both as like reasoning over time into the past and into the future um and together that really improves uh the robots ability to reason about how to uh address the scenario this is sort of analogous to MPC uh but at a really much higher level uh Elm uh planner rather than over lowl robot actions um there's also many open questions here uh we used a fairly simple approach for kind of attaching language to Locomotion policies uh where we essentially took a took the controller actually that just comes with this robot and attached verbal uh uh verbal tags to them uh but there's a lot more that can be done there to try to ground language into low-level Locomotion policies and it's also clear that there it might be challenging to describe different robot behaviors in language and so thinking about ways to expand on the language abstraction while still getting the benefits of vision language models is a really interesting open question cool um so I talked a bit about Locomotion and essentially like different ways in which we can affect the the vm's reasoning to improve it for these challenging Locomotion scenarios that I would like to talk a little bit about manipulation and I think that the same lessons hold for manipulation but we found that actually applying VMS out of the box is more challenging when we're in manipulation scenarios and the setup here is um we're going to be looking at really long Horizon by manual tasks like putting all the objects into a bag or making trail mix um it's really expensive to collect data for this and so using a vision language model or using some sort of highle planner um that might be able to reduce the amount of data we have to collect um it can be really helpful uh so the problem set up here is we um at each time step the robot will kind of look at the scene um we'll again have like a high level planner that's outputting language and a lowlevel policy that's predicting actions based off of that and then at a given time set we may also Al have a person interject and change the instruction that the robot should take um the high level instruction that the robot should follow at a given point in time and ideally we'd like this language correction data to um both be able to improve the robot on the fly as well as improve its performance through some sort of post-training procedure uh and so we want to actually kind of be able to leverage highle language Supervision in these two different ways both um like literally at test time as you're running the robot as well as uh for further improving the policy um so like I mentioned we're going to have a Hier policy and specifically the high LEL policy will take us input RGB images and output language instructions um so you can think of this as a vision language model we're not actually going to use a pre-trained vision language model for this uh because it was simpler to train something from scratch but uh it's definitely something that we're excited to do in future work and then separately we have a low-level language condition policy that we're training with imitation learning now um the key Insight that we had in this work which is in some ways pretty simple but I think quite neat um which is that if your low-level policy can follow a wide range of instructions then you can actually just improve the overall system system simply by improving the highle policy essentially by improving the high level policy to be able to kind of provide language instructions that can compensate for uh inaccuracies in the lowlevel policy and one thing that's really cool about that is that you could this means that um you can actually improve the overall system only with language supervision without collecting further demonstrations you can essentially update the language that the high level policy that the output of this is language so you can give it language annotations and labels um to improve this High LEL policy without actually having to collect any additional demonstrations with tele operation and so the method that we're going to be taking here essentially corresponds to dagger um on the high level policy and freezing the lowlevel policy if you're familiar with dagger um if you're not familiar with dagger uh no worries uh essentially what we're doing is uh as the robot is trying is executing a long Horizon task a human might intervene and correct the robot verbally with what it should do these language Corrections will override the high level policy prediction uh as you're running the the overall system and then you can find tune this high level policy on those interventions in order to say in this situation this is the correction that you should have provided to the lowlevel policy in order for it to succeed cool so we have this Frozen lowlevel policy and then we're updating the high Lev policy by supervising on these language Corrections um we gave this system a fun name uh yell at your robot or yay robot uh because we're simply providing these verbal corrections to the robot so um we can look at again some kind of qualitative results I think this is like really the coolest part to see in uh most robotics papers uh the base policy is a language condition policy and um it's it's the kind of the full system of a language condition lowle policy and a higher level policy that outputs these instructions that are shown in yellow and we see that the it can make mistakes um and it fairly rarely issues corrective commands and so if it's trying to pick up a Sharpie for example it might uh repeatedly fail uh and still be outputting this high level pick up the Sharpie command um and then after fine tuning with language Corrections we find that the policy is able to correct so in this case it fails at the insertion um and the high LEL policy is actually uh telling it to move towards the camera go higher which is kind of correcting the behavior of trying to Simply put it into the bag we also find that it's able to self correct for grasping where uh the high level policy uh tells itself to move to the right before picking up the St the Tape Holder and then also to try a different strategy for putting the sponge in the bag so right now it's kind of trying to uh kind of shove the sponge into the bag it eventually releases the sponge and pushes it into the bag um by poking it instead um and then overall uh by using these language Corrections and fine-tuning the policy on the language Corrections we see that the overall behavior is more successful at the full long Horizon task um this task is pretty long so I won't play the full video um but we can see that uh able to really kind of fully complete this task of putting the objects into the bag uh which is in kind of Robotics land a really really challenging task because it uh requires kind of executing a lot of things in sequence and involving objects that are really hard to model cool and then the other thing that we can do is we can actually improve performance on the Fly uh and the this is something that uh I think is actually really cool which is that uh I'm going to the video right here this is you can actually see that the robot's about to make a mistake here where it looks like it's about to pour the peanuts uh on the other side of the bag uh because the the scoop is not successfully into the bag we can actually preempt this uh by verbally interjecting and telling the robot to stop um and sorry let me play this again so um we're going to interject at that point and the the text on the top left will show how the person is interjecting so they're going to say move the arm to the left go higher and then move the scoop into the bag uh and then the robot will continue autonomously and we see that the person is able to kind of preempt the policy from failing and ultimately succeed at the task one other thing that's cool is that after fine-tuning we actually see that the policy doesn't just rely on the human Corrections on the Fly it'll actually self-correct in the same way where it was about to put the craisin on the other side of the bag then it moves higher and successfully put some into the bag cool um so those were some of the qualitative results um quantitatively again we also see uh in this case a 20% gain in performance just from using verbal Corrections uh which is pretty cool and it closes a lot of the gap between using kind of on the-fly human Corrections which are kind of an oracle shown in this lighter orange color um also worth mentioning that the low-l policies have a lot of room for improvement the kind of overall average success rate is around uh 55% uh which is a lot lower than 100% and the um and this can really just be explained by uh the room for improvement in these lowlevel policies cool um so the takeways for this part is that you can now productively yell at your robot uh and the robot can improve only from language feedback by fine-tuning the high Lev instruction policy without requiring any demonstrations um limitation here is this relies on a really performant instruction following policy as a l level policy and improving this will uh I think really improve the ability for um the high level policy to uh succeed uh and really follow through on the the kind of planning that it wants to do cool um yeah so that's it I'd like to really acknowledge Annie Alec Andy Goen and Lucy who led the works that I presented and uh yeah happy to take a question or two if there's time should come up here Dr F can you hear us yeah I can hear okay awesome hi professor Finn thank you so much um my question was like I noticed that you asked the BLM prompt um and for reasons that the plan might fail and I was wondering does this help the robot evaluate went wrong and then react dynamically like why do you ask it specifically why it might fail yeah that was not something that we like did detailed quantitative oblations on because the evaluations take quite a bit of time to run on the real robot uh but we did um kind of anecdotally find that uh asking it to Think Through the failures was helpful for it to um improve performance okay I think uh we are out of time so yeah let's thank uh Dr F again yeah okay uh next we are going to have the hour paper presentations so the M Workshop has selected three outstanding papers from three sub uh of the workshop for this session and first uh our first uh present our presentation is is by uh Scott Murray um special representations in um M model AI system and let's uh welcome uh Scott to the stage thank you my name is Scott Murray I'm a professor in the department of psychology over at the University of Washington uh my background is in uh visual cognitive Neuroscience so using brain Imaging to understand the visual system recently uh my research scientists and I uh Bridget Leonard and I have been looking at special representations in various multi AI systems today I'm just going to be talking about uh gp4 our approach is very similar to how we do experiments in humans we start off with some observations uh generate hypotheses and then do experiments um one observation that's relevant for today is that even though gp4 is brilliant at coming up with a complex description for a really simple image like this um rich in all kinds of spatial understanding like a person with a ball a barrier and a Target no problem good you right here oh yeah yeah cool so even though I was great at this we notice that if we turn the uh image upside down and included the prompt imagine rotating this image upside down what's going on does it recover any of that previous description does not uh the description is very different so it describes the upside down image as being a collection of geometric shapes and it's imagine rotated version of it still appearing as a a collection of intriguing geometric shapes so there OB observations like this that made us wonder whether gp4 was even capable of transforming spatial representations um we know from the developmental and um Developmental Psychology and Neuroscience literature that there's really two broad domains in which to consider spatial Transformations one is in an egocentric reference frame a classic example is mental rotation and the other is an allocentric reference frame a classic example being perspective taking so we did a number of experiments in both of these domains in terms of mental rotation um you have to be careful about how you do these experiments so for example if we give these two images to gb4 and ask for a comparison it gives nice long winded description where at the end it accurately concludes that these are two different views of the same room now even though you and I might make uh or use Mental rotation to do that comparison gp4 might just be generating a long list of linguistic labels and comparing that list to come to that conclusion so for our uh for our mental rotation experiments we turn to classic uh stimuli from cognitive psychology um that are more difficult to solve using linguistic labels and using these stimuli on the experiments that we describe in our poster demonstrate that gp4 does not behave in a way that's consistent with mental rotation in terms of allocentric reference frames we also don't see evidence for spatial transformation so if we go back to the room example and now prompt with if you walked into the room through the door with the plant be to the left or to the right of you for the image in panel a uh gp4 is wrong 90% of the time it responds left it's basically just staying in terms of the image information or a viewer centered reference rame in panel B there it's interesting because in terms of image uh information the door and the plant perfectly line up and appropriately or consistent with that gp4 is answering left and right 50/50 um so overall we don't see any evidence for gp4 being capable of uh transforming spatial representations one reason uh this might be the case is that language itself is a cumbersome format to make spatial comparisons so for example with these two images of objects that have very different 3D configurations superficially at the level linguistic labels they're kind of similar um they have you know three arms one of them's long one of them's short CC so it might not be surprising that gp4 had difficulty discriminating objects like this now as a neuroscientist I'm prone to make uh analogies to the brain so our visual system is comprised of two visual processing streams a vental processing stream that's important for perception and object recognition and a dorsal uh visual stream that's important for um guiding the motor system and what current multimod models seem to have done is glue together the computations that occur in the ventral visual stream with language what's currently missing are the computations that are occurring in the dorsal visual stream computations that are really important for um spatial reasoning and motor control so I think um an open question is in their current form in this sort of translation from pixels to words can multimodal models ever really be able to spatially reason I think there's potentially reasons answer yes maybe they just need to be trained on different kinds of data like videos and maybe spatial reasoning would fall out but I think there's important reasons to answer no and that's that language is a poor represent representational medium for spatial information Vision evolved to guide the motor system so maybe we need systems that interact with or simulate interacting with the world like robots um and the final point I want to make is um spatial reasoning itself is trans modal we often link it to the visual system but it can occur without vision for example in people who are blind it has Rich auditory and tactile inputs so ultimately um to make um progress in spatial reasoning multimodal models counterintuitively might have to separate um the domain of vision from the concept of spatial reasoning so I think that's my five minutes um and I look forward to talking more at our po our second presentation is by D Georgia who will discuss the Deep equilibrium of rmic Reasoner uh let's give him a warm welcome how do you go next and three left and right yes can okay okay can you can you hold the microphone okay sorry for uh slowing down a little bit so I want to tell you today about the a bit more about algorithmic reasoning for which I'm very thankful gave a very nice introduction to you and uh if T did not mention it so far in our reasoning you know for good out of this tradition generalization you need to have a good what we call algorithmic alignment it's like the direct similar to how in GD you have in variances and invariances here we have aloric alignment and particularly in our data we want to align the th3 that after several iterations you hit a termination of the algorithm this is going to be an equan further iterations of the algorithm let's say you have a function that visits the unvisited nodes here would make no change this is an equ and we want to know this with the your neuron Network by uh in an intrinsic manner by training it to find the equilibrium model now at 10 second shout out to the N Institute in monreal we are not the only one working on this and if you want to get different Viewpoint of uh this line of work feel free to check out their drop post now what we did in our data uh it's fair to say as we agreed with our collaborators was reinvent the equal wheel for the N car because the D models and finding points have been like longstanding research and zik who just the recently came had of C any like has a long list of sters feel free to check out their research but in essence like it's that simple the if F Theta is your neuron Network and the the terization you just this as a root finding and you can use any root finding Alm to find the zeros of of G of thet X and do find the equan point and uh against our regs I'm going to flash some dat at you uh we compared on four algorithms here and uh we saw that the performance with respect to the baselines including more exessive baselines is comparable and what SES do know is that on tasks such as insertion s we were able to even outperform the Baseline and those results are on the CRS attention that are introduced and the last result I prise is that when you see a secan finding you don't need to align to every iteration of the underlying algorithm and uh you can use fewer or sometimes even more iterations to find the solution that uh in most of the time this would also give you uh faster inference signs and in the case of insertion sort we found that our model is about logi the number of steps which is the theoretical lower down that you can have for a s for a sorting algorithm now I think my five minutes are also running out of time so if you want to hear more or if you want to point me to the titles I have in my poster or you want to argue with me about the Meed catch me at the poster session thank you um thank you so much D and our final our presentation is by J Ken who will discuss um um the autonomous evation requirements of digal Agents let welcome yeah oh hello everyone this is Jan I'm a first student working with I Berkeley so yeah today happy discuss about our R Recon our normal evaluation and refinement of digital agents yeah thank you yeah so uh how how can I make it full screen oh I I think it's full screen yeah yeah okay let's get started so by digital agents here we mean like models that are conditional natural language instructions to like browser or control the digital device to get things done for you and we care about such task for many reasons for example it's useful in our daily lives but more importantly like the vast open Digital World is also an exciting place to build general agent itself so we can just first look at how the community has been doing right so on the evaluation side I pre previous Works have been built some fantastic hand crafty benchmarks they are very useful but because of its hand craftiness so first they are hard to skill and secondly there will always be a mismatch between devel valuation and real world deployment while in our work we actually show that Vision language models are effective open-ended agent evaluators which enables us to transform any in the wild digital environments into a benchmark or training environment on the capability side driven by recent progress in Vision language models rent SAS are usually built by prompting or supervis fine tuning with human demonstration data well in this work we show actually a normous valuation in labl or normous agent Improvement we can we can significantly improve upon these base models through autonomous evaluation without any without any additional supervision so we can just I'll first describe how we like build this autonomous valuator then discuss about AG Improvement so here is a like typical trajectory we sample from web Rena so given a using instruction like tell me the code of my latest cancel order the agent will execute on the browser step by step and final derive at the final response and just like how humans will be how human judges will like judge the success of the response basically we just try to feed all the information into a model and ask for it judges uh we experiment with two approaches the first is a end to end evaluator basically we just feed all the information into a pretty Advanced v v language models like gp4 vision and as for judges and it turns out to be pretty effective or if you prefer open weight solution we find like in the open weight domain like if you first transform the visual information like the screenshots into captions first and then do the reasoning in a text only language model usually use better performance yeah and here are some numeric results of evaluator so we tested tested on two domains web Arena and Android so on the web Reena domain so our evalu usually achieves around 80% agreement with web Arena's handcrafted evaluation functions well on the Android domain our evaluators achieves around 90% agreement with human judges so these numbers as we show in the paper basically illustrate like the evaluation is effective and next we'll talk about how this autonomous evaluators enables like agent in refinement during both inference and training time without interal supervision so to demonstrate inference time refinement we choose a very Str follow algorithm called reflection so here is a like typical agent execution and we first after the agent finishes execution we feed this trajectory every all the information into the evaluator and like in this case like the evalua thing it fails and so and then we trigger the reflection algorithm which is basically prompts agent say hey hey you made made a fiure reflect on this fiure and try again so in this case the agent tries again and deres at a new response so we feed it again to the evaluator and if the evaluator think like like in this case like it succeeds we will use this as a final response instead so it's what mentioning like we do not rely on external evaluators so there's no no external supervision at all so we tested it on the challenging web Benchmark so the Baseline is the previous s the lgb4 based agent which has around 60% success rate and you can just look through the graph so basically all Valu improves upon that Baseline with the best gb4 Vision based model improved by around 30% uh we also tested with training time refinement so I won't we probably won't have time to go into the detail here but the short summary is like we tested a reinforcement learning like algorithm called filter BC with our evaluator as a re reward function so we train we find or we Post train agent on both Android and environment and we find it basically use around 70% relative improvement over base models with our a normous evaluator and that basically ends my presentation today so we hope to convey you with two messages first like Vis language models are effective agent evaluators and secondly a normous evaluation enable significant Aon normous agent Improvement and yeah I'll be WR and happy to discuss more in the poster session thank you good just just wait okay good okay first a short announcement and then uh I'll introduce the speaker so Professor Tom Griffiths was supposed to be speaker today and unfortunately uh he's a little unwell and is recovering and in his place we have uh uh Dr Mark ho uh who is an incoming assistant professor in the department of psychology and New York New York University and is currently faculty at uh Stevens Institute of Technology he directs the computation and decision-making lab there and uh he was actually a post with Tom Griffiths uh at UC Berkeley and Princeton um where he received his PhD in cognitive science um so research in Mark's uh research group uh combines approaches from cognitive science social psychology and computer science to study the general computational principles that underp human problem solving and social cognition so let's please welcome profess Professor to um Mark please go ahead oh oh you can't hear can't hear right great can you hear me awesome great uh yeah thanks so much for um inviting me to talk um yeah Tom was feeling sick so uh has asked me to present in his place and uh I'll be presenting a bunch of work uh that we did together so hopefully it uh will be related to the original version of The Talk um but uh yeah so uh today I'm be talking about abstraction in humans and machines but I'm really going to be talking more about uh abstraction in humans first and then we'll kind of circle back around uh and talk about abstraction in machines um and uh the kind of overall context in which I want to think about abstraction in human cognition uh is just this observation that you know when we look at human cognition it's very general and very flexible and that's particularly in contrast to say other uh other an animals um other species for example we know that other animals can do very sophisticated behaviors like use tools and can you know do things like cooperate um and traditional AI systems up until very recently were also uh uh largely very specialized so for example alao is very good at playing go uh you know Google translate up until pretty recently was uh only able to really do translation um but you couldn't get Google Translate to play go or get go get a go playing algorithm to uh Translate um but uh humans are are kind of remarkable um and when we talk about the general purpose uh capacity of humans what we kind of mean is that people can do all of these kinds of tasks they can learn to do all these tasks uh they can switch between them and uh the set of tasks that I've listed here is ultimately a very small uh set of um the possible tasks that a person could engage in um and uh you know I think the the a big focus of this uh Workshop is thinking about okay how do we what are the uh basic elements of general purpose cognition in particular what is the role of multimodal representations and how how are kind of uh the needs of multimodal representation um shape oh you can't see the slides um okay uh let me know if you need me to do anything on my end I can't hear you can you hear me please go ahead yes okay um yeah and so uh right so you know in trying to unpack and understand what makes general purpose cognition possible in humans uh we really have to look at a lot of different uh processes at play including multimol representation um uh but I'm going to talk about abstraction and representation um from a particular perspective today uh in my talk and in particular thinking about what is the interaction between abstractions and planning in human cognition and human problem solving um and planning is really important because it really underpins uh all of this capacity for flexibility uh flexible problem solving and flexibil reacting to contingencies in the environment um and uh in particular planning you know is often characterized in Psychology literature in contrast to say habitual behaviors um so for example you can imagine that you drive to work every day um and typically you take a particular route to get to work and maybe you even memorize that route you don't really think about it much when you're when you're driving but uh the world is not static the world changes constantly and perhaps you know one day you're driving and the road that usually take is closed um and so rather than just being stuck and and not knowing KN what to do you form a mental map of uh the town you're in kind of bring bring you know details of the town that you weren't thinking about to mind uh and uh start you know planning in that representation and thinking about okay well since the road is closed the road I usually take is closed what roads are probably open how can I navigate along those roads to get to a destination that I want to get to um and so clearly planning is very useful it's a very important part of how we uh deal uh with uh you know everyday problems um and it allows us to adapt our Behavior to to changes in the environment and and novel contingencies um but at the same time planning is also a very computationally complex and uh difficult problem and in particular uh the the difficulty of planning from a computational perspective is is really summarized in the so-called P of dimensionality uh which you know in in this case uh basically says that as you you know search further and F further into the future uh uh the space of possible sequences of actions that you could take grows very quickly um and so the usefulness of planning is always uh kind of balanced against its computational difficulty and so a big question from a from the question of how do we uh you know understand human cognition how do we understand human psychology and also how do we reverse engineer systems that uh think and act like humans is how do we how do humans organize and access knowledge in a way that allows them to efficiently plan how does how do how do we to the extent possible overcome uh the curs dimensionality to get uh usefulness out of planning um and uh within psychology there's obviously been a lot of work thinking about all the different ways in which people uh you know different different mental strategies people use to make planning efficient so people uh will you know not even rely on planning they'll rely on as I mentioned use model free computations uh you know re-encode the state in a particular way use predictive state encodings things like the successor representation um and so there you know there are many different strategies that people use to make planning efficient chunking actions uh being selective about computations using puristic sub goals um and uh constructing simplified models of problems and so this last one is what I'm going to focus on uh for most of my talk now um so this question of how do we uh think about representations in planning and abstractions in planning um I want to contrast uh I want to start by kind of outlining the what you might think of as the classic Model of planning how people uh in Ai and in cognitive science often think about uh the process of planning and the typic and the typical model of planning involves you know you're engaging with a task in the world um and you have a kind of default you kind of have a a representation of that task that's uh typically fixed so for example if you're trying to plan a move in a game of chess you you have a representation of the game of chess of how pieces move um you know uh the goal of trying to capture your opponent's king um and so on and then given that representation of the task you perform some kind of computation over that representation you plan you do research um and simulate possible sequences of actions that you take and then choose the best one um the alternative model or kind of uh augmentation of this classic Model of planning that we explored in some recent work was uh to introduce this notion that uh people are not necessarily planning in a fixed representation of a problem um they're not necessarily planning in the the kind of given representation of a problem uh for example in chess um rather they're forming what you might think of as a as as an ad hoc or temporary abstraction or construal of a problem they're construing the problem in a particular way and then performing uh these kinds of planning computations over that uh representation and so to kind of illustrate this uh or to kind of give an intuition about how this works formally we can think of this process of uh construing or forming a representation of a problem as a kind of meta reasoning or or meta choice of how do I how do I choose a good representation of a problem in order to plan within it and that choice is really going to trade off two two key things so one is how useful is the plan that I get out of that representation and the other is what is the cost of the representation um so I'll walk through this uh kind of intuitively right now um and we can think of the the value of a particular representation or construal or abstraction of a task as uh uh you know we're going to represent it as a kind of value function or utility function over different possible conrols given a task and so I want you to imagine that you know you're you're riding a bike for example and you're in a town and you're thinking about okay how do I get from some start location to some goal location um and you know uh you would you know plan and then take a path but uh if you think about it you would never really use this kind of representation the actual map or this kind of actual photographic representation of a problem to do planning in um because there's just too many details to actually uh reason about and represent and so it's very costly even though if you even if it if it were possible to use this kind of representation the plan that you get out of it would be very good um and so uh from this perspective if we think about the value of a representation as really having the two components one is uh the utility of the plan that you get out of it and the other is the cognitive cost this photographic highly detailed representation uh it could give you a very good plan if you were to plan within it but it would come at this High cognitive cost it' be very costly to actually represent this uh this this amount of detail um and so you know you could have a another kind of representation of a problem a simpler representation so for example this is a representation of the same uh part of town except uh you're only thinking about the roads um and if you're given this representation you would uh plan within that representation and get out an optimal plan that looks something like this and it would tell you to take the sequence of actions to get from your goal to your destination um but the problem is that this even though this is a nice kind of a good simple representation that uh really removes a lot of the details uh the the plan itself is not as good as it could be the rather if the that you take here kind of takes you along a different road to get to your goal rather than takes you along you know the most efficient path possible um and so this kind of uh is is a good cognitive cost but has a bad low behavioral utility um so what you really want is a representation that both uh is cognitively uh efficient cognitively cheap and also um behaviorally useful has high behavioral utility so for example if you're on a bike you would want to think about uh the bike routes because that kind of gives you the affordances in the environment that are most useful for getting through uh the environment to your goal been thinking about to get through the environment to your goal and so uh we can kind of think about again this this trade-off in this space of different abstractions uh and refinements of of a situation where if you have a very highly refined detailed representation of a problem you might be able to get a good uh a good plan out of it at the at high cognitive cost um but also if you and then alternatively you could have a simplified representation of a problem but it might be the wrong simplification which would give you uh poor utility but uh good cost um but what you really want is something that optimizes both of these terms um and so the the kind of key idea behind this account is that uh people when they are forming abstractions they are uh balancing this behavioral utility and the cognitive cost in a way that is optimal or near optimal um and so to study this in humans we uh developed a simple task that is similar to the navigation task navigation situation I talked about where uh you know people are given a maze with with a bunch of obstacles in it and need to navigate from some start location to a goal location um and what we're interested in is what is the underlying abstraction representation that people are forming of the problem so for example uh rather than you know representing all the objects in a grid they are representing some subset of the objects in a grid like this on the right and so uh by casting this as a kind of optimal Choice problem um or as a decision problem with a value function and so on uh we can ask well okay what is the optimal conrol in a given scenario and so uh we can we can calculate for these mazes what the optimal conrol would be um and then compare those to a human responses and what we find is that people uh there is a a nice qualitative correspondence between what people uh respond paying attention to and what their uh what the model or what our account of kind of optimal conal optimal abstraction uh says and so the I'll so this is kind of a qualitative comparison I'll just show you very quickly uh the breaking it down by the objects that are predicted to be ignored by the account versus uh human responses and what we can see is that things that are predicted to be ignored have a people report not paying attention to whereas things that people that the uh account predicts people would pay attention to um people report having high attention to awareness of um so uh yeah so this notion of optimal construal uh is uh it's it's one way to kind of think about the relationship between representations and uh or representations and abstractions and computations like planning or reasoning computations like planning and uh we can also kind of revisit and think about uh other other kind of puzzling phenomena in in human cognition and uh human uh problem solving and how we seem to represent or abstract problems um so for example uh there's an old set of results in psychology on what's called functional fixedness and if you're not familiar with this idea of functional fixedness um the kind of original kind of canonical experiment for this was done by dunker in 1945 and the basic idea is you you bring in people into a lab and you tell them okay here's a table there is a candle on the table a book of matches and a box of tags and your goal is to mount the candle on the wall and light it and when you give this to people um they try a bunch of things they do things like try to melt the candle to the wall they try to attack the candle to the wall but none of these work given the kind of causal properties of the candle and box Texs the candle crumbles and so on um what you're really supposed to do in this situation is empty out the box of TXS tack the Box to the wall put the candle in the box and light the candle um and what's what's kind of funny and puzzling about this is that when you tell people who've been trying to solve the problem for a while that this is the solution they often kind of throw their hands up and say wow I should have thought of that before that's so obvious so obvious in retrospect so they're not really learning something new about a situation rather they uh they seem to have thought about the wrong things in the situation and one way to think about this is that they're using the wrong task representation um of the problem they're kind of thinking about things they're forming abstract representations of things but in ways that are not correct for the situation uh in particular they seem to be construing the Box the box of tax as a container of the tax and not as a support uh for uh for the objects or for the candle um and so to to get at this idea uh using computational tools and kind of uh understand what's going on From computational perspective we developed a version of this grid navigation task uh uh based on the idea that we can form different kinds of abstractions of the same object in a very flexible way uh so for example if you imagine that you're at the airport and there are a bunch of terminals you could think about each terminal at a very high level abstraction just as this kind of Big Blob out there in the world um that you maybe have to take a shuttle bus to and from um or you can zoom in on any particular terminal and think about uh you know very more specific more detailed uh parts of it for example you know where the bathroom is where uh the food court is where your gate is and so on um so we designed a uh maze one of these uh simple tasks that have this structure to it where uh again you're blocked off by certain objects in particular these 3x3 blocks here and the and the Black Walls there um but the blocks also have a kind of more fine grain more detailed structure they have notches in them that you can walk over um if if you want and so uh this kind of having multiple levels of abstraction of the same uh scenario uh leads to potentially different behavior um and different possibilities uh so for example in this uh example in this maze on the left um you actually don't need to think about The Notches to solve the problem if you only think about each object as a block as a 3X3 block you're able to figure out oh I need to navigate over here to the right and go up um but now imagine that you moveed this block up here so the block is uh blocking this path well if you only form of blocks only can stroll over this maze then there's no way to solve it there's no way to come up with a plan to to figure out how to get to the goal and so what you really have to do is for of blocks and notches conule of the maze you don't necessarily have to pay attention to all the notches in this maze but you have to pay attention to at least one of them to figure out how to get to uh the goal um and so we ran a pretty simple experiment just trying to understand the Dynamics of this uh abstraction process in people and we find and so what we did was we we put people into one of two conditions so in one condition we had them do a bunch of these uh mazes that uh where you had to go through a notch to get to the goal these Notch necessary Maes and so those necessitated uh forming an abstraction that involves both blocks and notches versus uh giving people a series of what are called Notch unnecessary mazes where you didn't have to go through a notch to get to the goal get to the goal um and you could only think about the the maze in terms of blocks and notches um and just to be clear both groups of people given these sequences of Mazes these different sequences of Mazes they were they were introduced to the concept of the notches and the blocks they knew about them uh they even did some kind of very early trials where they had to use the notches uh so they're coming into these situations with very diff with with very similar if not the same uh kind of knowledge of the of of how the world works but we're interested in whether uh they end up using different abstractions and so in this test maze uh we gave them they or this series of test mazes they had this structure where if you formed the blocks and notches uh or if you blocks and notches can strle of the situation and paid attention to the notches you would realize that oh you can go through a notch to get to the goal more quickly uh than not and if you formed the blocks only constru you would uh only think to go uh the long route to get to the goal and so there's an actual behavioral cost to uh using a an abstra the wrong abstraction of a problem so we were interested in whether as a function of uh this uh exposure to notch necessary versus Notch unnecessary problems people would be more or less likely to take these different routes and what we find uh was that when they were given the notch unnecessary uh sequence they were much less likely to uh go through the notch indicating that people are really kind of uh really kind of locking into the much more the simpler representation of the problem only in terms of blocks and that there's a kind of stickiness to it that prevents them from solving the problem uh in the most efficient way and that this actually persists over time um and uh even by the end some people are um not taking the optimal route uh so from a computational perspective trying to understand what's going on here uh we can uh there's this interesting kind of dynamic between between uh action planning and representation or abstraction that's going on and the way we approach this and try to explain what's going on is to say that uh people are kind of coming into the task with different kind of abstraction mindsets or abstraction strategies that they're using and so for example uh if on some initial maze they're only thinking about the blocks then over time they uh are you know continuing to think about the blocks there's a kind of stickiness from problem to problem um until they get uh to the end and kind of realize that oh there's a block maybe I can take a shortcut to get through this and so uh the approach that we take here is to really think about okay what are these like latent construal or latent abstraction strategy dynamics that people are using in this kind of uh sequencing of of problems um and we can think of this again as a dynamical system where there are kind of two mindsets or abstraction strategies that a person could be using uh one is only blocks the other is only notches and the Dynamics here are governed by different costs so one cost is that switching is costly it's easier to stay within a particular abstraction than to switch abstractions the other is that complexity is costly blocks are simpler than blocks and notches because to think about The Notches you have to think about the blocks um and so uh we were able to show that this these uh the these costs these kind cognitive costs on the representation on the abstraction strategy uh end up explaining uh behavior um uh uh and you really need both costs to explain what's going on um and a kind of side side uh result we found here was that uh uh reaction time is actually predicted very strongly by uh the complexity cost so people seem to be really uh you know taking so they take longer to plan when they have to think about more objects or more details of a problem which is consistent with this idea that uh it's a costly like representation is costly and that uh you the human brain has to kind of manage those costs in an effective way when problem solving okay so um just to wrap up uh how this you know some of these ideas around uh general purpose cognition in humans and problem solving kind of relate to uh you know some of the themes of this Workshop um uh as I started up I tal started talking out um you know when we think about human cognition it's it's General it's flexible but it's also very efficient compared to a lot of um AI systems and um there are many factors that could be shaping the processes of abstractions and type of abstractions that people form and I think we need to uh really consider all of these when uh trying to understand how how how are abstractions in humans going to be relating to uh abstractions in machines um and so for example a very common uh way of thinking about abstraction um and that I think is really nicely summarized and demonstrated in this in this recent paper by uh Lake and and waking bong um is uh that multimodal learning just the data itself trained on uh uh very general uh tabasa uh representations can actually pick up on you know abstract representations um so training on both text and language uh or sorry images and text uh actually gets you quite far in explaining uh human data but not all the way um I would like to Pro or argue that uh really a very important factor in the types of abstractions that people are forming is this need to efficiently reason and plan and control and meta reason about uh problems in a way that uh is adaptive but also reliable and so that's kind of the the theme of of some of the work I talked about today um and another uh Dimension to thinking about abstractions in humans that doesn't necessarily uh come out of the multimodal thinking about things from purely multimodal perspective is that a lot of human abstractions are the result of cultural Evolution and are kind of designed for cultural uh cultural sharing and uh interpersonal interactions um so for example uh it's work by uh Bill Thompson and some folks from uh Tom Tom's lab looking at uh how do different uh algorithms evolve in Social uh cultural processes uh where people are kind of uh organized into different generations of Learners who learn from one another um over time and I kind of key finding from this is that it's not necessarily the most efficient algorithm that wins out over time but uh the one that also can be transmitted uh most effectively and most easily and is kind of robust to uh errors in transmission um and so in thinking about uh the kind of abstractions that are relevant for machines uh it's it I I we should you know be thinking about well if we want them to be humanlike uh we should also be thinking about how do humans what what what are the actual factors that shape human abstraction and uh you know in machines we typically focus on you multimodality and kind of the nature of the data set but also uh efficiency could be very important the kind of efficiency of the representations as they're used in algorithmic processes um and then also thinking about the kind of cultural or social Dimension to abstractions which is their interpretability whether or not they're aligned with human uh cognitive representations and then um also their explainability so I want to end by thanking my collaborators on the work I presented today Tom griffits johnan Cohen Michael Litman David able and Carlos kah um and thank you all for listening and happy to answer any questions if we have time uh thank you Mark for that excellent talk and you know special thanks to you for for stepping in with such a short notice as well uh we have time for maybe one question if anybody wants to come up and uh ask Mark wait hi uh can you hear me yes yeah yeah great talk um I was wondering it seems like humans um might change our construals of a of a task over time during the task uh for example the Bic route you know where you are in the route depends how you can through the task have you considered that in your research yeah so we're starting to look at that um if I'm understanding your question correctly uh yeah as as we are uh doing a t Okay so I guess there are two ways to interpret it one is as we are doing the task maybe we're even adapting our representation within Tas right okay so yeah that's definitely something that uh we're looking at and uh in in the some of the papers I presented we actually do have some modeling that looks at the Dynamics of construal overtime and how you can think about you know there's there's Better or Worse ways to uh kind of stage the representation over time uh and kind of integrate that with planning um but I think there's a lot more work to do there in particular thinking about um abstractions and uh how the organization of abstractions can lend themselves to easier or harder uh uh uh kind of modification over time so for example uh you know if if you're navigating to a if you're trying to get to the airport you'll initially think about the airport in a very abstract way but then as you approach the airport you'll start thinking about it in more and more detail and uh there is very likely a uh kind of more General principle at work there which is that we need to organize our semantic representations of things in ways that allow us to kind of zoom in and out very flexibly um to support you know long-term planning for example uh but yeah it's a great question yeah there's more work to do there excellent thank you let's thank the speaker once again thank you Mark thank you okay a brief announce uh we have a very short 10 minute break now followed by two more exciting Keynotes by lean Wong U who's going to talk about the recent advances in multimodel Foundation models and also uh emelan dupon who will talk about uh new dis how to do mathematical discoveries from uh program search using large language models so yeah see you in 10 minutes both those kotar will be in person so so the Keynotes will start at 1035 how are you glad you're here can I actually pronounce your name around so the nice to meet you so so I'll I'll switch over uh so just need to connect this HTM do you want a charge charging cable or I do have a are e e you May on the video Yeah hav tried yeah3 wait for a few more minutes yes you access something B yeah it should be possible conne so yeah also something want that's PR SC yeah so hello everyone uh welcome back after the break and I hope you're having having a good time at the workshop here today uh so next up it is my great pleasure to introduce Dr Lian Wong uh she currently serves as a principal research manager uh leading her uh own team within Microsoft geni uh after earning her PhD from chinga University uh Lian began her tenure at Microsoft research Asia in 2006 and later joined Microsoft research in Redmont in 2016 her research primarily focuses on multimodal understanding and generation encompassing a wide range of areas from 3D Talking Heads to Vision language pre-training Vision Foundation models and image video generation as a pivotal contributor in Vision language pre-training image captioning and object detection her work has been integral to the development of various Microsoft products including cognitive services and Office 365 she's a senior member of the it E and has published over 100 papers in top tier conferences and journals and she also holds more than 20 us patents that have either been granted or authentic with that let's welcome Dr liim uh hello everyone so it's a privilege uh to presting here I'm Legion W so uh I'm working uh I'm currently working on the multimodel generative model at Microsoft so today I would like to share um my understanding on how the vision Foundation models get involve over the past few years I want to talk about three models here the first one is the clip model the second one is the large uh multimodel model lmm the third one is the diffusion model I think the fundamental question here we want to ask is how to extract intelligence from visual world so when we look back a clip represent a paradigm shift when it was introduced in 2021 FIP extract the information at intersection between image and text and training both image encoder and tax encoder together the past year has been marked as the rise of large multimodal model so large multimodal model extends large language model uh with the sensory skill such as visual understanding so that it can achieve stronger or generic intelligence also we want you to pay close attention to diffusion model not only not o not only from the perspective of image and video generation but also um view it as a vision vision centered more Vision centered representation learner I'm sure a lot of you are already familiar with clip so clip was a big paradigm shift at the time it came out because rather than needing handcraft labels to train a good classifier which is um time consuming and painful to do um now we can leverage free form text um from the web to learn a model that is simultaneously um a good classifier for all domains so clip has proven to be an efficient learner to extract information from image tax paays the past year was the year of a large multimodel model here I want to make connection between the large multi model today with the early Vision language models early Vision language models serve as the Prelude of the large multimodel model these slides basically show you the evolution of early Vision language models from uniter proposed in 2019 to Flamingo Koka and git which are proposed currently in May 2022 first I want to uh take git as a case study for the image to Tex generative model which takes image as an input and output is a Tex sequence most of today's lmm model variants share a very similar model architecture and a training objective in terms of the model architecture the model typically consist of an image encoder to extract the visual features and the language model to decode the text sequence the vision and the language modalities can be potentially connected by trainable connection modules the image encoder and the language model can be either trained from scratch or initialized from pre-trained Models training objective typically employs Auto regressive loss on output text tokens for attention map in the trans Transformers image token can attend to each other and the current tech text tokens attend to all image tokens and all the previous Tech tokens Flamingo is another example to train L lmm with interleaved image text sequence so it connects the Frozen pre-trained image encoder and the language encoder by adding novel architecture components in between flaming is trained on mixture of complimentary large scale multimodal data coming only from the web without any data annotated for machine learning purpose after the training is done Flamingo can be directly adapt to Vision task without simple future learning without any additional task for specific fine tune now we come to the year of 2023 open ey released d before and the Eun of large language large multimodel starts from the gbt 4B model in the paper the D of lmm we anal analyze the gbt VV model to deepen of understanding of the lmm open ey acknowledged that the D lmm preliminary Exploration with gbt 4V by our colleagues at Microsoft covers a plethora of practical observations and strategies for using gb4 V openi has included this paper in the gb4 v body of work in our exploration of tb4 V our Focus has been using qualitative results to offer a snapshot of GPT forb new capabilities and potential emerging new user cases our objective here is to un cover and preview what GT4 V might already been capable of even though these novel capabilities may not yet be fully reliable from our study we see that gbt 4B had significantly Advanced the state of Art in areas that have baffling computer vision Community including zero shot open vocabulary task unification free form image text interl input and in context learning here I would like to highlight a few scenarios and new user cases that can be potentially enabled by the emerging capabilities of this model the first one is the visual pointing we know that the pointing is one of the earliest hand gesture a baby learns for communication we found that GPT 4V can well understand the visual pointers that d ly drawn on the image based on this observation we propos a novel model interaction method we call it visual pointing the core idea is to directly added image pixel space to draw visual visual pointers or syn text as human referring instructions so as the example show here gbt 4V can comprehend the question and accurately respond that the blue arrow is pointing to the running shoe so it intuitively links objects indicated by the arrow to its respective objects also other emerging capabilities like a coding V with vision so GT4 V can generate a python code to reproduce a line chart we also find the GUI navigation is quite interesting gb4 V can interact with and navigate through the graphical user interface of a computer or smartphone in the paper we also explore the potential of d4v to complete complex task such as web browsing online shopping message uh responding to Notifications it can also handle like incoming call and message notifications to a computer screen effectively also it shows potential for embodied AI to actively engage with Dynamic physical environment to paint a clearer picture so imagine gb4 V is powering a home robot in this setting the AI can interpret Appliance operating instructions and also perform task oriented navigation through the house in the months following the proposal of this model we observe a surge of instruction to the open source lmm emerging as a new line of research here I I want to use lava as an example offens Source lmm example a similar idea is also proposed in its current work min4 the network architecture of lava is very similar to the network introduced in git lava connects the pre-trained click Model A Clip visual encoder and a large language model vuna and also add a simple projection Matrix which is a linear projection layer it has two- stage instruction tuning procedure here in stage one the pre-training for the future alignment so only the projection Matrix is updated in the second stage fine-tuning end to end both the projection Matrix and the language model are updated today the landscape of lmm can be well tracked on the lmm leaderboard as of last week there are 30 open- source lmm and the 21 Preparatory lmm listed on the mmm U Leaderboard on the mm V leaderboard we also observe rapid progress daily and monthly with the latest dig gbt 40 reaching a total score of 69.7 Open Source models are also catching up quickly can achieve a score of 62.8 here um I want to talk a little bit about the io modality of this new model gbt 40 so well large multimodel extends the large language model by adding visual modal ity it's natural to further extend it to go beyond just vision and language so gbd4 is a step forward uh moving toward a much more natural human computer interaction it can adapt inputs in any combination of text audio image video and it can also output combination of text audio and image so it would be interesting to investigate what additional capabilities for a model combining all these modality together or what can be go beyond the current capability it also need we we want to understand how to make the native inte integration more efficient and also let different modalities can enhance each other lmm also have also inspired many new research areas such as visual prompting with its representative work on S so and also multimodal agent multimodal agents for video understanding for generating um automatic generating audio descriptions for go navigation as we can see in the paper mmbd mm narrator mm Navigator as well as for visual design and Creation with feedback next we will talk about the diffusion model if we see lmm are adding image condition to language generation diffusion model are the opposite is more like adding text condition to image generation so we see that diffusion model are important to Vision Foundation models not only from the perspective it's uh it's for image generation and audio and video generation but also it can serve as a more Vision centered representation learner a paper came out a while ago called your diffusion model is secretly a zero shot classifier the basic idea is here the basic idea here is even you are modeling the distribution of image given text the model can be converted into classification model this approach is somewhat similar to clip given image and a candidate caption you can use the diffusion model to computer score for how well the image matches the caption while this method is more computational expensive than clip it provides a compatibility and similarity score between the image and the candidate caption the paper also demonstrate that stable diffusion can achieve good image prop which is a surprising result in SE in September 2023 di3 was released by open eye modern tech 2 image system often ignore certain words or descriptions forcing user to learn prompt engineering d three represent significant Leap Forward in generating images that precisely aere to the provided text I'm very proud to part of the di3 team working closely with open ey to develop this model the key takeaway here from di three is that training on alra descriptive captions make the diffusion model more computer efficient even when measuring the quality of samples produced with shorter captions here I would like to quote a details words here the observation also suggest that we can get a better unconditioned model by using Ultra descriptive captions as scar folding even if we don't use alra descriptive captions at inference time in the past year we also observed diffusion model has made significant advancement in video generation progressing from low resolution to high resolution from generating 4 second clips to one minute long video with character identity preservation much better coherence and also 3D consistency the most important message I think from the Sora team is that their results suggest scaling video generation model is a promising path towards building general purpose simulators of the physical world I personally am a strong believer uh in this direction so to summarize here today we discuss about three models which I think are representative of the recent advances in Vision Foundation models clip learns an image encoder and text encoder together large multimodels connects image encoder with a text decoder it also called a gener a language generator well diffusion model consist a text encoder plus an image diffuser so there are open question open discussions for the word model so we are not sure which model will ultimately lead us to the perfect word model but we're pretty sure it will require a lot of innovation from everyone here so yeah that's all of my talk thank you um we have time for a few questions and please come up to the podium to ask if that's possible do you want to come up and ask yeah you can you can ask there and okay yeah sure first thank you for the um so looking at GPD 40 and Gemini one 1.5 the the the capabilities in as llms are LMS is a really outstanding where do you see the Gap today like what what do you think is the their main Gap to like a general understanding of of vision and multim multimodality I mean where do you see the pit fors today you can repeat the question POS so I think the uh let me try to repeat a question so um you see uh you say that the the recent um um model from openi and also from Google actually show very impressive uh results so uh the question is that what is the p uh from the current model to the a ultimate general purpose um multimodel model yeah that's a very good question I think the it's a very open I think this is very open question as well I I don't see that we have good answer to say that the uh large large multi model is the will be the the right direction lead us toward the world model because I think uh eventually we need to solve the word model problem so still um um my view is the large multi model is still a very language centered model so it do have limitation like uh with in this Workshop we discuss all the abstraction abstraction images and also like when we uh analyze uh for example gbt 4V when we prompt the the model try try to read the the clock the an analog clock it's very difficult so um I don't see that the model can solve many uh real world still uh many real world problems are unsolved so um uh that's why I I I so so I think the um it's very open so I don't I I I I don't think that today's uh large multi model will finally lead us to the word model so we we need Innovation here um yeah yes so I agree with the have seen models but I do have a question is like how do I mean given that these models are being train on billions and billions of images or like like how do we make sure that we are not ping ourself like in ter of like all these benchmarks that we see like how do we make sure that we are not actually testing on training set in such a way right like how do we make sure we TR time that's very similar to the things we test any thoughts um I'm trying to reference your question so um uh the question is that the model right now is training on a very large scale um data set so billions of images and text from the internet how can we make sure that when we do the evaluation we don't have any leakage uh from the training data right so to uh the those benchmarks so for those benchmarks how how how can we make sure that we truly evaluate the quality of the model so I think that's very very good uh very good question so I think the how to evaluating those new models actually right now I see it becomes a very uh active research area so we do need to people to propose and uh develop uh new Benchmark new evaluation because they also have new capabilities which on which that the current um Benchmark cannot evaluate um for example when we work on the paper the down of lmm uh we try to make sure that all our examples all our samples are um absolutely not included in the training data we use a lot of of photos on our own capture photos uh or the or like the images uh after like April 2023 to make sure that um there is no no chance that those images have been seen in the training data but uh still I I I I believe that um now we have the 40 right 40 uh have more modalities inte integrate together it definitely need a new Benchmark new evaluation metrics and um yeah I I I think uh yeah people are working it right now so I I don't worry about that so any other question now don't IDE and the futur then we combine basic D and um yeah um let me try to rephrase okay so I think the question is that um do we see that Sora like model can um learn the physical law or any physic true physics uh be learned uh in the model and yeah so that's a very good question so I think the um uh OPI sent a message they they believe in that direction but does that not mean that today their model can do that so uh when people look at the current Sor results when it's tried to do 3D Reconstruction from the video actually the the quality is very poor that means right now the 3D consistency in that model still very po um but U it's just point us to a path that maybe maybe when you when we scaling up this video generation model that's a way for us to learn the world simulator to learn the physical word because we need a way because if we ask the question whether the larger language model is the way to learn the simulator uh versus Sora this type of video generation to learn the word simulator which way do you think is uh the more Pro promising way right I think that's a that's a open uh very open question so yeah about how the Capt the mmic consistency will like that's like first and like that but there are lot of consist like this person to something yeah so um let me try to refas question so how do we see that uh Sora like U video generation model can Ultima learn the multi-layer logic consistency right yeah so I think that again I think the I feel that the the world model really uh have many um share many com common feelings so people want to see that uh how we can have a model can truly learn um this kind of physical consistency logical consistency everything right I think that's the world model so I think um it's a video generation I I think currently it's a promising path like uh imagine if we can um given a image if we can predict anticipate what will happen next with this static image that basically answer your question so like um at a at a one level you you say that people will which lag the people will move in next right and uh also it will answer that maybe um the ball will bounce the floor and and then what and then what right but there are many things were happening when you try to uh Force the model to predict the next next frame or video so yeah I see that it's a promising direction to learn the world model question so it seems to me that shortterm logic consistency is a little longm consisten oh that's a very challenging question so I I uh I I think that try to rephrase the question is that um the video generation currently we can try to model the relatively shortterm right predicted the what what is happening next but what about the long-term planning long-term logic um um yeah that's a challenging question I don't have a good answer honestly so I I see I see that the all all the future directions do need all researchers in this room and also in cvpr to to think about and uh to solve it together um so in the interest of time uh we'll thank Dr Wong right now and then move on to the next speaker thank you yes thank Youk e e e e e e e e e including online bin packing um which is kind of a root node problem for lots of Downstream task like scheduling jobs and clusters uh and everything the vo is okay or we're still figuring out the online sorry um now it's okay okay cool cool yeah um yeah so um kind of at a super super high level what fun search does is it combines a large language model with an evaluator um and so so uh a lot of problems kind of allow for fast evaluation you can kind of check if a solution is a solution quite fast but actually finding this solution can be pretty difficult and so what fund search does is we combine an LM that tries to generate loads of solutions with an evaluator that tries to guard against incorrect Solutions so LMS you know famously hallucinate and say all kinds of crazy stuff but if we have a formal evaluator that can check if something is correct or not or how good it is then those two can kind of work in tandem to to improve things and so what we have is some kind of iterative improvement Loop where like an LM tries something an evaluator tells it whether it's good or not and then they kind of continue working together in that way and so at a at a super high level um get a pointer here um you kind of give as input an evaluate function that evaluates a solution for a certain problem you care about and then you have this fun search Loop that does loads of things and then eventually uh a solve uh or a solution comes out and that solution hopefully uh good and useful for your problem um so this is obviously all like very very high level um but uh I think the easiest way to illustrate it is through an example because it's actually very simple when you just have something Concrete in mind um so the example that I was thinking of focusing on is this caps set problem which is one of the mathematical problems we try to tackle um and it's quite simple to explain uh so the problem is just to find the largest set of points on a 3 to the D grid such that no three points are in line um so for example in in 1D a 3D grid is the three grid is just three points in a line in 2D it's a 3X3 Grid in 3D is a 3x3x3 grid and so on and within this grid you try to place as many points as you can and you want to make sure that no three of them are in the line um and you know that can sound like a very like toyish problem but it's actually surprisingly very deep and uh yeah many mathematicians have worked on it including uh teren T who's obviously very fa mathematician um and he once said this in a blog post that's perhaps his favorite open question so it's a like mathematically interesting problem um and so we're trying to see like can we use fund search to to to tackle this problem and and improve the best known constructions within this and yeah just some technical detail this 2D grid is is like we're doing addition module 3 but it's not super important um but just just to be clear about what we're trying to do um so okay we can start with an example again so this capset problem in dimension D equals 1 is just three three points here in a line and we're trying to place as many points as you can without creating three in a line so first thing I can do is add one point all good no no three in a line add a second one still no three in a line but obviously if I had a third one here then these three are going to be in the line so it's no longer a valid cap set so then from this we know that the largest possible cap set for Dimension equals one has size two um and then we can repeat the same thing in dimension equals 2 for example here so now I have a 3X3 good um I'll try to add some more points I'll add the point here all good this one all good as well I can add one more one more um but at this stage if I try to add any other point um three are going to be in the line so for example if I add one here the diagonal is going to have three points so we're no longer uh allowed to do that so it turns out in 2D as well that the largest possible cap set has size four um and at this stage this is when I first saw this problem I thought okay well this is super trivial all you need to do in any Dimension is just to take any axis add two points and then you have a set of two to the D points and that's the largest cap that you can find um but it turns out already in dimension equals 3 uh you can do better than this trivial solution and crazy there's actually more space for you to put in points without creating three in line and so the largest caps set in D equals 3 has size n and this is Illustrated here on the side so all these um gray highlighted points are the Caps set and you can count the there nine and you can also verify that none of them are in line um and maybe some of you recognize the card game set here which is uh yeah very closely related to this problem as well um but yeah so basically uh this quickly becomes non-trivial and there's lots of interesting structure that comes out and what mathematicians in general care about is what what is the the largest capset in size arbitrary D not not just a given one um and it turns out that even four dimensions uh SE I guess seven and above the size of largest caps dis is not known um and it's kind of an active research era trying to find better uh better solutions to this and also ultimately like ASM totically as d goes to Infinity uh what's the largest cap that you can you can have um and so what we're going to try to do is to just discover better constructions with this um and so just two properties that are interesting about this problem which we'll use uh our setup is that we can easily verify correctness of a solution so if I give you a set of points um I can very easily tell you if it's a capset or not I just check if there's three points in the line or not if there are three points in the line it's not a cap set if not then it is a cap set and I also have a signal of how good my solution is so if you give me a set of points I can just count how many points there are and since we're interested in getting the largest possible set um this is kind of a score or a measure of how good or how well I'm doing um so these two things together just means that we can write a very efficient like evaluate function um so given a proposed solution I can evaluate it very efficiently and and and tell you if it's good or not um okay but now we know how to evaluate something but how can you actually find a better solution how can you improve Solutions here and this is where the large language model comes in so the kind of key idea that we didn't had in this paper was to use an LM to kind of guide a search in code space uh in the sense that we describe Solutions in code and ask the LM to improve these Solutions um and then we use the evaluate function to test whether these Solutions are correct and how good they are um so um here's a super super simple example of how this works and this is I'm not even simplifying this this is actually how it works in practice so it's it's very very simple but um so here for example I just wrote a python function that builds a cap set so say in dimension D equals 2 for example we had before the grid what I'll do is I'll just iterate over all elements in this grid so for example 3x3 grid and then I'll add an element to the cap set if this function is in set it's true so it takes an element and tells you whether the function is in the set or not so all the kind of uh like reasoning and intelligence happens inside this function here and then I'll have an evaluate function which just takes a set of points and then builds it using this function we had up here and then tells me is it a cap set I'll check if anything in line and then how many points are there just the length of the list um and then this isn't set function here is where all the the cool stuff happens and in this casee I just wrote this there's no LM I just decided a good her STI might be in the First Dimension we want to make sure that's zero and then the other dimensions may be less than one um but it doesn't really matter what you start with the LM will kind of improve on it but I just put this in here as a simple example uh okay so now we have some code that that you know tries to build a cap set um but how can we actually use an LM to improve it and so we did something that's yeah extremely simple and yeah to this day I'm still surprised it works but um you basically first uh you know take fun search run this V cap set and then you take fun search here and run evaluation and then we modify this prompt just to add uh a v0 to this this function we had here and then we copy the function definition and add a V1 and then we just add a doc string that says this is an improved version of the v0 but we leave the function body empty um and maybe yeah at this point you can kind of see where this this might going uh this looks a lot like a prompt to an llm where you have kind of a context and there's something missing and you want the LM to fill out whatever is missing um and so that's exactly what we do we just uh use an llm to complete this function given the LM is told to improve stuff so that's what it's going to try to do um and so for example the LM I generate something like this um which is you know a slight modification of what we had before um and then we can check how good is this like does it does it work better than what we had before and because we have an evaluate function we can really easily do this right so we just run the evaluate function both of these and it turns out the first one gives you a cap set of size two and the second one gives you a cap set of size four so the second one is actually better than the first one and so we now have an improvement but we want to continue improving right there's no need to stop here so what we do is we simply take the V1 function and replace it uh the old v0 with with that one so we swap it up here and then now we just have the same prompt again where we want to improve the improved version um and we again promp the LM to improve the improved version and then hopefully get an even better solution and then you just continue this forever or until you're happy with your solution basically so you just keep iteratively improving every time and you make sure that this action Improvement by having this formal evaluator um so yeah so that's basically a the whole thing that that's how it works um um so it's very simple um there's a few details um that I kind of didn't mention here uh one is that in practice we don't have just like one prompt that we just it improved we have kind of a population of prompts and then we use uh evolutionary search to kind of uh like evolve the space as opposed to just following One path so for example if we had this v0 before we might sample say five b1s here from the LM uh we'll score each of them using the evaluator um and then we then kind of keep each of these five and then for the next iteration we'll sample randomly some of these uh these ones we had uh usually proportionate to the score so we want to sample something that's better more often um since that might help uh further and then we just chain together these mutations and then eventually um The Hope is that we'll get close to somewhere good so here we sample something we sample next one and so on and the hope is that eventually we'll get close to some optimal or uh best known solution um or ideally even better than the best known solution um and so uh yeah does this all sounds very simple and the question is whether it works and uh it does work uh at least for this capset problem um so what we managed to do is first of all match the state-of-the-art up until six and for this these are known to be optimals so it's not possible to actually improve but for Dimension sevens and above it we can improve and in dimension eth we're able to find a better construction than the best one uh known by mathematician um and so yeah I guess there was also a question earlier in in this Workshop about you know um you can just be regurgitating the training data but we know that this is just a simply not a known construction so it's for sure not in the training data so this has definitely pushed something beyond what is previously known uh which yeah we thought was was pretty cool um and another cool thing is that we're also able to improve like the ASM totic bound as n goes to Infinity um and the way we did that is to some other kind of finite construction that's a bit different from a cap set but similar in flavor and if you recursively apply this construction you get an ASM totic bone but the cool thing is that mathematicians usually care a lot about this one and we're able to improve uh this also using this fun search method uh and I mean this would be like a like a publishable mathematical result if this were found by a human um which is yeah which is pretty cool um that we can we can do this with an mm and that it has the kind of reasoning abilities to do that um and another cool thing about this is that we can also kind of have a human in the loop uh for for a lot of this stuff so in general if you have computational search methods what you would get is just a solution so here I printed uh this like cap set with 512 so it's just a list of eight dimensional vectors and there's 512 of them and so I mean if you look at this it's not really clear like I mean there might be some symmetries there might be some structure but it doesn't pop out at all and it's it's just not clear what's going on um and the cool thing with fun search is that we don't have um we don't have we don't just generate a solution we generate a function that generates a solution and so um uh here uh with fun search we can actually look at what the LM has written and try to understand what's going on and the really cool thing is that it turns out that there's lots of very interesting structure and it gives you an understanding of what this set actually is like this you know lots of symmetries there's lots of cool things like that that um that that we found and Al some links to how some lower dimensional sets were constructed by other mathematicians um and this this this kind of part of it is is is quite exciting and to mathematicians who worked with this was actually the most exciting part that you can actually look at what's going on um because the Alm just writes code that you can read um and so uh yeah to put this in some context uh to some extent you can call this like a first scientific discovery with an LM in the sense that it's it's provably new like it's definitely not in the training set it's an unknown construction it's verifiably true we have a formal procedure for checking this is actually a new a new construction um and it's a relevant scientific problem in the sense that lots of mathematicians have worked on it and it's quite like a a yeah it's an active research area um and the second part is that this allows you to kind of discover understanding about uh about the problem not just like a fact like we could just say oh we discovered a 512 size capsa in dimension 8 but this actually says oh it has all these interesting symmetries there's all this cool stuff going on uh and that's what mathematicians ultimately care about uh they care more about understanding than facts um and we actually worked together with a mathematician Jordan elenberg um who's also written lots of cool books um but yeah one of the things that he mentioned was that he really like we we sent him lots of fun search functions and he would look at them be like oh yeah but that's the Symmetry and then there's this like thing that's related to some other construction um and yeah as he says here when he studies these things he actually learns something and um one cool thing is that this asmic bound we found was actually from us generating a functional LM looking at this function noticing that there was this like kind of symmetry in the dimensions and a permutation and then then thinking oh maybe this is the kind of solutions we should look for with the Symmetry then we enforce the symmetry so the L starts looking only for Solutions with the Symmetry and that's how we ultimately found the better solution so this is yeah literally llms and humans working together like giving each other ideas and then uh eventually getting a cool new result um so yeah so um and it's not only for for math that we're able to to to use this approach so we also tried it for more kind of a uh practical problems I guess you could say um so one of the first ones we tried is this online bin packing problem which just ask if you get loads of items coming in and you have a fixed number of bins uh how do you place each item into a bin such that you waste as little space as possible um so here the red one come in you try to pack it somewhere then the blue item comes in you pack it somewhere else and so on um and yeah this this this problem has loads of applications so whenever you try to schedule a job on a cluster anywhere you load containers onto a ship you design chips and so on like this this thing happens in the background um and so we're trying to figure out ways to improve this and see if if this was possible um and so the way this is usually solved is using these things uh these kind of heuristics where uh you basically given an incoming item you choose which bin to assign the item to based on heuristic um and one very classic one is the best fit heris stic which just says whenever a new item comes in just put put it in wherever it will fit the tightest like after you put it in what's the tightest fit uh and so here uh it would be putting it in the the red thing on top of the blue one because after you put it in there it's like a relatively tight fit in this box um and the cool thing is that these tics are you can just basically write as simple python functions as well so um in this case uh we uh can just Express theistic uh as giving a score to each bin so if I have an item coming in and a bin I first check can the item fit in the bin if not then this is a terrible bin so I'll just put infinite score and otherwise I'll just return the remaining space after add an item to the bit and you want to have the tightest fit so you want to pick the one that has the minimum score here and again this is just a simple python function that we can use to uh to start like a a self-improvement loop with an llm uh and so here we can just improve this function again with fun search but we evaluate it on like data sets of Vin packing instances instead of a single thing like we did for the cap set so here we also need some generalization right we need to not just make it work for one single set of items but for General distributions of items so we also do some more like train test evaluation which is I guess more of a classic machine learning approach uh but otherwise the the procedure is basically exactly the same um and yeah so a cool thing is that we're able to find heuristics that consistently improve on these classic heuristics uh particularly for large data sets and here's an example where at the end we pack with five bins instead of six with the best fit htic and one kind of cool thing that again we can look at theistic right and and see what fun search is doing and one thing it does that's pretty cool is that best fit just always tries to find the tightest fit but fun search tries to find a tight fit but only if it's really really tight otherwise it doesn't so for example here it puts best fit puts a green item here because it gives a tight fit but the tight fit is actually not good because it leaves a very small Gap that's unlikely to ever be filled with another item so what fun search does when the green items comes in puts it over here and then says okay now if I have a really tight fit with item then I can put it there but this this space is actually wasted now because we might not ever see a small item that fit here and this kind of reasoning is is quite applicable across lots of lots of things um so again the interpretability factor is quite quite useful um and uh yeah this is also true just you know across data set so we have a few benchmarks here this percentage is basically like you can think of as the the excess bins over like an oracle optimal solution um and you can see that we we have much less excess than these classical heris stics and particularly for instances with like 100,000 items we're near near optimal um and so this also means that for Downstream applications we can save resources and yeah hopefully uh make make things better um yeah and so one other thing I just want to touch on which I think is quite important is also some of the limitations of this approach so um one thing is that experiments are quite expensive so for example to get this cap set we need to sample the LM a million times and yeah I guess LMS are big models and yeah they can be expensive to sample from so sampling a million times from LM to get I mean arguably to get a new mathematical Discovery could be maybe worth it but yeah it's not always clear exactly what the trade of and I think it's pretty clear that there's a lot of wasted uh sampling like we kind of um yeah if you look at the samples the LM generat some of them are just like garbage and some of them are very similar to what was there before and you know there might be better ways to actually sample from the LM in a kind of a smarter way um another thing is that we're we're kind of currently quite constrained to generating quite short python functions uh so we had this like V1 v0 setup and usually we generate you know python functions that are like 10 to 15 maybe 20 line um but this is not yeah maybe you want to generate an entire code base you want to generate new libraries of functions and uh I guess yeah this work doesn't have too much to do with vision right now but um I guess with in the spirit of this Workshop as well on multimodality we also might want to have things like you want to generate uh say visual signals or you want to evaluate visual signals um and so um currently we're we're quite restricted in the sense that we just have these short python functions but what we ultimately want to do is make it much more General like natural language uh yeah multimodal inputs and so on um but we're just constrained to being able to to like needing a a kind of an evaluation function so that's also another kind of open question if if you use other modalities um and the third point is also we need like a fairly fast evaluation function so um for example um what the if few things we've tried maybe evaluation is on the order of minutes but every time gener an LM sample you need to evaluate it so if takes a day on a th gpus then yeah you're not going to be able to to run this because it'll just be too expensive um and so I think like points one and three are quite related in the sense that we want to make it more efficient especially if we want to move to like more Vision based tasks that are that are probably more expensive um and one last thing that was quite interesting to me is that using better LMS actually doesn't improve performance so I feel like this is a very um standard thing across all llm literature is like use bigger better llms than usually Downstream tasks always get better um but we did not see this happening for us um and this kind of if you use a really bad tiny LM it doesn't work and there's a point where there kind of a pH shift where if the LM is good enough then it starts working but after that moving to like the B like the biggest best Frontier models doesn't actually improve performance more um and that also suggests that maybe funar is not really taking advantage of like the the intelligence or reasoning capabilities inside the illin and so this is also something we we want to work on and maybe if the LMS can be a bit smarter we also would need a million samples to begin with um and another kind of aspect that's interesting here is that we also have like an inference cost uh intelligence trade-off I guess here so we could use a worse model then sample it two million times for the same cost at a better model sample the million times for example and it's not always clear um what the what the right trade-off is so yeah compared to some other llm uh tasks this this there's a bit of yeah there's some some different trade certain things to think about here which is slightly different um and also another thing I don't know if I have time or if I can do that but um one thing that's quite cool to look at is um actually looking at like an example trace of what the LM does uh maybe this works does that work for yeah so um so I guess the LM tries loads of different things so here this is one of these capset like problems and uh to see what it does so the LM start this is what we just feed the LM this is the function starts with it's just really stupid to give zero on every input doesn't do anything interesting but then we can look at what kind of mutations Elm does so it might do something like this which improves the score and then you can go further down the tree then maybe it does something like this here and one thing is interesting is you can kind of uh see at some point it kind of starts to converge on something interesting so this this looks like it has some interesting structure and then you know it slightly improves it here by adding some other uh crazy condition um for for this particular problem and then uh for the last one it gets something like this and this one at the end is uh the construction that gave us this ASM totic bound ultimately like this best result um and so yeah maybe this is not the most inreal function ever but you can look at it and gain some understanding what's going on but it's also interesting to see how the LM got there in the first place and also to see things that tried like over here that just didn't really work um and yeah so it's quite fun to actually look at this evolutionary tree and try to see um try to see what's going on when the LM uh creates its samples uh yeah so this is kind of um what we been working on so far mostly in in in like more mathematical and computer sciencey type stuff but the idea would be to also expand it more broadly to to to other other domains um yeah that was it uh happy to answer any questions thank you picture that you that you put up blue and red sort of smooth picture it looked like there was sort of an optimization surface yeah but and maybe maybe I missed it but it didn't look like there's any place here we could calculate a gradient no yeah so also this is a very like um yeah it's a bit maybe Miss side missing it's like a very Tory kind of uh di diagram here so I guess this would be like the space of python functions and we'd be kind of like starting at a certain python function doing random mutations around it and um uh then we can score them and the hope is that you know higher scoring python function will be a good stepping stone for an even higher score python function so in that sense there's a gradient in the sense that you can like iteratively improve things but it's not necessarily true that like a solution that gets you % there will actually be good to get you to 100% um and so um what we do in practice is that we keep like quite a a wide range uh of of points so we'd keep something like even fairly lows scoring ones but you know they could have some good idea that eventually leads you to the to the really good stuff um so in practice yeah that's a good point in practice I don't think it would look like quite quite as smooth um but our assumption is that the score does give you some signal that will point you towards the right direction yeah that's that's a very good point per one one more question yeah yeah I don't know how to choose yeah but make sure to repeat it oh sorry sorry yeah good point yeah does always generate Sy code and you do non Val yeah this H that that that's a really good question so the question was does the LM always generate like syntactically valid code that you can even run um so yeah that that's that's a very good point point and the answer is it doesn't sometimes it generates code that uh you know doesn't even doesn't even run sometimes it generate codes that you know it runs but then it has some kind of error um and what we do right now is we just uh basically discard it uh but I don't know if that's optimal to because like it might have some like really clever idea but then there's just some like syntactical error somewhere um so it might feel wasteful to just completely discarded um but right now that's what we do we just we just discard it if it doesn't run um and I think in practice in experiments I'd say something like 80% of code is syntactically correct and runs so it's a relatively small fraction um but yeah that's a good point okay we can thank the speaker have some a quick award session here for best paper and challenge award this WR this you want to announce the poster session yeah yeah can't e okay so uh excuse me excuse me folks uh we we have some stuff here please take your seat for the awards presentation um so yeah we are at the final session of this workshop and uh we have uh some awards uh that we would like to hand over um so we have a best paper award from the three oral talks we saw in U earlier today uh we have selected one paper for the best paper award and we have been running the smart 101 challenge which I'll briefly talk about soon and we have two awards for uh for this for the challenge winners um so the best paper award um goes to this paper autonomous evaluation and refinement of digital agent and uh G um G Pen would come over and take the award this award is sponsored by math kangaru USA which is a um organization that has been um doing um olympiads since some time um and uh so then we have this challenge award so the smart volan challenge is some is a challenge that we have been running uh since about a year and uh it's actually trying to understand the the abstraction deduction and generalization abilities of large vision and language models for solving problems that uh kids can solve very easily um so llms can perhaps solve maybe a 12th grader International math Olympia question but is it actually capable of solving problems that a a Kindergarten kid can solve and is there a Gap in their reasoning abilities and that's something that we have been working on and trying to evaluate all the large language models including gp4 4 o and such we have a paper coming up probably next week on this um but but uh yeah you can wait for the paper but then for for this uh particular challenge we collected uh about 100 puzzels uh from so we have this smart 101 data set which we released in cvpr last year and we um created a test set which is not publicly released and we conducted this Challenge on evaluating submissions on on on this test set and it has 100 questions it's all of this children's Olympiad or CH children's puzzle nature can see here these are questions that even kids can solve pretty easily and the question is basically can all these U large language models able to solve these problems and um so we we had uh about 15 participants from all over the world um and we had nearly um uh about 15 submissions so um and uh the evaluation was so it has 101 pule 100 puzzels in the data set and the valuation is using something called um vosa which is like so you have a a problem and every problem has uh has five options so a submission has to select the correct option from these five options and every question also has an Associated um score so the score can be either three three points or four points or five points depending upon the difficulty of the problem and the some of the half of the these puzzels are only language so you don't need to use a visual visual reasoning pipeline in in that and remaining half is is having vision and language reasoning requirement so we actually um create uh did this evaluation using this metric this V metric which is actually the accuracy weighted by the points attributed to that problem and U um and and average averaged over the the entire set of 100 pels and so we have from the 15 submissions we had uh we have two two submissions which have very similar Po accuracies and uh they are as listed here um one is from South Korea and the other is from China and uh so they apparently have the same VOA performances and it's actually pretty low which is surprising because they have been using all the latest uh uh large language models except um we had the requirement that they cannot really have internet search or kind of query GPT 4 during the devaluation so there was no internet access so whatever large language model they can package in a Docker image and submit to us that is how we did the evaluation so that there um we're not exploiting something beyond what what is out there that they have created um and uh so one thing is basically uh to note here is that the text fosa which is the performance on text puzzle so some of them some one of them have better accuracy on the text fosa and one the other one has uh the the vision and language one part as well and other submissions didn't really perform that well so we had a tie and then all the the two teams were asked to actually submit archive papers explaining their method and then we had a review of their papers to understand which one seems better novel more novel and such and we selected one paper one one of these submissions for the the challenge winner and the other one for the runnerup so uh the challenge winner award goes to jinu an and the team uh team's name is is is given here here hyu mm lab uh KT and uh please come over and take your award thank you and the runner up award goes to Zan Zang and Wu uh zian is is unable to come to take the award but um somebody was uh was there was a replacement but I'm not sure where where the person is well we we we'll send the the word to them anyways all right uh so we will have the winner talk a little bit about the approach that they have uh used for this um so a five minute talk you can give okay come over please being shared on Zoom Shar you have to um okay I start okay thank you okay uh hi my name is Jan and I'm Master student at H University South Korea and it's great to be here and I'm really excited to talk about our solution of the smart 101 Challenge and during the competition we aimed the large scale Vision language models to achieve very high level of visual reasonability with this localized multimodal representations so I'll talk about briefly how to extract and utilize these representations after the during the five minutes okay so to start with our the main problems our problem is to solve visual linguistic puzzles like on the right side and we all know that these puzzles composed of eight categories for example like counting math algebra and pth tracing Etc and these categories requires different skills and their combination so in order to solve this Pro puzzles uh we need high level of visual reasoning ability so to tackle this problem and with this approach our main idea is to utilize reasoning ability of large language mod model so in order to use large language model in multimodal problem we use language grounding which is a well-known approach which convert images into text token format for the lm's input and likewise we also converted the input image to the text captions that capture the content of the image so in order to obtain highly detailed text captions we built a two-step strategy like on the right side uh first we generate question and answer pairs uh about the image and then by using these pairs we create caption of the of the image so although we use Elms and this Lang grounding we we obtained a high level high performance rate uh but moreover we utilized object detection based image feature to prevent potential information loss from language grounding and this feature gave us to capture geometric patterns maybe overlooked in the capturing process so we can see above the image like this is composed with not the real world scenes and rather than just diagram form so we adopt to segment anything model to obtain fine grain visual perception and as you can see at the right side the feature of V it and fam could be extracted from the input image and then pluged into the C forer with the caption that is pre instructed at the before stage so I want to explain about the overall multimodal reasing framework as with combining the previous two method so starting from the left side with the given image question and their options we first generate three set of visual question and answering Pairs and by using this as the history we generate actual caption of puzzles and then by we enhance the visual understanding ability with the fine R visual features from the vi and S and finally the image and text embeddings are processed through the C forer and the llm which is from the instruct play FL T5 B mode and additionally we we during the training phase we utilize the additional data set to in understanding synthetic puzzle images so 118 2011 additional instances from the mass fista and science cre and other data sets are used during the training phase so for the experimental result on the test phase during the competition uh we can see like when we independently adopt each of the method we introduced uh the model gave us the good performance on specific evaluation metric and with the overall combining the method like including the language grounding caption s features and training with additional datais we perform the best on the total total wara and to conclude uh reutilized we focused on utilizing reasonability of llm to solve the complex video linguistic puzzles and to use LM to solve multimodal problem we suggested two main approach which is generating High detailed captions with the given images and to obtain and to use object level visual representations and through the competition we show that combining these ideas result in very high per performance and other training details or miscellaneous are on our archive paper you can search with like smart to 101 solution for cvpr 2024 and our code is open on the jeub so send me an email for other question or answer thank you for listening thanks Jin um so that's uh that's all for today and uh I hope you all enjoyed our uh talks and um paper presentations and uh so uh we would like to thank you all from our team uh for attending this ceremon this this workshop and uh we all look forward to meeting you probably for the next episode of this uh um of this workshop and we are not concluded yet we have a poster session in the arch building so please um ahead please move ahead towards the arch building for the poster session which will be for for half an hour from uh which will conclude in half an hour but it it should be set up now so please go ahead going there um uh that's all for today and uh thank you so much bye Zoom call so this one we did this no no no no they got they the winner second sure sure one this is behind it's the same the same one behind is that two pages or one this one they so thick oh yeah so congratulations want come another picture you you you want to come to side as well yeah oh yeah sure um M you have to come to side the picture with all all of us together em no thank so much you have to come mria oh you want to take picture the same same as my camera yeah he's got there thank you very much thanks so much and thanks for organizing really cool work thank you I read your work on equarian neural rendering years didn't realize it was you that's so long ago yeah actually for

