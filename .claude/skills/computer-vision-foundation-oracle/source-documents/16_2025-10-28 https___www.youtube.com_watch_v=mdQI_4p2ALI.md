---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=mdQI_4p2ALI"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:03.964Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=mdQI_4p2ALI

46864815-3f3a-433e-bca9-47cc97f2fc22

2025-10-28 https://www.youtube.com/watch?v=mdQI_4p2ALI

f5baa747-84b6-4477-ab2f-20cd2e9a851a

https://www.youtube.com/watch?v=mdQI_4p2ALI

mdQI_4p2ALI

## ComputerVisionFoundation Videos

W all right I'm Anthony Francis and I would like to welcome you all to the fifth annual embodied AI Workshop um and we have been uh running for a while and I'm going to give you just a brief introduction to the workshop and to embodied AI first I want to give a shout out to our organizers uh in addition to myself our lead organizers include Claudia Perez darpino from Nvidia and luk wi from ai2 we are supported by a wonderful committee including ad famar Angel Chang changan Chen Chang shui David Hall Devon Helm Shel Jan lamberto Balin Matt D dka Mike Roberts naaki yukiyama Alexander maitz Ram ramaka ran gong Ren metf zor perk and Yaman bis our scientific Advisory Board includes Alexander Tov Andre kabov anara Kaki duu batra kman Ross John Trung man Sava Roberto Martin Martin and Rus mataki we are also uh blessed with a whole number of challenge organizers um and I'm going to let them introduce themselves during their own challenge presentations later during the day so we have been running for five years starting in 2020 and uh over the years we've had a number of challenges a number of speakers and one of the questions that came up um and over the at the third year we did a retrospective paper uh which you can find on archive Which documents the first three years of progress on our embodied AI challenges um even though things continue to improve with the different workshops I still get asked this question of what embodied AI is and why it just isn't robotics or computer vision and so first I like you know to distinguish what isn't uh embodied Ai and classical open loop robotics is not it uh still to this day um there are robotic systems where if everything is not perfectly aligned and anything is is wrong the robot is going to execute a fixed trajectory and it's going to bung up as a manipulator or it's going to weld the wrong part ask me how I know about the latter one um and it's also not classical IM image classification we're not taking a point in image space and trying to classify it or extract a segmentation or do a comparison with another image embodied AI is about interacting with an environment and here you have uh um a robot uh from Google or from Fetch but this is at Google where it's trying to get to this corner over here and the path that it originally planned was blocked off by this uh cart and it dynamically replanned and this is a technique called prml where we're combining probabilistic road map with reinforcement learning for it to do Dynamic replanning and generally when we have these trajectories we have some notion that the trajectory was good but as we've recently seen we're going Beyond this and going Beyond a a solitary embodiment where we can readily examine the trajectories so here we have um in an open set problem this is a system called Vader from uh Google where uh the robot is performing open set tasks and it use a uses a language model to plan but a visual language model to detect issues that it can't cope with in this case it's not able to quite reach the coke Cam and the table it asks the human for help the human does so and they have a little negotiation when the human thinks that the robot can pick up the task and then it checks itself and then moves on so because this task didn't necessarily exist before we don't precisely have a notion of of whether or not it's just good like it like we got to the corner of the room the human and the robot have to negotiate um a a shared meaning of whether or not the robot's behavior is right so classically we talked about embodied AI as agents that can see hear act reason and Converse and it's been putting together through increasing complexity beyond the functions of images to interaction with environments collaboration with shared meaning um and this increasing complexity has been enabled by a whole bunch of advances in the community robotic even moving the arm uh in the fixed paths required inverse kinematics getting the images required labeled data sets but then we started doing things with markup decision processes model predictive control to enable robots to move in spaces and now we have Foundation models and effective simulators that enable even more complicated systems but to me what makes embodied AI embodied AI is the fact that we have these challenges where people put together a set of problems and a data set or simulator and then we try to solve them and initially the community starts out did something just happen to the this is yeah it seems to be happening in a lot of the rooms um are we back okay um is there a way to okay so I'm back on okay so when the challenges start they're difficult to to tackle people don't get them quite right but then over a while the uh the challenges get solved we have solutions that are deployed we have findings such as whether or not maps are required for navigation and these lead into new um tools that we use and the challenges in turn get updated so I really see emited AI as not just AI which is looking at the world and trying to a world that it has to interact with it's a research field that's being reshaped by its own work we don't just like recognize cats and then deploy that on the phone we collaborate with the human and then say well we need to make this collaboration problem harder in the next iteration so how's this Workshop been shaped by that well our theme is open world in body AI uh the three sub themes are embodied mobile manipulation which is arguably the foundational task of uh open world embodied AI generative AI for embodied AI which is um a using generative models to create policies or training data sets and language model planning where we use uh language system language models to attempt to direct the robot what to do and we especially with the papers you'll see that we accepted on the website which will be going up after cvpr we have perennial themes around simulation environments visual navigation rearrangement and embodied QA Vision language and hearing we'll be presenting six challenges today including uh navigation and Social Challenges like multi-on multiobject navigation Hazard uh rescuing objects from fires PRS which is social navigation uh manipulation challenges like many skill which is a visual tacal uh um uh simulation uh task this year and Arnold and OV mm which are mobile manipulation challenges this year our speakers include anara kamb a Brian ikor Sheron song Chris pton Eric Jang Richard Nukem Olivia Norton Richard beiki and Ashley lawren we have 31 accepted papers the poster session will be at 1M in Arch 4E posters 50 to 81 and the papers will be on the website after cvpr concludes I deny all knowledge that we're scrambling to get all the camera ready copy I didn't say that uh our sponsors this year include uh Project Ara Microsoft and logic Robotics and we want to thank them very much for their support uh and bringing this conference together um and they are going to be providing an award of 300 to the first place winners of each of the embodied AI challenges so thank Microsoft thank project Arya and um I'm not going to thank logical robotics because that's me and I'm that' be kind of awkward uh so our schedule after I stop yapping we'll talk about our navigation and Social Challenges at 9: um and R kimbabi will'll talk about generative AI at 10 U Microsoft a will lead a panel on advancing body embodied uh AI at 10:30 Brian ier will speak at langu about language model planning at 11: and then Richard Nukem will talk about project aret at 11:30 uh we'll break for lunch and then the poster session is again from 1 um in arch4 posters 50 to 81 our manipulation challenges will be at two embodied mobile manipulation uh talks will be at 3: and 3:30 from Chris from Sheron song and Chris Paxton and Eric Jan will speak about humanoid robotics at for and we'll conclude with the speaker panel um moderated by Claudia prz Gino from Nvidia so uh without any further Ado I would like to welcome you all to the fifth annual embodied artificial intellig intelligence workshop and we can get started with our navigation and Social Challenges e e oh I'm so sorry H sorry um so um we have a constraint on the maximum number of steps which is like set to 2500 in our case um uh the agent has access to uh RGB images depth images um relative GPS Encompass which is relative to the start position of the episode uh it has access to the gold specification in terms of language instruction this year we have set our um data set uh on habitat synthetic scenes data set which is a collection of high quality 3D synthetic scenes and it has a diverse set of objects uh specifically uh each of our episodes contain three goal objects and which is made available to the agent one after the other and each of the goal is described using uh language instruction uh so for example find the Candlestick on the buffet uh so each of this instruction might have a spatial relationship uh for example Candlestick on the buffet uh or uh like it has a varying granularity of object descriptions so it might say go to the candle or it might say go to the red short pillar candle so in case of course coely specified object description we have multiple uh goal uh correct goal locations and the agent uh if navigates to one of the goal locations correctly it is considered a success here are a couple of examples uh from our episodes so for the Baseline we uh use we combine two methods uh mopa and VL Maps so we base our Baseline on the uh mopa architecture which is a modular approach and it has uh different modules in it object detection map building planning exploration and navigation but we substitute the object detection and map building with VL Maps uh to support the open vocabulary nature of our U approach of our task so V Maps basically builds a map with a clip image features and our approach selects the location which maximizes the similarity score with the object description coming to the leaderboards uh we had three teams submitting to our challenge this year uh but in the end only one team finalized the submission and it outperforms the uh Baseline congratulations team in Telo Labs so Franchesco is here I will uh call him up uh in a few minutes but uh like in a few seconds sorry but uh before I do that uh let me thank the workshop organizers for organizing this uh the eval AI team uh the habitat team for supporting us in this challenge uh as a reminder the we'll keep the challenge open uh so I'll uh uh ask all of you to check it out and get back to us if you need any help uh now Francesco can uh go in detail into his winning entry need e e e e e e e e so next up next up is the ha uh Hazard uh challenge who are remote hell hello uh yeah I'm remote hello I'm hono from the ummer uh today I'm going to introduce the hazard challenge the embod decision making in dynamically changing environments oh sorry okay uh hazer is a challenge about rescuing objects from disasters including fire flood and wind here are several examples of agents and the te and this challenge focus on the embod decision making problem we especially focus on the dynamically changing environments uh let me briefly introduce the motivation on it this first uh the decision making is a in a dynamically changing environments is an important way to evalate the embod agents because this type of environments is featured by rapid changes great uncertainty and complexity to overcome these difficulties and make good decisions the embody agents are required to First perceive these environmental changes then reason the Dynamics and future developments of the changes and finally they need to consider these factors during their planning and therefore find the best plan in the dynamic environments secondly this kind of things uh dynamically changing environment uh is closely connect to real world applications such as the search and rescue however studying this topic in real world is not very proper because we uh the implementation of the rapid changes is both dangerous and expensive therefore we need to simulate simulate the changes in the simulators uh in this work we choose the 3D World simulator uh to from the am this simulator is featured by its powerful Dynamics and simulation and the realistic image rent uh here's an overview of this challenge uh we name it Hazard challenge because it includes this three disasters uh the fire flood and wind uh we first implement the Sims of these three disasters based on the three world uh in an indoor fire scenario the Flames spread rapidly throughout the house potentially destroying the flamable target objects and in the indoor flood scenario the large amount of water continuously P into the house in the non-waterproof Target objects and in the out wind scenario the strong winds blow lightweight objects far away on the road making the retrieval difficult for the agents and based on the simulator of these three disasters we make data set of with hundreds of things with procedure generation pipeline also based on three world and here's a brief introduce of how these disasters are simulated uh this uh can be separated in uh this page uh including the physic physics based imulation uh and the image surrendering and for the fenario we built a temperature system which maintains the temperature of all objects in the simulator and update them according to these equations and the temperature difference of an object in the next time step is determined by the background temperature and temperature of other objects and finally an object will be ignited when it reaches the ignition point and after burning a constant time the state of object will turn to burnt and for flood scenario the objects are affected by buoyancy and drag forces we calculate these two forces according to the right two equations the water surface will rise with a constant speed and when a non-waterproof object is submerged by water it will be flooded then we render the visual effect based on the physic simulation these pictures on the top shows the visual effect on of the fire and flood and in the hazard challenge the objective of an embod agent is to rescue a predetermined set of Target objects and bring them to a safe place uh such as a back held by the agent or a sing cart as this page shows at each time step an agent receives an rgbd observation sematic OB segmentations of the observation and blurt environment specific uh observation like the temperature in fire scenario and uh and the water level in the flood scenario uh we also provide a perceptional version of haard which excludes the semantic segmentation from the inputs uh given this observations agent must make a proper choices from the available agent action space at each time step which will be introduced in the next page uh to finish task and they'll finally be evaluated by these three matrics including uh the value which is the proportion of the rescued objects value to the uh total value and this deps which measures the speed of uh the rescue and also the damage which measures how many objects are damaged uh during the rescue uh dur during the rescue and uh in Hazard we use this humanoid uh agent with actions in these Pages for the High Life actions we include an explor explor action and robust pickup action and job action that helps agent uh put down an object therefore uh once the agent Reas object it can use the pickup action to hold object hold the object and then use the job action to position it in a safe location the haet also support direct utilization of lowlevel actions including the move by turn by which can be a certain distance or a certain degree and also other low level actions the hazard challenge mainly focuses on the uh evaluting the uh dis decision making abilities of the embody agents however we also provide some additional settings to support a comprehensive evaluation of embody agents for example the perceptional version of haard does not provide the ground to segmentation as input which requires the agent to perceive by itself and the effect on agent setting will let Hazard affect agents uh which makes the challenge much more difficult uh for example the agent uh have its time limit uh have its temperature limit which uh keep it away from the file and we also provide a auto aut of domain test set which includes new objects to assess the generalization capability of embody agents okay and uh that's the introduction of the hazard challenge thank you for listening okay next uh is the ERS challenge I yes yeah are you connected to zoom you can but you need to be connected internet can see it the link yeah I one moment while we get connected on the zoom l oh no it's not recording in progress you have share screen yeah good yeah thank you much yeah I'm howong I'm an assistant professor from ping University and it's my pleasure to share our challenge human Center in building um the integration of embod AI into humans life has been a a trend and um we have already exposed various type of embod skill like manipulation grass pain and uh navigation and so on um but however we have noticed that there is still a gap between the current eii paradigms and the humans expectation and uh we believe that the human robot interaction should be the answer and it also build new challenge and new topic to the embod AI communities so we propose our challenge human centes in building human Center in building de and it come from a real business business scenario and a robot stay in a building for a long time periods and serve a group of people basis on their instructions um for this challenge we build a virtual 3D story building and it is inspired by a real Polar resar station from The Real World um this building contains room with many functions including a Weare house a supermarket medical room office the bares and and so on um we have built a small community with in this building uh including 10 human characters and each one had their own individuals settings profile and behavior generated by large language model we also provides hundreds of items for interactions we have also uh build a m prototype that can gr move and deliv objects with two rgbd camera as the sensor input and let's see a case for running expand of this deliver service uh first of all um we have the large model j NPC for example John go up and way out of of his room he asked the robot to bring him a bottle of water and put it in in the in the office so when we see this instructions the robot took the elevator from the first flow to the thir flow to se for the water uh basis on John's profile uh when the the robot gr the water water and then the robot will will navigate to John's office and uh and uh hand hand over the water to Dr uh using this building environment we can actually generate masses number of the task and uh by sampling the items characters and in different carers uh and uh we also uh generate a lot of task case for evaluation because all of the tasks are generated by LM so we can actually generate Infinate number of tasks but we also have the task a large number of task for evaluation purpose and the evaluation matod is quite simple just to tell whether the taret object is successfully delivered in limit time aot and uh we also provide a list of API for user to for to control uh in in lowlevel wave or in the llm style and uh we we have also proposed a simple model as our Baseline we use the rlm to pass the human instruction and the profile to obtain the target objects person and deliver deliver relocation we we use LM and the Zer shot object detection model to help robot to Gras the object and after capture capture the objects the robot should uh navigate to the op to the to the Target location here here is some uh Baseline results we have achiev around 32% success rate and um we have also for the challenge we have invite many team to participate and we have a winner team called PDA they are from JHU and USC uh sing W and F W uh we we will still uh working on this project and we will release a new version of the environment which will may contain many new features and uh yeah so in the end let's let's build a virtual human real Community together for import This research thank you very much the yeah I I I don't know whether the team is yeah so um so if team's not here coordinate with uh the organizers me to get them their after the challenge um and I'm supposed to be Hing out Awards when the challenges hadn't worked so we have a we have a Lov if the challenge teams are here and please forward it to them and coordinate with me if they aren't here today so thanks to our sponsors for making we are now going to be having the uh social and navigation challenge panel so I can ask um the uh coordinators of these um challenges to come up to the uh panel area here now we have questions a to be submitted through SL do we have a SL uh no slide um and we also have the uh the hazard challenge organizer who can join remotely if you're around oh yeah we are run so anyone in the um anyone in the audience if you have questions on the embodied AI um organization website there's a link to our slack where you're able to ask questions of these challenges under the um Tag questions for navigation um and we'll be asking questions of the panelists okay for already um I'll start with a um fairly General one so why is it that as a community for embodied AI that we still struggle at navigation what is the big problem that we still haven't overcome by this point I think for yeah yeah I think foration the the zero shot problem is still very challenging and uh also the perception problem is also quite important for example if we if the robot uh scan scan the room once uh before before the robot navigate in the environment then I saying if we have a 100% accurate uh per perception model that can solve a lot of problems so from my perspective I would say that we have solved the point to point navigation but what we haven't solved is the semantic navigation and mostly uh long Horizon navigation I guess uh that's one thing that is still working on as a community and um that's something we need to think and uh quiong anything you'd like to add oh yeah I've seen the question in the zoom I'm reading this sorry oh yes sorry I didn't hear it very clear clearly uh the question uh the question is essentially why are we still struggling at navigation in embodied AI systems uh yeah I think it's a a big question uh I um many agree with uh Professor Don says uh and is a possession is a big problem and also for some long Horizon so we need some like hierarchical to uh like finish this long horizontal task uh like in the hazard is a a very complex task is need some strategy uh in the uh problem solving uh so that's my that's my point of view thank you very much um next question is on the sorts of challenges that um we've been creating as a community so there's been generally a lot of focus on tackling this problem in indoor environments where a lot of the time the challenge is finding the objects or avoiding obstacles but what do we think about outdoor navigation and the fact that you also have to find Reliable terrain that's suitable for your particular type of embodiment um should we as a community focus a bit more on these areas rather than always pigeon hauling ourselves into um an indoor environment the reason we doing that because we haven't soled into that's EnV I usually object to PR an object under that building soers when H do you have a take oh no uh think the is much more complex than the indor as the the same kind of problem uh it's uh from the perception kind of view point of view also the terrain and much more factors to consider in the auto okay thank you all let me see if I can summarize this one it's a bit wrong um so essentially in uh our current research we are primarily dealing with two types of simulators habitat rendering a 3D map of a real world environment or an asset based simulator like Isaac Sim um however there doesn't seem to be a seamless transition from world based data sets that are based on 3D scan and objects based ones that actually allow you to uh manipulate the environment um is this um a correct assumption about how current simulators work and if so how might this impact our development of embodied AI the fact that with some highly realistic scans we've got um no direct interaction with objects but anytime want to add interaction we downgrade our realism yes uh latest habitat has the ability to add objects into it and interactable objects I think uh people uh are aware of this thing and they are they are working towards it um so I would see have something like that very soon yeah I think for the long longterm goal uh we we we we want we we hold single simulator that um have more Gap in the v show busis and also one F that this is our goal but um but but actually at this moment I think this go is quite challenging even even though is isap Steam and omnus have show some promising results but in the short term I I think a better way is to have two TI of simulator one one is good for learning the manipulation policy which have a very more s whe Gap in in physic and visual but maybe it will take a longer time for R for rendering the the data and uh the are the other simulator is for the for the the high level planning task like litigation and like the high level interaction with the human just like our P PRS PRS challenge oh yes it's also a step problem from the uh interactive uh the level and also uh from the SC like the from the indoor to outdoor from the single room to multi room is like a step by step uh uh and it's still a long way to go raia should be should be the the last Direction uh but as what I I I say previously then I the longterm goal we we should have a single simulator but at least for for this moment to if we want to learn such policy then we can actually have two kind of simulator depend C so yeah the long able to interact with but course we need to I'm trying I'm trying to the we're not showing up on the zoom so I'm trying to get the okay that's not gonna work sorryy I know like um navigation say is the first step and then uh the manipulation is again dependent some some some dependent on the navigation because yeah that's why that's what I think that because for for mobile manipulation when you already uh navigate to a specific vision for example just in form of a in form of a door then you start your manipulation mobile manipulation policy and uh but while you manipulate the door opening a door then your your base actually Lo won't won't for a long distance question big part of navigation right is actually getting into the spot oril is being in the spot where you can actually grasp the object and that's itself a very hard problem it's not just object navigation and so is um do you do you think there are other task for which navigation is like actually very important um or are we really just trying to do a manipulation task and then um you know getting towards the object is good but should we be really thinking much more heavily about manipulation um or is there something that can use like to fild the gaps there I think we can tackle these two problems separately and then find a will to some sort of mod approach and activate different policies do different things so we can like utilize the navigation policies which we've done past PL it but I think the all right the next question I might start with Queen um many navigation methods um created trained in test environments that don't humans do you think that sets them up for failure when they are inevitably deployed in environments that have humans in them would you suggest changing the general training Paradigm or do you think this actually won't lead to any performance issues oh uh thank you and um uh I did not uh didn't have not done any uh human experiment so uh I'm I currently have no have no idea on it uh but uh I guess that uh it is it will be a great Gap uh uh for human and I think the habitat three has also involve the pattern like human and the agents and uh it uh and this pattern will like I think is very promising uh for the future human in the loop learning yeah thank you um and from your experiences what do you think is the human in the loop critical part are we going to have our systems fail if we don't involve some human involved in our environments or is it only a Min that don't involve yeah so like because we've got these environments that are going to have lots of um humans involved in them do you think if we set up our navigation type challenges that don't involve humans just interacting in the environment that we're going to set ourselves up I think I think actually we can we can evaluate uh defend defense from task in the with for sample the speak a bit louder into the microphone yeah I I say uh we we we can actually evaluate different subtasks individ for example for for the for the physical interaction C by grasping or manipulating the articulate objects then we have we have one set of the matches for example whether object have been successfully Quest and then pick up or but for the for the for the human interaction actually is a high level task so for for all the high level task navigation or or uh object object navigation visual language navigation or even the demand G navigation then we can have an other set of the evaluation matage for that but of course we can also have the in in our you our challenge actually we we have achieved the B have ass 32% success rate right but in that success rate we assume all the physical interaction at 100% we assume the robot always can successfully gr that object always can successfully open the to or push the bottom of the evaluator and that's how we can we can separate that information so uh I think if we train n systems uh that's probably going to fail when come in the real world but uh that's where I again I advocate for like modular approaches not um so maybe if we use a robot who capable of for example I think uh it should it can be plugged into a high L semantic planning module and then yeah so all the all of the techniques invol that get um created as part of these various challenges get used as the high level planner but for your basic sort of Dodge stuff that's moving around in the environment use another the system traditional systems work pretty well right so why do framed policies for that as well makes sense um I'm gonna ask a a question so we've we've brought up a a couple of times the issue of U realism physical interaction versus non um for the different types of uh different types of scenes that we create vers of scans versus you know fully simulated environments is po do um any of the presenters here think there's potential in the world of neuro Fields Gan SPS particularly those that are able to include physics good um those are able to include um physics that might be the solution to this problem or is it just a potentially interesting source of technology to me it's very interesting potential sources of Technology but I person haven't tried any of it so yeah actually I have tried uh like uh some uh some construction method to construct like allo thing uh but I think the current technology is uh and is still not capable for the outoor uh uh embod environments um but maybe we can try from uh some simple things like uh single room or other other kind of simple things saying the problem is scale or it's um the realism factor that you've just not been able to get for your particular types of challenges uh you mean sorry sorry is your issue with the um technology one of being able to generate scenes at scale or is it that they just aren't suitable for your sort of um challenges specifically the inclusion of Hazards and things like that oh no it's another another one uh is like I was trying to generate at a scale but uh uh currently is not successful on it um you okay um lost my list of questions um could it be of interest uh in ejecting navigation tasks that cannot be achieved in any case so the agent at some point has to understand and communicate to the human like you're going I want to get to this location but there is something blocking my way or I do not have the right embodiment to get past this problem would that be an interesting uh part of a um future navigation challenge what problem between you yeah I I think the the indication of the embodiment may be may be a a challenge and the future solution we have two way the first one is to utilize the llm VM we hold the VM to answer question that whether the robot can successfully achieve that for example an object that is too high then cannot GR in okay we hold the VM can actually answer that and other solution is we if we don't use the LM then probably we have some other methods for for for for that for example ordance measures if if that if we want to open a quest and object but unfortunately then the the quest all cannot be predict that is because when we we predicting the quest all we also basis on the condition of the robot location that should be that should be the way there is also what we call the condition sorry I have no fur ideas on this question yeah okay for e e e e e e we were muted that's my bad trying to mute and unmute whenever I go to Que um sorry que sorry you might not have heard the question then so the question was basically what is the most important design aspect um that your team sort of came up with when you were trying to um create your challenge what did was the most critical um design decision you had to try and think of in terms of accessibility or challenge difficulty level um was the most important sort of decision you had to think oh yeah uh I think for our team is the critical problem is uh the difficulty of the challenge like we can uh design the challenge like the input signals or the task uh to uh set it to a proper difficulty but uh I think it's really uh I think is really difficult for uh for a challenge like that uh like haard it's uh it's full of Dynamics and uh like uh and it it is requires agent to have strategy uh so uh we have to like to select which which input signals we shall give to the agent and which are not like which uh what task it shall complete uh sh shall to do uh I think it's uh it's a critical problem for our challenge thanks very much um and yeah some question audience if there was anyone right so for people online so um uh summarizing the question are we focusing on the right thing in uh having all these challenges purely in simulation will they translate well to the real world when we can perform machine learning on uh real world platforms now is this the right direction for us to go as a community to keep focusing on these simulated charters for yeah Min do we just now we can see that ofation meths are Bas model so us model to just it set but just apply that that I so why Sim and dat simulation and I think that should be uh hey take on this K oh hello uh I I think it's a problem of the simulator and uh there's always be trade offs and I I believe uh it is also it is always uh like important uh for the simulators but uh uh but we need to find like the uh uh we need to find a better uh better simulator and we also uh need to find find the uh optimal the tradeoff point uh from the S and real and uh but I believe it it is always necessary to for the simulators in the inod AI thanks very much time all right uh thank you everyone who has been on the panel if everyone in the audience please join me and thanking them for participating in I move things aside awesome um awesome uh it is my absolute pleasure to uh introduce Ani kavi who is the uh senior director of computer vision at the all Institute for artificial intelligence and and an affiliate associate professor at the computer science engineering department at the University of Washington uh his research interests Li an intersection of computer vision natural language processing and embodiment um and probably remember him from last year CPR or he stood on stage and accepted the best paper um award and as well as a previous Awards hello okay it's if I unmute it is um oh I switch on my camera sure all good okay all right let me get started Okay so data has been a key driver of progress in AI right so we're seeing uh large volumes of data being critical to creating large language models models in Vision models in the multimodel world and we keep coming back to the same finding as much as you know as a community we try to fight this finding every few years and every few months we keep coming back to the same finding large volumes of Highly diverse but more importantly high quality data result in very very simple architectures trained end to endend giving robust generalizable systems and this hasn't yet been our finding in robotics and one of the reasons is that Gathering large volumes of high quality and diverse data is extremely painful right taking a robot in the real world you know we do experiments in our lab and it just takes minutes and minutes to even perform one episode so here are some really influential work that came out of Google recently and this is you know the first of many influential Works in this line This is rt1 and I found this line very interesting they said you know we collected one of the largest data sets ever 130,000 robotics episodes unprecedented but it took them 17 months with 13 robots right so collecting this volume of data in the real world is is just not going to plan out the answer really lies in simulation simulation can give you high quality data simulation can give you diverse data but hang on a minute we've been saying the same thing for a decade or even more right and if you look at our papers simulation has not delivered we have very poor numbers for tasks which realistically I had I would have expected that we would have been very close to solving but basic tasks like explore this environment go to all the rooms tell me when you see my keys right you look at papers from all of us we're seeing 20 30 40% successes we're not building clean mod model anymore are we have all kinds of auxiliary losses to get this to work we work in simulation but we're collecting human data again we're using modular architectures this is very different from the findings in the rest of AI and we've also now started making assumptions right we're saying oh you know actually you do have access to the test scene at test time but only for five minutes with a mobile phone or actually I'm going to assume that I know where all the objects are placed and so I'm kind of cheating in the task of navigation so today I'm going to present two models these are our latest models of the press right one is called Spar and a latest model that's not even an archive called polyform these models are trained only in simulation but at massive scales there's no real world adaptation these models have never been in the real world they don't use any depth they don't use any liar they don't use any map they don't use any modular architecture and they're only trained with RGB observations they not trained with any human data and so all of the data is cheap and in a sense unlimited as much as you can compute with and as a result of all of these constraints we're still able to produce agents that are proficient at reasonably long Horizon tasks in navigation and manipulation not just in Sim but also in and so here's a teaser this policy has never been in the real world it's been put on a stretch robot it's seen in an environment that that is not present in simulation and it's able to go around it's able to uh find this object try to pick it up it fails for a second and then it succeeds eventually and the key to getting this result or these results has been scale scale scale and scale scale up your environments find new ways of creating interesting data and scale up your models okay so about 18 months ago we decided to scale up simulation you know even though simulation came with the promise of scale actually all of us were working in 10 rooms 20 rooms 30 rooms 100 rooms and so it was actually low scale right and so we said 18 months ago we said the way to scale up simulation was using procedural generation and so you have this little uh specification which says I want a one bed one bath apartment with this connectivity and then we wrote a bunch of procedures that allow you to iteratively sample a float plan given a float plan sample some objects place them do lighting Etc and this allowed us to create hundreds of thousands of houses and we saw that as you increase the amount of houses in training and simulation in simulation and in the real world on Downstream tasks your successes go up so how can we extend this how can we first expand upon this so that you can get even more flexibility to the user and second how can we extend this Beyond Apartments without having to write these procedures and towards this at the CVP are we presenting holc holc is our new framework to create Rich 3D environments using natural language So Gone are the days of having this little tree structure and instead you can simply specify a uh long sentence or even a paragraph of what you're looking for and then holc will create the simulator field right and so large language models are key to doing this and we found that large language models can input this natural language and importantly they're very good at determining layouts that are consistent with the user query and they can they can tell us what materials to put in and then of course we can use off thes shelf computer vision models to match text and language and start constructing the scene they're also going at telling you what objects to put in right unfortunately this part requires an extra layer of constraint optimization that's because large language models aren't really 3D aware we were already surprised that they were 2D aware but we did not expect them to be 3D aware and so you have to put some constraints on you know you want to make sure that objects are reachable you want to make sure that robots can navigate between objects in the room and adding a few constraints allows you to go from you know from Bland environments to this so here's an example three Professor offices connected by a long hallway and the professor in office one is a fan of Star Wars and I will wait for it to pan around and you can you can see that when you go to the professor's office number one you can see some Star Wars member right and you can keep doing many such user constraints the the the system is fairly efficient you can generate really really diverse environments you get tremendous amount of customizability so you can say well I want to change this apartment and this apartment is actually you know someone has left the apartment and the person who's now come unfortunately needs to use a wheelchair and the model makes the doorways wider or you can say you know I want to create an environment which is an aerospace exhibition Hall it sources the right objects and so on and so forth of course we are interested Beyond navigation we're also interested in manipulation right as a community and so in order to get generalizable agents for manipulation simply scaling up environments isn't enough you have to scale up objects but we've been using at least in Thor fairly limited number of objects over the years and so last year we released obes this is a universe of more than 10 million high quality objects and observers of course has many more implications Beyond embod it's become one of the most popular ways of building 3D models and has been adopted by many people uh in this space but it also helps enrich our environments and really make them more realistic and reduce a semantic domain gap between Sim and Rio this is what environments look like before obers nobody's house looks like this certainly mine doesn't right because I live there there's this this stuff in this mess and my kids throw around stuff and so Obe allows you to create this and this is more realistic and this allows you to train robots to go and clean up your environment and you know uh navigate around obstacles and so this really reduces the semantic domain Gap that we've all been uh worried about okay so when you're in simulation right of course you get access to these environments but what you really get access to is privileged information you know exactly what objects are placed where you know the depth maps you know surface normals you know the masses you know volumes you know all of this information you do not have access to this information in the real world you do not want to rely on this information at test time but nothing stops you from using all of this information at training time and so the way we Source data is to write privileged planners if I already know where the object is I can use a shest spot if I already know how high the object is I can decide how to move my arm in order to manipulate this object and so we write privileged planners in order to Source lots of training data for many different tasks and this is the way we think about tasks you can have a behavior you can have a way to specify the goal and then you can come up with a natural language instruction and invoke the right plan so of course object nav is very trivial find your toolbox but you can say I long going tell you what to find I just want something where I can place my hammer it could be a bag it could be a toolbox it could be something else right you can also say find me the largest toolbox and of course you have privileged information on volume and weight and so on and then of course you can keep building this out to create more and more interesting training data you can say well find me a toolbox but if the toolbox is full then find me a bad so here's an example of a training episode the instruction is pick up a pillow and then this is training data the robot goes around navigates around objects finds a pillow and then grasps it and picks it up and this is for the stretch robot okay so now that we are able to source so much data how do we uh train models so this is our model called Spock which stands for shortest path Oracle agent um it it involves a Transformer encoder at every time step we give the language goal description and we give two cameras the stretch is an interesting agent for people who haven't yet played with the stretch because it has a camera on the front but its arm goes to the side and so it also has a camera on the side and so we input two RGB images at every time step and the Transformer encoder allows us to create what we call a goal conditioned visual representation and then these representations are created for every frame so every time the robot takes an action you get a new frame and all of these representations are provided to a Transformer decoder which then predicts an action this in my mind is a very very simple clean architecture this has worked in many different domains particularly in language and so we've adopted this we have to make small modifications giving the previous action helps in terms of the positional embedding helps a little bit and so what okay here is a result which I find extremely interesting so um this is a controlled experiment where we train Spock using a privileged planners for a single task this task is object goal navigation find me my keys find me a plant find me a bed etc at training time this is trained with shortest parts right and so at training time you already know where the object is in your planner and you just take the shortest path often like you know just a few terms straight towards it as you train with shortest SPS at extreme scales you start noticing that at test time even though you don't know where the object is the model is able to find the object but interestingly it's able to explore multiple rooms and when it gets into a room and it doesn't find the object it's able to backtrack and then move into another room this has been very very interesting to see and of course we don't train it on a single task we train it on many different tasks and we're able to create this large multitask model so in Sim you can start seeing that you can navigate to you know all kinds of things like you know these are synthetic tasks find the fruit in the kitchen but then go near the fruit which is at the highest y coordinate and as I promised this also works in real so you can go you can start looking for apples you can start fetching apples you can find toilets all of this is done with no real world adaptation so on the left you see it trying to find a green apple and on the right it uh navigates around his house and uh looks for a to okay so what matters and what doesn't so importantly scale matters with a little asterisk scale matters but depends on how you Source these houses The crucial thing is diversity if you start scaling your episodes but keep your houses fixed or keep your houses at the levels that we used to work in this doesn't work you end up learning shortest path experts and you end up not learning backdrop but when you start scaling up diversity then you start getting generalizable agents another thing that's important is context window so lots of lots of us who work on training manipulation policies tend to notice that you don't need large histories in order to decide how to pick up an object that's because once you've got your arm in a certain position your next action tends to be determined based on just a few previous actions and observations but that's not the case with like slightly long Horizon navigation and we see going from window size of 10 to 50 to 100 and we've also gone up to 200 you keep getting improved success rates and as has been seen in vision and language over and over again better visual encoders continue to give you Improvement in performance and the best visual encoder that we found in this work was a sigp model by Google interestingly Last Mile detection continues to be a bot so if you only use RGB images but this is just an experiment simulation by the way but if you provide the coordinates of the object only when it is visible you get huge improvements across tasks and this doesn't um and and my hypothesis here is that agents are already learning to be proficient at exploration and navigation but they often tend to walk past the target object and when you start giving privileged information only when it is visible they're able to make the right uh choices and go towards the target object so that's one explanation but I think the second explanation is that I think learning overall improves because the agent does not have to use its parameters for any level of object recognition it can then focus on the tasks of navigation obstacle avoidance Etc and we see faster learning when we start providing last mild detection so this isn't something that's uh that I advocate that we do right now um but it is it's a nice learning that you know is helping us make choices in our next models okay so Spock has allowed us to you know show that simulation uh training at scale can produce proficient agents but we still aren't at the level where we want to be and one hypothesis that we've been having as we did Spock and you know move from RL to Il was that IL continues to suffer from insufficient State space exploration right this is something that we've known as a community for many many years this has gone out but it comes back okay okay on the other hand we know that reinforcement learning agents learn via interactive trial and error and they are able to explore so much more of the search space however when we look at RL approaches that we work on as a community particularly in navigation tasks we tend to use very shallow Gru architectures it's like RL work in navigation is stuck 5 years ago and never made it into like modern Transformer architectures and I think there's a reason for this it isn't laziness we've all been trying it's extremely slow as you increase the size of your model and as you go into larger and larger Transformer architectures we've noticed that you have a lot of training instabilities and so we try to fix this problem and our latest work is polyform so polyform is a large Transformer based agent it's trained with on policy RL there is no I involved it is initialized from scratch and it achieves 85% object n success across two embodiments both in Sim and in Rim the architecture of polya looks very very similar to Spock it's just drawn in a different way because it was a different paper submission so at its input is a Transformer State encoder it takes into as its input an RGB observation um polyform was only trained for navigation we just wanted to test this on navigation because it's only navigation you don't need the side camera and so this only accepts the front view camera so you have an input RGB observation you have a goal specification and you create your goal condition visual representation and then as it was in the previous diagram you provide these representations over time into a Transformer decoding the key thing here is that we started using KV caching and when you implement KV caching correctly and you use it appropriately you can get huge improvements in your training speed and this was crucial because RL is very very slow of course there are a few small differences because between RL we need an actor head and critic head but these are you know minor differences in architecture to spot there have been two critical pieces that are part of this training recipe that that got this to work one is hugely improving the number of parall rollouts and the second was having a curriculum on B size and so in the beginning of training you want to make sure that your roll outs are very small and of course this in influences your bat size and every 10 million you keep improving or sorry increasing the length of your roll out which means you you start with small bath sizes and you go up to large bath sizes and this has been you know these choic are critical in getting RL to work exceptionally well on object go navigation okay so this is what we have been able to obtain this graph interestingly looks exactly like you know the ddpo graph that we saw a few years ago for Point coal navigation when it was shown that as you go towards a billion frames of uh training you start getting you know 99. some 999 uh degree of success and and we thought as a community that okay now that point goal is solved we have to change the goal specification remove the you know the the little GPS sensor and we'll get the same thing with object goal navigation but that hasn't happened until right now so the horizontal line which is the lower line um is uh is a you know state-of-the-art past ARL method the second line which takes it from 38 to 60 is spot which is what we were able to obtain with hugely increasing the amount of data and doing imitation learning and when we use this same amount of data but we go back to RL with a new training recipe we completely break this ceiling and it is my opinion that right now we're just compute limited I think we can easily hit 90% with small modifications of the training recipe as long as somebody gives me more GPS that that is the real bottleneck right now and this works this training recipe has nothing to do with stretch we have two robots at ai2 uh stretch and L about and it works well on things okay finally I want to end with a a small extension to polyform um and this is called boxn um so one of the goals that we've had in this project is to really create what we call a foundation Explorer right a foundation Navigator and so what we consider here is we like a model that learns to avoid obstacles that learns to explore that learns to navigate but what we instead have created is an object goal Navigator we want to take a slight step up and create a universal Navigator and we want this Universal navigator to then be in a sense prompt based on the task that you want and so the change that we made here was to Simply remove the goal specification train this policy with no goal specification so it has an RGB input it has no goal but sometimes we give it a bounding box and when we give it a bounding box in the uh in the observation um if it is close to the bounding box it gets a high reall so what does this mean we're training an agent okay go around keep looking around search for this magical bounding box the bounding box will be as Bigg as room the bounding box will be as small as a little pixel the minute you see a bounding box you found some gold go towards it and you will get a high one right and this turns out to be a universal Explorer because now what you can do is you can Source this bounding box based on the task you want using an auxiliary vision and language model this allows you to do interesting things so the task here is find me a book whose title is humans and so here's a robot it doesn't get a bounding box from Vision language model it just says bounding box where's my bounding box where's my bounding box it keeps going around like a crazy person until it finds a bounding box and that bounding box only occurs from you know an off the shelf model for a book titled humans and I think this Paradigm is very powerful because it can be generalized you don't need to do bounding box any sort of image annotation that can be provided by an off-the-shelf model can now be used to train your Universal Navigator Universal manipulator and so on in order to produce generalizable policies here's the second one um this is going to do multiobject nav it is never it doesn't know what multiobject NIS but we show the Bing box and then we make the bonning Box disappear and so like again like a crazy like you know like one of these dogs looking for the tennis ball it says Bing box Bing box where's the Bounty box and goes into another room seees the Bounty box goes towards that agent um and here's a quick video showing you that you know this is nothing to do with stretch you can take this policy train it with this same recipe and now we can learn to avoid obstacles and you know in the boundy Box okay I'm going to end with same learning that I had on slide one in the rest of AI we've come to this conclusion that get me lots of data clean it up really well let's not go into hugely modular architectures let's have simpler architectures aams razor let's train it end to end and this is going to work and I really believe that this is the way we should be thinking about Robotics and um these projects are you know uh I consider you know steps in this direction and with that I will thank you you're perfectly on time so we have time for few questions um if there are anything in the audience SL but yeah in the scalability slide you showing scale as a number of training steps how do the training steps relate to Epis so right so um this is oh um the question is on this um and correct me if I'm wrong on this graph we're showing training steps how does this relate to the number of episodes so typically our episodes tend to be I would say the beginning of training episodes tend to be longer uh so you can have 100 200 Step episodes and towards the end of training the model starts getting more efficient and so it starts getting youro faster I would say amortized over the whole thing probably about divide by 50 and you get the number of episodes uh one thing I forgot to mention is this is log scale so it's not as quickly Rising as you might think yeah yeah that's a great question so the question is in in box naav are we with disentangling perception and control and so are we now losing something there so I don't think we're disentangling perception control we are still doing low-level perception because you need perception to do control what we're disent tangling is sort of high level object recognition based on outside World Knowledge so for example if I was in a bookstore and I asked the robot you know can you go get me a book I like to read you know mystery novels can you get me a you know novel that was on the New York Times bestseller um this requires outside World Knowledge uh but the robot still has to do perception um so the second part of your question is are we losing something and the answer is yes and the reason is that when we take an agent trained on box naav if you say can you find me something to eat right the object goal Navigator is like okay something to eat this is going to be in the kitchen I keep walking the minute I see a kitchen I'm going to take a right and hopefully I will go towards something that's edible in box naav it doesn't have any idea what the task is and so it is a uniform Navigator maybe that's the right word it'll go into the toilet first because it maybe the bounding box is in the toilet and so that is something that can be improved by saying I'll give you a bounding box when I see a direction that I like and so you can go towards that and then I'll give you a bounding box when I see a an object that I like right and so and this doesn't have to be done with bounding boxes you can do way points and you know there are papers that are already exploring these directions um but this was something that um we felt is a is is a way towards Universal explanation yeah so when we train in simulation the models work exceptionally well in the real world we don't see degradation but there are two asterisks the first asterisk is that in order to get this to work you have to use visual encoders that are trained in real if we train every parameter in simulation Sim to real does not work um but when you use a sigp model or a dyo model that's trained in the real world and Frozen at training time then essentially you're saying you know these are Universal visual encoders they work as well in the real as they do in simulation and we find that the embedding spaces are aligned within simulation and real and that is sufficient to get transfer they're aligned but they're not exact and so the second asterisk is that the key to getting Sim tooreal domain Gap to essentially disappear is Extreme augmentation in syion and so you just like mess up your image to the point of like there's only some information there all the you know there's huge amount of Jitter in terms of colors in terms of brightness in terms of contrast and so on and the models one more short question so the question is longer maybe after so thanks for question about the in navigation you mentioned you need a very long I was wondering is it about what you remember could you you get much yeah I think it depends on the task so we're also interested in tasks like can you count how many apples there are in the room so you need larger context lines unless you have Excell memory to write into but if you're just saying you know if you're doing box nav you know you just have to remember well I've already been to the kitchen I should not go into the kitchen again we're also exploring interesting ideas on how do you have a dynamic compx life let's like to speak one time um uh four people yes that is including the moderator uh one plus Mo yes basically I'm putting it there and this it's on um okay cool sound pleas loudly move it as close as possible for the podcast here we go yes okay uh good morning everyone uh I'm ad fotti uh I'm the moderator for this panel and uh and also one of the organizers of the fifth andot AI Workshop here tvpr in Seattle uh would like to start with a a warm welcome to everyone uh for being here at this uh panel session so this panel session is themed uh advancing inata AI uh towards simless integration of perception and action so in this panel session uh we'll delve into the fascinating world of embod AI uh we'll look at some of the remarkable advances uh that multile moldo Foundation models that brought to the field of Robotics uh we'll look at some of the opportunities and uh perhaps challenges to the research and scientific Community been bought Ai and perhaps we'll also explore um some of the opportunities that AI has brought to the field of Robotics so perhaps impacts like uh Advanced uh higher level reasoning perhaps lower level controls so um yeah so before we start a little bit about me I work in Microsoft research uh I lead research incubations efforts globally in the Microsoft research accelerator team uh my background is in computer science uh in autonomous systems and my research interest caught across the intersections of AI Robotics and simulation so I'm also one of the leaders of the embod AI working group uh in Mar research so really pleased to be here I've got a fascinating lineup of speakers I've got two corporate Executives from Microsoft and one executive from Sanctuary AI uh I'll let them just briefly introduce themselves say a little bit about their backgrounds then we'll get started all right um yeah so all these Executives this will either be the most interesting panel or the least interesting panel of the day um I'm Ashley Len I'm uh with Microsoft research um I've been in and around the fields of AI and uh robotics autonomous systems for about 20 years um and when I first started working in uh in this space nothing in the robotic stack was learned and we were sort of weird uh you know in the you know in the engineering and in this in the systems engineering because we were using kernel methods and support Vector machines to start running classifiers on systems uh and so it's been fascinating to see how much and what the really potential is for most or maybe even all of that uh robotics stack to be learned um when I think about embodied agents I think about agents that act on our behalf um when I think about um embodied agents in particular I see the need for of course these fundamental capabilities of perception uh planning and action and we've seen now perception evolve first right from uh you know from kernel methods to to do classification to to cnns now to to generative architectures um and even language uh grounded models uh kind of at the perception um and then you know now we also see an evolution at the control layer right from uh from model predictive control to I think it's been interesting to see methods like deep reinforcement learning sort of quietly reduced to practice uh you know at that at that low level and so now what I'm really interested in uh is you know potential scalable foundations that connect uh perception capabilities with low-level control in this area of planning and uh it's interesting to see architectures now that you know kind of have this perception and action learned but still kind of explicitly constructed in the middle um you know one one such experiment was recently conducted by Microsoft colleagues manipulation example where where um GPT 4V was used for learning from observation so demonstrated manipulation task GPT 4V annotates the steps along the task uh and then those steps uh are then repeated on different zero shot transfer to different robot robot platforms that are essentially activating kind of motion Primitives learn from reinforcement learning on the bottom but in the middle so learn perception uh learned control but in the middle then you've got this explicitly constructed uh almost like a PhD thesis worth of uh you know kind of explicitly constructed action language in the middle and so what I'm really fascinated and interested in is what kind of architectures can allow that uh you know perception action Loop to be clear uh essentially closed with with scalable learned foundations for perception uh for for planning and prediction and uh for low-level control so that's just just to frame up my my Fascination and then Focus recently in the space well thanks Ashley I think you got us up to a fire start thanks uh hi everyone very nice to be here uh with a group of like-minded individuals all furthering the discussion on embodied AI as you've heard my name is Olivia Norton I'm one of the co-founders at Sanctuary Ai and Sanctuary is a company with a mission to create the first human-like intelligence in general purpose robot so if you're familiar with our Phoenix robot you'll have a sense of what I'm talking about um we are well I've been in in this space for 10 years now over two companies the first of which was a company called Kindred AI it was founded on a very similar mission and vision ultimately acquired by Cado in 2020 and Sanctuary has been a redoubling of our effort to create these intelligent embodied machines we have a particular focus and are somewhat unique in the space in that uh we are interested in the interface where the mind or the AI meets the world of work so in the context of applications in labor augmentation the hands the dextrous end defectors that we provide our Hardware platforms are this critical uh uh piece to allowing work to happen uh by these physical systems the incredible impacts in the digital world of these Transformer models over the last seven years with the particular acceleration in the last couple has uh reinforced our belief that embodied AI is going to be the next major Frontier for impacting the way that we live and work at a huge scale so between declining birth rates um uh aging populations a changing view on the world of work in in in in people we uh expect these systems these intelligent embodied systems to be a a huge Factor in global productivity measures in the next short while so this is the application of all the work that that this group is is doing um we are very excited to be collaborating with with Microsoft in uh innovating in in AI models in embodied research areas like reasoning like planning like human interaction and developing these uh uh AI models for for humanoid robotic control we do have a booth here starting tomorrow so if you're interested in learning a little bit more about Sanctuary I'd be very glad to have you combined meet the team and learn more about what we do and I'll pause saying uh thank you Ed all right thanks Olivia thanks great um really privileged to be here next to these esteemed colleagues my name is Stephen batish I've been on Microsoft for 25 years I'm a technical fellow and I manage a group called The apply Sciences group uh we support uh the windows and devices division uh we're a multidisiplinary team that works uh across the entire spectrum of of of the various Fields uh we develop most of the AI that ships in Windows an example and a lot of different Hardware Technologies our main main goal in Ultimate vision is to actually help create the next computer um and and I think in this space in the field of Robotics I think we can stly see that uh you robots are potentially the next the next form factor that's going to impact our lives in many different areas which are really excited about in my former life uh before Microsoft I uh worked in um in bioengineering and neurobiology ology um combined with uh with robotics I built a because I wasn't really a good programmer back then so um I built a um a hybrid robot that used the mo as a control system so the moth was basically uh glued to the front of a car a remote control RC car uh and um uh in the moth with using the evoked Action potentials of the flight muscles of the moth it would actually uh steer the the car around and control it so was a really easy way of uh stealing a control system from nature which was extremely sophisticated with an onboard power supply an onboard Vision Processor and a neural network that was quite sophisticated at the time this was in the 20s um and uh it was a pretty pretty cool but anyways now we have um artificial systems now that can replace um such things so pretty excited about uh seeing what we can do in that space all right great thanks Stevie Olivia Ashley so we'll uh we'll have an interactive uh conversation with um the panelist here we'll also pause at some point for questions so we're can make it more interactive uh perhaps without further Ado I think a great place to start I'll start with you Ashley uh so Ashley you would agree with me there's been tremendous innovation in the space of embod AI uh we''re seeing remarkable progress across several Dimensions uh what's driving this progress you know and what more do we have to do yeah I kind of opened with with some uh fiery I guess comments uh on this on this before but um you know the the Advent of of the you know Transformer architectures and just generative architectures in general that are able to uh be scaled with data we saw a really interesting talk uh before uh before this one on uh use of of simulation environments to uh to create data sets I think those are really interesting of course we've got uh the sim toore problem to worry about as well and thinking about uh contact physics especially for uh things like soft body uh manipulation you know folding uh folding a tablecloth and things like that so um I will I will agree that in general we need to figure out how to get these scale effects especially for um at that intermediate uh planning uh planning layer in these embodied AI architectures got it okay sounds great uh so Olivia so I think some of the emerging mythologies we're seeing for embodiment embod a about AI uh they're increasingly humanoid form and uh I know Sanctuary has this highly Dex humanoid robot Phoenix so I think it'll be interesting for the audience if you could describe how you're thinking about briding the gap between perception and action yes let me start with the humanoid form factor I I suspect that folks are generally following This Thread but we uh as an organization are really focused on this world of work on augmenting uh labor areas where we don't have the folks to support those roles where we U are looking at safety related applications and you don't want people in those jobs or they're just fundamentally things that that people don't want to be doing that they could be doing something more interesting and all of these uh all of these applications are in this perception action context that is human Centric so the objects and the environments are defined by people for uh work by people and the humanoid form is the the perfect form for that design uh constraint and so that is one of the reasons to bind ourselves to this human form factor the the multimodal data acquisition capability of a hardware platform like Phoenix is of course just very interesting from an academic perspective as well uh Vision touch uh audition and propr reception providing that uh uh robot firstperson perspective that allows us to learn new skills and automate activities that the systems can perform we as with everybody in the audience the last talk uh suffer from the same bottlenecks of data acquisition that that everybody else does uh we do some of it the hard way in the sense of a second per second data acquisition approach uh tele operation on physical Hardware but of course they're always looking for opportunities to augment the the data that we have in terms of deriving the most value from perhaps limited amounts and to uh search for alternative perhaps lower quality but but uh still valuable uh data sources uh you know simulation is a great example certain tasks and applications are um suited to to leveraging simulated data and some are less so uh to Ashley's Point earlier around soft body and contact physics um the the data variety piece is something that I do just want to touch on because we also believe that this is an incredibly important inflection point in regards to developing these large Foundation models large Behavior models for a system like Phoenix and we expect that variety to be coming by uh virtue of deploying in in real worlds in commercial applications in operations um out in in in industry and we began this process last year uh we began in a in a front of house and back of house retail context with a large retailer in Canada and are seeing that translate now to manufacturing assembly and Automotive applications here this year so um all of the same data related challenges but uh but but looking to that that final application space to direct all right can I can I chime in it's really it's a it's a pleasure and a privilege to be working with Partners like sanctuary and others it is interesting to think about whether the humanoid form factor is kind of an end state or a waypoint in form factors um it does seem to be relevant right now that we have all this large scale language data and it contains all this inherent knowledge about um operating in humanik environments with humanik affordances and capabilities and so as as a waypoint it's really interesting time to be looking at the humanoid form factor given uh the kind of implicit alignment with a lot of the a lot of the large scale language data that we have right now very well said yeah no U I really like that we're going to come back to Phoenix uh there are several folks in the room I'm sure who uh perhaps would love to learn more about this highly dexterous uh embodied uh agent but uh I do want to uh so Stevie perhaps this one's for you um so I think uh a lot of folks in this room will love to make this real so we're in a research environment but you know let's talk about how to make this real so how we do describe the sort of evolutionary phase where we're at for now and perhaps the sort of computational or Hardware requirements that we need as a research Community to unlock advances in this space yeah that's a great question yeah um yeah in my intro I talked about the you know building the next uh the next computer form factor and if you actually look at all the VAR um uh uh forms of computers we've had in our history you notice that they all follow a very particular pattern um and uh you know every one of them even all the way down to uh you know even the slide ruler uh you have a step function increasing compute that allowed us to essentially write software that abstracts the functionality of that compute um and changes how people interact with that computer uh for the most part and that allows us to essentially build a different type of form that does different things goes in different environments and you see that like over and over again if you go from you know like you you know hardcoded computers to vacuum tubes to when the transistor came along to gpus every one of them basically introduced you know a step function increas compute today is no different um today you see especially with uh with tensor accelerators you know a massive increase in compute not just not just in top speed and ability to inference and train large models but but but also at the same time to do compute efficiently and I think the combination of high compute in in with computational efficiency is going to change the field of Robotics drastically so if you can imagine like you know especially in a humanoid robot the amount of data that floats around inside um inside a machine the tactile sensors the vision all the encoders the muscle and how fast the competition has to happen so you get a nice closed form compute and control system um and so having local compute on the device in conjunction with cloud of course um allows you to design a a uh a computer but not just high computational workloads again efficient because we want these things to be mobile you can't walk around with 200 watt you know uh processor uh moving around it's not very efficient um so as an example uh you know in fact today we just GA um uh kind of a new generation of of uh silicon in the windows ecosystem um and it is uh basically defined as a a neuro processing unit that can do about 45 trillion operations per second but does it under four Watts which is tremendous to think about like so before you imagine like you know we used to you know be happy with you know two teraflops and about 20 30 watts of power um now you know you have a massive step function increase in compute with comput compal efficiency and and I see no end to that sort of thing in fact a lot of the a lot of the um you know our computational SK laws have kind of shifted towards domain specific silicon Innovation such as ensal accelerators that are really just designed to handle um you know neural network workloads um and they do it well they may manage data well through the system and as a result you can do some pretty uh pretty profound things so I think that's going to have a profound impact in in the field of of Robotics um especially as more and more of the robot control system shifts towards uh neural network architectures oh that's fascinating um so I'm going to pause and perhaps if there's any questions or comments from folks in the audience we'll take it and uh we'll go back to the panel list with some questions yeah go ahead yes go for it enforement learning and invitation learning um does that make simulation kind of less important here because it seems like the winners into space are benefiting from scaling these real real world uh like training offline training data like Tesla for example and then does that imply that like the challenges in this Workshop could take more of like a like like you know getting you offline reportings as data sets to simulation oh yeah so uh for can you do you want to for the zoom can you repeat it I think you you repeat it from microphone yeah yeah so for folks on the zoom the question was more about simulation versus data sets offline dat offline data sets and then AO context right yeah who wants to take that you want to start talk about that yesterday it's a recurring discussion in this Workshop I was just to say I mean it's absolutely both and the companies that we've seen that are doing well in the space view the combination of simulation to bootstrap and and and learned but but uh real world data brings you diversity and scale especially when it's actually put to put to good use so we've you know we've talked to a number of different companies that have started that flywheel um uh there but I I think in conjunction to the real world data you have kind of simulation as well um to help uh manage things and and certainly I think you've seen that you know even just simulation alone isn't the only solution and you you found the limitation of the result of that yeah and uh one of the other factors that feature strongly for us um as you're making architecture learning architecture decisions simulation is a huge Factor um validating new morphologies is another big piece for us as well so it's it's not a single uh uh single benefit that is being derived from Simmons and aggregate Yeah Ashley I think they covered it well okay uh Anthony we had a question from the SL what does human life intelligence you uh for the folks in Zoom what does humanik intelligence look like let me describe how I think about this but but there are many opinions so I'll start I'll start by by cting um in the context of the the work augmentation the labor augmentation Focus that that we have it's really in this space of are these systems behaving as you would expect a person to be able to do uh in the context of the SOP the operation in the context of perhaps The Edge case some sort of failure mode that comes up applying common sense to the decision-making in uh the the constraints of the the operation um broadening that to something more definitional anybody want to take a cut out well yeah it's interesting because I I I think humanlike intelligence or the aspiration to achieve humanlike intelligence has been an inspiration for the field for many years but as we sit today I I often wonder if it's a bit of a red a red herring um relative to so so I think I think a few things are important so the ability of a system to accomplish human goals uh in a variety of different environments uh under a variety of different conditions variety of different tasks I think is important the ability of systems to do that in effective collaboration and with effective oversight by humans uh is also important so those are the two things I think of as uh you know most important now in some cases I think it's good for systems to be able to do things in a humanlike way so they don't freak us out uh but but but this is kind of a I think a nice to have in in some situations versus the the penultimate aspiration yeah so uh perhaps maybe another round of questions so we could spark uh and elicit some more um questions or feedback um I'll go to uh Stevie so Stevie 45 tops uh that's pretty non-trivial can you speak a little bit more about these npus newer processing units um which work based on tensors and array of arrays it will be fascinating for the audience to learn more about this yeah I mean as I was I was mentioning the you know we're uh you know I think this year we kind of crossed that threshold where we're able to run um you know these uh uh Transformer uh models um at U uh very low latency um uh uh scale so for example we run if you're familiar with the small language model fee and fe3 that Microsoft just released which is incredible performance in terms of uh its um its um performance relative to some of the much even larger uh language models that are out there uh you know we are able to execute that model on these devices again under four Watts continuously in real time with a first open latency of less than a second um and that is uh pretty impressive especially when we start shifting towards multimodal models that uh want to incorporate um you know even even more diverse uh functionality and performance a result of that so I'm pretty excited about and they're they're more difficult to use than than than gpus are um and CPUs are um but uh but the tools that are tools uh are out there that've been kind of uh brewing in the ecosystem for at least uh you know 15 years um Microsoft has a uh uh inferencing runtime called um ort onx runtime that um interfaces with these various neur processing units quite well and tries to make it simpler and easier to use so um I think you know I think more and more people will use it I can imagine you know just like we used to have the Intel Nooks that people were kind of enamored by these little small compan devices that people are going to have these kind of you know um you know High powerered AI um uh boxes that they're going to be able to use to prototype up all sorts of things on on on the robots all right thanks Stevie so uh you know Olivia um it'll be interesting to learn more about Phoenix for the audience we've come a long way uh there's several new paradigms introduced since the early days of humanoids for folks who are familiar with the televox or electroman uh we've come a long way uh but can you speak a little bit more about Phoenix just you know give us some Polish a little bit more color about Phoenix yeah certainly so this system stands about the same height as I do so six or 5 seven6 I wish um and uh it's it's a a very fast actuation it provides a high degree of Freedom hands so 17 degree of Freedom hands um verging on the degrees of freedom that you'd start to see in a human hand in terms of the geometry one of the things to call out is and this is a little bit different from other form factors is that all of the vision related sensory information is coming from that first- person perspective in the head so one of the questions that we often get is you know wrist cameras or finger cameras or other ways of providing additional information uh to um you know close proximity grasping type of tasks for manipulation and this brings up the the the whole discussion around touch sensing and the way that touch sensing um uh provides us as people with incredible awareness of our environment uh uh if you anesthetize your fingertips and attempt to do something like light a match you can do this visually it takes some practice because you have to cage that object using your your eyes uh and you're operating at about 20% of the performance of your your normal unanesthetized uh sort of physiology and this this is the same type of thing that we see in systems that are equipped solely with with vision in the sense that as soon as they're working with small objects you're running into occlusion issues everybody has probably dealt with this and you need to either move to a different modality that is closer to the object or you need to be providing that touch sensor information of the type that we get so compliant uh touch sensors providing Force set of granularity that is useful for doing in-hand dextrous manipulation and object reorientation so that's one of the the key aspects of Phoenix that is um worth highlighting and is sort of unique in the field got it oh that's great and Phoenix is not in the building Phoenix is not in the building all right I think we're running uh we're running on time I wanted to throw one more at Ashley so Ashley you've been in this field of autonom system for a while so I'm going to pick on you uh we've seen through Menace advances uh going from classical theoretical robotics now body AI um now we're seeing um language condition models uh that are now and now this evolution of action as a language can you speak about what the future hes we want to hear a prediction from you a prediction wow these are these are really hard these days um yeah I think just on this note of of of action languages it's really interesting to think about parallels with natural language and we've got natural language systems that are clearly multilingual um and maybe the promise of course there's things like low resource languages that we have to figure out and get right um but we can see the Promise for some something like a universal uh language uh or at least a universal translator among languages and so one interesting question as as we enter the embodied AI space is do we see the potential for that in embodied AI something that is fluent and uh universally fluent in action languages whether that's in my household or industrial manufacturing uh or even some of the consumer experiences that STV works on and so I think this is a really fundamental question we going to have things that are kind of domain specific or do we see the potential for a kind of universal uh action language that so that that's not a prediction but it's a question that I find fascinating and think it's relevant to the kind of architectures we'll consider in the space absolutely no thanks add I mean I think just to add to Ashley's point I mean we're moving in the world where uh we're moving into much more agentic World agents are going to be the mode and the the programming model of future for a lot of people and and those agents are going to be manifested in all the different form factors of computers that we have so you'll be able to to have um to interact in a natural humanlike manner with these agents and they will do be able to accomplish tasks for you in an automated manner whether it's in the virtual world or in the physical one and I think uh that's kind of the exciting thing world that we're kind of moving towards I think fairly quickly okay so we're we're on time uh perhaps uh maybe we're gonna take one more question I'll like to do a round up and summary we're on time okay uh I do want to say a big thank you to my panelist ashle Olivia Stevie uh fascinating discussion uh thank you to the audience and I do want to say special thank you to my fellow organizers and Lead organizers Claudia um Lucas and Anthony uh so let's continue the dialogue as a research and scientific Community uh so the Innovations in emata AI is evolving and emerging to be a team sport so we're all going to uh solve some of these trackable problems together so thank you thanks great job okay next up we're going to have uh thank you thank you um next up we're gonna have still there Ryan I think can you hear us I think you can present uh Blake can you mute thank you okay can we Brian can you Brian can you hear yep can you hear me all right we we can hear you I'm GNA put you on the the speaker cool great and I assume you can see my slides and everything yes looks great yes looks great so this is uh Brian nixer who is going to be talking to us about Foundation models for Robotics and Robotics for foundation models take it away yeah thanks Anthony um yeah so I'm basically gonna go through a couple recent lines of work that are about basically you know how deeply can we use Foundation models for Robotics and how can we pose robotics in a way that can benefit the most from Foundation models and then also what kind of lessons from robotics can we apply to uh improve Foundation models um and downstream performance so uh a lot of this work was going to done at Google deep mine but uh right now I'm at physical intelligence I I'll talk a little bit about that at the at the very end um but yeah so Foundation models obviously have made just kind of huge leaps over the last five to 10 years uh taking in the internet's worth of data and translating it to Downstream really high quality performance on reasoning tasks and a broad range of very complicated tasks now this is sort of this like you know I I think some people have said like almost an alien technology dropped on us as especially as roboticist that there's suddenly this vast knowledge base that we can use to enable us to do all sorts of tasks and all and get a lot of generality that we didn't think necessarily was possible for robots where we don't have them in a wide uh variety of circumstances right now and places in the world now they've obviously uh touched so many different things from starting from just the language models where you know now they can act as a very high quality doctor to doing vqa tasks which bring us even closer to grounding for robotic tasks but we can also take in um audio start being more embodied like within a phone with some of the a lot of the recent announcements around bringing AI to uh more physical worlds uh we can generate images generate videos and even generate audio so I think like you know a lot of these things are extremely natural for the way that a robot would interact with the world it needs to be able to see and hear and move but there's also a couple challenges that that exist there like a lot of these don't have real interaction with the world they're always this um very uh pulled apart use case uh versus having had experience in the world where you change something and your decision really affects it so this is some problems that I think like robotics can can really bring we we do a lot of work on optimization and planning and I think these are areas where we can sort of uh help Downstream Foundation models but you know there's uh I guess at at the core of it we have this like wave of uh this technology going and I mean I think I've used this like image a bunch of times but I really believe that at the the Crux we want to just use as much of it we can to propel robotics forward but I also think robotics has a chance to sort of like prop up Foundation models and fill in these gaps this like embodied experience the uh knowledge of the physical world the knowledge of decisionmaking and the like this actual experience in doing it so I'll talk a little bit about some of these like um problems that that Foundation models have and ways in which uh lessons from robotics or data from robotics has been able to to improve them so one of the first questions that I that I really want to think about is how can we make a robotic control as close to Foundation models as possible so you know there's if we if we take some early work like Sean or others where we do this sort of highlevel reasoning you know it's pretty clear how to pose robotics in the space of language or in the space of vision language models where you have this high level task and you output something that the robot can do so this makes a lot of sense there's a clear natural language for it but when you get to something like um say manipulation or navigation becomes a lot less clear how we can get the most out of robotic control so a couple ways we might be able to do this is sort of like a rt2 style where you just act like um actions are language and train the model to do this now I I think like one framework that I like to think about here is if you're the human you're you as a human are put in this circumstance what what control modality would you be able to use and and I think this is a good proxy for what a foundation model is going to be able to use where we're going to get the most out of the foundation models so here if I then ask you you know you have these two blocks and I want you to pick up the the red block and the rt2 framework I would say now just put out some numbers for me like uh you know the move in the X YZ this amount this is a you you know it's not super clear exactly how you would be able to do that you don't know this mapping to actions it would take you a lot of experience to me to do this at a really high at a really high rate now if I said uh you can respond natural language you might be able to say move to the left or right but actually like the Blue Block is a little right and a little forward uh the red block is a little left so I think we really need to think about what sort of framework uh we can build that allows us ourselves to be able to control a robot and if we can do that then probably a foundation model can do a lot to learn from it so the idea that I guess that I'll talk about here is we can actually use visual prompting so if I now draw on this image and I ask you you know which these actions which of these arrows would you follow to do it it's extremely clear to you that for the red block you do one and for the Blue Block you do number two and indeed we've seen uh you know some from some previous work uh one about uh what clip understands about red circles you can actually visually annotate images and VMS understand them pretty natively you know without any sort of I mean this is clip so it's not even a more complicated VM it's it's and it works quite well to identify These Fine features that are otherwise a little bit different difficult to explicitly understand now there's these are a little more like object recognition tasks but on the case on the left the path actually matters too but I think this is something that we can see that we natively can understand and that a uh Foundation model can probably understand that this is the trajectory that you want to follow it's it's something more nuanced than necessarily just object recognition so the technique that I'm going to talk about today is called pivot prompting with visual iterative optimization and there's a couple things going on here one is this idea of visual prompting and the other one is something that's uh you know more akin to like cem where we do an iterative optimization and the way that this works is given a a robot so maybe the robot is a um is a an arm with a wrist camera or a um or a uh something doing navigation and I say go to a specific task I basically want to sample a bunch of actions that I could potentially do so this is sampling in this AC space that I presumably have well represented I then annotate the image with a visual representation of that action so you can basically project that action into the camera frame that this is the movement of the end affector or this is the movement of the vehicle that's driving around to be able to do the task I can then ask a VM which of these ones should I do to be able to say pick up the red block which of these should I follow to uh move to the green block or to avoid something or to you know navigate between them and a VM can actually at a decent rate predict you know these are the best arrows for this so it picks maybe arrows two and seven for the red block and three and five for the for the green block um and what we do is well first of all sometimes it makes mistakes and other times this is still you know not the explicit action to take so this is where this iterative optimization this almost like CM like approach comes in we actually fit a distribution to the respon the VM we resample it and then do this task again so we iteratively basically hone in on the optimal action for the robot to take and then we execute it now what this looks like in the real world you can kind of see the distribution Converge on this microfiber cloth where at first uh it answers a couple ones that are over it we fit that distribution to it and then we uh eventually select something that moves Us in the correct direction and it's important to remember that like at the underlying level the action that's being represented really is in the action space we're just projecting into the camera space for the uh for the VM to understand it it's also important to realize that this is a actual trajectory you know it's it's an action movement it's not just necessarily a um a final object recognition you really need to like move around things at times so in the navigation case and I I'll show some examples where where that matters so this is what's nice about this is very general all you really need is this like sample action space and then some way to put it in the camera frame frame so we as as part of this work we put it on five different tasks uh four different types of robots one doing navigation uh where you have some ego frame View and a um and you can turn or go forward um one where we have a third person camera view for a robot arm so the action space initializes at the end effector we can also do it in a rist camera frame we can do it in something more like a Transformer Network style where there's just like a pick and place location or we can even do more referring expressions from um say the the the more VM Community now when we actually apply it on the robot you can see that it it works pretty well and this is again all zero shot nothing's trained to do this um we can say find tissue paper uh we can also say avoid a wooden avoid the uh obstacle or something like that and now this is where this like object recognition varies from something more from uh like from pivot or visual prompting you can Rec you can actually uh represent something more than just a final location you represent the entire path and then optimize over it so this is like a very nice I think General approach to being able to get actions out of uh out of the model we can also do manipulation so here where to put the orange it's very clear that you should should probably put it with the other oranges and we're able to extract the semantic knowledge out of the VM and do it uh zero shot without having to train anything or collect any data we can put the grapes there we can do the banana and it can do this relatively robustly in part due to this uh iterative optimization which I'll I'll talk about next so again the way it works is we sample some actions ask the VM fit a distribution and then do this repeatedly and so so on each of these plots we show both how many steps we do of this iterative optimization and also we can do this over multiple parallel calls at each iteration so sample from this action distribution a bunch of times and each of these are ways uh methods from sort of robust optimization in order to fix mistakes that the VM might make and so in each of these cases we see that the performance actually improves with number of iterations and for the most part it improves with the number of parallel calls so at times uh down is better I think the top two rows so we have um referring Expressions where we want to be closer and indeed uh we get better and we're also more accurate which is the second plot um the number of parallel calls seems to help kind of across the board in each of these so really clearly we're like robustifying this in in a in a very general way now another thing that I'm sort of particularly excited about within this framework is that it scales really well with model size so as our as the VMS become more performant we also get zero shot a much more a much better answer so not only can you kind of like improve in this way but we can also go and find tune the models and have them do even better on this task so I I think this is something that we can really look at in the future is basically what do you get zero shot in this task and then how well does it do fine tune because this zero shot performance is going to be in some way indicative of both highle performance fine tuned and also to a degree the ability for the model to sort of generalize to new tasks when you go go and deploy it in in the world there are however some limitations that we if we look at it right now so these are places where I'd say Foundation models are somewhat limited uh in in their current form they haven't had this uh sort of interaction with the world they haven't had this embodied robotic experience uh some of the common sense that we have from from really interacting with the world so if we look at the progress over the episode uh on the the left here this is basically you can you can find some really key points so at first on the approach to the object the performance is quite high but it degrades as you get closer and closer to the object in part because it's hard to see what's happening and it's hard to really distinguish where to make the the grass Point um once it's picked up it improves again or once it once it's grasped it it it improves a little bit again but then this like lifted off the ground so here is like pick up an object um it it the performance overall decreases and you can see how this is kind of hard you know first the image is fairly uded um and by the gripper or by even our like visual annotations so I think there's something in this we need to really like fine-tune these models or add this type of knowledge this um you know potentially video data uh some some way to get this like kind of embodied in action experience into the models to improve these particularly hard things and you can see this really on the the bottom here where you know maybe with the the um optimal many iterations and many parallel calls we uh are able to approach the object you know at 100% across a few trials but the grasp still isn't at 100% And this is like a relatively easy task so I think we see that we we move in the right direction with this but there's still a lot of missing 3D knowledge interaction knowledge um within the underlying Foundation models i' also I'll quickly talk about a couple other works that sort of have leveraged the same similar thing so this work uh Convoy um has a similar setup but in this case the uh the visual prompted results then go down to a motion planner which can resolve a lot of the errors that we would see so this motion planner obviously has more experience or has more understanding of the uh dynamics of the world and I think need to think about algorithms in this way where we can uh sort of pair them with uh more optimization and um knowledge that we gain from Robotics and interaction uh throughout through the world another thing that we see that that fails pretty badly uh both natively in Foundation models and similarly in pivot is this like spatial reasoning understanding so it might know okay you can approach this way and get close but the actual you're very close you're a couple centimeters away this depth and general like semantic reasoning is something that's been fairly well known to not do very well within um VMS now there there's a lot of issues with some of the pre-training that can cause this but in general this is something in robotics we can do quite well we have sensors that that track a depth um we have maps and occupancy grids so one of in this work what we did was we built a semantic filtering pipeline where we uh generated data with space where reasoning with both uh relationships um depth estimations we also use depth data from uh collected on the robot and then we trained a VM to be able to do more semantic tasks and you do quickly see an improvement overall in the ability of that VM to do spatial reasoning tasks depth estimation and it generalizes to things that it hasn't seen before one I think like very important thing to highlight from this work is where we really saw the most gains were on robotic data where we had good depth data um and like well understood scenes so I think the more that robotics can kind of collect this wide range of data and then um trickle it down into Foundation models can work quite well now another thing that I think we've seen in Foundation models is that and particularly in language models is that reasoning starts to work fairly well at some scale so we see in a chain of thought that if you break the problem down into several reasoning steps the performance overall improves um we also have seen within the like uh I guess both through something like Code Pilot but also works like codus policies within robotics that language models can code quite well and use this as a way to interact with the physical world uh clearly like these reasoning capabilities through code work fairly well so what's nice with code is it has this like very general I guess structure that allows you to build fairly complicated programs and then run things explicitly and get um you know a deterministic clear um computational number but some tasks are really hard to to do in this way so you know this uh tennis ball tasks might be a little bit easy but if we made it more semantic and said you know uh for maybe the second one if there were apples oranges and other objects that how many how much food does the does the cafeteria have I actually can't see see the thing over the zoom uh window but the uh essentially there's a lot of things that are very difficult to express through code but somehow you know if I'm given a very complicated task I often might solve it through writing code so there's something in this sort of like interweaved um knowledge common sense and code and so the the idea in this work and we call it chain of code is that we use language models to both write code um to solve General purpose reasoning tasks but in times where it's maybe difficult to express something in code we can simulate the output um of that code so I I'll talk to what that means in this next slide so this work chain of code uh has the language model as I said both write the code but also try to execute the code if it's not executable and so what this looks like and if we look at this uh list below for example how many how many countries have I been to and a list of a bunch of places if you just throw it into um you know GPT as a question it makes a fair amount of mistakes you know it's somewhat stochastic whether it gets it right but for the most part it doesn't get it right this is like a really long list of places that you might have traveled to and it's difficult because you need to track uh every place in which country it is there's clearly this like back and forth between like counting and and some more like arithmetic and also this Common Sense knowledge Chain of Thought improves a fair amount you're able to kind of write write down you know all these like locations and the countries and and and what cities are there and then make the count and I overall it does it gets closer in its estimate but I still see it make a lot of mistakes so this is where something like tool use makes a big difference but it's a little bit difficult to do unless you really are working back and forth between the code and these reasoning steps so this is where a chain of code comes in where the idea is to First generate a set of code um to run to answer this question so in this case you know you make a list of all the places and then you Loop through those places but you get to something very semantic what country is this uh is this place in you know maybe you could have some map API that you look up but you can imagine a much more General semantic question where this where this doesn't hold very well so instead while we're like running this program instead of erroring out that this uh you know this function doesn't exist or is too hard to write semantically we instead simulate it with a language model and the language model can do quite well at this this is something that was known from U for example scratchpad prompting previously that you can simulate the output of code um and then we can amend the program State and continue on so the the way it works again is we generate a overall reasoning Trace in code and then we go through and execute anything possible with the python interpreter and if not then we do it then we have the language model uh generate some a trace that it believes is reasonable now this turns out to work really well it has the state-of-the-art on big bench hard which was a subset of very challenging reasoning problems from the big bench Benchmark uh we get a pretty good gain over Chain of Thought and particularly we actually outperform human Raiders in in like a I guess 18 out of 23 tasks so a very large proportion of tasks compared to something like just writing python code we actually uh see it performs so this is the uh this one here it performs uh 20% worse uh and even you know just writing python code actually does worse than um or I guess this is compared to to human Baseline um but compared to so Chain of Thought out performance human Baseline across in general um but just writing code doesn't work for a lot of these tasks because they're they're fairly semantic and Common Sense and I'll break it down by those in the in the next slide but we actually see that that our approach that this interweaving of common sense and code together performs much much better than both humans and Chain of Thought now if we break it down by the type of task we see that in for example the tasks that are like fundamentally natural language processing we perform about as well as something like Chain of Thought uh but importantly we don't lose anything even though we're expressing in code which we might think of as a less natural way to do this versus just writing down your reasoning but the syntactic structure code is so uh so General and useful but then on more algorithmic tasks we obviously perform much much better we actually perform better than than the human than even the best humans do on average so this ability to leverage code is extremely powerful when you get to those more types of tasks it performs really well uh compared to even chat models that use tools um this is I think in a large part because this like explicit code structure provides uh a lot of a lot of um powerful uh reasoning abilities and it performs well for both small models and large models uh we can see that as the model scale and that's like the x-axis in each of these Chain of Thought uh famously only emerges at scale so you see that the blue lines for example do about the same as direct prompting without Chain of Thought until you get to the really large scale really performant models but chain of code actually performs well whether in the small models or the large models and a lot of this is driven by the algorithmic tasks where this like coding uh emerges a little bit earlier uh and with less performant models uh though though to a degree I guess it's the coding and the semantic reasoning because you can see this red plot does a little better but the uh our chain of code performs much much better uh sort of at all all scales here now you can also apply it very naturally to a robotics tasks because robotics often has a a lot of kind of semantic and algorithmic reasoning together inter interweaved you also have these apis that you might want to call perception and control ones uh and you want to interact with with the user so here's uh a task where we're sorting compost and recycling across two so here you really need to both call these apis but also reason about given the detection what objects are there that an orange peel is compostable for instance so what I'm working on now is thinking about if we scale these ideas uh to really bring general purpose AI into the physical world and so I'll just a very brief slide but recently with with several colleagues founded a company physical intelligence and we've been focusing a lot on how can we scale robotics data how can we scale robotics Foundation models and the research breakthroughs we've seen to be able to do new tasks we see uh shirt folding working pretty well dextrous tasks uh busing which requires some semantic knowledge of what to throw away and what to bus um we can see fine gr control so we really believe that at some level this scale will both allow uh robotics to thrive with Foundation models but also contribute back to Foundation models as they're a big source of high quality data and interaction with the world so in summary I think we should really think about both the the strengths of foundation models for robotics but also Robotics and things like optimization and robustness uh and reasoning and planning that that can contribute back up to Foundation models and I think that's it but uh like to thank all all my collaborators and colleagues um for and I think I guess I have a couple minutes for questions yeah so we have um we'll take some questions from the audience but I wanted to start with one from um uh the slack what can you do at physical intelligence that you can't couldn't do at Google Deep Mind I mean I think there's um a degree of really like scaled focus in this idea that that's very important I think uh dine has a lot of ability to do exploration and obviously very good access to Foundation models but one thing that we think is extremely important is the ability to put robots out into the world to really experience and uh interact with the world and build up this general purpose uh data and uh experience I guess uh repertoire and I think that's something that can relatively challenging um otherwise any questions from the audience so the question for the zoom folks is uh what's the bottleneck with Foundation models this is the data the the model the code uh I mean I think it's probably a mix of of each of these uh I mean I think on the like I guess let's say like code level there's like inference speed and things like this I think these are very uh approachable problems but still big ones for deploying in the world uh I think there's a lot of missing data of this interactions I mean this is sort of the nature of uh training on this large scale internet database versus actual interactions with the world and I think that that contributes both to the overall data that there isn't this experience but also the models that aren't necessarily built to understand natively this this type of uh embodied reasoning so to speak all right well I think we're at time so thank you very much and we uh um uh we really appreciate your presentation thank you for e e okay there's it's a strange Loop and I think I have to be unmuted here is that right yeah that's right okay introduce you one moment one moment um I jumped into the opportunity to introduce him because I know have such an easy job you know I don't even have to do it right now but well let's please welcome Richard uh newom he's a be hello well he's a BP of research s of research science at meta reality labs and he's Leading The Surreal team in reality Labs research I really want to have all the time for him yeah you can hear me good uh so please take it out it's a pleasure to have you all right um you might be able to hear me or not certainly you're getting loud when the Wi-Fi is come back apparently this is a Wi-Fi issue not a microphone issue okay um so very pleased to be here thank you for inviting us along uh to this very broad uh you know space of really the future of AI and I'm going to talk about always on contextual Ai and in particular uh from a point of view of the kind of devices that are coming next and this is the wearable space of egocentric capturing devices and also egocentric Computing devices um very related to robotics so hopefully we'll see some like great themes along here when we start getting into how we're actually solve for the always on uh problems and in particular on the small form factors with sort of limited sensing and compute uh just quick overview here um uh I'm going to start by taking us from where we are with AI today which which apparently is here and you know solving the touring test and all that sort of business so that's done good uh to contextual AI which is a different Beast all together um and I I'll claim that that's probably a more useful kind of AI that we really want um I'll take us into sort of like the introduction of project area which are the glasses that allow us to capture entric data eye tracking head tracking and lots of other good stuff in a glasses form factor and what that means to sort of like helping us solve the data Gap uh to essentially enabling contextual AI I'll then talk about uh building models of reality and in particular sort of a very 3D Centric view of uh predictive models reality both World models and user models and in particular using a lot of spatial AI that's been maturing over the last sort of decade and like I say has a lot of overlap in other disciplines and if I have chance which I doubt I will then I'll bring this on to the new problems that we're trying to solve with new data sets uh that are coming as well okay so let's start with the big picture um certainly during Chan's time before there were any kind of graphic user interfaces mobile phones internet scale data sets or any of that um he was certainly you know precent in pointing out that what we really wanted were not machines that we'd program one off but that would learn from experience and potentially continuously through their lives and that's fair because at that time of course all there was were humans interacting with each other there was no model of the graphical user interface that would come to dominate our human oriented perspective Ive on Computing today um well it looks like we're back to sort of like where we were maybe in the 1940s and people are thinking again about the future of computing not through the lens of literally you know 2D screens but through embodied AI Robotics and all so you know that's good news for all of us here today another important figure of course is Kenneth CRA who you all know who in particular pointed out and again these are sort of like you know from the old times but they're exactly right you know over a 100 years later that if an organ ism is to carry a small scale model of their external reality and of its possible actions within its own head and its ability it's again try out its various actions predicts the consequences uh before they arise and utilize the knowledge of past events to predict future events and in every way deal with the reality that the creature faces uh to make it basically safer and more performative um now Kenneth Craig when he was writing that in 1943 you know right around the time when Computing as was was being developed again had no perspective on where robotics was going to get to or the kind of language models that were going to come over 100 years later and solve a lot of intelligence problems so we have to kind of map our idea of a predictive model of reality that's in a creature's head now into this new space that somehow does solve it if you get enough data and doesn't look really much at all like creatures and their brains okay well anyway AI is here and so like I say good job everybody who worked on that um it looks a bit like this you know you type into a text box anything you want and somehow it gives you an intelligent answer based on simply predicting the next token and so it goes you know there's a predictive model right there and it was trained on internet scale data and uh you know that's very useful and certainly I think you know we we'd uh it would be wrong for us to imagine that it might be impossible to do it again with all the other modalities uh but there certainly is a problem in that something is missing in the data sets that exist online and whether or not they could ever capture the kind of human experiences that would allow us to build predictive models built on that kind of technology is I think tenuous at best if not you know questionable is to why if anyone would put their life experience uh on the internet at the scale of kind of like what the internet is with respect to text and images so what's missing from AI St well it's you know it's your real life context and our job is essentially to get there that's where wearables come back in they start to close the gap by being able to give sensory perception to the kind of AI that exists today whether it be imagery through things like R band meta or certain various microphones that might record what just you're saying or in otherwise like observe the world from your egocentric perspective as we call it in fact if people are using health apparatus like rings and watches if you're a sports type person I see Lord is is in the room she's a Runa I'm a cyclist we use contextualized AI all the time from a point of view of continuous capture of our performance metrics how many WS I'm putting out you know how how my sleep's going so we're actually used to this kind of like pervasive capture and the question is of course could it extend to the sort of much more extreme modalities that have much richer data like imagery and video well that's where Ray band meta come in and certainly it's a product you can buy it I andan it certainly lets you ask questions about reality you say hey meta look can see and then it does visual QA again that's a sort of a great thing that's happened in the last sort of 10 years from when visual QA was first introduced as a problem to where it really is already deployed in products but I'm not interested in this kind of like triggered vers of or of AI where you have to ask a question and only then it turns the centers on because my contextual AI is defined by what I think is going to be useful and I think you'll agree is about all of your context even when you're not interacting with an AI so for it to be useful it really needs to have a you know a timeline if you like of all of the things that have been happening all of the time and it needs to do so in a privacy aware way and it needs to be able to put that together with all your Digital Data and then you apply the AI and reasoning system and you know I I particularly claim I don't really think AI from a point of view of the reasoning system is really problematic actually I think if anyone is here is interacted with chat GPT today there are several ways of having interactions with it the main problem is it literally has no context about you so it's going to make really dumb mistakes it's going to generalize with its kind of a world model whereas if it did have access to your continuing like life then actually there's a good claim that it could think of your life as a novel being played out you know one sentence at a time it's fine you know there's lots of people who have written about things over the last 100 years and they have been consumed by these llms but getting the context from your actual physical interface to that system is the real problem that I think embedded embodied AI has to solve okay so always on contextualize AI is what we really want to get to not just sort of interrogative uh you know reactive Ai and we need to go from these internet scale data sets that don't have the data manifold that we need to contextualize Ai and this is where project area comes in so ARA is essentially by the way go along to project ara.com in case you want to try and get a pair they're free for industry and academic researchers otherwise there's data sets there is a pair of glasses you can wear them they're wearable they're light enough they have no display they're literally just there for doing research and they have eye tracking and head tracking and microphone arrays and Imus and all the good stuff that again Maps very well onto sort of like the robotics domains as well and in 2021 when we started to hand these out and get research teams across the working with us and independently producing their own data sets we were you know excited to see for the first time the kinds of data in this real diverse domain and it's very different from internet scale data you know photographs are photoed from a point of view of purposefully capturing a thing that's not what Ecentric data looks like you know your head and your body are moving around to keep you you know mobile interacting with the world and that's a very different problem like I say we've been interacting with a lot of labs and I hope if you're not on here you can join us soon uh a lot to do um and its General Mission Project area is to help accelerate contextualized AI uh by advancing the wearable machine perception and AI research that can't be done without this new kind of data okay so what does a diverse data that look like well you know if you haven't had a chance to do this in your own life you know why not take a pen and paper and write down what a day in your life looks like because in a sense that's where your AI has to work um this may or may not be my real life I'll let you choose I woke up at some point it was around 6:00 a.m. I might have read the news actually the first thing I did was put my glasses on I walked a dog I don't have one and eventually I started work and so on and so forth until I went to bed again now these are the events that happen you know all the time in our day and every single point actually is augmented by ever finer grain things events at some point they are physical events we interpret them in a semantic way over a context window and we reinterpret them that thing that was dinner turned out to be a mediocre dinner when you got sick that night you know your best friend who sort of betrayed you was no longer your best friend so what we have our physical interactions that can be reinterpreted again and again and again and our job effectively is to collect the context not as it's understood in that moment but as it may be understood you know indefinitely that means it needs to be captured at a very low level of granularity without interpretation much more like physical events actually okay and now just picture this excuse me I think we can all agree that llms have shown that you can do next token prediction so here's a big picture of embodied AI build systems that allow capturing of that kind of fine grain context and then allow systems to essentially predict the next token or fill in It's A M Auto encoder type problem and I personally have no doubt that if there were enough data sets enough scale then we would be able to start to predict over events of Horizons of certainly hours days if not weeks years why well I don't know why not you know in the same way that we are within a manifold for you to exit that door there at some point you know you do have to walk through one of these doors the predictability of humans is actually a much tighter distribution than I think we really have thought about until perhaps recently where language models from the internet scale can kind of do a very good job of capturing intelligence through next token prediction okay so can we just get the raw data and you know basically give it to these llms and this is this question of like will llms get us there if they are enhanced with multimodal data well this is a nice study 2023 it's been updated since by a few different groups um I claim that the answer is absolutely not U the reason is because there's a big problem with simply trying to augment llms with the modalities that come from things like it tracking and so on and and not least because the data sets cannot exist at internet scale they'll probably never exist it into NEX at least not for the real world distribution um okay two things data and computer few there's two different types of data there's the data that's needed to train a future llm or other AI model uh in your data center and that can be very big but at some point the distribution has to match what is possible on a wearable device or a robot system and they can be very different because while she might be able to capture megapixels of video for offline processing possibly then certainly you can't use that same data Distribution on the device so actually it's not the same distribution in a very ser ious kind of way the on device inference versus offline inference problems are very different and you can't take a cloud and put it on your computer device on a wearable and you can't even stream the data off the device there's too much data there I can go into the details in a second same is true for compute you might have an absolute Army of a100 somewhere harvesting gigawatts but actually at the end of the day you've got to do the inference on the device in real time when you need it and in fact I think what we're going to see is these two are interrelated what we really need are devices that allow us to tokenize in real time reality so that that is compressing what is exactly going on and that's what we'll be computed with okay excuse me so data constraints Let's Get Serious for a second take yourself a project area device and ask okay well maybe my a100 farm is big enough to do some sort of massive multimodal llm learning with it like next token prediction and so on well here we have a look we've got slam cameras RGB cameras if facing cameras Imus microphone erases seven microphones on a project area device um even when you're you know reasonably compressing that data you're looking at nearly a terabyte a day per person just for an eight hour day now that's not very diverse from a point of view of a sample distribution of reality we're used to data sets from the internet scale that are more on the order of millions of people over Decades of input it's no very compressed already by the time that somebody's making a Wikipedia page this is raw data and it's redundant and even when highly compressed essentially it's it's too big to store uh if you do want to have interactive data sets you know lots of people interacting with each other to understand what the distribution of that kind of data is you're talking about petabytes of data per year just for a year of interaction of 100 people these is Tiny numbers of people with tiny sort of like very narrow perspectives on what reality is you can't scale this to the equivalent of net scale egocentric uh so in a nutshell um no you can't store the data from egocentric at you know fast enough quantities you uh can't actually input them into context Windows no matter how quote unquote infinite the context length is um you know seriously there isn't enough working memory to put it in there um you can't transmit it over you know coms in real time unless you are happy with like 10minute battery lives for your devices which means then of course it's not going to be on is prohibitive even from the point of view of the sensors themselves the underlying uh capture of a mobile phone video will take a good fraction on the order of 10% of essentially your phone battery if you were to leave it on for an eight hour day now that's doing nothing just just the raw data not even compressing it then add-on compression and transmission or computation and suddenly your 10 wat hour battery is going to die in your phone and yet a mobile uh phone is 10 times bigger than the battery that can go into a able at the size of a ray band meta in fact it's over 10 times bigger and of course there's priv privacy concerns that raw data has bystanders in and whilst I might consent to my timeline being captured and used with AI you didn't all consent to being in my sort of like context window so let's get away from raw data which brings us to building a model of a person's reality the user's reality this is back to Kenneth again I think if we just flip a few things around and replace the word creature with AI what we know is that what we really want is for an AI to build a predictive model of the user's reality and then all the rest is true so the question is how are we're going to build a predictive model because that predictive model can be highly compressive and effectively that's the main job for us to do in in embodied AI I think okay so what we're going to talk about is a 3D World model a user agent model and the way in which the two have got to interact I'm not going to cover absolutely everything here um but I'm going to try and cover a lot of we can get that 3D World model and how we can do it in real time we've been uh talking about 3D models 3D World models for a while remember all of this has got to be obtained built and kept up to date by the glasses devices themselves or other Ecentric facing data we don't want surveillance from for example devices I want to own if it's my model my model of My reality without it needing to be shared necessarily so we'll talk about that we'll also talk about the user agent model and again we'll start actually from a a full physical point of view I really want to know where my body is you know the full pose of where I am what I'm doing in reality but I'm also going to talk about it from a point of view of a users's personal timeline so this is where for example what they say where they are what they're interacting with and those sorts of things are highly compressive of the users's reality State and so you can probably have a think of you of these types of things that might be important we call them functional Primitives we want to know not all of the raw data and how it might be interpreted we want to actually try capture Primitives that then when they're reinterpreted later in context in a you know a short window a Long window maybe months years then actually they can be reinterpreted so I'm not going to put an exhaustive list there but think of how compressive reality might be if we make judicious use of functional Primitives like this for example and this is you know usually thought of from a point of view of Digital Data who can have a estimate here of how many bytes uh per day for everything that you on average fridge May write whether with a pen or with your keyboard or with a mobile phone everything you read whether it's on a poster or on this screen right here um everything you've said or You Know spoken to someone and everything you've heard spoken to you all of your physical basically written and spoken interactions who's got an estimate of how many uh gigabytes megabytes how how much is it there's a secret prize for anybody who can get within an order of magnitude m you're on the right track it's approximately a megabyte a day um if you're really sort of like allowing for all sorts of keystrokes that is to say you know when you press delete you include those things and so on honestly these are tiny numbers and when all utterances so not just spoken words in terms of sort of like pure syntax but also sort of like the markup the inflection the intonation and so on but it is absolutely tiny and think about that in comparison to the raw data that actually may be substantially more than that same is true for everything else so everything you've touched and held you only have two hands you can only interact with so many things at a time and you can only see so much at a time in fact this is one of the biggest differences with where I think computer vision was to where we're going to get to with embodied computer vision in AI you have a two degree field of view phobia and your eyes phobias carding around in space that two degree of fre phobia is where you see high resolution I.E to allow you to read everything else within your around 120 degree field of view is is incredibly low resolution when you take a 2 degree field of view and try and obtain 1.5 arc minutes or what is necessary to see 2020 Vision you need about 128 by 128 pixels think about that that's actually what you need now you also only secard up to or fix a up to about eight times a second especially if you're speed reading that means that your eyes are ticking around going tick tock tick tock tick tock they're not going at 60 frames per second you can perceive motion and you can interpret it but actually what you're looking at and changing is more on the order of up to eight things a second think about that for a second incredible compression from humans of course we evolve this because we have limited compute we' got to get stuff done and your entire body pose you know be generous give yourselves 200 degrees of freedom times a certain sample rate multiply it through by an eight hour day it's still tiny in comparison to the raw data that would come from you know a 12 megapixel image okay so we're on the right track and we would like to have a model of interaction okay our team has been working on this uh we've been calling this live maps for a while and it's really around building essentially the system that captures the environment keeps it up to date from a 3D point of view captures the user and keeps that state up to date and keeps the interaction between these two models all up to date and does so in an online fashion I.E no offline processing and remember this has to be done using just data from a wearable device this is what we're going to go through okay building context with spatial AI so this is what we've just seen here this is actually Julian stra one of our lead researchers in this space and uh that's that's him walking around his apartment somewhere in Seattle here and the model as you can see it takes just a few seconds to start to build it now this is really classic stuff this is slam and reconstruction in an online mode together with some very new stuff for being able to infer entire body State what you'll also see is those sort of like trails that are going on those are the objects that Julian is interacting with in his with his in his environment and uh in a sense everything you see there of course it's not photorealistic but that is the state of his reality you know he walked into the kitchen he picked up an apple he took it to the table he put it down he picked something else up he took it to his bag and it went on and it went on and this Dynamic scene graph essentially is the compression of what otherwise would have been for that clip you know at 12 megapixels you know actually megabytes of data it's like Les than kilobytes of data in fact uh the underlying World model that we talk about the 3D World model consists of uh essentially this kind of set of external Maps if you've got them use them you know Google's done a great job and uh other other mapping teams have done great jobs of saying that we know exactly where this building is the question is can you connect it to a location system so that when you with a pair of glasses are about to walk into this building it knows that if so that's great connect location maps from slam systems with external Maps then essentially build physical surfaces because whether it's a robot or a user knowing where the physical surfaces are in reality allow you to predict what might happen there but also essentially be able to track more effectively the sorts of interactions that are happening in a space they happen on this surface this is my desk Etc which brings us to the point about building structures and other kinds of 3D indexing essentially if we can tokenize reality at the level that humans already describe it at least at a course level things spaces then actually we doing a great job there's nothing stopping you know as then anchoring other bits of raw tokens from various multimodal encoders onto that stuff but there's really no reason not to make use of the fact that a chair that's rigid from a point of view of a user who picks it up in this room and moves it somewhere is like you know not a useful Atomic thing let's just give it six degrees of freedom and then start to track other properties on it you know now this might all emerge from giant data the problem is see reference the beginning of the talk that data is not coming from the Ecentric perspective we cannot capture Ecentric data at internet scale okay then the dynamic model of the user and their interaction with the environment for the location maps um this is really sort of like the core technology that's needed for 3D embodied AI this is a real-time visual inertial odometry system that works from just the Imus and the slam cameras so low resolution cameras that can be always on and essentially provides a you know centimeter to millimeter accuracy trajectory of the device and hence the person's head uh if they're wearing it now you're you know you're in trouble if you're head and your glasses are not near each other and they should be and there's a problem for where the rest of your body is uh but this is where everything starts if you have this this is a space time frame of reference that we can use again this is sort of like an inductive point of view let's use space and time let's not have it need to be emerged from machine learning models and now we can start to Anchor things in that space the first thing to Anchor are the physical surface is uh now what seems like a very long time ago although it was only 2018 2019 we were building dense 3D models using structured light cameras um but I just came from the XR Nerf Workshop earlier today where you know effectively it's now perfectly fine to take a pair of area glasses with no depth camera on and produce beautiful photorealistic 3D models uh static of that environment and a lot of people are well in away trying to make these Dynamic models there's big problems with that due to the amount of data that's actually there but yes building surface structures and even photorealistic ones is a generally solved problem at this point for the level of what we would assume is good enough uh to get on with the rest of the problems the thing is a lot of those techniques are offline and what we want are online inference techniques I can't wait for all of the data to come in before I start reconstructing it I need it right now and uh so what our team has been working on is a new generation of essentially Ecentric Foundation model um informed Fusion techniques so everything I'm going to show you here again is using only data from ARA the way the technique Works uh is that essentially we take the multicamera video from ARA and we take the point Cloud that comes essentially for free from the slam stack and we push that into what we'll see is essentially a convet that will predict directly the occupancy of a volume it actually also predicts the object in space but I won't talk about that here this work is now online if you want to take a look the I think the most interesting thing about this work is that it's trained purely in simulation using the area digital twin and ASC data sets uh that have been released on the project carer website that's you know quite a big data set it's hundreds of hours but it is simulated data and then it transfers over to real area glasses um the technique in brief is we take the underlying inputs together with uh essentially the uh slam sorry thank you two minutes together with the the slam map and we use already bat bones have been frozen for 2D image techniques and then we back project that into a volume and allow a unet to essentially decode it through ENT to end training with the simulated data to a Bound in box head and also an occupancy head and then we just push that through good oldfashioned tsdf Fusion uh okay static indexing um this is simply enough if you've got a point cloud and if you've got techniques that give you beautiful segments like segment anything or in this case muda you can color those Point clouds cluster them and Away you go with respect to producing bounding boxes of objects and this works very well in practice again you can see all of this is happening incrementally as the system works online Dynamic indexing is a new area so this is where essentially if you've got hand tracking from Ecentric uh cameras like from ARA then the problem here is even if you don't know what the object is can I change the state of that model so that I can keep track of it and that's what we're doing here and this actually just uses a very simple 3D technique which is if I know where your hands are and if I can tell that what you've just done is change whether or not you're holding something or not holding it then I can start to trigger very simple mechanisms for detecting if that object is the same object or a different object actually just at the visual image level and so then you can essentially start to keep track of a library of those things and because your hands are in 3D then effectively those objects are being tracked in 3D so that's a Nifty trick and certainly a new area that very few people are working on finally human State um what I'm going to say about this is that this is really a very exciting area because if you start with hands and then start to ask well the rest of the way that the world is moving by all the humans is because their bodies are moving and then their hands are moving the rest of reality so from a point of view of keeping a model of reality up to date getting the whole body State decoded from just egocentric data really is a key area of embodied AI robotics this have a slightly easier because they have the appropriate set we need to sort of like infer all of that just using the cameras so what we've done is we've built a very nice data set that's just been uh well being released online you can type that into Google and find it now on the Project Ara website and what we did is we took a pair of ARA and we paired it with an exent full IMU bodysuit to generate the world's biggest in the wild human motion data set this is indoors Outdoors day night and it also includes a thir third person P perspective of a second arala watching all the time the first ariala so that's some interesting data includes narration of the things that they're doing um it's useful to know that sort of you know your wrists move uh something like you know two and a bit times more than your head I don't know if you ever wanted to know that stat but we do know the stat that your wrists move more than your head it makes a lot of sense and uh that's a beautiful data set now given that data set that ranges like I say indoors Outdoors over hiking and so on we've been able to train a diffusion model that decodes directly just from the slam pose plus the point Cloud that comes from ARA together with a clip embedding of the RGB essentially the entire body and this was very surprising uh effectively because that RGB camera hardly sees the rest of the body but there's just enough information in the Vio trajectory that's got a lot of Dynamics together with the context that comes from clip and the underlying Point Cloud that we provide as well uh local Point Cloud to decode this is lingy the lead researcher on there it's actually on archive now this is actually my favorite sequence um as a cyclist I hope it I hope this one shows let's see oh he may not it may not show this is my favorite sequence of uh of essentially a person cycling and decoding you'll have to come and see me later if you want to see that okay in a nutshell that's it um I guess these shouldn't be highlighted I said we wouldn't get to the new problems we didn't um you come see us later on and thanks everybody and I'll take questions five extra minutes uh if anybody has any questions here from the audience otherwise we have a bunch of slide go ahead please do we have do we have an estimate of like when you're considering only the vi and if you would sa instead like it more manable you know I'm glad that you said that because that is exactly what this section was about um and so yeah in a nutshell I'll just show you when you've got ey tracking as you do from ARA in this case this is shown together with segment anything just to start to highlight the power of you know online fiated segmentation but when we think about the raw data which is what you're talking about this is where it really gets exciting if you start to drive imaging systems in entric wearables like ARA through fated capture you can get on the order of 3.6 th000 you know times reduction in the amount of data for the principle that I said now you might want to capture a low resolution you know rest of the non fiated region and you can do that actually quite well actually if you just take a 64 times down sampled version of the two to four megapixel area RGB camera and then push that through h264 you're back at kilabytes of data it's very good at doing key frames and so on and then that high resolution bit from the phobia driven by ey tracking is tiny amounts of data as well so yes um by the way if you've got you can do this yourselves with the you know the eye tracking that comes from the machine perception Services if anything I've been most interested by the fact that very few computer vision people seem to have thought about eye tracking and humans and how they see relative to the big problems of big data and particular multimodal data and yet this is exactly what may get us over that next hum we can capture lots of this data and even give it to our you know multimodal predictor models if it's in this form but not in the full capture form that's a great question so that like we are we are actually able toine and progress like track level and also making that so is any yeah that's great um so yes um and one way of thinking about this is that if you start to take those physical events and start to align them with effectively textual summaries that are a small enough sort of granularity and then Bunch that all together with when it happens and then simply give that you know Richard walked into the room you know Richard picked up a can and Richard drank and then Richard turned his head and gestured and then give that to an llm today what does it do with it well exactly what it would do with director's instructions you know to a play it does very well and it does things like summarization and so on so you can keep track of the higher level not by having to in real time implant what it is being interpreted as but literally after the fact and that's the power of sort of capturing at this lower level of events uh that you don't have to spend all of your time trying to interpret in a narrow window what might actually only a few seconds later be absolutely obvious what is happening one more there is one from slag how do you estimate that megabyte uh of of uh compress data and the question goes as is that culturally standard like go to different cultur SP difference oh yeah so it's actually significantly less the chap over there who said 20 kilobytes um it it is between 100 kilobytes and and a megabyte even when you add in all sorts of variations for speakers and it it's not massively culturally different it can be a lot smaller than a MAG let me tell you that I I added an extra zero on there simply because it's so ridiculously small the reason is we evolved to not be able to easily hear whilst talking we evolved to essentially have two hands that require attention mechanisms to be able to interact with reality because we only have one body that can manipulate the rest of the world through or act in the world so it's it's very real that effectively these things are you know you know these are social across the planet because it's part of being an embodied creature with this type of morphology yeah well thank you so much what oh I can't dolit screen uh I think you can you probably just need to set up it's probably easier just do it without a split screen if unless you have timings or something um I had not but okay I think I got it okay oh perfect it's working okay does a zoom look like that Zoom great uh perfect so uh yeah it's 2m so why don't we get started it's my pleasure to uh kick off the interaction challenges speakers um with the the Manny skill challenge um this these speak talks will go from 2: to 2:30 and then we'll have a speaker Q&A where all the speakers can stand here and we'll we'll field questions from the crowd and uh hopefully have some fun discussion take it away thank you thank you um so yeah hi everyone my name is Stone I'm a PhD student at UC San Diego advised by houseu um I'll be covering kind of two things today uh one is on the vision based T challenge results and I'll also be covering some of the new features in our new simulator m 3 which is the fastest date visual G simulation robotics um without further Ado um introducing the man skill visual tactile challenge the goal is to train a poliy capable of relying on just visual tactile sensing data to perform tasks like charging a phone opening a lock in more without actual rendering cameras or like State based information the two challenge tasks here we propose is Peg insertion and lock opening oh sorry this is covering a lot um where both these types part controller or policy given only the visual tactile inputs you can see below in those images uh the challenge is quite difficult since for the test setting the peg insertion task you aren't told what shape of block you're holding and for the uh open and lock task you're not even told what shape the key is you're going to be two ke three keys you don't know you have to use trial and error and tax sensing to be able to solve these tasks effectively so this is what the real world setup kind of looks like uh there's kind of like four dimensions of control there's one one rotational axis and three axis translation stage to kind of move this robot gripper up down left right rotate in one angle to try to open locks and and attach that Griers to what called gel site mini senses which provide that tactile information um this challenge has a custom simulator based on fbm and IPC supporting intersection free and inversion free simulation it can stably simulate the elaser which is the gel site main sensors large deformation during reinforcement learning training and also supports diversity geometries VI mesh based representations sorry there's just something blocking the way oh sorry know you're good where is my oh I think I can CL apologies very great okay hope I can Ty this to sometime uh yeah great okay um so in order to process the data we provide all competitors the efficient tactile statement representation which is effectively just appointment operating on reference positions and current sens pctile information this is permutation enironment thanks to point net architecture and it's fairly robust tracking laws and achieves fairly good generalizability um in fact the generalizability is quite good because of Cent real Gap is quite low um here is a transfer direct transfer of a simulation policy to the real world world of the peg insertion task um and the success rates are fairly similar um so we are fairly confident in our simulation to be able to effect uh efficient accurately model this um T simulation before I announce the winners and conclude some about the results I would like to thank the 18 different represented institutions that participated in a challenge this year uh we hope to get even more competitors involved with these simp real typ challenges as they provide great accessibility for labs and students without access to expensive Hardware without further Ado uh congratulations to these key teams um Second Place team Luan from CH Young University team touch innovators from T University in Kings College London and congratulations the first team Place team team illusion from gu University um the overall result is as so U for the peg insertion class the best success rate was 70% um and the best uh open lock accessor is 40% so clearly lots of room to improve um and turns out the best Solutions the one to scales best uh Behavior colony and Tas specific R Al tuning um yeah so we plan to kind of launch this compet uh launch the next version of competition somewhere around the end of the year um and we'll also include a new sensor design track if you're inform uh interested in more technical details about the future challenges you can email that address there or you can also stand that QR code um this QR code will show up again at the very end of my presentation and big thank you to all organizers and students who help develop and organize this challenge as well so onto the next part um I'll also be kind of showcasing man scope 3 a massive upgrade from previous iterations of Mana scope that included incredibly new features and results primarily around paralyzing almost everything onto the GPU for robotic simulation so the first big feature is GPU paralyzed State simulation similar to Moko mjx or ISAC simulation um but the main difference is that we support very very fast state-of-the-art parallel Runing speeds more details on that later but on the right there for example that's actually a visual policy trained in about half an hour with reinforcement learning using pure RGB inputs no State um so yeah I'll couple more details in performance benchmarks and comparisons with other simulators in terms of rendering but this is now possible actually um differ also Improvement is we have a ton of new tasks and robots spanning um quadrip heads humanoids as well as standard to manipulation tasks um we even have F noise developed by new startups uh there's a little Stompy robot for K SC Labs a couple other robots in there from unry um quadrupeds all sorts things um we also support large scale conflex scenes like those in Allen AI score simulator as well as a replica CAD data set and for the first time these tasks and these complex randomiz scenes can now be simulated entirely on the GPU uh we also support a very interesting feature known as diverse parallel ulation on GPU where each parallel environment can have different objects and articulations of different degrees of freedom we provide a very simple and easy to use API to build tasks of this property enabling more generalized SCH robot learning the task on our right for example is actually a task for from our very first man challenge in 2021 and used it take like many many hours to solve and served as a fairly good Benchmark until now uh these days we can use GPU paralyzation to simulate every cabinet in our data set 20 times each um and learn to poll open every cabinet on the order of minutes or hours um instead of days or weeks that used to take before with reinforcement learning the policy shown there is actually an AR policy trying to open every cabinet another example is learning generalizable mulation for let's say cleaning and house the video on the right is a recreation of the habitat rearrange Challenge from a few years ago where fetch robot must clean up objects in the room and place to design locations um and here we're just rendering every pel environment each one has a slightly different room layout like different objects Place different places this is kind of like a bird eye view um and some internal lab work that we want to release hopefully at the end of the year uh we are training a visual RL policy from dep images to kind of solve this task completely on GP simulation and again our new diverse uh GP simulation software is able to enable this training at scale much much faster than before um the last kind of highlight of the new simulator is everything is open source for the first time ever um so man itself is like a task and apis plus assets this is open sourced safein is the underlying rendering system and kind of simulation framework um that's been developed since 20220 um in our lab also completely open source on GitHub you can find it online and for first time also physx which is the physx engine we use was recently open sourced by Nvidia um and so our entire simulation TCH app is open source if you ever have a bug you can always figure out what's wrong um yeah and so we really want to embrace open source Rob body community in this way and get companies startups academic Labs all involved improving performance usability and more in the simulation um so for more concrete numbers uh just for example with a fairly big camera relative to simulation standards like 256 x256 we can render close to 20,000 is FPS on a 3080 GPU and much faster with better gpus no task specific rendering f is done here um and this is being compared against Isaac simulation Isaac we have much better memory and FPS performance and we support rendering RGB dep and segmentation at the same time at this FPS just for a quick demonstration on the right is a task where a robot has to try and pick out an object out clutter note that each pale environment has different geometries and a different number of objects so now we're just going to render four of them at a time 16 64 132 of them at once and so on you can even run about like, 1500 of these in parall each one of RGB death and segmentation being rendered at the same time um all quite fast obviously I'm not going to show the 1500 one here because I cannot possibly fit such a large video um but you can try this yourself right now this is released another big feature you want to come to the Man 3 is just trying to support more reproducible robotics research and digital twins we had a recent collaboration um with people at Stanford and Google deep mind working on what called the simpler project where we built evaluation digital twin TNS of environments in the openx data set so for the first time you can evaluate policies like rt1 rt2 um and oo in simulation without having to have the actual Hardware at home to try to test these things again trying to support open source Robotics and reproducible Robotics um yeah and so for that kind of sorry that's an example of the digital plan uh so to conclude this presentation the big takeaway is not just you know the new platform or like cool visual tax simulation but the fact that there's great progress as shown in past talks today and this talk um in s to real and real to Sim in all modalities especially vision and ttile simulation um and as the domain Gap strengths I believe that simulation become a bigger and bigger part of the future robotics evaluation and training so we are hoping to steer this training platform and our challenges in the future towards this direction of Cent real um and by improving visual data collection speed and diversity and embodied AI feel free to stand your but P it have the Arnold e e e e e e e e e e test test got itare sorry yes okay uh where are we so Arnold includes eight manipulation Tas situated in the virtual household envir environments such as rigid object manipulation articulate object manipulation and the fluid manipulation tasks we also like for each task we provide template based language instructions specifying the goal states such as lift the bottle 30 cm from ground and for 80% liquid to the cup and to collect asate demonstrations we first obtain human annotated task configurations then we use data augmentation to increase the diversity and scale of the configuration finally based on the configuration we use a p Point based motion planner to generate 10K demonstrations across the a tasks the manipulator we have in arold is the sex 7 do Frank MAA Panda robot with a parallel gripper we place five rgbd cameras around the manipulator the visual observations and Camera parameters are provided to the robot as inputs and the model needs to predict the Factor translation rotations and the action of the gri we create a total of 800 test cases for evaluation the test cases like cover in domain tests and the generalization test for novel objects s and States we further withhold a part of the Tas case and do not show them show their results on the leaderboards to protect potential overfitting the test case is considered successful if the object state is within the respective tolerance threshold of both St continuously for 3 seconds we compute the average success rate of all the test cases over three trials and use that as the final evaluation matrics the challenge was open for submission from April the 2nd to uh June the 1 we receive a total of 99 submissions from nine participant teams and the highest performing model is able to achieve 45% success ratees in comparison the Baseline solution based on the fora model can only aieve 22% without further Ado let me introduce the challenge winners the third place team is the SSU reality lab affiliated with the songo university I'm not sure whether they would able to like be here today but there is a short video like introducing their Solutions okay our PSA train motors we're at times so you could just can't play the sure yeah so the second place team is the fun guide team from like affiliated with Shian University and the and the first place team is the robot AI like from University of science technology of China deep space exploration lab and the Sanford Research Institute and I like to thanks for all the organizers and participants I would also like to thanks andiva for the GPU C thank thank you so much e ni all right can we hey everyone all right so I'm here to talk about the home robot open vocabulary mobile manipulation challenge uh so I'm presenting for uh amandra who's a student that I work with who's actually um now starting stford in the fall but couldn't make it today to actually do the presentation he did all the actual work putting the cvpr version of this together so uh yeah so what is this challenge about so good question that's funny okay well that okay what are our goals so uh we want to make robotics research a bit more accessible um we we would like to evaluate multiple different skills necessary for building a robot that can work in your home uh standardized real robot benchmarking so we want to be able to test these skills in a way that you you can actually that you can try them out and that we can we can standardize them make sure we're making real progress towards problems that we really care about um Pro provide a platform for sharing ideas about how to solve these interesting problems and also build a community of Robotics researchers interested in deploying real generalizable home robots into different locations so what is what is this challenge actually so let's start with previous stuff like the habitat rearrangement challenge uh so in the past we've done a lot of work on geometric rearrangement of homes with known sets of objects like you can see the ycb kind of uh cheesei box there and and these are small environments known environments known objects all of this kind of stuff goals given as particular positions that you need to move to in the world that's not really what homes look like they look so really what you want to do is you want to have a robot which is something like pick up the box on the table and put it on the sofa um so so that's what our ovm task is our open vocabular mobile manipulation task so on to control this mobile robot a lowcost mobile robot that I can deploy into different home environments I want to move it move that object from a start location to a goal location uh the start location is given as an is open vocabulary meaning that I have not necessarily seen that object class before it could it's a we have a very large different range of different objects that it could possibly be we also have not seen the environment before it's a it's a apartment or home or something like this that that we have we have not previously explored it you have no training data in this you have no training data of the objects that are going to be involved in the challenge so it's very challenging as we'll see later uh but so and the idea is that we can we have actually tested this stuff out in a in a apartment that Facebook owns so this is this is a case this is an example of the robot of a robot performing this task so picking up a stuffed chair a stuffed toy from a chair and place it on the couch um and then it has to go it picks up the toy it has to actually successfully go find look at localize pick up the object pick up the toy it then has to go and uh place it on the couch so have to go drive over find a couch that it can place this object on and then it actually has to uh place the lift up its arm and place that thing on the on the on the couch that it doesn't fall on the floor so we invited participants to uh submit their agents to this competition uh so basically you submit you submit an agent you receive egocentric rgbd you receive base you receive the base pose you receive uh joint States from the robot uh so just sensor data essentially so no State information no privileged information that you would not expect to get from a real robot because we're not doing anything like that the robot is running slam so you get oppose relative to where it started and that's about it and then you have to predict actions like how far the robot should move what what the joint should how how the robot's joints should move whether or not it should try to grasp Etc so it's a very hard task lots of lots of Parts um so participants are are asked to develop their agents using our simulation stack so the simulation is based on habitat uh particularly habitat 2.5 now where we where we have the robot moving around in these large simulated home environments where it has to go and pick up these different pick up um it has to explore this environment there's lots of different things that it could see it has to find the target object and then move it to any valid location from a valid location to a valid location so if you pick up if if the task is to pick up a cup and you pick it up from a sofa and pick up a cup from a chair and you pick it up from a sofa or table instead you're wrong so it has to do this all correctly and then uh we test them using this so you're supposed to develop and test using our home robot library and simulation and then we can evaluate their Solutions zero shot unreal robots so that so they're tested on in in our unseen test environment with unseen test objects uh based on so we so we ranked our our different teams based on overall success so basically just number of times it manages to move the right object to the right location without colliding with anything failing right so just what you would expect success to actually look like partial success meaning like whether or not it manages to accomplish each of these individual subtasks in order without failing and also um that basically if there's a tiebreaker we can we can look at the number of steps that it took to the episode so uh we did a full version of this with with the real world part of the challenge at nurs we didn't have time to run the we we weren't able to run the real robot version for cvpr so we're just doing the cvpr uh CPR version of this challenge uh but I will talk actually I have a keynote talk later where I'll talk a lot more about that the the whole real robot side of this thing uh but just to keep things short we're not going to go into that now uh but so we provide a bunch of starter code which provides a modular Baseline for chaining these skills together U so finding objects G looking at them gazing localizing picking them up Etc um who participants submit their Solutions as doct to EV we evaluate them remotely and rank them um uh so we had six participant teams uh 17 total submission to test to the leader board uh they were evaluated on 250 episodes meaning combinations of start goal and uh Target object open their object H using 10 held out test scenes and unseen instances unseen objects so the mixture of unseen instances of known classes kind of totally unseen classes and uh so this is the the Snapchat from the leaderboard so we ended up with a so this is so the success rates were all less than 10% so this problem is definitely nowhere near solved a very challenging task you have to move through a very large environment find these sometimes very small objects that this that the robot can grab and move them from different locations lots of potential for failure partial success is notably higher as so it's like being able to actually find objects partly part like in in the environment is much easier sometimes or at least feasible but completing all of this is difficult um so we so I'd like to so congratulations to team uni team uh who got first place on our leaderboard this time they also got first place in the real world challenge so you're probably not too surprised I guess maybe their confident their solution at this point so about those Solutions uh the top three solutions so two-thirds of them built on aristic Baseline so they actually we provided both a reinforcement learning based and just a motion planner based solution reinforcement learning solution was not was not the top Solutions the top two both used most used just motion planning essentially uh one third uh built built on the RL Baseline they're the one that was below our baselines actually in this in that previous thing so sorry guys so all of them worked on improving those vocabular object detection though which is based on Foundation models and that's a very important thing to see uh they all they all had changes to high level planner meaning that it was important to do things like Air recovery and uh making sure and better exploration things like that so um made improvements to different to all the different component skills uh so what's next so the leaderboard is still open so it's great if you guys can keep submitting this is a very challenging task so there's a lot of progress to be made it's also a very realistic task that's part of why it's so hard and why everyone's scoring below why we have like below 10% still because like it is actually the case where you have a robot that has to move around in a home and find objects that are pretty small right it's very hard for for current perception methods um so yeah so stay tuned for updates on F future competition grow the community please let us know otherwise thank you for listening and for this minut there see all see each other okay great Al so I want to uh open this up to the the crowd in a moment but uh maybe just uh kick us off I'm sort of inspired by Ro's talk yesterday where we talked about of real and fake tasks um and sort of made the point that we should be um sort of really emphasizing working on real tasks those are tasks that actually have economic and cultural value um I guess I would want to start off a little spicy do you think the current tasks and your challenges are real tasks or are they fake tasks yeah for my task definitely not real task yeah I mean we have possibility to make a real test but it would be like too challenging to like combine them like a navigation part and manipulation part like into one challenge so we kind of like split it like in and focus on liation part like specifically for this like challenge in the future like it makes like perfect sense to expand it yeah I I think that our challenge is pretty good better than most I do think that it is not it's not it's still easier than it really should be than a real world version of it is actually I think uh like the the real world setup is simple and so it's we're trying that's I'll definitely say I think there's different spectrums of real you have one there actual real world you valuation then you have also two what not is something actually someone needs like I don't think you really need a lockpicking robot but the point of research is also to investigate problems at maybe a smaller scale first so that we can actually do Center real robustly before we start tackling maybe crazy crazy hard problems sometimes so both are very valid to um investigate so I an example of a task we don't want a robot Al maybe argu maybe lockmith really wants a lock taking robot but aief we don't have one um but if you were to think about sort of the next incarnation of your respective challenges towards realism or towards having real tasks what would that look like for your you individual challenges I guess I can start I think something really excited about in the future for our simulation platform is we want to support a lot of like open source and reproducible digital twins so we aren't as focus on let's say mobile manipulation and I think one problem with mobile mantion is that it's hard to set up in your own home sometimes um but with reproducible ones if like a lot of labs already have existing like say franker robots Kar robots whatever just buy a few things off Amazon use a couple rendering tricks to make it look as real as possible in simulation and try training it that's something I hope to try to do something similar in the future yeah I think one big Improvement that I would like to okay mic so one one big Improvement that I think that I would like to see from ours in the future is that it's we very very uh we simplify the manipulation component a lot in particular grasping is not really simulated in habitat which is a simulator that we had so that so that means that um you can't really learn that s toal that you can't really uh do really that which really limits the amount of utility you can get in a lot of cases uh that being said the task itself is like it's perfectly achievable because of the simplification it's all fine it works in the real world too but but yeah that's something that I feel like is really grounds for improving it if you really wanted to make sure this went robustly in real about it yeah uh for my challenge like as I mentioned like currently the initial configuration of the task like the location of the robot and location object it'ses like determined based on human annotation in the future like it makes sense to like allow the robot to freely navigate the room and determine like what's a good position to start like start this like mulation tasks the other like exension would be like adding more like diverse things objects and like robot manipulators awesome um and so as the sort of the organizers and people that are working on some of this technology I feel like we sometimes have the the best view into what is actually most annoying about our particular framework so of course when we sell it it's all easy you can installs done um but what is the sort of the major P point for getting one of your challenges set up and running and training and actually getting Mel working yeah for my challenge I think the paino is really like U yeah on the higher level like the challenging part is like how to you make the make a right challenge basically how to like make the set the correct like difficulty level because if you like make it too difficult and like there's would be like no like mods would be able to perform like that's like a nonzero performance on the like tasks yeah on the lower level I think like we have like U there's a lot of like technical Gap like we need to feel for the simulator for x simulator yeah I I really do think that getting the getting the difficulty right is very important and very difficult and I think I think it's entirely possible that we skied a little bit too far to the difficulty side if it's below 10% but yeah anyway uh in in favor of being realistic or somewhat realistic um so I appreciate that I will also say ours is not P solvable it is actually like a little bit of an ordal to get everything working properly sometimes so I think that's a there's a lot of stuff like this yeah um I think in the basically every iteration man skill we always run to evaluation challenges uh partially because we don't use evalu we always build our own system actually cloud service U which is quite complicated but I think another thing is like also trying to pick what the right metrics are to show so I know like picking hard tasks I think is important but sometimes you know success rate maybe is not the best metric maybe like say partial success or maybe measuring some other measures to show incremental progress figuring out what those measures should be is not super obvious um and obviously we don't want people to like game benchmarks just to optimize a few metrics um so I think ultimately picking the metrics is one of the harder problems too uh have more questions but I want to also make sure the audience gets to have some participation so does anybody have any questions for these challes what did just to to oh to repeat the the question um for SL or for for for Zoom um so what are the road blocks for each of your challenges that are sort of uh gating progress I'm happy to start so yeah I guess we'll start from the middle this time but I think that uh the the main thing that we see that people still have trouble with is actually just simple object detection doesn't work that well in a m robot and part of that's because things are not actually very fra very well framed and so if you're trying to do this generalization kind of stuff that we really pushed with this challenge a lot of the times you'll see that just it doesn't fail it doesn't work it doesn't detect the things that you want it to detect it's hard to train these things it's just like there's a lot of problems honestly and for long Horizon tasks training is training is slow and expensive and just issues my large part of a lot of the challenges and work we do now for manisco is mostly around like Sim to real real to sim that's always like one of the biggest problems all the time and it's a big research topic now obviously we're not going to solve it um but every year we design a challenge we always have to make kind of compromises to make sure you know we can transfer something to simp to real so for example this challenge there's only four axes of movement and it's already quite difficult you can only move three directions in zxy um and one rotational axis and beyond that you know Center real Gap starts to show more and more so we are trying to confine as much as we need without making it too uh simplistic as well yeah for the aror challenge I think like we're seeing like uh like in terms of success rate like we're seeing large difference like across the task like we have like for the simple task like reorient object and the pick up object I think the success rate can be quite high but for the articulated like manipulation task we're seeing that like the first The Horizon is like very long and the second like we're seeing that the robots can open times can struggle like in you know detecting the like object Parts it actually needs to like manipulate such as the handle of the cabinet or the drawer and for the and the the like fluid manipulation task such as like transfer water is also like quite challenging due to like as very like fine Bring like motor control otherwise that water can feel like all over the place um so one quick observation and then a question the observation is I was a huge fan of Ross's talk yesterday too and I'm all going after real problems but he worked on object detection for a long time so perhaps his need to pursue real tasks is sort of more urgent than these tasks because you know like maybe we have the opposite problems in our are to maybe they're too real question is uh suppose uh GPU performance double uh like overnight and you have the choice of investing that that extra performance into making rendering uh better or making like more realistic or making physics more realistic or making training uh go faster how would you invest that 2x GBU performance that you just automatically over that so I personally would focuses I think the biggest problem is actually diversity in data because if you looked at the talk from Allan I earlier today you know they just take Dino V2 features and it transfers fairly well not perfectly obviously so I would think that Gap is not the biggest problem and in fact my personal opinion is if you just train on simulated plus real world data the feature should align right I mean I don't know if anyone's done this yet but it should in theory um so I think the biggest use case is actually diversifying assets even further right we have like hand annotated assets from ai2 Thor sapen what other benchmarks you're looking at if you can get that GP formance you know procedurally generate and verify like these are all usable meshes Collision meshes visual meshes you know or use all those gpus to you know scan every room in this conference center and build all of them um because I think training time is uh not a big issue I mean I work on GP simulation now so I don't think it's a big issue um but I would say diversity is definitely the biggest problem for simulation yeah I mean say I agree with that basically entirely I think I think we could get a I think diversity is the biggest issue yeah on the same to real part like if I have more like resources I will probably like um know try to like identify the physical parameters of a lot of like these articulated like objects such as cabinet and drawers because I think that's the like bottom neck for the S transfer instead of I think the like visual part is like partly like solved so so obviously I've been a bit I'm a bit biased from coming from A2 where we do a lot of procedural stuff and we use language models to generate houses and whatnot um but I guess on this theme of scale are there sort of uh how do you sort of Envision scaling and if you want to scale diversity how are you thinking about that what are the the pathways that you feel are most promising um you know maybe maybe procedural is not the way to go I can't take a first shot um I think one of the biggest things that language model have provided us is like kind of open set generation like prior to GPD whatever end um it was mostly very much like Clos set generation you had a fixed set of tokens you could choose from right now we all of a sudden have open set generation there's a lot a lot of companies now including one for my own advisor actually we do 3D mes generation right um so I think generating things from language is one way to scale things up I think a better way to think about is maybe not just scaling up but also just thinking about find tuning paradigms like to I mean I'm too early stage if I were to start my own company I would focus a lot on taking those extra gpus giving one to each person that buys my robot and finding another their household um because I think if you want like for example like in robot you care about like 99.99 success rates sometimes you can't get there just by a lot of data alone you know if you look at GP for what has acccess rate of like 50% 80% that's not enough for robotics um so maybe focusing more on fine-tuning may be a better approach instead of just scaling right yeah in terms of uh scaling I think proced generation of the thing like could be like could be the way like to for the like navigation test yeah but for the manipulation T are see like a we are seeing like like larger gaps like there yeah I think procedural generation onlys to a point because we still need to make sure that the that scenes are like visual procedural generation isn't all isn't all there is to it right we also do need things like I think eventually we'll need to be able to simulate different types of carpet and different types of like textures the robots will need to move over and like walking through clutter and all these kinds of very very very complex and unusual scenes that that fall into this very long tale of just possible weird outliers I think that there's a lot of work to be done on how to best procedurally generate things maybe you can get them to work well uh it seems like the only thing it seems like uh I do think that the best route at some point is going to be to be able to collect lots of data in the reg World especially as robots get more accessible lower cost I like the idea of just of collecting lots of data having hundreds and thousands of robots out in the world and being able to have people send data back I think probably the best example of this has been what us actually been able to do with autopilot right where you can collect lots of data have start to work impressively well even if it's still not 99.9% yeah thanks for organizing all those great challenges I want to raise a concern and ask question about the Rob Rob and U I guess the first question is that should we assume like a fix ement for and second perhaps more important for mile manipulation right what's the what's the First Choice Cheng and you solici and so I have pretty strong opinions on this someone works for now a company building mode manipulators but so I like I'll have more to say on this later but I think that uh something for for for mobile manipulators I do think there's a lot of value to having everyone using the same ones I think one thing that we did in the competition which I really liked is having basically just be able to download a Docker image from someone and test out their entire robotic stack and have it like go around and try to pick in place in our environment that kind of thing is really valuable uh so I think that on this so there's a lot of stuff on the software side on the hardware side it needs to be uh reasonably capable six soft manipulator it needs to be light enough that you can move it and position it in different environments if you want to test it in homes it needs to be about a person sized right meaning that it has it can't be it can't be like a Boston Dynamic spot like a really sorry like a really big robot industrial robot that makes a lot of noise and damages tables and things like that it's got to be small and safe and quiet and it's got and it can't be dangerous because like there's people in homes and things like that so there's all these other those are the so all of those things are very important in my opinion to be able to deploy but I'll let these guys talk I think go on this for a while yeah and Will from my perspective I think it's beneficial to have like a different like kind of robot like for like kind kind of tasks like for example in my challenge like for the franer robot we are using I think like it's like reach is limited so that they cannot able it's not able to reach like objects on like a higher shell for example yeah and also I think like based on the like capability of the motor there are also some like certain Tas like carrying like very heavy stuff that's the current like it's not able to yeah um I guess coming from who works a lot on simulation recently I'm personally not attached to any particular robot I also don't work at a company so I don't to sell one necessarily um I personally am the opinion like in simulation the biggest use case for is you can test any robot basically um if I were to really work on real robots more often I would opt for one um Sim to what Chris was talking about basically um one that's safe that's big part I think safety consense slow down a lot lot of teleop um like if I was watching a robot bump in every hour I would need two like one or two Engineers monitoring it that's like $200,000 a year just pay to look at a robot so definitely safety is a big concern um and also for embodiment is something that's easy to teleoperate and collect data with um so something really cool about stretch robot for example is there's a really cool project out of Miu where they get a really simple two finger $20 gripper you collect data for that robot um so um robots have kind of afford that kind of easy data collection system is something I would be very interested in were you finding Engineers that may take 100K a year yeah topic to discuss um so we have this debate about you know so for um B that you guys got what was kind more given that tasks and also tasks sorry you said Trends about what like the models yeah I feel like it depends on the feasibility of like a data collection and the availability of the data if we have a like large like amount of data like and the task is relatively short Horizon I think like yeah it is okay to use the end to end like a base approach yeah but if it's the other way around I would like prefer to use the modular based approach yeah for our task and stuff was just completely infusible did not work at all yeah totally modular um for our task I guess we were quite focused on robot learning and more focused on like maybe the modern way of learning not classical so we did quite reduce task a lot so that end to end with behavior cloning pre-training plus reinforcement did work um no one used motion well I don't really think it makes sense to use motion planning our sense um but yeah we designed the challenge to support you know NE networks and end end learning effectively one other question so uh what would you I throughout your challenges the the types of embodiments you have and the way that you control them differs quite a lot um I guess what is the what is your perspective on the right way to do control it what what is the action spage we should be giving to our agents and why why torqus or velocities um so this is actually pretty important problem because something that work in our own lab we find that when we try to transfer like really really dig deep into the we of Cent real choice of you know like learning with ector control so where you just move the so it's just Six Dimensions rotation XYZ Delta actions super easy to learn sometimes not easy to transfer what I mean by this is because a lot of companies um have their own internal inverse kinematics Solutions and we found that we can't acate transfer simulation policy sometimes because we don't know what their ik solver is we ourselves use Pinocchio but they might be using something else that we don't know so we find that you know sometimes position control actually is what you should be just be using another short comment on this is in my own experience when you do GPU simulation solving for ik and GPU simulation is so slow to the point that learning with joint position control sometimes just faster so maybe this is not a debate to begin with anyway who knows so I'm of the opinion that you should probably just provide multiple different interfaces to the robot and have a low level control stack be able to handle multiple like I think that pH is often the best for generalization I think that there out skills where that's not the best representation but especially we're focusing on pick place like joing position control really made made a lot of sense for us yeah for under challenge we're mainly using the pH like based control of the an factor I think for certain T like a you know like the transer water task I can see the benefit of you like velocity control like for example Jo St any audience question not I'll ask a very related question um so we talking about action spaces and so naturual thing to think about next is observation spaces so there are lots of different ways that uh we can observe our environment and you know can use T to sensors we can uh use RGB depth lar um we can have multiple cameras so something we did for the Spock project or the RL for on so we use our own sort of mounted camera onto the stretch because we just found the stretches Viewpoint is too narrow to get the generalization we wanted so um yeah what are your perspectives on the the sensors we should be using and I guess is this something that you plan to change with future iterations of your project or your challenge or your your robot so at least for ours uh we we use mostly the sensor tenses that are on the robot so rgbd and but we also added slam like basic state poess estimates for for where the robot is instead of simulating a lar which you could do instead um I think that having a good slam system is kind of a feature of most robots that you mobile robots you can buy these days so I think it's a reasonable thing to to say is part of the observation space um I also agree very much like I think rgbd or at minimum RGB is quite important I mean Tesla has shown quite powerfully what you can just do with just only rgd cameras no Lar um I think one they're cost effective you know you can take your iPhone camera too to collect data that's the main thing the universal modality for robotics is almost always just Vision whether it's you know this is how humans perceive things you know robots perceive things almost the same way um and I mean the other small reason it's cheap to do in simulation now sometimes I think for the AR challenge right now we're using the five like rgbd cameras like around the like manipulator but sometimes I think it's also like beneficial to have a like CER VI camera like for the like environment I guess I have one other small comment I guess if you're coming from like doing s to real there is still a lot of use case for using ground troof data particular with asymmetric training methods um so like doing GP simulation whatever simulator you use um you know ultimately does a lot of interest recently in like trying to distill a vision train policy by using fast State simulation and then take div Vision simulation for find tuning I guess on the on the point of speed things that are fast I feel like there have been a few claims today about very high simulation speeds and uh both for rendering and physics simulation so um I think we we heard some of the the highlights in terms of GPU uh batching and all this but what were the sort of main insights that yall might have to generating fast uh simulators that actually can train with the in parallel um I guess I can call this about our project to extent um I think one of the biggest reasons why Mana 3 has such fast art rendering all of a sudden is because we actually upgrade to the GPU State simulation so that means we transfer State data to our renderer directly on Cuda no need to move to the um CPU memory so that so our last year FPS was like maybe 4,000 we just kind of doubled down sorry went up to 20,000 now um I think the other thing is you know GP simulation is super cool but you know you can't actually GP simulate everything possible um I'm probably overc a crap ton but like um there is a lot of caveat that come with GPU simulation for one the per thread speed of GPU simulation is slow so like for example 16 CPUs way faster than even 32 GPU threads in some sense um so you have to kind of pick and choose the tasks that you care about like if you want to do fluid simulation yes they run in GPU but they're not very fast um so I mean for the most part though like for a lot of like you're still working at pick and place for all things and GP simulation is very very helpful for that um the other caveat that came with developing this new simulator is also that you know you had to switch your brain a little bit to write GP paralyzed code um so I had to write all my code in batch which is you know it's like a trade-off you have to make right do I spend more time writing code to be fast or do I use a simpler CPU variant um definitely try off think about aw in the interest of time and we're getting close to the end so I want to give you all one chance to maybe say one sentence or around one sentence about what you'd like everybody to take away from your presentations today and um maybe maybe save something BR probably like need more time to think about it sure I I think you should try out our competition it's nowhere near solved I think it's a really challenging and interesting problem especially since you can try it out in home environments if it works you could try it out in your house for example which is pretty cool but um I think another takeway I like to giveways like you know there are a lot of simulators out there regardless of which one you use simulation I think will end up being very very important in the future whether it's for training or evaluation um I of course I'm bias to my own but honestly any simulator you use is better than no simulator awesome thank you so much give one more round of applause to our keep this laptop for sh I think it should work oh perfect yeah hello maybe we I think I have to turn on my sound okay um shiron can you say something hello can you hear me yes it sounds great uh let me let Claudia introduce you oh I with I think I need to pin your your view one moment and you're able to see my slides yes the slides look good at me just we're figuring out this one first and find her here and then P oh it has to be okay pan so you hear us well yes yes excellent okay so we are ready to go we're on time cool well hey everybody it's really my pleasure to uh introduce shiran song She's a faculty at Stanford currently and she leads the Robotics and embodied AI lab uh real for sure uh I have been following Shan's work as well as as many of you she has done excellent work on trying to understand what is the right data for robotics what is the uh right data mix as well as uh learning progress and devices and tools for data collection to also address this problem she has done some pioneering work on showing to the world what diffusion models can do for robotics which I've been very excited about and uh with that I'll just uh give it to her so uh she has all the time please take her away thank you so much yeah thank you so much for the introduction and uh very excited to be here it's a very exciting workshop at was I was sitting in the panel discussion that's a all very good points made today so today I'm going to in my talk I'm going to talk about uh in the wild robot teaching without in the wild robots so really kind of getting into the question of how to get robot data so to start uh let me first show you a robot that I think many of us would like to have in our homes uh that is this robot this dishwashing robot right so you can see this robot is able to kind of pick up dishes wipe off the dirt uh and uh it's also pretty robust if you add more sauce to the the plate it's able to continue to wipe it until it's fully clean and then by the end of it it still also remember to close the faucet and if you open it it will like close it again so um I think this is if um everything is automatic is train the policy and in case you didn't realize uh let me tell you it is a very challenging task it's one of the hardest tasks we have done and then and it this T really pushed the boundary for robot capabilities in many fronts right so for example first of all it's a relatively long Horizon task that it has many steps each step depends on the success of the period steps and also in order to successfully do this task the robot need to perceive and manipulate complex fluid for example the water and catchup they have very different Dynamics and then similarly the robot need to manipulate like constraint articular object like this faucet and also use deformable tooths like this deformable sponge and finally U the robot also need to be semantically robust to the concept of like for example cleanness if you add more catchup on the plates you need to understand that I need to continue clean before I end so so all these things I mentioned here is just what makes this task hard and challenging and if you think about how to like write a program to program this robot to do this task reliably and consistently it could be quite challenging so how we can make this system work right so the hello idea well the basic idea is actually quite simple it is just learning from demonstrations right so just like how you teach your kids to watchh dishes at home but here we just do the same thing with robots however while this idea of learning from demonstration is intuitive and simple to think about actually making it work well in practice is not easy right so in fact if you ever take a robot a robot learning course uh the first few lectures will basically tell you behavioral cloning and actually why it doesn't work or why it won't work well in practice there are many issues for example like the the um the difficulty of modeling complex action distributions how to collect those uh data and then eventually how to make your policy generalized so today in my talk I'm going to touch a little bit on all these three challenges and see how our current system can kind of bypass those those challenges so in a high level I think there are two technical questions we need to address to make the system work right so first is on the demonstrator side or the teacher side which is how to demonstrate a new task to the robot effectively and efficiently and second is on the robot end is how to learn from those demon demonstrate data so that it can the robot itself can do the task next time when encounter a scenario that's Sly different and then we're going to look at this two question one by one so first uh is how to demonstrate or how to collect the data right typically in your robotics life the most common thing to do is to T off your robots for a new task for data collection right so um however in order to like really have a good and nice set up for T operation that often times requires either like very comp complicated uh device or at the very least a very well Tred engineer that knows the robot how or how to T off the robot and and also another challeng um like limitation is that the data you collected through this interface is often limited to only the lab environment so if you want your robot to work at your home it's often very hard so on the other hand you can also just show the robots a video of how you washing dishes however um the trouble the the robot will might have trouble learning from this CLE video because a big embodiment Gap right so for example your hand is just very different from the robot hand so instead uh what we do is kind of somewhere in the middle basically using a sensorized handheld gripper that is very similar to the robot gripper and use that for data collection so that the robot can easily learn from that data and here is how it works right so basically uh you can take a pair of this gripper let's just hold it on your hand and then do the Tas that you care about and now each of the gper we add a GoPro camera to record the visual observations uh during the task execution and then by tracking the camera poses you can get the corresponding robot actions in the form of antifactor trajectories and then uh also on on the each side of the gripper we actually add two small mirrors to provide two slightly different perspective view so that it can help the robot to kind of help them estimate the 3D depth information without adding additional cameras so here is how the mirror looks like in the input um like fish eye camera and also this is the view that the robot will see during execution or this is the input to the policy and then from this view you can also see that this is almost no difference between the human demonstration data and also the the the visual observation the robot will receive so as as a result it help us to bridge the embodiment Gap so in the end of day all the demonstration data is stored in a single m for file that contains all the information needed to train an effective robot policy right so for example those multisensory observations and also decaled robot actions and hope hopefully in uh we really like this mp4 file is actually something that's really easy to transfer and share so we really hope that this MP4 like this kind of data collection format can be a standard and sharable data for robot manipulation in the future so now we have the demonstration data the next question is how to learn from this data so here the robot policy is modeled as a neur network and uh what it the input is a visual observation that is from the goal camera and we want it to Output or infer the proper robot actions from the observation so one of the challenge uh at this step is to model the action multimodalities or those complex multi uh distributions in the action right so which means that um given the same visual observation there might be multiple valid action so this is actually a very common problem in like human demonstration data since that there are naturally many different ways to do the same task right so for example in this simple U case I show you if you want to push the T you actually can go from both the left and the right and if your policy not able to handle this kind of multimodality it's very likely to get stuck in the middle which is pretty bad so to address this problem we are making off uh making use of one of our earlier work which is called diffusion policy so the idea for diffusion policy is actually quite simple we basically on to use a similar idea for diffusion model for image generation but now use it for Robot action generation right so the difference here is that the output uh space for the diffusion model instead of image is now the action trajectories and then we can uh do the diffusion process on output actions in order to prict robot action so intuitively you can think about this whole uh iterative the noising process as a gradient descent process in the action space where the predicted gradient field can have apertur number of local minimum and then each of the local minimum can capture a unique mode right so this is why or how um by using diffusion model we are able to capture those multimodalities in the action space so long story short uh by using the same learning algorithm which is diffusion policy we are able to train a wide variety of um different manipulation skills for different applications like folding CLA or um passing objects assound you can provide the right TR data so for example uh if you look at the tossing task this is actually a task that's really really hard to T up if you think about it even with very expensive teleop teleoperation device because this action requires actually very fast reaction speed and also very smooth actions um however now anyone um without knowing how robot works as since you have the scrier you can teach Rob about the skills by just demonstrating the passing action and then also in this tossing task uh is where we start to realize that um the importance of properly handled the different latencies between uh the the robot OBS sensory observation and the hardware execution so here for example uh it's a system that's without handling uh uh the latency matching properly and you can see that uh the robot will have a lot of jiter behaviors so this kind of git action is probably okay if you care about only pick and place task it probably still able to complete the task however this G action will destroy all the momentum that the robot trying to build up um when it's trying to execute this tossing action and therefore in in this video you'll see that all the objects actually have trouble reaching to the deser tossing velocities and object actually fall outside the beam and then U by holding give you another quier we can easily kind of extend this uh system to perform by manual task so for example in this video we are showing how we can demonstrate um the class folding task which by using two grippers and the robot is also able to learn from uh this uh this training data um and I think one of the trick uh like a tricky part for like doing bmanual demonstration is that now you cannot actually press the GoPro to start and stop uh so we actually use voice command so that you can actually start and stop so the data uh demonstration okay so that's uh the tasks that we actually select and picked to highlight the difficulties um of the task and also highlight the capabilities of the system by showing like what kind of complex task we can do with wumi Grier so but then how about in the wild generalization so I think one of the biggest criticism that people typically have with behavioral cloning policy is that they oftentimes has very limited ability to generalize to unseen scenarios and we all know that uh a behavior cling policy if you test it with Auto distribution scenarios it's going to be like it's it's often times not going to work well right so for so and also so far for all the experiments that I have just shown you is what we um tested on what we call narrow domain evaluation which means that um the algorithm is pretty much tested on a very similar environment where the robot where the training data is collected so although we vared the initial configuration of the robots or initial positions of the object the environment itself is very similar between training and testing um however Now with uh hand hogers like wumi we can really get into many different environments to collect very diverse training data and what we really hope here is that by using those very diverse uh uh training data or collected in in many environments now we can train a policy that's able to generalize to unseen environment that the policy is not original train on right so that is really the hope of why we want to build this hand Grier so in order to validate this idea of see whether uh this is true we test the system on a relatively simple task which is this cup rearrangement task so in this task uh although it's uh seems simple but it's actually still requires the system to kind of push like rotate the cup into the right orientation and perform a very precise piam place and then uh you can see in this video you can easily carry this handog gripper to many different environment to collect data right so overall within just 12% of hour we are able to collect over 1,400 demonstrations in like 30 different environments for this C Arrangement task so basically the students just Parry this CER with them uh go back home and then uh whenever they go to a restaurant before the the the t uh the the food is served they can collect some more data or at their home they can collect some data so that's the data collection process is actually quite easy and then with all this data we can train a final policy and then to test the generalization capability for the policy basically drag all robots across Stamper campus and then evaluate the policy roll out so to our surprise actually a single policy trained with this diverse W data is actually able to work out of the box for many unseen environments right so for example uh the fountain like we we actually test the robot to do the cover Arrangement task in this fun setup that is actually not really even a a table which just put the cup and saucer on the fountain and the robot still able to perform the task regardless the water and then also um oops oops can I play this yes so because our system only used a rism camera so the whole policy only needs one camera and it's rismon camera and then the policy predict the relative motion the whole system actually does not require any camera calibration and then it's also as a result it's actually robust to the base movement right so in this video what we are showing here is that you can drag the robot a little bit around and the robot still able to complete the task desp the base movement so because of that I think this uh W setup is also very suitable uh to be a deployed for mobile manipulation as well so now you can uh you have seen that the policy can generalize to different environments um but now uh I in the next uh few slides I want to show you to uh dig a one layer deeper and it really asks the question about where does this generalization capability come from right so because one thing uh is that for all the policies that I showing you today all start with some kind of prrint visual encoder so for example most of the policies are prrint with clip visual encoder and we find it with the data we collected so there is a possibility that the generalization capability is actually come from the visual encoder um and instead of uh so we actually don't really need inter wild action data so that is the possibility so in order to really answer that question uh we did another experiment where we still use a pre-train visual encoder in this case clip but now we actually finding it with very uh with the narrow domain training data which is the action data collected in the lab environment and then here is uh the comparison of the two train policy with and without the in the wild functioning right so as you can see that the the model on the left is the one that only trained with in the lab narrow domain action data and then the system actually barely work it almost cannot finish this task so I think um based on this limited experiment I think uh one of the important takeaways here is that at least for today I think funing a large pre-train Vision model with just the narrow domain robot data is actually insufficient for generalization which means that we still need very diverse action data if you really want your robot to generalize so just r on large Vision model is not sufficient okay so lastly uh I also want to show you uh talk a little bit about a slightly different type of generalization right so this is generalization instead of two different environment but actually two different Hardware embodiment so as you can imagine because of the the design that we did for the hardware interface that only used a wrist man camera um the system actually like the data and also the policy can easily generalize to different robot arms right so as so here I'm actually showing you the same policy that is deployed on the Frank arm and also a smaller ARX arm so as long as you have the robot arm that you can processive compute I Solutions and do very good position control you can directly deploy the same policy and then also thanks to the IND wild training data it's uh the same policy could work almost in any environment with any expresso cups that you can get on Amazon right so we actually released uh the pre-rain weight for this particular task so you can actually download this weight and then directly Deploy on your robot and then just buy some cups from Amazon and it should work and if it doesn't work let us know um and then also to to take this cross embodiment idea one step further we actually recently test the train policy on a very different like drastically different embodiment that is actually a quad pad with a arm so here you can see this is a little robot dock with arm on it so the difficulty here compared to the earlier example that's uh when we deploying on Industrial arms like ur5 is that for like cop pads we actually no longer have a very good controllers or ik solvers to give us a very precise antifactor pred control so as a result we actually need to bring a whole body controller to uh help us to realize those predicted antifactor trajectories but the good thing is that um the training for the whole body controller can be done purely in simulation without the needing the need of additional Real World Training data and then once you have that H body controller training simulation you can actually easily deploy any existing manipulation policy trained on the womi framework to a very different embodiment like quadropad so basically making your manipulation policy mobile and here it uh so that is the tosing task and in this video is a whole body pushing task and then we actually call this system Umi onle which indicates that you can directly put any Umi policy on a lged robot so we want to highlight how simple it is so that's why I think you should probably also try this policy and also the system as on your favorite robots as well so we actually open source everything uh including the code uh the hardware assembly tutorial and pre-rain weight so please uh check it out and then let us know if you have any questions and we are happy to like see uh the system running on different robot as well so if you have new system running uh show as the email that's the end of my talk thank you thank you thank you can you hear us yes I can hear you okay we will take some questions from the audience and by the way Shan I don't know if you can see people from on the other camera but this is a very full room like I'm just standing here okay now we need this for her okay please pause one second can you see this yes yes hello everyone okay there are like three rows of people standing okay thank you so much for being here um okay so questions from the audience go ahead I have a little bit trouble hearing the question maybe you should come to the front repeat it for you we will repeat it in a second well please pick up because I don't we know that policy do yeah correct me if I'm wrong but I'm GNA pack the question as in um how do you compare your approach is based on diffusion models which are very robust for noise and multimodel and so on with sort of kind of U another approach in which uh models are based more on pre-train models kind of what more what Google is doing take a pre-train model with a lot of semantic knowledge already embedded there and then keep keep training from there yeah okay yeah I think I think there are uh two part to that question so first is that uh I want to clarify our our diffusion policy also actually rely on some pre train models so the vision encoder as I described also r on pre train large uh like clip models that's already kind of have some benefits of pre-rain models um but I think the question is more about what if we do like kind of like more modular based approach that's for example use VM LM and to do like motion planning like uh that and in comparison to like diffusion policy directly doing behavioral cing so I kind of feel this two type of approach they are kind of handling slightly different parts of the challenge so for um like high level planning and reasoning those large Lang models and vision language models are very useful by providing those Common Sense knowledge but this like very fun lowlevel manipulation task um I think that's probably easier to learn directly from Behavior coning so it's a little bit harder to like squeeze out those um um action uh like very detailed orous manipulation action from those vrms or RMS so that's why maybe I'm pretty sure eventually it will be like combined that you have lar Jun model for planner and then kind of triggered to call those like lowlevel Behavior cloned uh policies in order to do a very long Horizon task I hope that I answer the question yep thank you go ahead Arsenal how do you deal with kinematic differences between the human demos and the robot kinematics embodiment Gap yeah that's a very good question I got this question quite a lot for for UMI so I think we have we have 30 different uh Solutions if you have different robots so when we just do Umi like with a very good industrial arms like ur5 when we have a really good model for the arm we have really good I sers or controllers for the arm what we do is basically do a very simple um data filtering so you have the human demonstration data and you can actually filter the data based on like whether the kinematic limit and you can filter the data if it's not reachable by this particular robot you can just kind of make those data invalid and then only train on the valid uh data however that approach won't work very well if for example once you move to like the qu pack scenarios where you don't have ik S and you don't have a very good controller so in that case we actually need to train uh like a controller ourself with reinforcement learning simulation and then in that case actually the reinforcement learning is trying to reach all the U an Factor trajectory that you demonstrate but of course there are PES that cannot reach and automatically able to learn like what other data they should learn from so that is uh how we uh right now handle the embodiment difference and there are indeed cases as human demonstration cannot be realized on the particular robot embodyment yeah next question y you you were a couple of things about the gripper for data collection is is the up closing binary or you control the the like length of the opening and what was the other one yeah yeah okay yeah that's a very good question I I'm so glad you asked so it's actually ban control is not sufficient so one of the reason that why I can do for example task like tossing is because we actually track uh the the Grier width because the opening moment is actually very important and critical it need to be very processive controlled so we have a QR code on the uh on the on each of the uh uh finger so that you can track precisely like the the opening width and then that is also transferred and learned on onto the robot so that information is critical for like a lot of fation task when you cannot just do ban control questions from the audience one over here then you get like 75 practical applications you might something5 guess question how do you see how do you accy the question is about the performance that you're getting success rate uh something around 70 80% how what is what is the next mile to get that to 90 you know 99 uh what do you think as the next approaches for that yeah yeah I think that's a very good question and I think if you ask this question to different people they will have different answer I'm pretty sure if you ask that question to Chung his answer will be like just collect more data and actually it's I think it's kind of partially true so for all the um examples I show you uh we for some especially for those hard tasks we actually add additional iteration of more data so for example for the dishwashing task we collect the first batch of data and it doesn't work that well and we actually add more data it works signant better I think that is um the the beauty of learning based method is that a lot of times the complexity just get into like collect more data and the more data you have often times the performance will get better but I guess another question is that how we possibly get the system to work like 99.99% uh accuracy I think that's probably require slightly different approach or like maybe something different than just getting more data um I I think right now I don't have a very good answer for that I think maybe for example the system can be uh doing a little bit of exploration and uh collected some more error correction data by itself um or using some reinforcement learning in simulation to help them to like get those um the last bit of performance I think those are like all valid idea but I don't see one solutions that really able to like solve all the problem so I think that is um still open research question I'll take a quick question from the slack and and then I I'll go to you um so one question from the slack from one of organizers LCA so building generalizable and performant models seem to require training large deep models which are slow to run inference on so the question is about the the tradeoff between uh sort of the the well let me read it because I was trying to summarize too much real time dexterous tasks competition however require uh require High late uh low latency decision making so how do we resolve the tension between uh these two these two aspects yeah I think that's again it's a like excellent question and something that we actively thinking about so right now diffusion policy is just a flat policy that I think it's actually not fast at all it's pretty slow um because it's using a diffusion process and we didn't really optimize it for it um but there are a few things that you can make it's uh like okay so for example by predicting a trajectory instead of just next single uh Next Step you actually can like get away by predicting it slower because you have so many actions that you predicted you can actually execute it for for a while um and however I do feel like at some point uh we probably need a hierarchical policy so right now diffusion policy predict the antifactor trajectory and we rely on the robust controller to do uh to do the like the more High rate decision- making and and actually executing the action or doing the linear interpolation between the action so that's is our already like a hierarchical representation like diffusion policy is slower but uh like more a faster controller uh in the in the uh lower level so I think in in the future once we have like if we really need to take in like force feedback if we need to have more fast RIS U like reaction it's possible that we probably just make for example diffusion policy or like different Nal architecture that naturally has this hierarchical representation some of them is slower that's able to consume more data do more common sense reasoning and another policy that is more reactive just doing like faster reaction and also control so I think that is like Pretty Natural direction to go great I think we have more questions in the room but unfortunately we are right on time right out of time so thank you so much youran for uh presenting remotely and um thank you thank you bye bye e e all right good yeah good okay cool little not where screen okay right all right so you heard from him before and now you he from again um we are very lucky today to have um CHR Chris Paxton from Hello robot uh come to talk to us today um he has been very big in the space of making sure that we have robotic systems able to operate in human environments uh he's been working on ways of being able to do uh using language perception planning and policy learning to make robots into general purpose assistance and is now leading the embodied AI at hello robot to build practical inhome mobile robots so look forward to hear what you got to say today thanks yeah hope everyone's not tired of hearing me talk about home R again because we're going to hear that bunch of this so uh the general idea is how can we build and again my keyboard isn't working so that's fine yeah so uh so how can we make robots that are that are General robust general purpose assistance that work in any environment right so particularly this is what I want for my robot I have a robot that can go into my house and can I can talk to it and have it go and perform any different complex task it can generalize different environments different room layouts different type it can do whatever it is that I might ask it to do um and it has to be able to do a wide range of complex tasks so this is so this is the dream that we want to build towards and what I'm going to talk about is basically how we can get there so uh way that robotics people like me would have gone with this is something called task em motion planning uh I don't know how many many of you know about this but basically you would have to know uh you'd have to build in all of these kinds of behaviors you can do really complex stuff like this but but obviously this doesn't this doesn't quite scale you have object models struggles with partial observability and so on a really great uh imitation learning work uh including from what the talk we just saw uh one of the issues that we still see with this is generalizing to different environments and different settings and being able to capture really wide ranges of different tasks preferences Etc uh so we need solutions that allow well to different environments robots embodiments can perform different tasks different types of manipulation uh whether it's dextrous manipulation or being able to do um navigation exploration and so on uh so so we have so I'm going to talk a little bit about different solutions that we have for this and why I believe this is possible that we can in fact do uh generalizable robot autonomy in homes and in a wide range of different environments using current techniques and and uh so on um in particular I think one thing that I that I like to that I like to note is that for a lot of the videos and stuff that I'm going to show and and stuff that I'm going to talk about is that we don't have we is that we really have this strong assumption that we really care about generalization we don't we we want to deal with objects that we haven't seen before instances we haven't seen before lots of different tasks and so on um the other thing that I'm going to talk about of course is home robot so how do we know whether not when these things work how can we assess this how can we share code how can we make sure that uh we can actually have these kinds of systems and deploy them at scale in the real world so and finally we have the robot itself that we're going to talk about so this is stretch three uh so it's a nice it's it's a very simple this is a robot that that I'm going to use a lot of especially later on and I want to talk about why we're going to do this so particularly there are a lot of different types of robots out there like there all the humanoid robots there mobile manipulators uh like dogs so but we really care about this being like portable lightweight easy to deploy in different environments and safe to use around people because that's what we're going to be doing we're going to be like running I'm going to be running experiments in my home and in other people's homes and so on so all of these things matter quite a bit uh we have a l cost robot that we have for this um so so particularly the problems that we need to address to be able to build robots like this that we can deploy into homes is that we want them to be able to we want robots that can navigate and navigate and represent scenes so we can answer questions about them we can move to uh open vocabulary goals like if I ask it to like if I ask it to cook something or get me food it needs to know to move to the right cabinet and open it up it should it should be able to we should be able to it should capture semantic knowledge about the scene and the purpose of things in the environment um it we need to be able to manipulate objects perform multi-step manipulation tasks on unseen objects in unstructured environment so I don't know what's I don't know what I've encountered yet I need to be able to perform I I might want to set the table or stack a bunch of boxes or something like this all of those kinds of tasks have to be in scope and uh we and of course we'll talk about benchmarking and bringing it all together and actually deploying it in real environments um and again generalization is really important to us because we need to be able to scale to homes um and to a wide Vari environments and your homes different from mine and so on and we don't necessarily have we're not looking at lab Lab kind of environments as much as possible so first of all I'm going to kind of quickly go through a bunch of different um sort of like how techniques that we can that that show us that we can actually kind of do these kinds of things at least to some extent now and that we should really be assessing on of these open vocabulary kind of tasks where we have where we have different kinds of objects and uh different requirements and things like that so with navigation so this is open vocabulary instru we want a robot that can follow which we to which we can give High Lev instructions go to the Bookshelf help me clean up the trash so we need to be able to so we need to be able to capture some knowledge of the world in in in our robots in a way that they can use it so uh so the question is really then how do we learn and build like useful representations so we go with these kind of spatially organized representations in a lot of this work so I can walk around I can either walk around my house with an iPhone or I can have a robot explore it automatically and then I can I can detect objects and I can take in a and I can build up a semantic representation out of offthe shell features uh so so in a lot of this work we're going to assume we have 3D depth sensors so that we can capture structured structured representations of the world but these about these representations are populated with with open vocabulary features from things like uh like clip or uh dino that we can so that so that we can actually make open vocabulary queries so if I get a 3D representation like this from having my robot explore or whatever it is on the left then I can I can uh if it's populated with clip features in this particular work is called clip Fields then we can then we then I can ask it for something like warm up my lunch and I can see an activation potenti on top of the microwave there which is where the which is where the robot should go to next and so so uh these kinds of techniques they're they're inherently modular because we have these spatially organized kind of representations which tell us which tell us uh sort of spatially where the robot should go where it can plan to but now I can start to do like compound reasoning but I I can also have it just I can ask it a wide variety of different questions having it Go and wash my dishes I can have it I can these repres presentations we can train them to capture um to to capture uh distance Fields so we can do object avoidance and motion planning on top of this so we can get lots of lots more robustness than we get just from training endend policies uh in addition we can use similar similar set of ideas as far as spatially abstracting out some of this by for for generalizable object manipulation so given a point Cloud description of a goal I want to be able to tell it something like put the boxes in a line or set the table if I give these kinds of language instructions to my robot then it can do is we can we can train models so that it can predict uh basically where these things should move in the world so like I this plate should go in this direction instead of training instead of explicitly training like end uh joint State positions or end effector velocities or whatever it is if we if we express things in this way it generalizes really well uh and again we can all these experiments that that that are showing it doesn't it wasn't trained on these particular objects or these particular scenes and so on and how so and it can do these fairly long Horizon tasks just from from language instructure um once we move on from that we have we have to uh so now I've got I've got basically the tools that I need to be able to take that robot and and have it perform long Horizon tasks but I still really want to be able to capture user preferences and Common Sense knowledge and so on it would be great to just be able to plug throw the Sol PT and ask it what to do uh these models that so once we can do that we can also use we can use these to be able as with with together with different algorithms for planning to be able to reconstruct user preferences and predict things like if I showed pictures I can I can I can then uh have it sort of assemble these assemble different structures based on what based on what my preferences are um and we also see that GPT is really useful in these kinds of cases because it capture because it can capture all the semantic information about like kind of how objects relate to each other and that means that when I that when I take this robot um I can I can have it perform multi-step I can have it perform multi stock planning as long as it gives us gbd may give us a bunch of contradictory information that prevents it from being reliable enough for a robot once we put into those kinds of planning Stacks then we can actually start to start to get real executable task emotion like full plans or uh that that let us solve very complex tasks all right so that was a whole lot of of different things at a very high level but my point is that the the point of saying this now now that we're about halfway through the time is uh that we is that we uh we basically have we have this kind of recipe that lets us perform U perform long Horizon open vocabulary object manipulation in a wide range of environments and yeah these aren't like as smooth the smoothest and most beautiful possible actions that we can do but things like uh tools like GPT open clip and so on basically all of these Foundation models that you've heard so much about today and yesterday and so on actually do give us enough information that we can start to deploy robots in Real environments on real kind of open tasks uh and so so now what so what we're GNA talk about um for the remainder of the time is basically like what what goes into actually benchmarking this and trying to deploy these systems and get them to work well so so that's where we can get back to home robot which you already heard something about so I'm going to go into a little bit more detail of what that is and what the thought process so um so so instead of doing geometric Arrangement like we used to uh we want to we want to do this open vocabulary task so so my so instead of um so I'm going to say something like pick up the box from the table and place it on the sofa uh right so uh so the reason we set it up this way um is that we really care about I want to in in your home there's a very wide range of different objects that you might want to be able to pick up and manul uh they like you cannot assume that you've seen this in the training data at all a lot of the a lot of the issue is the difficulty in this comes from the from the intersection between perception and manipulation and planning because I need to be able to explore my environment and find the objects that I care about so we're really pushing on like we should test on objects you do not know what objects we're going to deal with um in particular when we when we put this together uh we wanted to come up with a with a with an embodied AI Benchmark which uh had continuous action space was Deployable on real robots easily that we could um that was solving a meaningful problem that looked like one that you might actually want to do in your real in in your house because for example if I this open vocabulary task may sound a little weird to you but like but if you think about something like setting the table or put cleaning up the dishes or whatever it really is just a sequence of these open vocabulary manipulation tasks right so this is a very foundational piece that allows us to to do to do a lot of really cool things if we can get it to work well in homes and that's the real question can we get it to work and so um so we set up our talent or competition to have two different uh two different benchmarks one is a simulation one is a real world Benchmark we have a lot of different environments and simulation the environments are created using U we'll get into that later but we have 150 different categories of object uh about 8,000 different instances Etc so it's a very so it's a fairly large data set not the largest that I'm sure you've seen but it's reasonably big uh created using open open object data set so licensing is all good and you can you can train it your if you want to um you can uh so four different compon four different parts to the task that you get scored for so finding the object picking up the target object which is a target object on a particular goal receptacle this this go recept the start receptacle part is because is to kind of give it a hint as to where it should explore first because you can put you can put something anywhere in your home and it might take a really long time if I have a really large multiroom home and we'll get to that in a second and then you got to move it to a gold receptacle uh and place it on there so so uh so start this is starting from the these are what the simulation environments we have look like so these are all from the habitat synthetic scenes data set the nice thing about these is that they were all created by actual humans they aren't procedurally generated which means that they actually look pretty weird in some cases but there's actually some human thought this was really what a house should look like they were using this um like home Home Design authoring tool to do this so we end up with uh so so we have a lot of different environments like this but they're all multi-room environments they're big there's lots of there's lots of clutter and distractors and things like that um so so in simulation so so this is kind of what the simulation looks like there's a bunch of different examples here of the robot moving around so a robot again roughly human size so it can move into all kinds of different kind of small areas to go and find objects the objects themselves are all small they're all like roughly the size of your hand because they can be picked up very easily by a two-finger gripper uh which means that in a house like this there's a lot of room to explore you've got to really move around through through these very large areas to find the find find the correct object you also need to move close enough to receptacles that that that may have may have the object on them to explore them and that makes it a pretty challenging task so oops so here we go so uh yeah so we so we provide different baselines for implementing this one of them is this uh modular kind of motion planning based Baseline that still is running an open vocabulary object detector of course so you can't do this without machine learning but like this is but there is a case where that's basically all that there is otherwise we do a bunch of reinforcement learning bench benchmarks which one's better kind of uh goes back and forth but in our base in our benchmarks usually the reinforcement learning ones were better leaderboard as we'll get to and if you heard the talk earlier theistic ones tend to be better but uh so what happens here so you can see on the on the right it's as it as it sort of the exploration process as it has to like move through this environment and kind of build this semantic map find the object find the different uh start receptacles the objects might be on find the goal receptacle and then and then it has to actually find and move to and grasp this object right so so challenging task long Horizon task um and then we tested it out in this real apartment uh in Fremont California which is owned by meta this is fairly fairly large test Department with three different rooms uh there's a kitchen kind of a living area there's a bedroom and office and so no no real world training no uh no Maps or Real World Training data were provided uh so we have the robot has to go around and and pick up these different objects and this competition at nurs with the prize being to give away a free robot so we got plenty a lot of different people trying to participate uh 21 ended up actually submitting a lot of people registered uh so we we had U 80 almost 80 79 total uh submissions we end up seeing the success rates for the success rates for about 10% this was on a previous version of sim four so little the numbers are a little bit different here uh but yeah evaluating on on these 10 heldout scenes again so so the the scenes that it's evaluated on have different objects that we aren't that aren't seen in the test um so the way the evaluation is set up uh it's totally agnostic to whether the it's being evaluated in simulation or the real world so we set up this uhu kind of Docker framework for being able to to pull and download people's code and you can test it out on the robot which means that we could uh you run the robot system and it sends over the all the all the observations so once we've agreed what the observations are you can just you can try out these different this different these different methods very quickly um I really like this as a framework because it allows us to test allow allows us to like share and test code very quickly on different robots um the top so when we evaluated the top three teams we saw that um two out of three of them were based on these on these motion planning benchmarks um so I'll show this is one of the the successful episode uh so we so we had a when evaluating the real world was it's a feasible thing to do you can test this stuff out you can test you can test other people's code in the real world which on on this robot without their involvement in in these on these fairly challenging tasks which I think was more work than it sounds like maybe and and it was uh it was nice to be able to show that we could actually do this so basically we can actually Benchmark real world mobile manipulation it's possible we should do it more uh the it's hard to say which methods are the most promising because we really saw things go back and forth here like in the first versions it was reinforcement learning was better in the competition it was this modular motion planning kind of stuff was much better and so uh uh these open challenges based on in this were all based on things like perception and robustness in particular things like Air recovery and uh and detecting objects and making sure that like if you get near something and it drops it what do you do if you get near something and then lose track of it for whatever reason like maybe because the angle Chang and now your detector is failing like that was a huge cause of failure here and so these are the kinds of things we're bringing together uh more intense type of solutions might do better but as we earlier entm training was not very tractable in this problem so there's a whole bunch of other issues there to resolve um and so the next thing we did because is uh so after that we have been since deploying robots into actual homes uh doing this stuff called okay robot so it got really simplified the exploration part to save time but you can now do vocabulary pick in place like this so we have robot robot that can move to like pick up pick up a purple light bulb box and move it to a sofa chair so on so moving around all these different real home environments doing pick and place again so generally again the success rate's not great in these kinds of cases we see like 50 60% success rate tested in a lot of different homes um the nice thing about it is that we can have these robots that deal with like home environments real home environments if you take they they often tend to be like very complex lots of distractors perception models can fail the so this is why these problems tend to be with like dealing with unusual objects clutter distractors avoiding obstacles grasping and grasping and manipulation manipulating reliably um so all so there's lots of cool challenges but again all I think the important thing is that these are feasible things to do you can in fact test in these real home environments and probably we should be doing more of it and less of it in Laboratories uh because like this is it's also there's something to be said for like this is just the robot running in my home it's a little messy so but the but so you can do this and and there's something magical about having the robot pick stuff up and move around in your own home which I feel like is kind of the dream right um so in the future is stuff like you can do dextrous skills so this is examples of using the robot to do dextrous tele operation and data collection so we can we can start to train policies that that are actually very complex using very simple uh lowcost Hardware uh so like so really I would say like Hardware is not the limitation it really seems like it is learning in generalization in general really always going to say generalization right and uh and nice thing is working with people so in this case this is like an example of stretch bringing someone a snack this is the kind of thing that we really want to have the robot able to do is like can I talk to it and have it go and move to different locations go bring get go pick up some food on a tray so it's not too hard to grasp and bring it over to this to this uh to this person tell operated of course yeah I think gr really exciting Place uh generalization and robustness are like really basically the going concerns we can start to do more evaluations in the real world on Long Horizon tasks uh and we're at a really exciting place in robotics in general I think that um there's a lot to do I hope that people can try out this Challenge and do better than we did just like there's still clearly a lot of ground to cover and thank you time for just a handful of questions so yeah there are there on Research that stretchy something where like which part to grasp oh yeah I think that's a so so the question is uh is there any research on using open vocabulary models to grasp like particular parts of objects so um not that I'm doing or involved in but I do know people are working on I think it is a really cool problem I like it is clearly an important one yeah so I was curious and I know uh a lot me we were kind of discussing different types of embodiments like different types of systems um and I'm curious more kind of like the AMR small Sy you guys have kind of developed and also thoughts perspectives around like we're seeing a lot with different types of human noise and different types and then there's spots there's uh embodiments and I'm curious on kind of from Hello robot the stretch three and also your thoughts on your systems and I know it's depending on different cases right so I think so the question is basically for my thoughts on different types of embodiments like these this robot versus like humanoids and dogs and so on so I think that because I I am fairly convinced that the Bas based on these dextrous teleoperation videos that are shared like I don't think that the limit factor is the embodiment right I think that the thing stopping a robot from being in your home is not that we need a cooler robot I think it's that we need AI to work and that that is that means that what we really need is robots that you can deploy and test a lot of interesting stuff with and that's part of why I I'm a fan of this particular platform because it's like relatively lowc cost lots of people have it which means that I hope that we can share more code and models and data from the robots and that means that we can do lots more experiments and really that's what's going to like data in the real world is in my opinion very large part of what's going to solve this going forward um but yeah of course all those platforms do have huge advantages in certain ways like I think everything is suitable for for different things also well better and second is if it's not the case like yeah prob yeah so that's a that's a really good question so the order and so the order in between Sim and real is was exactly the same actually in terms of like which team did well like the ranking ex that didn't change which uh so it sounds like I should probably say that simulation is good enough but I actually gonna say that no it's not and I thought that that was just that was just chance personally um I think that I think we really do need lots of real world evaluation to drive these kinds of things forward uh the simulation in particular there's a lot of shortcomings of simulations they're good at different things in my opinion so the real world is still irreplaceable in my opinion but you do need simulations yeah why why uh so so cost and robustness are big things right so like if the robot's moving around so the nice thing about that gripper it is Fairly reliable and compliant I don't think that I think that a two-finger gripper can do tons of different tasks like U shiron was just showing all the videos of of uh of two finger grippers doing things I think two finger grippers work well they're also mechanically simpler they break less right there's a there it's lightweight it's yeah things like that yeah yeah so the question is about whether or not like we should just be using uh human hands because we can just like take human data I think this is a much longer disc discussion that I feel like I probably don't have time for but but the uh but but I think I think that's an interesting point but I actually think you can do this with two finger grippers too for a lot of different tasks I think there's some good work from from CMU that I'm not involved in but that that uh that basically track human motions through through the environment and is able to use those to train robot policies on these robots and other robots so I think I think this is a good point I don't think that it means that you have to use human hands personally yeah so there was aine in framework have these different stages soltion but I notic that you kind things down traditional receive and then plan how do see like that comparing to what people now so I think that this question oh sorry so the question is uh so we had this very modular stack with like sort of a pipeline set of skills where some were RL and by the way the ones that were RL were either RL or fistic there were some that did not that RL training in the simulator was not really feasible for various reasons those were just yeah but um so the question is uh so so we had this fairly fairly um traditional robotic stack where perceive and then act right and so the reason that we the reason that we went with that partly is because we really were not able to train things end to end and get these kinds of generalization on these tasks yet I think one thing that you can do if you do this division is that you can train models that do just that do just the perception part based on web web scale data and that's that's what these open vocabulary object detectors can do really well so I can take an open open vocabulary detector like um alv or dedic or um Lang Sam or whatever it is and I can run that and that and that that is kind of its own problem and it works really well and then I and then and then like I can use that the output of those to parameterize skills Downstream and that makes everything more robust I I think that in the long run it seems like end should be the right solution because you don't have to have all these different components but um but I am of the opinion that we need to get this data flywheel turning and get robots out there and that means getting robots to work whatever it takes if we do have one more question that has been really sh safy I think you are the only one to talk about safy and in all the simulation we we've seen today uh we saw a lot of HS with no hum even if the purpose is to help hum so I Wasing so uh to be fair e nothing e say something uh testing can you hear me from zoom and then one not showing there I don't if you have to mirror Miss great all right so and then this is for rest of the room hey everyone uh I want to welcome Eric Jang who's a former colleague from Google n1x uh at Google uh you always count on Eric to have a very thoughtful approach to problems and he's also the author of uh the recent book um AI is good for you so uh please take it away great thank you and thanks everyone for coming here I'm really excited to see such an interested audience um so today I'm going to talk about a new effort we're doing at 1X called the world modeling challenge and um this is this is a fresh content so you're the first to hear it um so a little bit about me I've done uh you know prior to joining 1X in 2022 I did a bunch of research on visual robotic manipulation at Google here are some of the projects I've worked on um and basically I'm a big believer in endtoend Vision to action policies for robots um uh sort of after doing this work I came to the realization that the uh bottleneck in my view was actually Hardware not the algorithms so I kind of wanted to work on humanoids after that um a bit about 1X so it's a humanoid robot company founded in 2015 in Norway and as of today we're actually dual headquartered in both Moss Norway as well as sunny bille California um the company is about 120 people and the AI team uh that I lead in Sunny bill is about 15 people so a bit about the AI stack as as we have it today we're always iterating on it but the current instantiation is this kind of decomposition of a highlevel planner with a low-level um n 10 policy so you can imagine if you want to the the robot to go over to the window and open the window you'll have this um text prompt that says go to the window it triggers a particular model that is responsible for daging to the window um then you might switch the task and say like open the window and then triggers maybe the same neural net or a different neural net to open the window and so um we do uh low-level dextrous planning with just n10 methods that just do imitation learning um they predict 10 Herz actions from raw sensor data and the highle stuff currently is uh sort of human driven where that the humans just enunciates the the task into the into the like a smartphone app but we expect that we should be able to automate this quite soon with the VM are you sharing your screen yeah okay so oh oh yeah sorry I think a zoom Shar yeah okay uh can the folks on Zoom see this great uh yes awesome so um just to give a sense of like what this is able to accomplish here's a recent video that we put out uh on on the 1X um you know social media channels show showcasing basically this architecture of using voice commands to chain a bunch of tasks U to do to do things autonomously everything you see in this one take video is basically autonomously controlled by a neural network except for the task switching part so a human is basically dictating U remotely which task it should do and um again it's not a huge leap of imagination that with the progress in VMS we should be able to um have the data collection on on you know what humans are seeing and saying and it be part of the imitation learning process as well so I'll just let this uh video complete um basically you know there's a lot of kind of contact Bridge tasks that we can do like wiping tables um picking things off the floor pushing in chairs um the the the Bots are pretty strong um opening doors and and so forth and on the bottom right you see the text commands that we're actually sending to the the robot in this video some cups were harmed in the making of this video cool so the video ends basically with a door opening task and um so that's that's kind of like the what we we're able to do with our current autonomy stack so um let me move on and um I'm going to actually motivate the focus of this talk not by what we're doing on on autonomy but actually a strange experiment that we ran into recently so uh for people who are in NLP and and vision it's very common to have this kind of expectation of a scaling law where if you add compute you scale up the parameters in your Transformer or you add um more data um this basically almost guarantees performance Improvement if you're kind of doing the right scaling and Hyper parameter search so um for example let's take one of our dextrous low-level models that we're training uh in in the previous video and let's just try scaling it up right so so what what what do we expect to see well if this tracks things in computer vision and NLP what we should expect to see is that as we go from like a 10 million parameter model to 100 to a billion to 10 billion we should see this Progressive decrease in training loss for for the model indicating that we're fitting the data better and better so what do we actually see this is just like an experiment we ran about a month ago we see something like this where um the losses all kind of go to the same place which is extremely disappointing right so um maybe to people who are in LMS this is actually really odd but a lot of people in robotics frequently run into this problem and and these are often negative results that are never published so I was trying to look on uh you know the literature for like instances where this happens and I'm pretty sure like every robotics you know imitation loaring person has run into this in their tensor board or W DB at some point but um it's shockingly like rare to see this kind of published so what's going on here right well um I dug up a few results from from the literature um suggesting that you know sometimes more parameters is not always better so there's a few works from the rail lab um at Berkeley showing recently that for example in the the Octo paper the 93 million parameter ooto model outperforms the 55 billion parameter rt2 X model um similarly the Suzie model which is like an image gold condition paper also does that with a um you know much smaller model and then um recently there's this open da paper that also gets you know outperforms r2x with a 7 billion parameter model um similarly there's other papers that show that more data is not always better so the Droid data set recently um also shows that you know despite being a smaller data set in in total size it also outperforms training on the uh compared to the oxc data set which was the paper that kind of got co-released with rp2x um and then another paper from CMU shows that in in terms of foundation model pre-training you don't necessarily always get automatically better performance when you pre-train on a foundation model that was trained on a lot more data so turns out that image net is actually a surprisingly strong Baseline compared to things like um ego 4D and so forth so there these weird negative results in robotic scaling that kind of defy the intuition of where scaling should go which is very concerning because a lot of uh you know people who are trying to solve general purpose robotics want to actually scale things up and if things don't just get better automatically then it's pretty risky to do that so here's a few hypotheses on why this might be happening the first reason and I think this is a significant one is just that real world evaluation is just fundamentally very hard to reproduce many robotics Labs cannot reproduce other robotics Labs work so it's just very hard to measure the effect of scaling there's also a data freshness compounder um this one is particularly scary to me which is that if you let's say collect data on Sunday you train a 100 million parameter model on Monday you evaluate the model on uh on Monday as well and then you train you know in the meantime you're training 1 billion parameter model on Friday you evaluate the 1 billion parameter model in the real world it's going to be almost automatically worse just because it's Friday and Friday is like far away from Sunday when you collect the the data and the world has changed so this uh this bias in evaluation is very very pernicious in pretty much every robot learning paper and if you're evaluating your your your tasks on like different days there's already going to be this like default regression in performance another one that's you know potentially suspicious is that if we look at LMS and um VMS they're often trained with a entropy loss whether it's some sort of mass token prediction or next token prediction whereas a lot of Robotics models tend to regress some sort of geometric thing like you know mean square errror of a Sixto pose um or some sort of loss like that so maybe there's some something going on there with with some some scaling bottleneck um relatedly maybe there's a lot of bottlenecks and Robotics that restrict parameter efficiency so maybe you have this cute idea to improve the sample efficiency of your robot model and it helps at some scale but as you increase the scale of data Maybe it actually has worse gradient flow compared to just a big Transformer and then therefore maybe your earlier layers suffer more if you're scaling up the the parameter size from from this B um one example of this is that you know pre- Transformer in NLP and vision it was actually very common to see this effect where scaling up the model didn't automatically make things better and this was the case with a lot of r&m based architectures um another way that for example you might see the B happen is that with an msse model um it basically regresses one mode so potentially if let's say the other parameters that you're adding to the model would let you learn more modes the MSC loss is kind of preventing you from learning the modes even as you're adding parameters so there are these ways in which maybe these architectural decisions in robotics can actually U prevent your ability to get improved performance as you scale things up um another hypothesis is that you know a lot of these baselines showing that smaller models or smaller data working better is that maybe the rt2 X uh 55 billion model was just very under trained with respect to the actual test domain that was evaluated on because it's it's a very big um you know U very big model with big data set and maybe similarly there's there's kind of suboptimal data cleaning in the uh in the oxe data set as well um so when people talk about like how does scaling laws work in robotics I guess it's also sort of unknown how we count data in robotics um in in language modeling we just count tokens but um it's not clear what the equiv of of a token is in robotics is it just like per per demonstration is it the um is it like the sort of log base two of your action space and then times the the number of seconds of actions you have do you count the image tokens um these kind of questions are still unanswered in robotic scale and and so it's maybe very possible that like robotic data is sort of overcounted in terms of its data size because maybe the the sort of interestingness of the data is not as compressed compared to entropy which sort of encapsulates all of human knowledge sorry compared to a language which Cap encapsulates all human knowledge and um another way to express is that maybe the robotic task itself is not hard enough to require that many parameters in data so if the effective data size of Robotics data sets are actually really really small then you don't actually want to have that many parameters and that much um that much compute and and so let's talk a little bit about tasks so so what what is a task in in robotics um oh I'm sorry uh let me let me talk a little bit about uh evaluation first so um in robotics if you're running evaluations in the real world getting statistical certainty on your valuation is very tough because the number of Trials you have to run to estimate something like a binomial success rate is very large so U the standard area of a of a binomial um variable is is basically square root of the um the the mean times you know one minus mean divided by the number of samples so if you want to get within let's say 10% of your your you know true success rate you have to take like 300 samples and then similarly if you want to go to within 3% you have to take 3,000 samples and and this this is very bad because it goes up quadratically since of because of the uh square root term here so um evaluating how good a robot model is if you don't know anything about the black box that is the policy is extremely expensive in fact all robotics papers today don't really evaluate with the level of rigor needed to actually know for certain whether an idea is good or not and this is further confounded by the fact that like if you have um you know you're trying to get 3,000 um trials you're not going to really be able to evaluate this in one day and so then when you evaluate something the next day you have to then deal with the bias of of the environment changing so evaluation is a very tough problem okay back to tasks um why do language models benefit from scaling laws well you can think about a language model as actually not just modeling the next token which it is but um what what it's really doing is jointly predicting many many tasks at the same time so if your context window in a language model or Transformer is like 2048 you're effectively on every single forward and backward pass predicting 20 48 different tasks for which each token that you're predicting is can be its own unique task let me give a specific example here suppose you're trying to model like a mystery novel and then the novel kind of has a token sequence that begins like you know that the novel is beginning and then person a does this and then person B does that person C does this a says this B saw c c c says to B so you know just a very standard kind of uh linear narrative in in a book um let's suppose we're constructing a llm task where we're just predicting only a behaving so we're only going to care about what person a does in the book and we're going to learn this probability distribution P of whatever a is doing given what a previously did and then also condition on all the data that you see in in the Noel right so um what you end up learning here is that uh you you can get information you only improve your performance on what a is doing based on A's interaction with other uh characters and and uh relevant data in the in the Corpus um if you get some data from let's say B interacting with C it's very unlikely that it helps you actually improve your predictions of what a is going to do next and so um if you think about the Corpus as being you know all kinds of characters doing all sorts of things any given random data element is actually going to not really help you predict what a is going to do next conversely if you predict everything together then basically any data element you're predicting is going to help some task somewhere in the data set so so there's this nice design of llms where almost all data helps some task in one way or the other because you're trying to predict every token and and I think that's why we see very slow data scaling when it comes to just um scaling up action data in robotics is that if you are um only trying to predict scaling with respect to the prediction of one variable then most of the data in your Corpus is actually not going to help you learn that variable because it's uh it's only useful for learning the sort of a conditional Distribution on other data points so we can basically rewrite this this language analogy directly into the language of robot actions so imagine instead of character a we just say a is the actions and the behavior of all other characters is all the other non-action tokens in your sequence right so so let's consider this very simplified control example where you have an image and you're trying to make decisions from the image the image is tokenized into a 4x4 set of tokens so you have like 16 Vision tokens they go into your policy you generate one action token they go into your world model you get your next state right um if you're just trying to predict the actions then seeing seeing data points from x0 and X1 the relationship between two those two variables may not actually give you that much information on how you um how you improve your predictions of a Zer so let's try predicting everything that there is to know just like in llms right so if we flatten the data into a onedimensional sequence and we just predict this language model style then we have basically this the sequence that goes from all your image patch tokens to your action and then you repeat this for every step in your episode um to to use like a Yan laon cake analogy basically most of the data in your data set is cake very little data is the sort of supervised action and even less data is the final terminal success criteria so so let's just ignore the reward for now because let's consider simple imitation learning case but if you're trying to learn to predict the frosting most of the data in the cake is not really going to help you to do that if you're trying to predict everything in the from the cake to frosting to the Cherry then any arbitrary random piece of data that you sample is like the going to help you predict something which is why I believe we see better scaling laws in in um in VMS and llms because we're fundamentally trying to predict every quantity and so so the the sheer number of tasks allows you to see this kind of benefit of adding incremental amounts of data um so I'm going to then like you know start to motivate why we might want to do World models uh where predict everything for robotics but first to set a precedent there's already quite a few Works U where we're seeing like AV companies doing this um you know wave is pioneering quite a lot of this sort of photo um image generation where they can even try to evaluate and check counterfactuals in their in their sort of world model um Wabi has done some interesting work in in a sort of different space and then recently Kama had a tweet from their CTO saying that they're training their um their policies now and on policy with with a learned World model so there's definitely now this this idea in the air that like maybe it's time to actually apply World models to self-driving cars so basically I'm going to uh it's uh it's already public as of today the um 1X World model Challenge and so this is basically our attempt to focus the community on an interesting problem that we think is now ready for general purpose robotics um it compress it comprises of three different um sub challenges the compression challenge sampling Challenge and the evaluation challenge which I'll describe now um so why why are we hosting a challenge there's already a lot of like interesting benchmarks for general purpose robotics um but I I think I have a pretty different angle on it that's kind of not satisfied by current evaluation benchmarks so we think that video generation models have improved enough that it soon may be possible to actually do end to-end hallucinated evaluation and potentially even training of millions of different tasks in a learned World model so no more painstaking simulation engineering you don't have to go and measure the spring constant of your door and then imp from that on your door you don't have to figure out how to do deformable cloth simulation you don't have to figure out how to simulate people maybe it's going to be possible quite soon that we just put this all into a Sor type model and then be able to just roll out these uh learned hallucinations to evaluate our robot policies and this would basically solve the general purpose robotic evaluation problem if we're able to definitively measure and compare policies in the simulator then you don't have to worry about resets and you know going from Friday to Monday and having the environment regress so we think that if this evaluation problem is solved it would greatly accelerate the progress of general purpose uh humanoid Robotics and be able to get these into the home very quickly um one more thing that I believe is that the set of people who are qualified to help is now much larger than the set of academic researchers so we are less interested in a benchmark that is published in papers and more interested in a sort of public U prize competition um and and we're very much inspired by efforts like the scroll prize and and kabq so so this is why we're doing it we think it's very possible that in the near future we can do we can solve the general purpose evaluation problem with training on real data um and why is this evaluation problem important well I think in order to see scaling in robotics you need to have many many tasks I I don't think it's sufficient to have a benchmark that has like 50 manipulation tasks and then you try to use that to estimate what techniques to use because of the sheer number of tasks needed to measure a scaling law with respect to data um at least that's the hypothesis then it seems like in order to actually see a effect from scaling you actually need to evaluate a million of tasks so if we're also going to make a big investment in data collection and compute as we are in 1X then we need a predictable way to see how uh you know money spent correlates with the capability increase it's very scary to not know that and so we're trying to figure this out um and and finally if there is a learning algorithm that um has a scaling bottleneck it's very useful to know this because um if it if it cannot get better predictably by scaling up compute and data it will never be candidate for large scale journalist policies at least of the kind of investment span that you see that like you know big players like um Google deepmind and and open the I do when they're making their generalist models so a little bit about the challenge um it's still a work in progress we're trying to iterate on the community with the community on this because it's it's hard to get the metric right exactly the first time but I'll give a high level um description of like what the core tasks in the challenge are so you can already see the uh the challenge on on this GitHub page here uh basically we're releasing a data set of 100 hours of tokenized images from EES collecting many tasks in in our offices so this is like pretty much every kind of manipulation task that you can think of um with a lot of like deformable object manipulation by manual manipulation pretty much everything um the tokenized images are 20 x 20 with a codebook size of 1,000 we think we can actually make this a bit more compressed but we were trying to trade off the um the sort of sem making sure we're not discarding too many semantics of of the token ier and making sure that it kind of is a somewhat General um image patch representation although in the future we we might look into kind of releasing maybe a different data set with a more aggressive tokenizer we're also releasing the raw actions um basically the joint angles the wheel velocities and the gripper um the gripper open and close commands for the E humanoid so if you're interested in conditioning on the actions that that's available to you um the the first task for compression is basically centered around predicting the next token however from talking to the folks um you know shout out to yes for kind of giving us a lot of feedback on how to make this work well we don't want to have a benchmark where it's just predicting the next token because that only admits one kind of solution of basically an autor regressive LM so so we're trying to make this challenge basically admit a slightly larger set of models that you can train so The Benchmark is to basically try to minimize the cross entropy loss on all of the loits of your next frame or your next uh latent frame of of um tokens um we've released two code implementations in the codebase which is a simple autor regressive LM as well as a genie implementation which is a recent um Google dine paper and uh again this was largely inspired by the comma VQ compression challenge the only difference being that in our compression objective we're measuring the cross entrop on your entire frames worth of logit okay so for the second challenge um uh for the Second Challenge oh yeah sorry the prize is going to be a $10,000 prize for the uh first uh contestant that hits a certain Milestone um we are because you know Stakes are kind of high we want to make sure that the prizes like both achievable but not trivial and also not impossible so we're still figuring out exactly what the right um you know score should be but um at least the code is kind of available for people to start working on it and then we'll we'll shortly release the uh specific criteria um so when you cast everything to an LM Baseline and you implement things correctly predictably you do see that scaling laws work so here's our basically our tokenized data and when we train a llama Style model on the data uh you know printing all the tokens and all the actions we do actually see scale which is very promising so you know here you can see the sort of um the the the flops versus the train loss and if you fit the lowest um the lowest you know minimum for for the curve for each flop you can kind of get the optimal um compute uh profile for versus parameter size okay the Second Challenge is a sampling challenge so you know we don't want everyone to just work on only um you know discret distribution modeling we think that there's a much larger family of interesting generative models that could solve the ultimate evaluation problem for example like latent diffusion methods um video generation models and and so forth so the Second Challenge is um basically to generate the best um the best quality latent in the future um and and we're going to use like a perceptual similarity metric to score against the ground Truth versus the the next frame that you're sampling um and and and the goal of this is basically to kind of encourage more interesting work on test time search so for example approaches like mask G can improve your prediction if you spend some time doing compute at test time as well as diffusion and potentially even like this kind of new age of MCTS might be able to give you better results at test time um and we see that the kind of things that are playing out in LM land will probably transfer really well to this again the prize here is going to be $10,000 but uh we're going to have to figure out exactly what is the middle ground between impossible and triv um we uh you know we're finding that next word l are too slow to run in real time for our robotic systems even at the high level so we provide an open source implementation of the genie architecture which is essentially this Transformer that alternates between your temporal and your spatial Dimensions um yeah it turns out actually that the uh qk Norm is really important for this architecture um that took us a few weeks to to get right but um yeah this this architecture is kind of our our best Baseline right now for the challenge um here's some examples of our model this was trained just for like a couple days so I don't mean to say that like this is the best 1X can do um it's more like here is a simple Baseline here's kind of how bad it is we can definitely do a lot better but the starting code lets you produce something like this okay and the final challenge is the evaluation one which I believe is the Holy Grail of Robotics so um you know we think that in in a short period of time it might be actually possible to do evaluation in in learn simulators so the way we're structuring the evaluation challenge is that we'll give you 10 um blackbox checkpoints for some task and we would like you to basically find some way to predict the the uh best to worst ranking across these checkpoints and this will likely incorporate techniques from both one and two although you can imagine there's actually even more broad ways to solve the problem such as using an off policy value function or something and the prize and criteria also to be determined because we want to kind of get some confidence of what is achievable and what is impossible uh but I can say that it will be a much bigger prize than the previous two um basically the structure of the problems that we give you the mdp and then you have to score how good it is at the end of the r out here's some numbers on our existing leaderboard and um let me just close with a few open questions so um you know it's still uncertain to us whether we should be scaling policies or World models right like just because we see better scaling on um on image tokens does not actually mean that's the thing you should be scaling up if the at the end of the day what you're trying to make is general purpose home robot that can do lots of tax tasks and chores for people um that's why we're actually trying to bet hard on this evaluation setup where there are millions of tasks so that even if it is the case that you only want to scale up the policies at least have the millions of tasks in a learned World model that we can uh measure scaling with respect to um you know Yan leun is uh well known for saying that predicting future images is kind of a waste of time and maybe that's true like the future does contain a lot of irreducible entropy so maybe those parameters that you spend and all that like GPU time you spend is better spent compressing something that's a bit more efficient with regards to like long horizonal reason and then finally there's a question of like okay if you scale things up and everything works uh what happens if it's 100 billion parameters how do you run it effectively on the real world um we'd love to have some feedback on how to extend the challenges so we've set up a Discord um if you have suggestions on what kinds of data we can add to our Fleet we'd love to help you do this um you know if you want safety critical tasks like carrying hot cups of coffee or something like you know pseudo conscious like dressing yourself in a mirror um like please suggest that and we're happy to add that um think of us us as you know your data operations team and we'd love to also have donors help us increase the Bounty and finally you can do this internally at 1X we're also hiring um so yeah um here's the uh information on GitHub and Discord for if you want to talk to us about about the challenge thank you very much all right I think we have time for just one question because we are at time um anyone got one e e e oh because the people that are remote yes yes pointing in there right was that this camera looks at them yes yeah yeah yeah and then Brian I think I yes I'll pleas down yep I'm here I'll use my laptop for question okay so then guess I don't need oh yeah no this should that's all right should we just shift I'm okay yeah right oh my God this is exciting so much to yeah that's as far as it goes okay uh thank you for being here all of you uh hi everyone so we are going to start this the speakers panel uh in order to save time I will skip the introduction they have all uh been talking uh today plus uh think they are you already know all of them um so let's see this is how we're going to do this today I will start for with a few questions hopefully we get the discussion and conversation going and then we can take a a question from the audience yeah we have an hour so plenty of questions please please please start thinking about them um I would like to start the the panel by by asking you uh this uh thought question um what is um an opinion that you think is commonly held by your by our community and embody theyi but that you think we should rethink about or we should at least explore in different ways I'm sure that will open open for some questions any order um fre I can do um I think there's a lot of excellent work in modular architectures in m and lots of us have built them and lots of us are building them and for those of you who heard my talk this morning it's no surprise I'm going to say I think we should rethink this completely I think we should have simple architectures trained into end and I think I really think that's the uh answer to build uh generalizable robs and and I'll have some random stuff just to follow out you mean end to truly end to end or modular by perception and action truly andto I mean the if you want to bring in outside word knowledge like for example you want to train a robot and you wanted to learn a you know say cook me uh you know make me a turkey sandwich you know this outside world knowledge of you know what goes in you're supposed to put mayo and you're supposed to put mustard or whatever I don't think this needs to be learned end to end inside the parameters of the embo a model and you should be able to leverage a language model for this but in terms of uh performing short Horizon tasks um I think the I think the true answer that you know we will all learn in the bitter way just like other AI communities have learned is everything's going to be a clean end to end model thank another another volunteer for a commonly help opinion that you would like like to invite us to think about again that's right there the L of the cable definitely um okay so I guess I'm representing sort of like um somewhat embodied by humans because we you know went into prodct variety so it's the human who's got the body um the humans you know already very good at interacting with the with the world and I guess we've already moved on from like fine grain and infation because humans just do it very well already to some extent there's you know I think a big question about why we're so interested in robots that do humans jobs um I think that's something that we should all question pretty severely versus assistance for humans um who would like to be sort of more performative or in other ways uh you know brought back from maybe injuries and that sort of thing I think the obsession with obtaining sort of human level humanoid like robots is is one that I think the whole Community should think about given the uh both the challenges that apparently are solved by end to-end learning at the fine grain when actually the real challenges of sort of like time scales of days weeks months years of Liv that actually then lead to the context it's necessary to actually do the right action it's not get started in robotics so from a point of view of like let's just assume a second that the fine manipulation problem is actually solved because actually humans can do it very well um the real challenges are going to come when we're looking at data sets that absolutely cannot be solved by endtoend learning you cannot get a year-long data set and do anything with it it can't fit in a data center from an egocentric perspective so I think if you think about it that way around as in okay fine tomorrow colleagues here to solve entend learning for fine gr manipulation now what and it's interesting to you know ask so what are you working on if you're working on that fine grain manipulation problem well humans do it all really very very well it's not a problem that actually needs to be solved as much there some Fringe areas where it be great very very dangerous areas where you don't want humans to be whereas actually dealing with context in AI in the embodied as in I'm an embodied human is actually one where we have no Solutions and you know sure if you could solve it for humans maybe it might good for robots too but I'd say let's think about the human Centric view of embodied AI more than we currently are what I'm just curious what do you envision as as the application of the so in context AI as opposed to like use the big models for learning for robotics manipulation like what do you envision the application to be for or assistance or like what what do we do when you when we solve your problem well yeah my my problem is essentially the same problem that you have when interacting in normal society um where whether it's cognitive or memory related or otherwise enhancement and regaining things like vision for those who may you not have the best Vision or hearing or otherwise uh in general you know everybody you start to move away in particular from sort of a demographic that deals with social media in particular of the current sort and says well hang on what are our applications as people age the answer is actually um you really want to get back functions that you already had just fine with your normal human memory and cognition so again it's about sort of like looking at reality uh you know look at your family and friends around you look at people who need help look at how teams of people need help with each other and talk about that embodied artificial intelligence problem um I think it's interesting because otherwise I think there's a there's a dilemma for embodied AI which is to scratch at the surface of what are really very small problems in the scheme of things even though they're hard technical challenges like find gr manipulation they're small in the scheme of things and actually you know the big problems are really at a very human level of interaction with each other and then at societal scales of AI so um yeah I'm interested to hear from colleagues who of course are solving these very technically challenging our brain interaction problems but I also want to understand what your motivations are Beyond it being a technical challenge I'll take B so would you argue that you know you know I would argue that you know as my parents age I would think that having a robot that would help them with the dishes would be something that the robot would you know be augmenting their lives but still be independently performing the tasks and so you still need to solve the lowle manipulation it's not really human collaboration it's human ey collaboration on a longer scale or you know well they are collaborating to live together but the tasks themselves don't require fine gr collaborations I I would I would give that as a counter example on why lowlevel manipulation is important yeah I'm also pretty much in favor with the lowlevel manipulation thing I think we work a lot with uh folks who have disabilities or uh aging aging people and I think a lot of them do actually want these robots to be able to help out around the home and things like that like that's that's a huge huge issue right even if something as simple as being able to pick something up off the floor is not trivial right um yeah I think if you like for you know if you were not able-bodied and you wanted someone to like swap the mic and pull it up talking to an AI about like human values it's noton um yeah and yet here we are so many humans able to help each other out in these areas so um thank you by the way human who's next to me and I hope you don't get replaced by a robot anytime soon um so I think that the challenge is not that it's not interesting and important I just mean if you if you look at the whole Space of problems for embodied AI given this is this Workshop I I do think it's interesting I'm from a robotics background myself um why we're so interested in the particular form of embodiment when actually humans are really like quite Exquisite embodiment even when they're not fully abled in that sense um what I'm saying is not that the problems aren't you know interesting or hard or they won't be useful I'm saying that just assume that you've soled the F gr manipulation problem so now what and I'm saying that there's a lot of fertile ground there where you didn't have to solve the fine gra manipulation problem to be tackling some incredibly hard problems that will also be useful not just for the robot but also for humans who you know are embodied via you know augmented reality and various sort of wearable devices well we we definitely got the thinking going here I will I will still we will get back to this definitely uh but I I still want to have some answers for the for for a commonly heal opinion that you would like to challenge and I'll pass it to Brian that we have online uh to give him a chance he has a less of a chance to to just keep the take the the mics right so take I mean I'm happy to let you guys uh duke it out about whether fine grain manipulation matters at all so uh um I don't know I think maybe I'll say one that I'm not sure that I like uh quite quite believe in but I I'm curious to hear people's thoughts um I feel like in the language model and Foundation model Community people have gotten extremely far with no planning um or real like deep algorithms beyond the like the main learned model uh as someone who's like most of my career has been around planning it makes me uh like kind of question to what degree do these things actually matter I mean there's a few like fundamental problems where you can point to and say yes like clearly planning is necessary here but then when we see this in action it it it makes me wonder to what degree it actually is because most things seem to be uh done pretty well without it can I add to that so I think that this this is a sort of a a belief that I'm trying to figure out how to articulate more formally but I believe that there's sort of notion of like a cosmic greediness to the um to the uh environment that we we live in where an extremely greedy policy due to the due to the very high dimensional nature of the environments we live in which is not 3D but it's really like a a high dimensional configuration space a greedy policy can probably get really really far with uh doing very useful tasks for example like if you want to tidy a home it might be possible to achieve this with like an extremely OCD uh greedy policy as opposed to like a long Horizon planner that knows where to get all the appliances and so forth and I think that like we we see this kind of in in deep learning optimization where optimizing in high dimensional spaces uh using first order optimizers works really well to find some good look it it might also be the case that like in in the sort of high dimensional unstructured world we live in a greedy Optimizer might also have similar benefits where you'll get a tidy home but not necessarily like the optimal tidy home yeah I think that's reasonable it makes me I guess if I look at myself as a as a human I feel like I probably am very of in this greedy local minimum and but if I then project it onto robotics it feels like maybe it's enough for robotics to not do it well I think so personally I guess I'll be the one who somewhat defends planning here but like I think I think that uh I do believe agree with everyone here that I think anend is the right way to go in the long run but I feel like you can think about planning as a way of building in um better generalization and data scaling and things like that and in particular if we think of robotics say as uh I think it was uh as instead of requiring like 50 to 80% success reliability as requiring like 99.9% reliability or something like that if you actually do have that robot in your home and we're not we're not in Richard's here where we don't need this at all right then you need to be able to actually make sure that you can arrive at a solution so how can we do that because currently I think that we see llms and stuff that work really well with a human in Loop and now I think that assistive robots are a really important use case and for a very long time we won't need robots to be 9% but if that is the goal then we need an answer that is not uh what we have now I think I that's um I'll defend planic as well just as a a class of of um solution that you know gets the job done if you need it um but I like your point about uh interaction with humans and closing the loop um there's something very important I think that's been found where we moved from assuming that you give uh some form of AI embodied or otherwise uh a task and then sort of it spit out an optimal solution versus humans interact and get there and possibly even construct what the goal is in a more precise way as they go about doing it um I think this again is another one of these examples where you know looking at at the problems of embodied AI includes the class of interacting agents that you know are humans first like if you go and talk to you know Behavioral econom ists or social scientists and ask them you know how is it humans are getting unsolved this task um it won't be that they're forming some formal planning stage but in the end the abstraction is that a plan got formed and got executed it's just that's not how they'd articulate it it got done through interaction and then you know sometimes there's emergence as well but um yeah it's interesting to sort of like if we forget about interaction between even if it's just a robot and the user of the robot um I think it gets even more interesting to talk about planning at the scale of like you know entire workforces you know what is the plan to enable that giant group of manipulators that might be thousands of people uh to actually get the job done it's not so trivial to think of it in classical planning terms uh but it is interesting talking in terms of like interaction between the agents I think the longer Horizon the task so for example at some point I need to go for dinner and then I need to go home I think this has to be done with some sort of planning but I would say medium range you know after this panel you know probably going to go out go out of this room and so I have to open the door and make sure I don't run into one of you here I don't think we should be I I don't think the answer lies in explicit planning I know this controversial uh but I if I had to bet money on it I would say the answer would would be without value I don't know well I'll take it away to to another topic Let's uh let's cover a few topics here um let's see I I want to bring you back to the definition of embodied AI as a research field with research questions and goals and U yeah sorry there's a question I really wanted to ask you mind if ium so what would convince you other wasse like is there an experimental result that you would see that would be like oh shit I was wrong uh we got to do planning I don't know that's a hard one to answer I guess so what would convince me that planning is the answer well I guess if it you know I guess the ultimate answer is if if it if it works in the real world when you know you know for complex tasks where people are moving around and things change dynamically I guess you know well if it works then it's solution that's acceptable so what would convince you is the planet people get to it first sorry like the planet people get to it first and then they're right yeah I think I think it h I think yeah I think everything that you know we've seen in uh you know yesterday there was a wonderful talk uh the parable of the Bara it's talking about how you know we built all these parsers in natural language and not we but you know someone buil it and then eventually you know all of that got you know you know all of that got consumed by N2 models uh and you know there was an argument yesterday saying that you know object detection semantic segmentation all is just another form of parsing which you don't really need all you care about is you know you have some task that you want a visual system to do like assist a human being and answers questions about you know maybe there's a blind person you want to answer a question like doesn't matter if you have object I think this this is in that ve is how so that's why so uh aren't we already bringing back planning into llms though with the MCTS stuff that we're seeing over the last like week or so like I said I'm I'm betting money on it but I'm not betting a lot of money we have somebody jumping in from the audience so let's let's get it going like to P back that like oh you know these LM are doing so great without planning why do we need planning no they're not you guys have you guys ever used an LM to try to like produce like something you wouldn't be embarrassed to say by email like I don't think they are working well I think they could use a bit more planning personally like to like look up sources or or or stop hallucinating I know Hallucination is sort of in the eye ofer but uh I I have not I I tried once a month or so to get some kind of out from L never what so I do agree with you and I think I said that is you know as you go longer Horizon I think uh planning can help but I think we are currently using planning for all kinds of short Horizon tasks and that's what I'm betting my money is I think you would those those methods would probably be you know consumed by uh an end to endend model that does not do explicit planning uh I know one of you is going to ask me where's the boundary uh I'm not going to take bake there because I really don't know but but short of the Horizon uh less the probability that planning is the answer what generalizes better today neither I mean I don't think anything generalizes as a comparison just as a comparison I mean you're comparing like you know forget all the benchmarks we have in the three experiments that we run for a paper and you're comparing like 3% versus 2 per it's all it's all noise well I'm sorry but if you paid attention to my talk you would see that the planning B the planning the planning enty got like four times the learning based one no I don't think that's really that just for the record I'm completely baffled I must be from like a two to older school I so planning used to define a a thing where if you had a goal and then you'd like find so so what does it mean to have this conversation right I mean you can Implement A system that effectively does planning in several different ways and it maybe it is contentious as to whether or not you can implement it with an auto regressive model um I think it might depend a lot on the Dynamics of the problem as to whether or not it's closed or chaotic or or whatever um without any kind of feedback but you know then there is feedback that's the point with llms right so an llm that has a human in the loop is no longer just a forward prediction system and agentic implemented llm where you know it can do a few loops and keep hold of what was asked at the beginning without it corrupting the context window um is no longer a trivial implementation of an autor regressive model um and all these are fine because it's a class of problem with a class of solution um so sorry what what is the problem again with with planning clearly we need planning because it's a it's a it's a definition of I have a goal and I want to get there and then I can implement it in different ways maybe I can implement it model because it's such a trivial problem I mean I I do think when we're talking about planning sorry but we're talking about mostly about like whether or not search like particularly like iterating onest have this having a a system for iterating solution inut right automatically is is useful right I think that's fair and you can implement this with learning like with the MCTS LM stuff right agree I mean it's it's trivially useful the question probably is is it efficient but to your compute resources right if if I can then I'll allow my llm if that's what we're playing with as saying something that isn't planning and I'll have the distribution over it and I'll check where it gets against relative to the goal State you know well I think the bigger argument is usually not about the the cute resources but about the human resources it requires to put that algorithm together and whether or not you need to adapt it to different use cases and things like that which is where I think in um in like 2014 to 15 there's like the really cool guided policy search work from uh Charles Sergey where um they basically solved some um you know simple manipulation tasks by combining like a system one and system two thing um where you know at test time you have to actually well sorry the the sort of local policies are trained using a IQR like kind of planner based algorithm and then you distill that into a a type a system one thing that can do the test but today when people solve those specific tasks no one really uses a a type a system to thing anymore they just like imitate it directly from scratch and I think like what will sort of always happen is that um you know system one systems will kind of get better and then people will say like oh no like it's hit a wall you got to add a system two thing they'll add it it gets better but then like a couple years later the system one thing will then catch up and do the thing and and I think it'll just keep being that so so maybe let me rephrase it and maybe the thing that's changing is that the definition of what a goal is um which used to have to be defined in kind of quite precise terms is no longer needing to be defined in very precise terms you know for examp example coordinate frames of arms and manipulators interacting with contact and so then what we get to is actually the social construction of what a goal is for a human doing useful stuff and we get back to it's about interaction between the two agents or more because the definition of a goal is actually never clear for more sophisticated things you know for example even picking something up you know Once Upon a Time picking it up under an energy efficient trajectory picking it up with Precision well actually people were just try and at all to pick it up right and so then have all these various ways of formalizing it but actually now that we've got systems that are good enough in several different ways maybe through you know the various different robot morphologies to solve it um you know certainly lots of different morphologies can pick stuff up without lots of planning simply by suction for example you know with a very trivial kind of uh forward model or planner but is that what it is that we are you know what we really care about is getting away from having to precisely Define our goals and if that's true then actually that's because you're going to do interaction now continuous interaction or are we just talking about fine grain planning and that that's going away um maybe what I was referring to was not if when the goal is precise or the goal is ambiguous but I guess when the environment is extremely well specified or when we can assume perfect knowledge of you know like for example I think if you're working in a closed World setting you have to have this robot that Works in a factory and this you know this red block comes down at exactly the same location every time then I think you know like a what what I was refering to is planning you know a planner can probably work really well it'll be explainable it'll you know it'll do exactly what you code it up to do um but if you want some a robot to you know walk around cvpr and do uh something in a more unstructured environment where you do not have perfect perception then I think uh you know I don't think those methods are going to work very well but it's interesting that you you've you've you've you've uh it's interesting that you have um instead of thinking about the environment as underspecified you're thinking about the goal as underspecified and I think that's also uh valid I actually think I think we do see algorithm or planning algorithms where that would work in that situation we see plenty of them like these pomdp planners which which can handle crowds have robots moving through crowds and so on I think the question always come with planning and the the argument against planning doesn't come back to any particular capability it's come back comes back to how much effort it takes to implement that capability in a robust way and that the question seems mostly to still be open because we don't have systems that are 99% reliable andoil so for example do you expect um to be able to solve go without some sort of system that defines a state even if it's complete just using a forward model what would that what would that look like it can beat most people but not the best people so maybe the best people use this other thing which is you know trying to get towards a goal um be defined or otherwise I'm forced to to interrupt you to change the topic we have an A we have many questions he asked first Christian I think yes go ahead please so someone was said that experience will tell us right experiments will tell us what planning is need to following do we have the complexity or the scale in our experiments which can answer this very short example for example um episode length and environment cycle so let's take a very extremely simple task navigation moving object ination if I need to run around in Seattle and find something and I haven't seen Seattle I will not not necessary I just need to go roughly this way and I will just have local exploration combined with lots of resarch I really like what Eric said that planning can be seen in very many cases as some really policy high right but now let's take side if I'm living here I have SE this neighborhood for lots of time and I will do planning I will be much more efficient and planning will help but all our challenges in a we don't do that our episode size are limited 500 steps 500 steps or environments H3 obviously we don't need planning in but a robot is not fired up when it starts the episode the robot has operated for weeks around it should have a mental image of what it did for weeks and then that's my opinion M can be helpful and whether it's true or not that's what I think is we we didn't do the experiments yet to confirm to be honest I think uh maybe my controversial opinion which I didn't say earlier is that I think we generally build benchmarks so that learning algorithms work so it's not that surprising that learning based endend stuff always looks better than planning I think yeah you probably end up with very different things if you picked some random hard task and tried to tried to do it like like solutions for like the DARPA Grand Challenge or DARPA Urban challenge or DARPA um Subterranean challenge like those are big goal oriented challenges right and none of those use an Learning Systems so anyway we're at Vision conference so I vision question uh how much do you think of Robotics is currently held back by visual understanding of the geometry World Dynamics is it contact modeling and control or is it just like nebulous third thing you guys feeling about where where where do you think the like biggest open issue is so I can I can I can go first so this morning I presented some work which showed that you know it was not a very complex task object go navigation and we showed that this end to end learning with imitation learning gives you you know some 60% and then if you just solve Last Mile detection right if you assume Last Mile detection is solved so you don't know anything about the scene you don't know where the chairs are nothing but if I have to look for a bottle when I see a bottle I tell you the coordinates only when I see it right you suddenly get like a 30% Improvement and this was sort of a controlled study that was you know that we were using to answer the question if some part of detection is solved or some part of perception is solved can we get a big jump in the ability of the robot to navigate around and we saw this you know you can you you can you can see this in Sim and I think we've some preliminary things to suggest this in the real world so I would agree with you that I think as we get better visual representations I think things are going to get much easier um there's another experiment that also proves this not just in our papers but in many papers is that we've been seeing over the years that as your visual encoders get more capable your Downstream class are uh improving quite a bit so that's the second piece of evidence I think it's not just a matter of like oh if I can see the object I can navigate to it I think it's a matter of if the visual encoders which let's say they're frozen during training are doing more and more of the heavy lifting the parameters in your neural network can spend more of their time on um on producing the right action and so we've seen that if your perception gets better your reinforcement learning Trends faster so I don't think perception is the only problem I'm a vision person but it is it's it's it's going to take things to the next level as perception gets better this is like a follow directly to that so a lot of robot I I completely agree I think I think you should just buy high quality cameras and put them on your robots it's just there there's no need to do 200 for 200 unless there's some Niche application where you know this you're doing like some sort of surgical thing you know some surgery and you need some camera but if you're constructing a robot today just put a better camera and we see this Invision that higher resolutions are leading to much better performances in multimodel Foundation models this is this is a well-known thing on the yeah looking so thatv discusses oh I can take that one so um so basically the question is like uh you know in deployment what happens what is the most likely cause of some sort of Scandal and then um what's the mitigation right um I think that in the short term the most likely thing is that the um the robot does something extremely dumb like fall down the stairs um and like you know fall on a kid or something so that would be the by far the like most likely outcome and um it would be potentially for extremely dumb reasons like oh you know maybe someone pushes it off the stairs but still it's like your fault if it's robot falls down the stairs right so um I think that is being pretty likely or for example you're doing the classic bring someone a Coke and then you drop it and the coke explodes like that's also extremely highly likely I think um which is much less serious but like probably going to happen um I think the mitigation is that we probably have to adopt a lot of the safety and alignment techniques from the LM Community to give um these general purpose robots a common sense understanding of how to be safe um at least that's the kind of best guess I have in terms of how to avoid these pretty General open ended uh safety SS I think that's a good point Eric about the common sense that's in these I think like you know in the past the way that we thought about safety in a lot of cases is like extremely explicit uh like hard constraints and some like clear representation in the enir I think this is really powerful that we now have the ability to have a much more like semantic reasoning over safety I think it can provide a lot of like additional layers of safety that were otherwise very difficult to express I think it's also going to be important I mean I mean the cases that you suggested are going to happen on day one the first day you ship a robot those two things are definitely going to happen right but um I think it's also going to be extremely important for robots to be able to say um sorry I can't perform that safely uh and so the ability to say no uh is going to be an important uh I think research problem we should all be thinking about well for that you should first be able to perform the Tas but once we get there then you should be able to predict if you if if if you can perform it safely we're going to jump into David just question um on the safety front and a lot of these systems more model um decide stand Rel can say system is going to work up to this percentage of rather say oh the system knows blah how do we systems that we can say you know we should work within this percentage of St um yeah David let let me take a crack at that one because I think um probably again this is this sort of like I have my roboticist colleagues um there's already there's already agents acting in the world way before we have humanoid robots in the home um and so here's a question who here you know within the next five years envisages giving their agent AI assistant Chachi bt6 maybe um their credit card detail so they can go and start to act on their behalf got one person over there definitely want to sign up open I to one on the panel that's great there's like I don't know 2% um okay so what you've got there is definitely actions that can be taken in the actual world that do deliver actual fiscal consequences and um from a point of view of um was wondering about those standards that is where it will play out first there are plenty of consequences even in sending a digital message uh long before we worry about an exploding cat you know or not frankly you know even in the driverless car Community way before those humanoid robots you know so I do think it's very interesting I think a lot of the things like we use 200 by 200 images in robotics why is that well because actually because of endend learning because if you don't do that then your data sets get too big not because you can't get better quality images than that um and yet a human requires on the order of well let's talk about it in terms of megapixels effectively because otherwise they don't have 2020 Vision which is necessary to see a 12o print on your mobile phone within the f8 region so what we've got is sort like a complete disconnect between where robotics actually is today in the real world versus sort of like where agents and llms are getting and of course it's question about does it need to be goal oriented will it have actions will you allow it to take agency through you know basically the various means it can already have and that is where it will play out first and if we don't see it play out there then we will not see it play out in physical robots it's as simple as that oh I if that's a controversial point of view but I think that's where it play out that's where it is playing out now absolutely absolutely else upon them follow Strangely I think it's the same Community um so if we see bation today in the community even though you know conference um the concept of embodied AI that has access via where is going to happen before you have a humanoid robot it is going to need to have an A semantic awareness of your environment it's need to deal with the uncertainty of stuff you can't quite see and tell you I can't quite see that thing you know when you're asking about it um all of those things are going to play out first in spaces where there's commercial advantage in developing those things so they can actually be in the wild to get real data at real scale with real commercial application and in the in the meantime you know there's actually some very exciting work seems to be going on in the sort of like space of making physical robots cheaper but they're still cheaper in a very kind of like toyish way you know they're not they're not cheaper in a sense of sort of like a robot car is cheap so you can go and play with it and even if you could the consequences are too terrible for you to go and play with it on the road so what we've got here is a giant Gap you know that has to be breached by or or bridged rather via like these other mechanisms in perception of systems that are worn by humans in actions that are taken by internet scale agents and all of the safety measures and all these kind of exactly following the current sort of chat systems we're going to see it play out exactly in those spaces and then we're going to you know it's not going to be a credit card that you can use it can open a door for you as well but actually by that point you will trust it with your credit card so as to whether or not like you know it it knew that that was the right kind of shoe for you and that that was what your pet wanted and you know it didn't kill your with the stuff that it already ordered for you that you then put into the mix or to feed your cat Etc a lot of those actions are very real with real consequences and you're the manipulator you're the thing that does the actions now if we can't bridge that Gap solving the fine gr manipulation problem so that you know the robot presses the Amazon button you know and unwraps the Amazon box is the least of our worries right um so I think that's that's probably how it's going to play out over the next few years like we have a bing question right here um I think that was a great question this is kind of might be a bit similar on the Lin but something I get really deep into like my team on benchmarking right now models that we have and I think it's really interesting what scale AI is doing but thinking more so on the benchmarking of these models these whether you call it a general purpose robotic model uh and how should we kind of look at that or think of that there's like kind of two sides of it and a lot of today we're discussing the simulation and the training of these systems and benchmarking there but also trying to do that in the real world but there's also much you can do because of the amount of employees or Engineers working and training that system so I'm curious of how do we really work think through benchmarking with these emergence um if there would be these emergence of these very general G4 style model so how should we think about benchmarking how should we think about benchmarking uh like a GPT for for the physical world I think that uh this is something that I feel like I've thought about a lot I think that we just need World benchmarks where we can like reproduce and test these things in in different environments like you need to be able to easily download someone else's model and try it out I actually think that like the way open AI does things where you just have API access and can run it on your own platform is a great way of doing this right people people can trust those models to the extent that they do which of course you can argue about but people do trust them for the tests they use them for because they're very easy to test and you can run it you can try it on your problem and you know what it's gonna you know what it's going to do because' seen it do stuff I also actually mostly disagree with what you said I think I think that I I just think that like a robot in your home is probably going to be far less harmful than giving a robot your credit card honestly I think that a robot like there's there's plenty of things like low hanging fruit that I feel like is very useful to have a robot that can do manipulation navigation and planning and grabbing things and just whatever like like the the nice video that Eric showed earlier of the robots like bringing you a drink right I don't need to give it my credit card it can find these things we can do similar to what self-driving car people do and have like multiple levels of safety to make sure it doesn't run over a kid or fall off a cliff or something right that these are all solvable problems I think but uh giving a robot giving a robot your credit card and telling it hey buy buy buy my wife a present or something like that is an insane thing to do that I cannot see myself doing in five years perfectly that's you could imagine having the stretch go to the corner store and then like you know using the contactless payment terminal and then bring the thing back for someone who's not ail bodied yeah if I told if I told it like go to this store buy me this thing then then I could I could probably see that I don't see a way of making that safe but you are also telling me to do this at which point maybe it's like maybe maybe it can tell you that's probably a bad idea but I could try it you can certainly put a gate on it you can say auor to spend six bucks yeah you'd allow to buy this it has to I don't know like if maybe yeah but then then you're setting the bar pretty if you if you can actually make sure you that's getting into a complicated uh before we design the final products uh let let's get back to the science of how we we're even trying to Chi this I want I wanted to talk a little bit about simulation because clearly if you see all the talks all the challenges we really use simulation heavily in this community one of the reasons is that apparently the only thing we all agree on is that scaling up helps um any any BS okay so so oh there's one okay but but we all seem to be pursuing more data and and um with the exception of having the the glasses from Project area uh having simulation is pretty helpful um at the same time it has the challenges of building the action simulation we saw uh talks on that uh there are new trends and generative models to be able to generate the assets etc etc but we still need the real dat at the moment that we want to do uh s to real so um I was wondering how you think about those tradeoff in your particular applications if we think about humanoid robots there have been a lot of work lately on trying to collect data from the humans directly less of simulation because we cannot simulate the tasks uh so talk a little bit about the the tradeoff between simulation and real data for your respective uh applications I think today so I'll just I'll condition that on today because you know technology progresses but I think today I would say if you're worried about navigation problems go here take a picture send it to me avoid an obstacle go find this you know go do you know here's a here's a you know the robot has a basket I put a Coke just go and you know give it to my friend who's sitting at the end of the room if it's navigation I don't think you need any real world adaptation um I think I think this is solvable only in simulation I know this is a B you know people might disagree a little bit but I think you don't need any real world adaptation manipulation because of the limitations of you know there's a trade-off between how high fidelity physics you want to simulate and how you know fast you want your simulator to be and that trade-off often means that you do not have high fidelity physics which means you probably have to do a little bit of real world fine tuning today in manipulation um or a lot of real world fine tuning I think that amount of real world fine tuning is going to keep reducing because simulators are going to keep getting better uh so hopefully asymptotically we don't need to real world find I I guess for me I sort of feel like scale in this case is really uh you need to think about what access do we need scale on uh if it's just sheer quantity of data then obviously simulation can can work quite well but really the ones that matter and the ones that seem to be challenging are like real diversity over tasks uh both like physical diversity um certain things that are actually pretty hard in semantic diversity things that are a little bit more difficult to express through simulation I think like if if I wanted to go and you know like manipulate a tissue designing that in the in a simulator is relatively complicated but here's a box next to me and it's like extremely easy for me to gather data with a robot doing that so I think for most of these like really challenging things the things that we're struggling with right now I actually see the real world as uh in a lot of ways a better place to gather this this quantity of data I think what SIM can bring us is really fast feedback okay like of evaluations that this is working and this isn't working it's really easy to iterate and debug in that but I'm not sure that outside of things that I mean I think navigation was a good example um but in manipulation that it that it has as much of a place as we might think so I think stimulation is most I think this is a good point but I mostly think that simulation is good at like kinematic kind of problems where you don't really need to deal with lots of complex complex interactions or like the weird physics of simulating something right now now hopefully hopefully of course simulation will get better but like let's go with navigation right so I think for 90 99% or 90% of of the time I think you're probably right but one thing that we that you probably have is that the the mobile robot is maybe the mobile robot is running like some kind of controller that someone has spent a bunch of time tuning based on real data Maybe that's not maybe that's even manually tuned uh if the robot is doing like accelerating and slowing down it's got like maybe it's running NPC or something on board which is we're getting into planning land again right so basically probably are not solving the whole problem in simulation you're probably still solving it with a lot of real data and I think that this gets even more pronounced once you get into things like I've got to have my robot go over go over carpet I've got to make sure that I've got a um it's got to accelerate and then slow down and ramp in this right way so that it can go around a corner and at speed or something like that or go Offroad if it's an an autonomous vehicle there's all kinds of stuff like this that I think is not well modeled and Sim so like so there so there's always going to be a role for real data if you learn a world model from real data and you kind of get the best of both worlds this is with a big you know asteris that like it has to uh work um but if it does work I think that's um that kind of gives you the benefits of simulation and also the real world feedback loop and not having to model tissues um and uh I think I think it is there's a reasonable chance that it actually can work with the sort of latest generation of video generation models and also the you know the robot actually might even improve Fidelity because U the data from contacts actually gives you a lot of information on uh what happens in in the world so unlike it might actually end up being sort of easier than the uh unconditional video generation problem if you have if you have a um a you know a system with high Fork transparency that you can see actual you know external wrenches on the on things follow yeah uh been brought get it again you talk about data is diversity of the data so what I want to ask is what are your strategies for improving the quality sets and maxing that yeah I mean here here's one for sort of like realizing the uh sort of potentially what comes before fullblown robotics which is solving a ton of the perception problems a lot of the you know what might be long range planning problems possibly implemented in different ways and doing it where the human is wearing wearables and you've got all the same problems um you just don't have to solve the fine grain manipulation problem so why is that important for the diversity point of view well because I mean you perhaps can see there Ray band meta today is a product it exists these are glasses that people can wear and start to capture data um at Scales of product ARA is our research device that's you know it's only a few thousand devices but they're out there free with the academic community and collecting you know tens of thousands of hours of real world people interacting um I really can't see a way of Bridging the Gap without going through that space I think what might be going on is that people don't know how big the problems are to solve once you get away from the fine grain kinematics and Dynamics problems like the the problems are far bigger than that that those are definitely problems but then there are the problems of exactly us interacting the fine grain problems of understanding the social like navigating the social which is like potentially much bigger than sort of like the problem as whether or not I can get over the carpet you know do it right and the human will carry you over the carpet if if there's a human nearby and so on but do it wrong so I think I think i' get back to like what do we mean by embodied Ai and the problems in it social sorry do you think the social dimensions of like how people interact with whatever is wearing the glasses would be very different if the person knew that the um you know the thing wearing the glasses is a robot versus a human and if so then does that mean that you actually need to collect data with those glasses on robots yeah I think that's a very interesting question and which gets at the heart of like the motivation which was very sincere on my part of humanoid Robotics and from a humanoid robotics background we were doing biometic robotics where we had tendons that were like humans and we were solving problems that made robots look and move like humans because we thought there was very interesting important problems to solve with exactly this that otherwise the Gap in interacting with humans is also unknown and interesting just technically interesting problems for simulating that stuff but I agree um the thing that I can't quite get over in my mind is uh sort of like what do we mean by the problems of embodied AI that need to be solved um if we could hold on to that for a second we might have a more diverse discussion even because I I feel like we're skewed in the direction of fine grain problems because they are the hard problems today in autonomous robotics but they're not the hard problems of tomorrow um and that's I think that's fascinating you know from a point of view there are many different morphologies of robots today that are nothing like as sophisticated as our humanoid robots today and in principle they can do quite a lot of manipulation tasks you know thinking there in terms of sort of other animal morphologies with very simple manipulators relative to what we have um so yeah what do we mean by embodied AI again we're gonna get back to the definition which is what I wanted to talk about but we have a b question here he has been trying hard so yeah uh so if you were given like all of the H1 that Facebook media recently and like had the entire Facebook AI research team and you had bunch of cart do you think that just like scaling up multimodality all the text all the video all the images like throwing every problem formulation into some really big modle you think the representation of that all learns is sufficient so then trains and policies on top will actually get that's going to give us a step function a scaling Sam so the amount they Sol I don't think scale uh when implemented in a very raw way is going to be the answer I think the key is high quality data so you know you said you know you have all the h100s and you have all those bright people at in Facebook uh and then you throw all the data on the internet at it I don't think that is the answer you need scale but you need scale with very high quality and so I would just my two cents there that if you want to solve the perception problem um I think stapling all the websites together isn't going to get you there sorry have Sol otherwise they would have soled also yeah I mean you know I'm I'm not suggesting that you won't get an improvement but um for for example I mean you know it's just I'm just guessing here but uh you know open AI visual perception models tend to work much better I at the benchmarks tend to work much better when you actually give it all kinds of tasks compared to other models and I I believe the reason is that they just spend a lot of time thinking about what data they're putting in I don't think they have any other secret Source um uh and uh and I think that's probably a I'm just guessing here but I think uh uh that tends that makes me believe that um whether it's in Vision whether it's a perception or whether it's in training these end to end policies I'm advocating for in robotics I think we don't have that clean data yet we don't have that high quality data yet and how to create that data whether it's in simulation or whether using a little bit of real world you know uh ego data I think that's going to be key so wait isn't that uh just to go back to the question there but isn't that an argument that our algorithms aren't good enough because like humans learn all this stuff off of pretty bad data like really whatever we see so I'll I'll take that point and ask an opposite question of what he asked but is connected with that if you have very few gpus what research would you be working on to think uh I can answer that because 1X has very few GPS so uh everything you've seen in our public demos done on on like basically single 4090 cards um and uh you know we we are a little bit on far side of like you know anti skill but um and we do need to scale things up quite a lot um I would say that like you can get extremely good memorization results on very small amounts of compute um and that can go a long way because like once you verify you can memorize then then it's kind of you verify the correctness of your system um then then you can like scale things up and I think a lot of people often scale up before they check for correctness um I think if you are also scale bottleneck you probably just have to like ride the open source wave of models or rely on apis and there's no there's no nothing wrong with that you can build great products and build really good working systems using using uh the API providers um so you said we had just very few gpus yeah like not not that the SC that you're probably working on but uh no we're working on lot a lot of researchers don't have access to massive clusters and the question is how again how do you how do you produce uh interesting research and probably there is a lot of interesting research there that's what I want to talk well let me answer a slightly auxiliary question if you had very few gpus how do you produce effective Solutions um I don't you know some people would consider that interesting and some people don't and I think it's I I would focus on uh data quality and that's you know that's what we're currently working on we don't have a lot of resources because we're not a you know a big company so um finding the right uh recipe for creating your data um I think it's it it it it it works as a huge compute multiplier I guess I'm sorry go uh yeah I was just going to say that yeah I feel I mean along with what everything's been said here which you can actually learn like pretty good you know policies for specific things uh on much smaller data and focus on sort of efficiency and areas like this but uh I mean we all have access to like apis and things like this that don't require you to have these like you know a ton of gpus in in the past you know you might have needed to be part of something to have any access to to what's now just like open source and out there freely available or or you know cost some small amount of money but I think the fact that like these large models are actually available to us opens up a lot of uh research progress for sort of everyone without requiring a ton of overhead and compute on your side if you're also compute Bo you can open source a benchmark challenge for people to donate their well we we are really actually almost out of time uh minus two minutes um so I'm I'm sorry we're really out of time I I want to give you a final row of thoughts and I actually wanted to know in your like in your most positive dreams if we have this Workshop next year why would you love to be here showing up next year what do you think the progress goes towards yeah I mean if I was to have my dream um there'd be a much tighter community of embodied AI really looking at the opportunity for um given we at cvpr where this embodied AI is Active Vision which um is a term I haven't heard in a long time but you know at the heart of it um we've got currently this kind of like dilemma in modern AI that uses absolutely insane siiz data sets relative to what we think humans post evolutionary scale learning need to learn to be you know decent humans or otherwise animals all the same um but it I mean to me at least it looks very obvious as to like what we can do here because you don't need 12 megapixel images at 60 FPS times like a day long times a year times whatever data sets um what you need are properly embodied data sets which make use of the fact that you know manipulation um of the environment changes sensory observation those sensory observations are actually incredibly sparse relatively speaking most of the environment is not changing most of the time most of the pixels in most images are redundant you know once you factor out various things that change like lighting is a function of you know weather and otherwise like the day and shadows and all this sort of thing now Evolution got us there um through like a very crazy tree of you know uh yes creatures that that solve lots of these problems before we get to the sort of um scaled learning from sort of like um you know a baby onwards and but I hope that we'd have closed the loop on understanding that the kinds of data that have been used for internet scale AI are exactly the wrong kind of data to scale to body Ai and hopefully we'll have closed that gap on sort of like the future computer vision in Active Vision as it used to be I like that I agree with that but but uh so my my thing is going to be a little bit less technical which is that I really hope next year that we just see lots of robots and lots of interesting like real environments and homes and things like that unless yeah yeah I was I was also going to give us more boring answer um and I I was going to give a very similar answer I would love to see next year um all the invited speakers bring a robot along and you know showing a you know real demo of what's working and what's not working I don't expect a lot to work next year but you know uh we're working in uh in embodied Ai and we're all saying oh real world is so great and then you know we we we don't do any live demos so I think I think proof is in the pudding so I would love to see that will drink yeah right this is something I really like about RSS is that they have like a big demo track now that's what I really think that we should do maybe Brian and yeah maybe that's what we should think about when we you know when we all sort of putting our brains together to organize the next is dedicate some time towards real robots maybe Brian and the sure I mean uh I think I just Echo what everyone said that yeah like having a real robot there doing something that I can then interact with and see you know how robust you're policy is uh I I'm I'd be pretty curious to see that and I think we'll be at a point where we can do that where we're in a place where we we really believe that what's it's that things are working a little bit Yeah plus one to what everyone else said you're bringing the human life I would like to attend next conference as a human well thank you everybody I think the closes of the panel pleas so I just want to thank say something I want to thank all of our panelists speakers uh our organizers all the rest and everyone that came and made this Standing Room experience thank you all so much this has been inv body AI 5 we'll see you next year at body ai6 interesting to see a position paper on whether or not thank you thank you so much

