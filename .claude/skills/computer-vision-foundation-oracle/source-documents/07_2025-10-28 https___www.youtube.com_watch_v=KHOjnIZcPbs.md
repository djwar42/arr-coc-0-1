---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=KHOjnIZcPbs"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:01.818Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=KHOjnIZcPbs

6033a520-81fa-46bc-8d23-ed7d30b529e1

2025-10-28 https://www.youtube.com/watch?v=KHOjnIZcPbs

932b6e2c-ff35-40cd-a787-74a7227b6d6c

https://www.youtube.com/watch?v=KHOjnIZcPbs

## KHOjnIZcPbs

## ComputerVisionFoundation Videos

today I'm here to present our work in nerve 360 text guided 3D consistent object imp painting on 360 Dee neural Radiance field this is a collaborated work by Don Wong Dr Tong Jang Al abud and Professor Savin sust the goal of this project is that given an input neural Rance field and a text instruction to remove a specific part in the scene we want to be able to remove the instructed object and leave a visually plausible region some of our prior work only works with frontal facing scene and requires of hand annotated mask as input other prior work on editing 360° neuro Radiance field have difficulty in selecting specific object based on text instructions due to the implicit nature and lack of semantics embedding in the underlying representation what makes this problem so challenging firstly obtaining a consistent segmentation for object across 360 Dee Viewpoint is difficult as the outline of the object changes drastically from different camera Viewpoint secondly it is also challenging to fill in the missing region in a visually PL manner after removing the object since the 2D image imp painter has no 3D knowledge it produces different per frame imp paintings in this work we propose our solution to each of the two challenges firstly we introduce our multiv view consistent segmentation module we initiate a set of founding boxes with an object detector through the input text prompt and the source data set images we then refine the prompt used by the segment anything model through the estimated depth information of the original scene to encourage a consistent segmentation of the model we use 2D image diffusion to remove the object with our consistent segmentation from previous stage and initialize a new scene in which there will be artifact floaters accumulated by the inconsistent 2D edit as the diffusion model does not have 3D consistent understanding we then fine-tune the new scene using our geometric prior it is a 3D diffusion model trained on shape net and has the prior knowledge of the density distribution of 3D surfaces to remove floaters and fill in the missing region and at the same time we fill in the color in 3D space with perceptually consistent appearance in painting We qualitatively compare our results to recent 3D segmentation ation method spin nerve and the video segmentation method dyo which is considered to maintain some consistency for object in adjacent frames for the top row as the less challenging case each method gives relatively consistent masks for a bottom row however Dino and spin Nerf produces incomplete segmentation for challenging cases on the other hand our method maintain consistent masks our Baseline contains artifact in the region with removed object while our method synthesize artifact free result we also preserve the surrounding background closer to the original scene here are some result or our measured on 360° scenes here are some results of our methods on frontal facing scenes we showed the effectiveness of our geometric prior on removing the floaters in the infed region we can see that the lpip SLS can improve BL background output due to the inconsistent 2D in painting with our accurate selection and the 3D priors extenstion to editing object appearance in 3D is natural as shown here in our example we can do much better fine grain edit compared to Prior work thank you for your attention I have presented our work in Nerf 360 Tex guided 3D consistent object in painting on 360 degree neuro Radiance field

