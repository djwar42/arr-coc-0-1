---
sourceFile: "23612   2nd Workshop on Compositional 3D Vision"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:05.744Z"
---

# 23612   2nd Workshop on Compositional 3D Vision

7a1d6bb0-f05b-4ec7-b099-2f1179a5a528

23612   2nd Workshop on Compositional 3D Vision

ea47c2d7-2cea-43d9-9e5f-073493b8a413

https://www.youtube.com/watch?v=n7H4vfk0YjM

n7H4vfk0YjM

## ComputerVisionFoundation Videos

right good morning everyone and welcome to the second workshop on conventional 3D Vision um today we have an exciting program and uh this is Joint work with wolf K Hedrick Peter vda Natalia and a group of uh nastic students and collaborators who have uh worked so hard to to prepare the program um so the today's program will start with a very exciting uh talks this is the an overview for the program starting from 8:50 to 6: p.m. we have a fantastic lineup of speakers Professor Andre Fidel Katarina uhu J stre and uh J Jun and Angela D um so we have also an two Workshop challenge that has been running a 3D compat Challenge and also VC that will be covered at 3 p.m. today we have two uh breaks as well as poster sessions that are going to happen at 10:10 as well as uh at uh 3:15 to to 400 p.m. this will happen at the ark building for e at the exhibition Hall um and uh there are in the program also there are 12 uh papers that has been accepted in the program uh seven of these will be orally presented Ed and this will be covered at 1:35 to uh like 3 p.m. right before the workshop challenges um and this is basically the summary of the uh Workshop we have 735 minutes talkes two challenges two breaks 45 minutes panel discussion towards the end uh oral spotlights or or presentations for 90 minutes and uh a total of uh 75 minutes of poster present poster presentation a but please note that this poster presentation will happen in the art building because this is how it has been organized here in in the by by cvpr uh thanks everyone and with that I would uh like to introduce Professor Andre faldi Professor Andre professor of computer science and machine learning at the University of Oxford and he leads the visual uh geometry group since 2012 uh his current work Focus is on generative AI for 3D computer vision for generating 3D objects from text uh and images as well as the basis for image understanding he's author of more than 200 uh peer-reviewed Publications in top machine learning and AI Venus he's recipient he's recipient of the student of Mark aam prize for selfless uh contribution to compar Vision community and the test of time award uh by the ACM for his open source contributions and the best paper award for the computer for the computer and and uh vision and bition Conference and today he's going to talk about structure and composition in CI please join me to welcome Andre yeah all the time yday yeah I don't know what happened and folks also confirm that the zo share is working hello nothing it seems I'm very good at the technical issues yesterday was also not working something was also not working so I guess it's um yes e see the board yeah wonder about Zoom that work same all toart well let good thing we are five minutes early I guess two minutes early now e e this levels is it yeah it's it's it's hitting here it's also hitting that but it's the actual speakers now they shut off so but I put a messaging and they're going down the line and you know everybody somebody has to call somebody and all this and then somebody pushes a button somewhere and they about to yeah yeah I think shout that's for sure uh um the question is will people people on Zoom be able to hear me I can can turn on the audio of my computer the mic is on should oh really yeah the mic okay um yeah so should I also share screen here oh hey so much okay perfect okay don't need to show anymore set opportunity um okay hello what should have said that um okay right okay um all right should I shout then and just on okay wait it's again yes just terrorized me shouting okay I'm going to wait a couple more minutes just to see what happens exactly somebody's playing in yes okay right so I'm here today to try to talk to you about structure and composition into thei um I have modestly put composition in parentheses there because there would be some of it but not uh much of it I think all right so I would like to start by just not IM of this machine learning and could beion coming together now so we have new ways of representing for example NE Fields mind geometry and and learning uh from the view point of representation of the 3D word we have tools for two generations you can take bit text like a house with a bit of clouds on a H or you can even take a single image or maybe video here and start Ang from that and of course this is something that involves geometry but it's also not something you cannot do you can do without suitable proi so that's where machine learning really helps by combining with geometry which just algebra feel like the statistical information that we require to solve this class of problems and more but I would like first to discuss how we can go beyond just Construction Three construction for example we N St but what what else can we do with it how can how can particular put some structure uh so maybe some some interesting things with it so just as a reminder what I mean by 3 construction here we need 3D construction through optimization so suppose we given a model of the scene think of that as Nerf that is to say field Maybe you would be using BL name um you have an image several images I I I here that you capure to try to get information about your scene and then there is a randoming function which is differentiable so given the model and given the camera CI you can render in a differentiable manner the image that you obtain from the 3D model and then you can compare that the image you get from the I and you can cast that as can set up problem which you try to make the render image as Sim as possible the image you measure and by minimizing this eror with to the model you're going to get stru everybody understands that this was made very very popular by Nerf uh because then you have this combination of learning the different representation and differ optimization sorry optimize us for example but once you have done in suppose that now we have a network here that can extract some features from your this could be some features like d could be a segmentation Network something like that so you can take your input image you can pass that to the network and also you get some features and what you can do is you can add the features as additional channels in your image and you can extend your models that when you render it not only you render the RGB colors of the pixels but you also render the values of the features and now you you sol the the addition problem but now you again start adjust not just the RGB channels but also the feature channels this is very very simple but in this manner what you can do is you can essentially fuse the information that exract on image image on an image man image basis into a single Cent mod all right so this is way of fusing the information into a single construction so why is that useful well because you're lifing this information and you're making it more consistent so this is an example of NE feure Fusion Fields so this is a paper which is already few years old now where we start with this idea and got us that these D features which are the features that you can exract from each frame dependently so you can see here a version of those the idea here is that the color corespond different of the same and you can see that are low resolution to some extent and also little BC consistent consistent which should not be the case because the semantic of what you're looking at the image should not depend on your point but if you the ties in 3D then they get cleaned up quite nicely so they become less noisy they become more perform they cannot do some things with you can start to have some the composition of the the model into different parts so here you see the F reconstruction it's just a nerve and here you can see the foreground which has been extracted completely automatically by using this set features so Dino features they not train using labels they're trained just using images even so because the are actually veryy good once youd you can get something like automatic segment component so you can get forr and backr in a separate man now okay so that's one example of is in 3D but of course you can use any so here left that was new last year where we look at the same problem but in this case what we wanted to fuse are not um semantic fees like semantic labels but rather segments which characterize instances That You observe in your in your your frames and these are just inst segmentations you like so these are not necessarily consistent across different frames to capture so you can see here that the segments they sort of stable really any particulars return to the that's not what he wants because you know that the always the same it's just that because you're detecting them independent frame they not consistent I confus the detections using press the lift and now the dets become better um more Global if you like and also they don't flash anymore because we solve sort inconsistency that you get by processing things one time just again to give a sense of how this works so here frames and you run your ination systems you get frame one and frame two you get C of the same object but the colors are different because the labels consistent one IM this this one is so they don't much so you can the model assign to the objects IAL problem that when you want to match your model what you measure every time you do that you need to Findation that we arranges labels with a compatible with the label from yourd segment now that's possible people have done it um but it's also a little inconvenient little slow because every time you thisx every time you your loss so that's annoying so what we to list was weate this in a conted fashion so you see here we have selected a um dimensional point the star and then we select all additional two points one on the same on the same object and another one in a different object and if you compute the distance between the embeding trying to learn so here we Ass Point picture Vector which is supposed to be characteristic of that particular instance so if you compare those vectors like that you see that the differ between them here is small that's why this is black just means that the differ value between the features you get here and there is small that's because they belong to the same object inance but if you go across instances that large if you think about it this property of Legos being small large the distance between Legos being small large holds through this space but also hold in a s varant manner each one of the views you are extracting so this now is a loss you can compare this sort of map get in your images and that's actually going to be pration varant and that's nice because then things learn faster and better so the point here is that if once you have use your semantic labels this way then the performance that you get actually improves so this is the case when you integrate 3D computer vision and you add it to your uh to the segmentor just again a frame by frame basis actually get quite a big boost in term of detection performance so this is a case where Division division really helps um to the machine learning and you can carry on like that and be creative so for example you can use language features so here we have way of taking CLE features and embeding them to the space so you might be familiar with the results of those has some great examples of that already but what you see here is that's with this new thing I've developed called NE pills you're able to be more much better sensitive to compos qu or quties that really try to combine a bunch of different cont using language so for example here you can see green toy chair and only the green toy chair lights up then you can and say I want the chair legs and only the chair leg like up etc etc and so this is obtained by essentially having so they found a way of summarizing different resolutions different semantic scales of features into a single dimensional embedding that then can very very efficiently once you have your Tex so if you're interested please have a look atable okay this is another example which language now in this case have to De compos SC into different Parts different components at different semantic resolution levels also different sizes physical sizes okay so moving on um next I would like also to discuss a bit issue motion so when it comes to this construction they work very well if your SC right but it's actually not the case if contains some dyamic content for example say this case so this is a video from group where she shows the difference between what she call Casual captures so this will be captur may be capturing your cell phone as opposed to the sort of existing capture for damic scenes that people like to play with papers so what you notice is that in both cases the scene is actually dyamic so I think this guy is going up down like the cat is alive so things are changing over time but the difference is that here the dominant motion is the motion of the object here the dominant motion is the motion the camera so it turns out that doing dat construction or should say for dat construction is much easier here than is that's because the camera mov is dominant so it's more easy easier to things like but really when it comes to reconstruction in a more realistic setting this is still very very hard think that in the the future of the division move sort of IDE construction to do that we going to need to have better tools for tracking motion so one thing we're doing m is developing a new class of chers so you here see an example from coer which is just the adjust actually work but it's a Transformer that is able to track together indidual points and get uh results in performance or at least now comfortable to order in class um tracking the same class but the point here really is that TR these points much much better than was a few years ago I think these are tools that will be help a lot understanding the Dynamics so what we about the composition well you think about when it comes to explaining or modeling the motion of objects then you're not just explain the motion of single thing you see in one video but you're you want to explain the motion of a object for example humans or maybe all of Ty like all hes or all lines you can do that by developing models like for example and small and that's that's fine these models work great but it also takes a lot of effort to be able to build them and train them so what we wanted to do was try to open up 3D computer vision in this Cas computer Vis understanding CES that in a way which notri just two categories that you need train by collecting quite a bit of man training data for for that so the idea here was to scale learning by using internet data inste all right so this is a set of works that we we been developing over the course of several years now in Oxford most in Oxford and Stanford um and there is in fact the latest of such Words which is Farm which is um sorry yeah learning the web that's out here present so you want have a look at this session check it out so anyway so this is a long series of words where we trying to be to develop this system that are able to learn damic object models on images that are not restricted to just few so the way this works at a very high level is actually quite simple so we use what we calling approach what this means that you have a collection input then you have that we call a d that comes up with atic factor that explain your image so shape material motion light and cameras and then you have a rendering function which is basically just deterministic if you like or manually that take that takes these fact andage cont as totally as possible and now you can train that just as as if it was okay construction L and this me you can train that without without now of course this is a easier set than done so there's quite a bit of work you need to do to be able to do that on SC on scale and enough so one thing we did here was um first of all to come up with a way of modeling the the shape of any object so we start from space here which is DM space that is differentiable um initial initialization that gives us essenti sort potato here in this space training what you get is a 3D mesh which approximates the templates of the object category or the object type and then you also have some f d features here which are just an augmentation this dimensional shape where we reach points a set F Vector that are theing D features the way the why we do those do because these fees are unsupervised but at the same time uh they tend to be sensitive to different parts you see here again thec B visualization that different colors T to pop up from different parts of the object so here see this even better so have bunch of training Imes here the coresponding D fees there you see that the D fees light up of find correspond automatically for you so this is essentially decomposing your your D this for you without having to provide any then we take an input image like that we have our base here which is a template for that image we have AER that tries to capture different facts that explain how this template fits the cont of that image then this goes into this ring function that produces theb pixels The Mask as well as the dam feature that then we match the D feature that actually measur the image D featur image D features in model and then we try to match them just in the same way as we matchs this is very similar to what I showed before when we were doing neure Fusion so then help set up some losses train and this is fine but this is only for a single category but as I said we really want to expl to expand this and do um thousands counts of different categories I'm not going to be fully General here I'm still going to stick to essentially but I will try to remove the Restriction that have to work on your report cows Etc so we want all this able to do all of them into one go which is actually quite hard so the way we do this is by ask asking our incoder with coming up with the same f as before but in addition to that we also produce with with encoder here a key Vector that then looks looks up in a sort of dictionary of Base shapes that we're learning automatically a corresponding small set of possible matches in this space s basis State like M with weights for those and then when you can Rec combine those umal shapes if you like into a shape that is the base model for this particular image so essentially on the Fly you try to construct a template was uh the C would look like here based on that image by combining sort of fixed templates that we are also learning fix in the sense that they always the same for all the images but of course they change over time while you train it because we don't have time so you can combine them you get template prod that and then you get your information articulation Etc that try to M that particular templat to what actually seeing the image and and tries tries to M them features and and in the shap so these are example of results you can get so this is 3D we again poster um on Thursday morning and this essentially works for um about 100 actually more categories of this met C is SP and it's actually quite nice because you can capture shaps which are very very diverse and still works here you see some examples this running practice so you see the image on the left then shape and rotate around you see this also a skeleton that helps to explain population and then because you have that you can animate those shapes these are not just single IM constructions but are map that takes your image into an instance category so now you have the cat which you can animate andate okay this is another example it works also for non objects like this C here this is the model um um this because the model as well you can get some crazy shapes still works and if you want you can can apply on a CR by basis and it works so the construction looks there pretty smooth but again it is not Dynamic strange that this would actually over time this is simply a sympt of the model being quite stable okay so that's one thing you can do in trying to automatically de compos in this sort of consider Parts which are damic and you use the parts to try to explain motion right but what about what if you want to go beyond that so this is actually question that I've been asking myself for a while and it's a tricky one I think so what is a universal modomics right so we have seen before that you can have something like which is uncrafted maybe you can have something like extension Which is less uncrafted still fixed but act shape and learn so this isra something just really carefully cut by hand and with a lot ofion uh but still it's just B bit what if you want to mcycle a model is able to understand a human a horse but also pie of paper a towel a door or a cabinet so all these things are things that have certain motions so how we going to come out with the model that can explain those so one possible way to try to look at this problem is to to work with a block so you train a model that you think understanding motion and now what you want to do is to have a way of prob with anything else and one way to do that is to do that through driving interace so here we have an image of chest which has a handle we draw a drug and we want our model to be able to predict what's going to be the effect of daging that part physically right so there's no physics here it's just an IM pixel then Dr is just be but what we're trying to get the model to simulate from the stand what be the effect of of that motion in reality and this is if you think about this completely generic because you can have a chest but you can also have a laptop and you should work just the same you don't need to specify any skeleton completely openend of course this is very indous so it start from an image and new what you want to do is to sample an image that is a possible response that particular D but is that interaction happens at the not the global you translating object you translating in a the small as possible part still makes sense physically right so the way we did that was still quite basic so we went out there collect a set of training object synthetic which have articulations different kinds and we have random mind STS and St this set pairs where you have starting image and image and then we label those arrows which correspond to the DRS we have a whole bunch of those hundreds of thousands and from that we can train a model able to the prediction the model not surprising it's just the fusion model I don't I don't want to go into the details too much so to say that you start from the conditioning which is start your drive you have a certain way of cing the Dr that I encourage you to look at the paper if you're interested then you Den noise start noise an image of what the chest would actually look like once apply that drug T you can have any number of drugs from Z to is quite less and here see some some results so these are segment three images so you can open might open here you can close the doors of that cabinets uh go lamptop close this this LD again this handle you can do one or more doors and Ms understand what you mean etc etc some glasses so you can bend the what you call the stick of the glass uh Etc and more more results and because we start from aain Mod St diffusion then this actually even though the train data is actually quite simple just Sy data which some rization is still okay to real images this is not to yet we are thinking about how we can extend this as well of course but this just give you a glimpse of what's what's going to happen in the future again this is completely opened interface in which you don't need to have a specified topology for your object you just want to get open understanding of what it means for object articul model their dyamic which is IDE next I like the point that a lot of composition we can get Geometry might comes from the US of language right so in particular we can consider of Tex which I think is quite interesting so you say and the go would be to samp from this distribution that condition PR gives you density over the to the object might respond this Con and if you think about language has obviously properties and the question is if you train a model that goes from Tex P to an object can you benefit from those composition so one thing we found that is quite interesting again this is also poster here is language control to the editing so what you can do here is you can take objects in this Cas we using I'm explain one seconding for those called shapy is a technique by open okay then we can specify with a Tex PR not what we want the object to be but how we want the object to change so it could be some Global edit like for example make it look like rainbow change in typ colors it looks different but it could also be a Lo for example add a little to it sweater so because the model language you can potenti understand some level of composition so how does this work so thing we want to do with this paper was study the problem of learning interesting related spaces for objects and uh because we did this inity we didn't think we could train this from scratch so we used the to ask which was sh again this is space object that was is provided by us technique so what idea here is that start like that there a network that that Vector another Vector sorry another n the Tak a little about your your object in this case it's wrong should be the same as before didn't yet sorry new slide okay but what we wanted to do was to actually change this lat into some uh add to this this Ting capabilities so the way we do that is by take the object we space which is VOR then we learn a box here called shape editor shap e um that takes if you like the instruction the way you want to actually change the object then find finds out different L here that get decoded the you actually want is the object that we have here input but change and the way we do that is by training this using an existing off the shelf editor for 2D images so we essentially distilling the editor into a editor so again we have the object we extract The Source lat then we apply our should the editor to get the edited L we decode that so we get corresponding construction for with without and with the edit we render that so now we have images um image have and then here we use the SS loss um and SS loss is trying to make your editor change the input image in such a way that the edited image looks like it's compatible with a tri on edits which is captured by a given Image level editor so here use two of that an image image editor that takes an instruction and speak takes some instruction and apply to the image or we just use diffusion surrogate for for to the editor with the SS we can then back into watch not on editing a specific object but in learning the weights that go network but then is able to apply the Eds directly into this space a little distinction between what are Global edits like the style object loal edits the parts we have a way of automatically figuring out tension Mar say which is the part of the object that the Ed is meant to change and then focusing our efforts in only change that part the rest J okay so why we want to do this why is anything well um I'm going to come to this point again a few minutes but one thing we can get immediately is that while editor doesn't work for all PS so you still have to train for one at a time a few PS so that's we lose a bit in generality the point is that once you train editor it's super fast so normally if you were to use SS L to try to edit to the object might something 30 minutes but once you are to do that space takes one second so if you are you want to added a whole family of objects you have I don't know old marks they want to change the color of their hands so their hands are them you don't need need to pay the cost of minutes by hundreds but you only need to do that once and then you can edit all your very very quickly by using the editing capabilities you have now acquired lat space so the Ed lat space is in fact much simpler than editing the 3D model directly okay this is a complicated table but the the message here is that you can get better qual results by editing all the together in this fion and eventually actually much faster than trying to one time see here's an example of global editing so changing the over style your object um making cyber like Lego and a lot things is probably more relevant to this Workshop hand sweater a little Crown here and Snowboard the quality is limited by the fact that we in thisat space is Shady so that is how good these objects are for that model nevertheless is interesting that we can learn very very effectively sort of LS that apply universally kinds ofits in fact the space is also quite interesting so what you can do is you can take two objects here like the cizer that are being edited by change it color subtract the seiz without that edit we're doing this literally as a subtraction Vector Stace because we have St codes so the ALB there then some the Cod for the shoe and of get a code that represents the shoe without affected it seems that this level of is Lar work because space is C it's actually quite interesting same is true here for Bo you can see that you can understand how the action works space but simple subraction once you have that Delta Vector you can apply to different objects and that same Styles can be apped to the other object okay I think I only have few five minutes left so very very quickly would like to conclude this by uh insisting once again on this sort of idea language can help you to learn structure when it comes to generating objects the St approach here would be you take an input you have a multivator that produces set of objects which set of views of the object which are respond to that input to be text or an image then you can have an constructions out the object there that's mod is what we ask ourself is uh what we take what would it take you take this and make this generation St of sufficient quality so that people in the gaming industry for example prod actually would actually want to use the object which are produced by regenerators there are bunch of are very important but the ones that I think maybe this work are two this control in term of compil and also material control material control by mean that you want to be able to specify your front what your object is made of so here say for example H made of metal so what you want to do is to produce the sh object but just want to produce the that would be the unshaded color you want to have roughness you want have which are two physically based property that describe how the drdf function work for and so how sh is and howal looks like okay so we did that and we train a model that is able to understand these factors and amount from language to the construction you have and these are some examples of the results we get okay so this is called meta regen and is the latest greatest from all group in in meta so this basically result of a year and a half of work I think so now works quite well so you can specify fairly detailed descriptions in language and you can get this very nicely looking high quality object correspond to controls both the shape and the alabs of the texture as well as the materials now what I want to make the point here is this level of complexity that we can get by having a part image generator combin the power language model and then eventually Maring everything so here for example the is stal stat of L with a l on one side and May 24th on the other side see here are three different methods3 are two commercial competitors and here and let me show this again to you the video sorry about that see one side M for the other side again an example of composition through language and this is a i it doesn't work all the times but when it does you're quite happy it's actually amazing that you do that a l truth okay but this is speaking the is very good at F instructions so here we have a character with a straw hat and superh costume and if you look carefully at what you get from different methods our is essentially the only one which really follows St very very carefully so a STW hat has super superh custume the other ones will have some mistake in that okay and here is again a composition where you have an foring hmer on and you can see here our me is able to get all the elements correctly and also get quite a nice shape as a result so to conclude again there are three papers that hopefully are going to come out by the end of this week or very early next week uh um if you're interested have a look there is one on generation from test another one on Tex generation us test and then there a meta paper of paper that two combines them together to get results inex generation then just uh thanking the team in Oxford a lot of people there A lot of people CA many of these people had a the SE today but of course you can speak about everything and then just summarize what talk about so one was using uh information into some space you get some benefits from that including having structur structure in first place maybe language that part drugging so this idea of developing this sort of universal motion models I show you the learning motion models for and then I want to try to generalize this to beace to prob it's quite inter part editing and space so developing this space for object which have this property where that sort of exposing sort of almost like fashion structure of the parts or the properties of your object quite useful for understanding the structure there then of course a lot of composition comes from language TR simply speak text Tod model and you get some compositionality for free just because composition is there in langage first place okay thank you um sorry for being one minut uh yeah it's interesting question um but I mean it's very openend question also I think you have most specific question or just uh complet open yeah so kind of people on like to induction using IND okay so I mean I can try to make some very very high level predictions here first of all I think the next step is 4 so 3day especially when it comes to to do also image understanding is more useful when you want to explain videos but uh videos are interesting uh especially when there stuff which is moving inside the camera and this is where you know still not very good at so I think that's definitely something we want to do better in the future and I think going to come very soon so 4 right um in term of the technology use Fusion probably for longer but I don't think that's fundamental or using nor probably that is also not fundamental and by the way these things they tend to be more interchange when you think not very efficiently can almost get not not quite but those the the deficiency ofing at least in term of how much memory takes for ex to train these things are not so fundamental I think um what uh is another interesting question to me is what does it mean to have a representation so now we use primitive like points GS triangles vertices blah blah blah right this is going to be very important for applications for example gaming or special effects because this is what software for professional um and any any computer gra sare use that's the language of computer Graphics today but project that in in the future maybe the way we going do the do gra is going to be completely different and the idea of representing something with a tangle MCH going be complet right so a lot of the stuff they're doing is trying to take machine learning and get to produce outputs in the space of representations which are beautiful computer Graphics because that's what we know and that's what the tools we have like PlayStation actually does use understanding right but project 10 in future that might actually go away maybe what we need is complete just pipelines where J is largely or entirely so I think that's really interesting question is that going to happen maybe my pleas to introduce Prof is ass Professor Department universalized in building maches that can understand the stories port in videos and conally using videos to teach machines about the world uh the goal is to work in building machines that overend movie blocks while the AL go is to develop machine that that would prepare watching film over joining he spent three years at the board at Ply with and also Google and she will talk today about uh memory pomping and feedback understand this do you know what I need to press so that there's no mirror I think it's uh yes thank you so much uh to the organizers for organizing the workshops again this year very second time thank you very much for inviting me and giving the opportunity to present a recent work from our group um the topic the word of compositionality is already very complex it Mak different things different people a very simple maybe explanation would be I give you a new uh picture and I can of course give you a label Center you don't understand what this means but you go and check the definition definition is with respect to things that you already know so I would say compositionality is how we compose things we already know to build our new Concepts that can quickly understand new Concepts as opposed to having training data of that New Concept directly so this talk will explore three types um which can give us compositionality the first one is through memory indexing we'll discuss some memory augmented neural networks that along with a parametric component there is a big nonparametric external component where your memories are there they retrieve them compones the second is to generative feedback how can we use rendering to and compositionality to lift monocular videos of multiobject scenes into 4D and the last is compositionality 3D through beautification of 2D image parsing and 3D SC parsing okay uh so compositionality through memory indexing um so this is this is a work that I really really like despite the fact that it hasn't really uh let's say picked up because it's only applied to to part it has very good results from parment as I'm going to show so we'll build M networks that along with the standard parametric component there is another component we called a retriever that has access to a memory to a set of memories every memory you can think of it as being a labeled Point Cloud okay completely point out Ling where the parts are labeled okay as you see different colors are different labels and we'll be part new Point L input by essentially reading from those memories okay here's the retriever we are going to fuzee each one we're going to do K as neighbors bring the ones that are most similar along with their party compositions and then we'll feed those memories to a modulator a standard you know attention based Transformer that um instead of just processing the current input also process the memory so you try to understand essentially the currency by relating it to the members to the things that you've seen before uh and then you Lael the output and you every every label part of the memory is fzed and is M to a query Vector so this architecture should remind you a little bit of the standard detection Transformers where we have a bunch of slot vectors or query vectors right so they're shared across sces and these featu Riz along with the scene and eventually they segment uh the the object so the same thing happens here but instead of just having those vectors that are Shar across standard detection Transformers you also vectors that have identities so this quy Vector is the back of the chair that you saw yesterday in your office all right so we allow those memory query vectors to also participate in the same paring and claim Parts um because some people are joining it's on the calend inv I see I just so it should it should work right um okay so basically along with those detection sorry queries that are s agnostic we have those you know identifiable queries that are memory part vectors so when that memory part Vector is used to capture one part of the current scene then the benefit is that this segment part is aut correspondence so this part is like similar to the part that actually claimed it so you don't need semantic labels anymore you don't need really uh you know the space is not anymore a language space is the space of your memories and their decompositions and so so what this allows you is to to do is instead of having one language label which is the back of the chair you can have as many labels as back of chairs that you've seen okay so you can recognize your grandmother's back of chair and so on and and this also explains how uh let's say esos know more about snow than us because they they don't use the language space of snow they use the types of snow that they've seen and they see much more snow than than the average let's say American or Western Civilization people so so basically memory provides the label space to paring s that's that's the idea of these networks and again this should remind us of those segmentation Transformers that you have in the image and have this colorful slot vectors that can explain the scene but our slot vectors now come from memory now along with the memory slot vectors you do have those gray guys there because new things can show up in a scene that are not part of your memory and you still want to segment and recognize those so we call those networks analogy forming networks because they form analogies to things that you've seen in the network we also call them analogical networks but unfortunately this term is not that good because it's like digital versus analogical is the loop overloaded so the way we're going to treat those networks this through with did see augmentation we'll train the label Point cloud and inform it in two different ways and we will use one information as the memory to par the other deformation and the good thing with this self-supervised way of treating the network is you have this LEL Point cloud and when you deform it you know exactly which part should claim each part so you can put very strong losses that tell each part of memory which part of the input Scene It should claim and as you see you know the the blue should claim the blue and the green the green part and the red the bottom so you're trying to this segmentation transform in this way and again here we don't use absolutely any semantical label so nobody knows that the blue is the top of let's say the the lamp and here is how we do our cross SE training where again we don't use any semantic levels we have access to this um set of memories and we retrieve where retriever tells us what to retrieve and then we minimize essentially a segmentation loss so we only trade the network to segment but not to label but you'll see that the label is going to emerge automatically through similarity because the most relevant memories are retrieved and are used to label the input in that memory part label space now the good thing with this memory prompty the Transformers is that depending on the memory you dri you have different granularity of the resulting labeling so the same input can be labeled differently depending on which memory you retrieve with less corser part grity or finer part grity so our baselines are the standard detection 3D detection Transformer we have a point cloud and a set of you know sostic queries as well as a t here for partner that tells you how find or course you want the segmentation to be and another very interesting Baseline is a standard memory augmented detection Transformer that has access to memory much like the analogy forming Network okay but so it attends to those part and Bings but doesn't use them as query to seg so allies are going segment SC but you know you're still allowed to retrieve and attend to uh things from memory and I mean needless to say that all this langu work is very inspired by retrieval of Meed generation R used in large language models okay uh we've seen language models doing much better when they have external hongus Repository so fact and so on so this is very much inspired by that the downside of this model is that unfortunately you cannot get zero shot uh labeling you can segment that it doesn't put things into correspondence because you don't have those nice identifiable queries uh to use you just attend to them but you cannot use them uh to to segment so in part if we compare our model to to the baselines you see that when many examples are shown from a category all models do well but when you have one shot or few shots the memory augmented models which is analogy forming Network and Ral augmented D do way better the reason is that from external memories allow fast learning whatever you see you just directly update your memory while parametric models need gradient based learning and this is slow learn you many many gradient steps for the W to update to explain your category and also you want this gradi St to be decorrelated and so on so we want to combine here fast learning with memories with a slow learning of grading descent over weights um yeah so detection the the standard parametric Transformers that everybody is using is this orange bar and as you see if you find tune it it does well but still not as good as the memory augmented neural architectures um yeah so now we have the same time we also have a label propagation without using any semantic labels simply because our analogical networks for cor explicit correspondences to Parts in memory you can also transfer the semantics of those labels in memory to at the paring of the current Point Cloud so basically you never train the weights of your network knowing semantic labels but simply because you correspond to memory you just inherit the semantic label of the part so so here is how this looks like you have an input the mem and here the corresponding output and the the most interesting results is when we keep to the network different classes so we asked you to segment the TV by retrieving a clock so you see what the the network did it said you know this part is not I think this this and also new part so this is also segmented by this uh seen agnostic qubs so so you see some sort of special abstraction to our eyes simply by using this memory of Meed learning so we're very you know inspired by this MH Bar who said that the way we understand the world is through analogies with things with things that we have seen and you know and you know if the situation is complex you need to do multiple analogies so don't ask what it is which is the language guided learning tells us but more ask what it is like so the takeway is we can compose concept by retrieving memories and we can adapt fast to new examples and you don't need to build different models for few short learning and many short Le morning like you see many papers noways they say few shot on the time and so on this memory of work the same way for men and few shot and this makes sense because you know when we see new example we're not supposed to know whether this is a few shot or not F shot example so this member of architect do this for you under the hood um yes so the next thing I want to discuss is compositionality through the generative and specifically we want to Mo from static 3D Point clouds to Dynamic SC and and those Dynamic will be the form of monocular Vos and the way we asking is can we use today's generative models positionality to Le monocular videos into of of mul objects since into 40 so we've already seen 2023 I think was for sure a year of open world 2D to 3D lifting right now we can uh lift it 3D any image using reprojection there on The View that we see as well as diffusion um prior diffusion of losses you render your differentiable representation to views that you do not see and you evaluate how lightly is that render for my diffusion model and this gives you additional losses that allow you to do this in the while 2 to 3D lifting right and the first paper of course that that is best paper award in iclr 23 drink Fusion okay that use the generic uh image uh diffusion model for for this uh you know score distillation objective and since then people have come up with a beautiful much more focused um objectives and here specifically we're using zero on to 3D where you condition the diffusion model with the view that you see to predict how alternative view points will look like and this is what supplies the sore distillation losses now unfortunately all those 2D to 3D lifting results are all concerning a object okay which is indeed is not surprising it's much easier to predict how a single object will look like from different view points that to predict how a whole symbol look from different viewpoints so the question we're asking can we use those object Centric generative priors to leave whole since um into 3D instead of a single object and and here is our our work that you know appeared on AR very recently it's called D scene 4D where we take moleular videos like the ones you see on the top to generate uh 4D 3D overtime um you know uh outputs um and the interesting part is that we don't do per frame lifting but rather those outputs are all connected so what we're using is 3D trajectories okay that connect essentially 3D points over time that are rented again the very idea they R and we use reprojection pixel reprojection as well as motion reprojection from The View that we see in the monocular video and we use score distillation losses for the views that we do not see and because score distillation losses are object Centric what we do is we would we de compose the video to apply the object entry cles and then we recompose it to make sure all objects are in the right place in the right dep so that three projection errors are you know are minimized so essentially the methodology of of this approach d c4d is first it decomposes a video into different objects as you see here we use a s segment anything plus temporal linking then we focus on each detected object and we use score distillation losses at every frame um as well as reprojecting rgp values and motion the motion of every object is explained through a set of 3D gocean trajectories so we have PIP plan features that are test time essentially trained to predict this placements D xyd for every point at every frame this is very much the same representation used in dream oan for the iclr 2024 and then we take all those objects and all those essentially 3D object TRS and put them together and do the projection ER so that you place them in the right place okay so I'm showing in this uh SL the original video and we show our reconstructions from the same view as well from alternative viewpoints okay and the important thing is that not only we see the rendered rgp but also you know you can look and evaluate the point trajectories and this is what we what we did now you may say why would you care about this why would you go and get monocular videos and leave them into you know 3D trajectories well this is extremely extremely important for visual imitation so if you want to teach robots how to do tasks and so on right now you need to use this kinesthetic teaching you need to move the arms of the robot or you need to use a teleoperation or other extensive let's say instrumentation and and if we're able to indeed to find great video perception as shown here accurately then we can infer how the world changes has the human human manipul the scene and then ask the to carry out a very similar manipulation in this way we can we have very very you know tense in time reward functions for our robots to for reinforce learning in the real robot platform as opposed to in simulation right so modeling F gr modeling of the V scene is extremely important for of course activity recognition and and robot imitation um and and here is some results that we show under heavy occlusions uh we compare here with a Cod tracker which is the method we really really like we were're very inspired a couple of years ago to for the task of tracking any pixel in a video so uh you know my student Adam Adam harle publish a paper called independent particles speaks and then contraer address contraer essentially pushes this to the Limit extend this by Tracking not independent particles but rather multiple particles at the same time and here I'm showing the results we want to see can we track pixels even better uh and and one big question we have is for the for the task of pixel tracking IND vide can we solve it in a fit forward way with big transformers train on insane amount of supervised data or actually we need to follow a inverse graphic approach which is try to imagine the 3DC behind the video and how the 3DC would move put as much PR as you can and render and would that inverse Graphics approach do better pixel D so so this is where this work is trying to to go essentially um and uh I mean we're not quite there I mean we are doing as I showed under heavy occlusions of course we do much better than the Fe forward approaches but there's still a long way to go here I'm just showing that yes our people prefer drink Gan for drink Ford over you know previous methods for scene lifting simply because there's no other preuse method that can LIF the whole scene there objects design for objects and once you the whole things brain uh and here without any training this is a training free approach this is like a test time optimization approach you are able to do very good tracking over you know C tracker and P plus plus other point tracking um and the last work I want to discuss is compositionality 2D and 3D unification um so so far one thing that that that is true and that actually bother us is that 2D and three division are kept separate and they also investigat are being investigated in different workshops from different communities with different mod architectures uh and we're try to understand actually I like the question where this is going where is the division actually trying to go here I mean clearly augmented reality and you know gaming and so on is One Direction so so yes for graphics it makes a lot of sense but what about for people that care about perception better understanding of images and videos how is 3D about helping in that in that way okay so this is what that's also what the previous method was trying to do use 3D as a way to get better video understanding more more fine video understanding um so one thing that we saw is while foundational model Foundation are taking over the world uh in scet the state of the art was still not a foundation model it was m m 3D trained only on scan and evaluate on scannet and then what we found is the following is that sced benchmarks actually use specific benchmark provided mesh example Point clouds as opposed to directly captur Central Point clouds where you just have IM with the dep and Camera POS that areain through some certain post processing and because of the tiny post processing the mix CLS are slightly misaligned with the input images so an approach that instead of trying to directly operate over those Point clouds instead operates of the images that actually give birth to the pl Cloud at the end of the day will be will suffer from that misalignment okay so so we believe that this is potentially one reason why 2D and 3D communities are kept uh let's say separate so we said you know what 3D data is not as scarse that people won't say all the time that we hear this standard uh quote 2D is everywhere 3D so Scar and it's so scars I mean it depends what you mean with 3D if 3D is just images with Cam poses I mean they're notal just some nothing can be moving so that's the other problem with division that you know the SC cannot be Dynamic but you know then you get get you get the death you get the B and you know what we can get a lot of this so in this conference we have this model called OD only dimensional inst segmentation that can both Tod images and 3D data and the 3D data here I mean rgbd images with camera F that's exactly what I mean uh so the way you think that that is uh is the following so has the following realizations 3D for visual recognition and not not for graphics just to be clear here only differ with respect to the positional en codings of the tokens by tokens I mean the PCH features so in 3 the positional en coding XY coordinates while in 2D the positional capture pixel coordinates because we don't have camera we don't have death and we can totally inter 2D layers attentions that use 2 position with 3D attentions that use XYZ position codings and benefit from using those two delayers because two delayers would have been pretrained on insane amount of of 2D images labeled with language um so this is what does so this is how it process the standard standard B with a M decoder much like you know the state-ofthe-art C segmentation models and then once rgbd images come in the only thing that changes is that there is a lifting 2 3D a projection and fusion and projection so the projection essentially takes the two tokens and uses the P of camera equation and C extrs to lift them into their XYZ coordinates that's all that projection happens there's no parameters there and it's a differential of operation okay now once you go to 3D then there's going to be a 3D Fusion which is a 3D attention um and the 3D attention because there are too many 3D tokens if you you know agregate across the whole scene we K&N relative attentions Cann Transformer which means we don't compare every 3D token to all the others but only to the 3D tokens in a radius around the to in the vicinity okay in the local neighborhood and the important thing is we use relative attentions meaning that and the aity between two tokens only depends on the relative Arrangement as opposed to the absolute XYZ coordinates because absolute XYZ coordinates are extremely arbitrary you can put the you know center of the system anywhere you li in the scene and by changing the of system you know you'll change the results of the absolute attenti that's why you need to be using relative attenti okay and uh and then once you in 3D now and you've F things in 3D you going to go and project so this is the the red layer again there's no parameters there but it is differentiable so that's why you can you back propagate through all these layers and the projection essentially takes three tokens and project them back to their corresponding Tod image locations so this is what Odin does it alternates between two 2D a projection 3D Fusion projection 2D Fusion a projection and so on and the 2D layers are shared between images and 3D data and the must be puts also shared so shared weights everywhere so this sharing weight helps dramatic for performance uh so now because scan labels and the that those Match Point clouds and because didn't uses sensor Point Cloud we're going to transfer the features uh there and put you know predict the labels and evaluate and propagate so the results look as follows um we were not able to beat a mask 3D in scannet uh scannet has 20 Level classes we came very close uh but we could not able to beat now at the same time evate on the C Point Cloud which is where we which the one that we're using it does much worse now once you go to I mean semantic segmentation work doing also better now as you go to scan 200 things change because in 200 there are 200 categories and many of them are Lo tail like they show up very early in the scanner 200 training data and there is exactly wherein uh WIS because it uses the super powerful let's say clip to the features it starts from very very important initialization so you'll see that uh for semantic segmentation uh no for is segmentation it does okay in the common classes but does much much better in the so so common classes and on the tail classes okay so anything that is not super common in the Benchmark OD does better thanks to two Dee training also Odin because it you know operates directly from the central CL you can use it as a perception let's say brain or body agent and here we have a helper this is our model for Alfred Benchmark or the te Benchmark is the state of thee art and we are able to improve the performance of our model simply by swiping the Odin you know detection as opposed to just do the detection and then post linking now having said that there are multiple works here open scene concept that also try to use Foundation features on 3DC much like but they do do this in a zero shot manner without being able to train this fusion and of course because it's also not zero shots right it can use 3D data it has better than those methods as we show in the paper so the takeways is that labeling and evaluation from three division benchmarks encourage right now 2D and 3D divide because of this smash Point Cloud that we just said but we do believe that three division should be evaluated and trained in the wild okay for bed agents robots that move around and so on uh 2 and 3D can be unified in terms of positional codings that as we uh said is one conrete architecture towards this unification and of course much much more should happen in that direction okay so the last thing and I think I need to close correct how many do I have any minutes or one minute okay yeah I just want to say that uh where is 3D uh useful so we had recently this learning from demonstration method 3D diffuser actor that combines s representations from that I just said takes to theut 3D use 3D relative tensions and so on for denoting robot trajectories and it directly went to the top of the benchmarks okay so so I do believe and this is a quite important because right now everybody in academ Industry does learning from demonstrations for robots you know there is this current robot of 2024 and 3C representations are able to to do so well and and it is very important what is the form of 3D representation it is tokenized right so every patch is a 3D token and there are tensions and in that 3D space as opposed to you know collapsing everything into single eding vectors that essentially you do not uh permit generalization because anything changes in the scene will affect that Ed so the summary of The Talk would be we can segment and track novel scenes with little to knowing domain training using compositionality rendering in Genera pre um thank you very much again to the organizers for this work and for giving the opportunity to present our work these are the three uh papers that I mentioned um and they all on archive and the first and the last also have a public available code right now so thank you very much loing work show that there's features you where you remove the 3D Parts yes the 3D Fusion right yeah Yes actually I I removed the app because I didn't have time but yes there is this ablation there and there is a big drop where in the instance segmentation not the semantic segmentation if the only thing you car is I want for this three depl to label a semantic label and you don't care to dis SE it into different inst is there's no drop this three diffusion is important only for instance yeah great question yes some object so the model also able to change the background for example as as very good yeah great great so this for the segments object and the background static for the background stic thing we use standard SL we use dust for initialization and then you know gra descent to find the right dep scales and Camera pose so then you have this nice background scene and then you have every individual object lifted to 3D and completed with SC distillation losses so the background is also there yes and as you move around you see different parts and again there is one part in this for I didn't mention is there is any painting part so the backround images are in painting because the objects create holes so to be able to change the you and see complete background you need to impt the background image as well as the image of object because the objects are often uded uh yes and we use standard um not B standard but we use the latest in painting models to you know go from partial View and to complete you thank you so I think but we'll be back here4 so unfortunately not able to than my say spot between so oh they should be out Mar right there it are can your Prett each don't that yes another have to think together the group how about you students that but then I I find that yeah starting yeah and then sometimes you can any point yeah have was sure have he was a part you Mone St any six resar I think yeah man and that's the but then I think they also don't I think it's flexible another we haven't hit that far yet so think could use prob so I'm sure I storage capacity think yeah I suggested it a to I'm actually yeah so this collab man system so my yeah it's not like it's easy to get J f spe for you show than not than that's I than to the school of AI and school for was ACH scientist research receiv PD from Stanford University under supervision his research specialized in Vision graphics and machine learning with focus on data generation processing and Analysis his academic serves include serving as a program Community member inra Asia uh 22 to 24 e Graphics 224 andic graphics and as well will talk about learning of please uh thank you for the introduction so I'm going to talk about this so before I talk about the my research composion I want toy what we mean by the composion know so we talk about the main things about 3D comp but what we exactly a for Le the comp so I'm LS I just try to look into some of the about the studies of the comp in the Linguistics and see how people talk about the comp Linguistics and I found this interesting kind of princi which is the com states that the meaning of a complex phrase is determined Slowly by the uh meaning part in the inex so is it kind very true that we are seeing this kind of the roofs in many kind of the natural enties that we using so what this means understand meaning and this kind of the provide kind example some kind artificial language which is not this so kind some examples for this so let think about some kind of system we are only having some kind of single single kind of making some kind of sentences with comments competing repeating so example sing file and this tce and open file and this might we some that means deleting the file so when we have this kind of langes and what we can see is that this isly kind of the the example that the the work here the m is not associated because when you see some kind of the multiple the know Nation so we can see that so this some kind of example of some kind of system which so while the kind of the can be synthetically deos into the kind of clicks we can see that the the mouse CLI is not associated we so see many kind of langage system we are seeing is basically not kind of the case so we can see that langage systems are Bally following this kind of principles in terms of that we can kind of the specificites into the words and good thing like this is that we can also some kind of sentences basically by coming you know compositing this kind of words and we can also try to understand some kind of sentences by having some kind of the understand meanings of these words and the thing is that we can see this kind of same sort of the rules in any kind of the 3D things as well the human the objects for example here we can basically identify these objects that a chair because we can see that this object is Bally com kind of compon form each CH you can see that this object is having some kind of and we part on and so when you see that the object is this kind of components compon some kind we can see so we can see some kind of same things for as well so when you see some the OB things like this many the things will be comprised of the kind of desks and so from this kind of some competion can see some kind so we can see some kind ofies between some kind of as well so then why we need to kind comp so weally see that some compl can be associated with the meaning we can basically utilize this kind of information any kind of application many place the same having this kind of compos use because we can easily meanings of each of the components and this kind of the rules can be us to some kind of for some kind of new compin of the first so talk about composition Linguistics there can be two essential comp so one is B the kind some kind of comp that comprise some kind of and the other might the compos so when you have any kind of of the they some kind of some kind ofes like ass kind of Rand combinations assemblers and chair like this so which means that uh rules the composition is very important so we can think of some kind of important questions in terms of the compos thetics the first question is that how we Define first so some essal building BLS and based on that we also def some kind of the compos rules how we can make some kind of combinations like this kind of words and other thing is that based on the kind of words and compos RS some kind of the new sentences or new phrase might be some kind of the essential this and we can think about the same question theion we think about how we Define some kind of components for we can think Define relationships some kind of corations and based on the person relationship this might kind of some questions in Al these kind of the three questions can be Bally associated with some kind of the three uh this kind of tests one would be spting the the pur and the second thing might be relating this Pur in ter running their kind relationship and the other thing might be combine these first to new object so I'm going to talk about these three M topics with some kind of the recent work for so I'm going to start with this sping the operation so in terms like this sping actually there are some kind of the Ms and these are the two Mains that I f one the first question is that what types of the components we are we going to Lear so what kind of the the the types the representations that we use to represent the comp of the objects so for this question the Bas answer might just having some sort of the region representations so from the very beginning that we start to see some kind of learning techniques like the point which of the person new network processing the point Cloud the easiest way that we can get some kind of the segmentation information was Bally loating each of the points with some kind of the labels so we could Bally learn the point seg like the reason representations for the components of objects so then we also consider having this kind of some geometries since many many human objects are on some kind of C models which is represented with the main kind of gec Primitives like CER Spar con or kind of things so when we have this kind of more like representations for components the good thing is that we can do some kind of the some applications into editing or manipulating like this and this kind of composition can learn from kind of C data set so now we have this data set or data set which is but this representation may not for some kind of the many other complex ships as well see many other types of comple ships may not be Bally precis this part of objects those kind of cases we can consider basically some some sort of the bounding which is not precise the service basically some kind of the information about how these kind of shapes can be some kind of abstract form in terms like some things kind comp structures some kind of that can be useful foration the operations some kind representations the key point so also compose the shap into some the some kind of key points this kind of information can be used to many kind of some editing oration the applications and especially for those kind of key Point presentation can be useful to person the uh the the non sh or or sh like or so when we have these kind of key points we can do some kind of the key Point based kind of the some information in the editing and can be useful any applications and speaking to the representations for the components one kind of the the key idea that I was want to use here was that they have some functional representations so this might be kind of example that we are some more General representations that can be represent many not only the part points the B idea is that now we are seeing shapes not kind of dat but it's kind of some of the domain that we can Define some kind of the functions over here so for example the key Point can be the cas we are having some sort of the you know sort of the function surface and one point only has be one and the other point be zero and also the the part can be represented as some kind some Vector right Bally having some like binary representations the the values of the service so in terms of that like having this kind of representation can be more like General representation that can rep other things and we can consider so having a Lear some sort of dediction or this kind of Def functions in terms de composing the shap into multiple parts so this can another way to represent this kind of the other question is that where obtain to learn is supervision to learn those kind of things so we can consider having some the explicit supervision or some kind of the some indirect supervision to learn those kind of things so the E kind of way to learn those kind of things by some more explicit locations so from the like the beginning like 3D Vision kind of the research we started to see many kind of the data set incl the shet or the part net which is providing kind of some sort of the reason based notations for person and also in this Workshop we are seeing also this kind of new the data set this comp the Plus+ data set which is providing lot of kind of the for for many kind of models but the challenges in terms of utilizing this kind of more explicit might is that because the oration the models is more like the canuti kind ofion might is the for example like some kind of Cas we can use the geom shap kind of the some supervision so when you have some kind ofes we can check whether this kind of the set of thees are fitting well to the given the geometric so they can be very weak kind of information that we can utilize to against some kind of the some The Primitives or we can also consider kind ofch this is the hour work which is Lear basically the te set the contr points that can produce all some kind of dations in this case like without having some exlic theis contr points by only using this kind of the machine we can also learn this kind of of the key point that can best of be kind of target shs al this is example that we are learning kind of the pur and also they Associated modation for the trans based on kind of animations 3 point CL in the S so this is an example that kind supervision to learn the not only the part but also Theo tions as well and one work that I want to includ in this talk is learning this kind of compostion some kind of holistic description so some part years they not only the you know time consuming and also the the labor intens in terms of the usual Bally know first this kind of interface but if we ask the people to Bally provide some kind of the description for each of the the the she they might be much easier because the human can say something about the sh without reaching any kind of interface and now we are seeing any kind of which is not providing any kind of importations or some kind of the Global Tax shap those are some kind of example which is the shade data set and we also extend this B data set into the much bigger S which is the shade data set so those are the cases that we are having some sort of the the tax descriptions for with ships so the the work that we presenting this2 the idea was that how we are going to utilize this kind of sort of the L descriptions for in terms of some parts so in this exle the input is some sort of the some description the tax description for the sh and then we are desing this kind of the part on that so here the key idea is that we are utilizing the reference idea in terms of collecting some the text so the thing is that now we are not only the T sh but some kind of shets and we two one is the speaker and the other one is listener so here this is that the speaker only knows the which one is the target shape and where theer is describe the target shape in a way that the listener can find the target shape of this kind of structors so let's this set so this know that this theet then what kind of the best way to describe this taret sh so there can be multiple ways that we can have some kind of description for example we can say that the target shape is the only one that has the armest then we can might say that the target shape is the the to is one or this can kind one example then the speaker can say that the target is one that before this so the thing is that if we collect some kind of some Global DET Tex description in this way then what we can see is that the speaker will try to Bally provide kind of description for some kind of the distin pur that can be used be find Target about the know distractors so basically this is our the key observation you you know some kind of taex the chairs for some here this kind of refence set up without having any kind of some sort of the interace that the US need kind of we can some kind of the descriptions B describ some kind of distinct Parts in these ships so the key idea is that we are collecting lots of these kind of the tax descriptions for the three ships using the reference schem and then we are making a year net that we rep place the lener in terms of defining Target sh the these CHS us some kind of attention so here the setup is that as kind of input we are providing like one sentence which is describing the T shap and then we are provide also the bunch of the ships and one of them is Target ship and we train Bally new network which is Bally the classification in finding the target shet given the text description and one while we we are making this kind of network we are having the B the model which will Bally highlight the parts which is associated with descrition here the idea is that we Tex description and also kind 3D model which is represented with kind of as a set of segments and then we having some kind of in for POS TX and shifts and having theen model which basically some kind of Pur the shap which is associated with each of the word the PO the text and then we are feeding this kind of feature into the the output the input of the text and then some kind of score like How likely it will be shaped will be the target shap thetion this is very simple the idea why we bun of the details in terms of the some implementation architectures so you can check out those details on paper uh but the idea is that if we have this kind of the some description that we can also get some really high quality some results without having any kind ofion for first and also in terms of some quantitative evation obviously we are not reaching some kind of address that we can get with this Lear but we are still getting some kind of 8% kind of address for the segmentation and the good thing is that this can be also you know generalizable to to be any kind of other other kind of categories as well and we use the same idea for the so this was kind of the one idea in terms of how we must the 3 object the first so if consider many different representation for the first then also think about many differ some kind of the some strong supervision Orion kind of uh so next part is how we going to kind of Al there are many kind of interesting the data set in providing some kind of the information about the relationship of but the our Focus was to see how we get know obtain this kind of relationship information Parts without having any kind of explicit supervision so in this example we focus on learn some kind of variability in terms how the other part should vary in terms of the PO of uh so for that in our Bally we basically try to basically learn this kind of met handles which is basically giving some kind of more intuitive handles for the deformations while also capturing some kind of the some full variability the you can see that there are some kind of the here so depending on how you control be handle you can see that we can make some kind of more intutive information uh so basically the uh the the differ we some kind of PR work is that previously we using some kind of the key point or some kind of as kind offormation handles which is some like geometric handles which is not some meanings but you have this kind some intutive de we canally factorize some kind of the while some kind of the corelations or some kind of function so the Bas idea for that just be Lear some kind of dictionary some kind of Point Ms kind of condition idea so if I skip some details here so idea is that know given some kind of the control points we are some kind of met handles which is represented as some kind of the combination with translations of the cont points and then the final information is represented also the combinations of those handles and to learn this kind of things us we are using some kind of the conditional Dan the architecture which is producing some kind of the set of the Met handles and produc some kind of random formation and theator is checking whether the random information is really making some real shaps and also we having another some part of the your network which is basically you know avoiding to make some kind of the you know the rest of the shape that's kind of the outut of the the the de isue those are examples some kind of the uh um some intive tations yeah so the last part my about the combined kind of part so I very for this part so let me with this one so I say some kind of the some other work which is like generally some kind of 3 R some parts and also those parts to some kind new sh but those are cases doing some kind of retrieval to basically produce some kind of new shape so those are all the cases that we are making new chair by ring to theart from the given database so here the question we have is that how can youate some kind of these shs by generating both the kind of part structures and both kind of part so see some kind of the recent work of 3D generation models those are the cases that we are 3D shapes with some kind of representations of the point cloud or the BS so for those kind of cases we cannot do some kind of editing with out so here the question we had is Bally that we had in our the ICC paper was how we Bally utilize some kind of the some part level some representations in terms of that we are making some division model can produce many kind ofers ises but also allot some kind of divers applications like the part level the completion or the popity mixing is taking the red part and the green part and making some kind of new shape and also do some kind of Tex generation and also the Tex guide depart so in this work the main idea is that we are basically utilizing kind of the hybrid representation which is basically describing both the global the level structure also theet as well so we this kind of and also some kind of feature that can be decoded specific so train is kind of a diffusion model to that this kind of representation we kind of tated models which is first producing this kind of some explicit structure in the part level and basically also producing some kind of the implicit some representation for def the the geometry so here the key idea is that we having the taking advantage of the both explicit and Inus representations in terms of that we can apply some kind of the some editing some kind of the techniques using the IM models so we could see many kind of recent work which is utilizing some kind of the pixel space to the model for some kind of in painting or some kind of editing which is over Cas is that combine some kind of the for process the reverse process theion models so the good thing of having some sort of the hyrd representation of explicit and implicit representation is that we can apply those kind of the techniques for the 3D as well some kind of the compostion or some kind of part missing while B achieving basically the generation so yeah those are kind of some list of the work that I also work of the comption this read and I think there are many kind of open Pro future research like one of the open question is that how we can also kind like scale up this compos so we Bally use some kind of which learn some kind of the components based on some kind of the some language description but we also the case that we collected this kind of data from of the people but how can you also utilize kind of the some Foundation models toate those kind of things to learn the compos so in our paper we present on the thir we also Bally going to present this work which is modifying this for dis sampling in terms of the we can utilize the IM priers not only for the generation but also for the editing as well this is kind of example that we apply the technique so compared to the previous work the good thing is that our work can some kind of some GE changes Bally like liting it copy so those are some kind of Community cases in contact Ting the N another example and the same technique can also be applied some kind of the factor changing the to into the space so this is the work that we're going to present in our DP that will be presenting the Thursday uh we we also into some kind of G Sy terms that applying this kind of indent not only for the N but also for the imaginating as well so this is example that we are making some kind of deformation only information we going to see this kind of some some this but if we use this kind of some IM def priers then we can see that we can fix this kind of the WR kind of informations so this is also the work that we going to Dev andion models may not be the only product that you can use also consider many other type of the some the trials from some kind of the foundation models we also seen lot of the examples that we using the model or some kind of to basically learn some kind of compos structures from the and the second is that how also learn kind of complex some rules so if we see any kind of objects you can see many complex structures like some symmetry or some liting patterns orm many other things so when think about this kind of some complex rules what can be kind of the goodvision the sour of the supervision that we can use so probably like some Foundation models can be kind good source think about like you know learning those kind of things from the many kind of projects and finally the the uh the last question might be that how we also you know incorporate this kind of some kind of Pur and competion rules for models so main kind of model that we are seeing is basically generating some kind of 3D shapes by learning some kind of par geometry but if you can start to see Bally learning this kind of the compos the structures in the 3D generation then I guess we will be able to see diverse kind of typ shapes that was Bally not possible with the current models so this is the uh my end of my talk so these are the list of the papers I this thank you for l need are now of understanding theous driving localizing regions were par how can we do this stage yeah I think the challenge here now might is that so we are having like lot of kind of be some some data set or information like learning the composion for the three things like for detecting the c or some kind of tre kind of things I know that there are many kind of data set but as we go down to the More Level having some DET small pieces of the parts in the objects I guess it becomes much more difficult to Bally have some kind of some data set kind for them so I guess how to learn those kind of things especially without having some kind of the some manual the effort for that colleting kind of data set but using some kind of the other source of the the supervision might be challenging but also intered questions I don't know maybe some kind of this IM segmentation model was kind of but kind of good source to those kind of things but here the CH might be like how U like this kind of some Qui for thank you very much very interesting I think I have a question about thisen and yeah for question so in this work uh I remember that we used the like thousand kind ofes to get the part but for this specific CL the ships so chair we think like thousands uh kind of the the the this but we also collected the much bigger size the data set for many class of the ships so we Bally the work that I presented yeah this the shap was kind of the case that we collecting more than like uh I remember like hundred thousands of the for the third classes so they can be good for that but question is that how we can also can also use some kind of the other types of some caping kind of things uh the thing is that so in this the key thing was how we can get some kind of Quality Pro uh to this kind of the the test if we have some very description and train model then we don't get any kind of results because because the description describing too many different person the happens so here the key was how we that destion for the one specific part or distinct part with the other the the data and for that weing this kind of set which is the refence not single person but two people in a way that the speaker will try to say some distinct person that can be will be highlighting so we spend of to get some kind of destion which is describing some e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e M it so how about now is this better yeah so so these kind of drag based editing approaches the limitation so far has been that if even if you define a bunch of points on the object the resulting edit that you uh that you get uh looks a little bit like this right so rather than achieving the kind of goal that you want which is to translate that particular object from one part of the image to the other uh what you end up doing is something that looks like this so it's not very easy easy to extend these drag based approaches for compositional editing of images so in this talk over the next 20 minutes so what I'm going to try to do is to give you a couple of examples from our group where we're trying to address these limitations the first one is about compositional Dynamic 3D understanding where we are you know trying to understand Dynamic scenes but not just Dynamic scenes as a whole but also the parts of this Dynamic scene uh in particular I'm going to be talking about grasping object grasping uh and in the second part of the talk I talk about some um new methods that we've developed for compositional 3D aware editing of uh objects in in images all right so let's uh let's start with the first part which is about compositional Dynamic 3D understanding so grasping is actually a pretty fundamental human activity uh imagine how many objects you've grasped since this morning it's probably in the tens and maybe even hundreds right uh so you know we grasp a wide variety of objects hundreds of objects every day uh these objects come in all sorts of shapes and sizes some may be articulating non- rigid and so on and so forth so unsurprisingly this is a very important problem of interest in computer um and you know we've we've seen a lot of Prior work that kind of does this kind of work um so you know I talked about these upcoming Radiance field like representations uh so you know can't be just model Dynamic scenes grasps uh human Gras hands grasping different kinds of objects using these Dynamic Radiance Fields uh yes we can and you know at the cvpr uh we have a paper that that that essentially a data set of dynamic scenes and a benchmark data set that shows how many Radiance field methods operate with this kind of dynamic data however the disadvantage is we don't get any kind of compositionality so we we kind of reconstruct the whole scene as a whole uh we don't know anything about the parts of the scene right so that's something that's missing in the context of uh classical representations if you will so things like measures template models and so on there's been extensive amount of work in computer vision that study is grasping uh and I'm not going to go over all of them um but there's there's probably like a couple of decades worth of work that tries to understand how humans grasp objects given applications important applications in robotics mixed reality and so on but the limitation of these these approaches so far has been that you know the representations that we are using today uh most common representation being meshes and template models U they are not particularly well suited for grasp capture because we uh first of all we kind of need to have a template uh for the hand and the object so we kind of need to build this compositional representation for different parts of of the grasping activity here and even if we have these templates they don't end up being very good models of the underlying um data that we have which tend to be RGB images so this is manifested usually as these misalignments that you see between the mesh and the underlying RGB images that you have right so that's something that's pretty common so in our lab we've we've kind of been thinking about how to uh exploit Radiance field representations uh to to as representations for grasping um and in particular I you know we think that Radiance field representations are particularly well suited for the grasp capture problem because Radiance fields are pixel aligned by construction so we can help avoid these kinds of misalignments that showed earlier with respect to these template models and the underlying RGB data and in addition they also have the advantage of being photo realistic and and so so uh I'm going to quickly talk about a paper that's going to be presented at cvpr on um on Wednesday tomorrow um which is about marketless grasp capture using articulated 3D CS which is a kind of Radiance field representation you can replace this with nerves or your favorite Radiance field representation the idea is the same and here what we'd like to do is we'd like to capture human grasps at a very high level of detail and we'd like to capture certain components of human human Gras we'd like to decompose grasps into these uh components so what are these components we are really interested in objects so you know as a we don't want to capture the grasp as a whole but we also want to understand what the shape of the object is what the appearance of the object is uh we'd like to understand something about the pose of the hand how the the shape of the hand and finally the contact surface between the hand of the object right so that's something that we care about so these are the three components that we want to decompose videos like this into and and this was work that was done by chandad who's going to be presenting this tomorrow morning that's paper 196 if you'd like to know more about it all right so let's let's say that we have a grasp scene right and and you'd like to reconstruct this grasp uh scene um the first thing that that we might be interested in doing is to try to understand something about the object so given grasp sequences we can use off-the-shelf tools like Sam to segment hand object separately right and once we have segmented the object separately we can then build these kinds of Radiance field representations so for those of you who may not be familiar in this example we are using 3D gussian splatting uh so essentially we model the object as a collection of hundreds of thousands of 3D gussian functions each function has attached to it some properties including mean covariance opacity the something about the shape as well as about the appearance of of these objects um each of these ganss can then be projected onto the source image a process called splatting or rasterization um and that results in a a pretty good reconstruction of the shape and appearance of objects so the Reconstruction that we get looks something like what you see here on the right okay so you know we've successfully reconstructed objects in these graph sequences uh but you know there are some limit so firstly because we are using these Radiance field representations we don't have this pixel misalignment problem so we're able to reconstruct these shapes pretty accurately but the limitation so far is that these representations can't yet capture articulating shapes objects are we focusing on Rigid objects here the hand is is is is an articulating shape we can't you know extend the same representations to the hand readily so in order to fix that problem we extend these ran field representations to also capture articulating shapes um in this case this is again 3D Gan splatting given a hand that we might have in a grasp capture scene um we model the hand as a collection of 3D gaussian function similar to the object um and we then attach these gaussians to an underlying kinematic skeleton uh the skeleton drives the position and the co-variance of these gaussians uh and and given a driving pose so that's the articulation of the fingers of the hand uh we can then um you know POS these 3D GS in this new post that we want to look at the hand at uh and then finally Rize this to obtain some kind of a representation of the hand and this whole pipeline is trained on a lot of training data um using some loss functions which I'm not going to talk about uh but essentially this you know using this kind of an approach we've managed to extend uh this representation uh to also model articulating shapes okay so here's here are some examples of how the reconstructions look like as you can see this captures both the shape as well as the appearance of the hand um and as the hand articulates we're able to capture the details pretty well and this is all being driven only by this underlying kinematic skelet the pose of the kinematic skelet okay so what do we have now given grasp sequences we've managed to reconstruct the shape and appearance of the hand we've managed to reconstruct the shape and appearance of the object uh and we have an articulating model so we have decomposed the input data into components now we can recompose the whole thing together to accurately capture grasps as well as Associated things that that that we can capture including contact area and so on so the way we do this is you know we uh we drive the hand model using offthe shelf hand POS estimators and given grasp sequences we can successfully reconstruct using the techniques that I've described earlier both the shape as well as the appearance of the hand jointly so we can compose the two different methods that that I talked about uh to jointly obtain both the hand as well as the object accurately and the advantage of doing this is that we can now start to reason about where contact happens right so being able to compositionally reason about these Dynamic scenes enables us to extract information such as contact points so the heat maps that you see here on the fourth column uh denote where the hand is making contact with the object where on the hand contact is being made and the heat maps on the object uh show the the the riverse which is where on the object the hand is making contact so we can kind of make these inferences simply by being able to decompose the scene into into it in into its constitu we can also accumulate the whole thing over time these sequences are temporal sequences so we can obtain like an accumulated heat map grasps are not static as we grasp an object the contact points change a little bit so we can kind of take that into account as well okay so here's a few examples uh what you see on the First Column is our data um so those are this a multiview grasp capture sequence so we capture a lot of that data and what you see in the second column is the Reconstruction of both the hand as well as the object um and we can and because this is compositional we know the hand separately from the object we can actually make estimates about where contact happens on the hand which is the third column you see as a heat map as well as on the object that which is on the fourth Colum all right so you know this this kind of uh uh to to study this kind of problem uh we rely on a lot of training data uh and to obtain this kind of training data to to build these Radiance field representations we need a lot of views and in order to do that we built our own camera capture system I'm not going to talk about the details here uh but essentially this has 53 cameras that that is designed exclusively for capturing and uh interaction okay so uh you know we we have this data set which is called the Manis graph RPS data set which is now public um and um this data set has 53 camera views uh overall it has about 7 million image frames so it's 7 million image frames showing different kinds of grasps uh it contains ground Truth for the hand shape hand pose object shape uh and most importantly it also has ground Truth for contact so how do we measure how good the contact is so this is something that we infer from separately reconstructing the hand on the object so how do we how good is this so in order to evaluate this ground Tru contact we took inspiration from uh work from the 80s kamak Kuran colleagues uh did studied the Hand by uh studied hand grasps in particular by painting objects in wet paint and then they would ask participants to grasp these objects uh the wet paint would leave a residue on the hand uh and you can then Analyze This residue to to infer patterns of grasping so we kind of took inspiration from that and in this uh in what we did in our data set is my students painted these objects with green paint green wet paint and after the completion of the grasp it would leave a resio on the hand and you can use automatic chroma keying tools to automatically figure out segmentation of how good the contact is and this is actual ground truth because this is um you know there is a paint transfer only if there was contact between the hands of the object ignoring capillary action so this allows us to kind of build like quantitative metrics for evaluating how good grasp capture methods are all right so so that's the first part of my talk where you know we we did a little bit of work on um you know using these kinds of decomposable representations and then recomposing them which gives us interesting pieces of information about grasp contact points and and so on and so forth of course there's a few limitations So currently we are limited to only rigid objects so extending this to non- rigid and articulating shapes is is an interesting problem um extending this to multiple hands uh and also looking at more complex grasping and manipulation could be interesting for the future and that of course involves a lot of different objects different hands which we need to take into account okay so in the second part of my talk uh I I I want to introduce uh some so I know the first part of my talk looks at the dynamic scenes question right so we have this new emerging Radiance field representation for dynamic scenes uh how do we also incorporate compositional understanding into these Dynamic 4D representations if you will right whereas in the second part of my talk I'm going to discuss some recent work we've done on editing 2D images but doing so in a compositional and 3D aware way so that's the the second part topic that I wanted to discuss so talked about examples uh previous examples of these drag based editing approaches so these are approaches where like drag drag diffusion uh dragon diffusion dragan and so on that enable us to edit images based on point selection so you can select a point on the image and ask the point to be somewhere else and and this produces an edit that is that is reasonable right um there's there's several kinds of approaches but the limitation of these approaches I I showed earlier is that they don't respect uh you know object classes or they don't respect objects as entities they kind of treat the whole image separately so it's very hard to um you know do these compositional object based edits with these approaches uh there's there's also other handle based editing approaches which suffer from same uh disadvantages and also they need depth information as well as language descriptions in order to successfully perform these edits so we kind of wanted to address these limitations and we were thinking about what would be an ideal object-based compositional image editor so what would be some of the things that we might want to do with something like this right so let's say you have an image in this case an image of a car on a road we may first want you know users May first be interested in figuring out which part of this image they may they want to edit so let's say they want to edit uh a the car right uh so this could be easily achieved using um you know state-of-the-art segmentation methods like Sam right so it's very easy to get the segmentation masks they might then want to specify that they want to move this car using sliders they may want to specify some kind of 3D translation for the car so for example they might say I don't want the car to be on the left side of the image I want it to be translated in 3D to some some other part of the road so that's something that they may want to do um or they may want to rotate the car in in slightly different ways and so on or they may want to completely remove remove the car from the image right so these are some of the compositional editing tasks that they may want to do so in this work that I'll briefly talk about uh my PhD student Rahul has developed this method called geod diffuser that tries to achieve these kinds of compositional object based edits so what what can geod diffuser do um so given input images U geod diffuser can successfully perform 3D translation so users can specify that given selection of an object how do we translate this object in 3D so that's something that you can do with geod diffuser you can do it with different kinds of scenes as well you can also do 2D translation um so you can specify that this object in this part of the image has to be moved to a different part of the image you can also do 3D rotation so you can say I I have this dog here but I don't like the I want the dog to look at the camera right so you can kind of achieve these kinds of 3D rotation edits as well so again I'm not going to go into too many details here because I think I'm almost running out of time uh but essentially we followed the general image editing uh framework of ddim inversion uh and the idea is as follows um so given some kind of latent noise Vector that you have here you know we follow the reverse diffusion process but then have two par trajectories or two parallel paths the first path is is what we call I think I lost the mic here I'm just going to be loud uh the first part is what we call the reference diffusion process uh which is uh essentially just the you know you can think of this as reconstructing the original image back from this noise Vector right but we also have a second parallel edit diffusion uh process uh which is responsible for achieving the kind of edits that we want to do that the user has specified so the reverse diffusion process operates as as diffusion you know typically does the the only difference is that the attention layers that we have in this diffusion model are shared between these two parts so the reference diffusion process and the edit diffusion process share this attention layer right and the benefit of doing that is that it turns out that we can cast these kinds of user specified edits as transformations of these of the attention layers and I'm going to talk about that in the next slide but by casting these as geometric Transformations or features of this attention layer we can achieve the U the desired edit that the user has requested so how does this Shar attention layer work so we have this attention layer that is shared across the reference diffusion process and the edit attention process uh and essentially we take the user specified Transformations and then convert them into geometric Transformations for the underlying attention layers so that's what you see here on on the on the top here so in the reference diffusion process we perform the diffusion but then we transform the attention layers depending on the the kind of edits that they user is requesting so this could be 2D translation but it could also be 3D Ed so it could be 3D rotation or 3D translation and again I don't have time to go into the details you're happy to chat about it offline uh but essentially this with with a lot of other stuff that that is part of this pipeline uh we are able to achieve these kind of attention sharing between these two parts and that results in the kind of desired edits that we uh want to do now one of the challenges is that there's disocclusion so when you move an object you also need to fill in uh the missing part so we we are able to uh do that as well okay so the most important feature of geod diffuser is that this is a completely inference time optimization method right so there is no fine-tuning or pre-training involved you don't have to train the diffusion model you can use any kind of diffusion model that you have access to right and and this is you know it's kind of like a test time optimization strategy here and our loss function incorporates losses that try to preserve the background that try to remove the object object successfully and recompose the object successfully in these images right so there is no training or or fine tuning that that you need to do so here's a few examples on the left you see the original input image these are natural images and on the right you see the kind of edits that that user has performed so these are all examples of 3D edits so 3D translation where we've move different parts different objects in this image to different parts of the scene and one thing to note here is that our method is successfully able to handle shadows and Reflections uh which we found quite surprising so in this case we've managed to move this uh lounge chair from the right to the left and you can see that the shadow kind of moved with the chair as well right so that's something that the method implicitly takes into account uh we can also do uh you know 3D rotation so in this case you see that the dog's head has been rotated the car has been rotated we can also rotate different Furniture pieces of furniture or animals and and so on and so forth and then finally we can also uh remove objects from images completely um so in this case we managed to remove some of the objects and and again here I I want to highlight that you know we're able to remove Reflections and shadows successfully so in this case the U the coffee Mark the shadow of the coffee mug has uh disappeared completely okay and the same thing you see that with the boat and the shadow on the water there all right so uh we did a perceptual study comparing it different methods again I'm not going to talk about the details here but essentially almost 85 plus% of the time people prefer geod diffuser to other competing methods uh as well as other methods for inpainting and so on okay so here's a few comparisons with existing drag based methods uh and again you know the the importance of understanding or operating on images at the object level uh is kind of evident here right so using drag based methods results in um results in edited images that that don't have don't preserve the original uh object as well as our method does this is also scalable so you can use any diffusion model that you like stable diffusion 1.4 2.1 doesn't really matter so this actually works with any kind of diffusion model or any model for that matter where you can use this kind of attention sharing mechanism okay uh last couple of slides so limitations so there's obviously some limitations of our method especially when it comes to 3D rotation uh because we need to with larger rotations we need to inpaint a lot more of the object which gets uh pretty tricky so that's something that our method isn't able to handle as well so that's something that we're working on sometimes we see aliasing artifacts that we're trying to address and and in painting isn't always perfect especially in these difficult kind of scenes with a lot of texture and so on all right so takeaways from geod diffuser um for me personally I think you know geometry looking at image editing from a geometric perspective and doing this in an object Centric way allows us to you know unify different kind of editing tasks into a single uh method and that's kind of what we showed with geod diffuser and having this compositional capability enables us to you know perform these edits the way the user desires to do and you can kind of extend this to multiple objects in the scene and so all right that I'd like to thank you for your attention um and and thanks thank my students and sponsors and if you have any questions I'm happy to take them yeah we've done a few experiments so you know one of the things that we can try to do is let's say you want to rotate by 45 degrees you can divide that into increments of 5 degrees let's say right right I think the problem that we encountered was that um the errors accumulate so every time we do the editing right so the image degrades just a little bit right and as you keep doing it over and over again the degradation results in images that maybe not are not at the level of quality that we might want to have yeah yeah I think there's definitely more more things we can do there yeah question these like the weights are never touched um so it's exactly like I described there's actually not much more technical detail apart from the details of the attention sharing mechanism um so we don't touch the weights at all so we simply transform the attention layers using the geometry transform from the user specified datab right so there is no change in the weights you can use the diffusion models as is without any any change thank you uh so next on our agenda is or presentations uh we're going to have seven papers presented so each presentation is going to have 12 minutes and incl questions and we need to make sure uh okay okay thank you uh hello everyone my name is Jam uh today I'm going to uh discuss our cvpr paper named the Gast grading field based autog graph sample for 3D part grouping uh due to the traveling issue I can only deliver my presentation through the online format and we also include our paper Link at the corner here so you can scan the QR code okay so let's start on my presentation so uh to brief introduce the contents we will start with a brief introduction to our proposed 3D part grouping task and secondly we will introduce the grade and field based uh autographic samp gas to solve this problem and we will finally introduce the Benchmark which includes the qualitative results and the quantitative results for this task so let me start introduce the definition of 3 part grouping task so in here we uh uh imagine you have several shapes here and this shapes are disc constructed uh with the parts but uh interestingly this parts are occasional or something else just by pae mixed together and shuffled into a mixed part set and now we want to use some algorithm to automatically separate them and find out all the potential groups uh uh from the mixed part set so this is very challenging for uh two uh for for two problems so first we don't know how many groups uh and here this is unknown so we don't so this brings uh difficulty to Define output of the grouping algorithm and for second reason we also need to uh use algorithm to understand the relationships among all the mixed paths so we uh actually we we believe the 3D part grouping may have some potential applications for example in archaeological area it is quite often that pieces are mixed together so imagine the piece here is the path of some chairs or tables and we can use grouping algorithm to find out the uh artifact pieces and separate them into the groups and uh in a very similar way when you get some Mass room or kitchen we can also use 3D part grouping algorithm to do the item organizing here and so we that's the application potential applications of fre part grouping but in our paper we use the 3D parts to do the add all the experiments so next I will introduce our main algorithm called the gradient field based Auto regress regressive sample gas so for the core idea we use a probabalistic method to model the 3D part grouping problem so to be specific we use a Boolean Vector C here to define the groups and we try to estimate a conditional distribution p c condition on P where P represents the input mixed the part set so let let me show you uh more into uh more intuitive way so we have the mix the part P here and actually all the group groups can be represented by a selection Vector so for example here we have uh in the first place here is two and we have the groups here is we include this parts and for the second part uh here and we can also find here it belongs to the group N and so on so forth we can represent uh all the group with simply with the selection vectors and uh please bear in mind with this definition we can actually uh use a problemistic way to model our problem so imagine here we have the uh all all the possibilities of selections but only some selections are valid so we can construct it uh conditional distribution that's the function here which is a discrete and we have the input mixed part and we can uh we can we have the selections to model this probabilistic uh function and with this definition the problem remaining is to use some tools uh to uh solve the probab uh conditional probability estimation problem and we borrowed the uh the recent proposed uh score based or to say the gradient field based method to constructive way uh our architecture so we first use a Point net to extract the features from the input parts and we got the per part features fpn and then we use a gradient field based selection G to construct our uh log gr gradient density function so the G here is applied to understand the relationships of these perath features which is uh identically understand the relationships of input parts and at every iteration we can obtain a selection vector and then use this selection Vector we can obtain our group N and we also have the rest pass PM Plus one we repeat this procedure until we obtain all the groups or the end reaches the maximum iteration so that's that is the uh implementation of our proposed the gas algorithm so in terms of the training and sampling as stated before our goal aims to estimate conditional distribution with score based model or to say the gr and field based model so in this case we can apply the standard score me training and samping algorithm so our implementation is use the score based model with sdes actually you can use other implementation such as uh diffusion model leure okay okay so we have the here we here demonstrate two algorithms which are the uh standard score based algorithm training and the the predicted correct sampler for the sampling procedure for more details you can refer to the original paper of score based models with SDS so now let me show you the uh some qualitative results so we except from the G we actually propose another two uh potential algorithms and uh with the comparison uh the comparison here show that only our Gast algorithm is able to produce the correct uh groups based uh for every possible input set it is hardly possible to uh for other algorithms to produce the correct groups so next we also want to uh quantitatively evaluate all the methods and in our evaluation we use Precision recall and F1 score to evaluate algorithms and in the evaluation procedure we match the ground truth groups with the closest groups and we can calculate TP F1 FP and based on this informations we can obtain our prision recall and F1 SCS and uh now let me demonstrate the quantitative comparisons as you can see the gas algorithms uh achieves the highest performance compared with other algorithms or gas variance so we also conduct uh appliation study to research uh the important components how the important components affecting the performance of G so uh in the table on left hand side we remove the gradient field we replace the gradient field with simply uh uh mean squared root uh root fun uh loss function and uh we also remov the graph neural network to test whether the uh algorithm can learn without graph learning and as you can see uh the performance has a great job down when we remove these two important components the table on the right we test the sampler with different Samplers and different number of steps and the results demonstrate that the predictor correct sample achieves the best performance with the sampling steps at 500 steps so here is the reference for our presentation thank you very much thank you so much do we have a question for a speaker okay no okay thanks uh let's move to the second talk hello uh so can you hear me H yeah okay okay thank you so the second talk is going to be uh about the learnable Earth parser and discovering 3D pypes in Aeros scans and the speaker is uh guess sorry about that sound sorry perfect okay so hi everyone I'm Roman and I'll present our work entitled Lor disc in 3D prototypes in scans thanks a lot for the organizers of the workshop for inviting me so to analyze 3D Li scans most methods either require very expensive 3D annotations or for unsupervised approaches can well rely on abstract feature representations that are often difficult to interpret and furthermore most of existing methods are often designed trained and tested on synthetic or highly curated data so in this paper we introduce an unsupervised framework for are passsing large real world to the SC into interpretable Parts alongside a new uncurated annotated aial ligher data set acquired in various environments so to perform semantic and instance segmentation directly through a construction we learn 3D prototypes alongside a way to position and deform them to reconstruct inputs our 3D prototypes are P clouds whose point positions are directly learned as threee parameters of the network along inside the prototypes the prototypical point clouds we will also learn neural network to select deform and position them uh to uh to to reconstruct input scenes so as you might expect learning jointly TRD prototypes alongside a way to select different and position them uh is very challenging uh because firstly we have to deal with a large variety of objects across different scenes and secondly it is very difficult and challenging to separate objects in a scene and each scene may also have a varying number of objects so in this presentation we'll focus on dealing with ambiguous object separations and varying number of objects in a previous papers we showed that we can explore the content of very large 3D Point Cloud collections by learning 3D deformable prototypes and that the learn prototypes represents well the diversity of objects contained in the data set so today our goal will be to go from one object to more complex scenes to learn to decompose a scene into simpler and easier components um so to achieve this we will train a highly constrained model to approximate a point cloud with a combination of learned 2D shapes so we said that we are able to reconstruct individual objects let's try to to do this uh for a scene so to reconstruct a scene composed of numerous objects we introduced slot models and each of the slot will be responsible for reconstructing one object one specific object instance in the scene so at this point construction model will just be the union of uh the output of all the slots so to Handle Variations of similar objects we will set the case output of each slot as a different version of the case prototype and our model can now deform and position each prototype but it cannot deal with varying number of objects in the scene uh as it is forced to reconstruct s objects and cannot select the prototypes to be used at each slot so to deal with this we will use a probabilistic modeling and each slot will be associated with Will predict from the input an activation probability alpha s which predicts whether the slot will be used in the Reconstruction or not and we do the same for for the Prototype selection the Prototype used for the slot is determined by a random variable BS um yes and finally the complete reconstruction model is a union of the activated slots where each slot output the deformed and poed select selected prototype so while this model can reconstruct very complex and diverse sces how how can we Lear can we can we learn alongside the prototypes the deformations and the activation probabilities so to learn the full model we use a probabilistic modeling of the Reconstruction loss the first loss encourages likely reconstruction of MX to accurately approximate X so it it encourag it encourages activated slots to be close to to the input um yes so to be close to actual points of the input and the second loss ensures coverage of the input by encouraging each input point to be well approximated well reconstructed by at least one activated slot in the end we use a probabilistic modeling of the rec construction loss with regularization to ensure that we learn meaningful and accurate prototypes so to evaluate our model we introduce the Earth passer data set a data set of seven annotated scenes acquired by a liers in various environments so it contains the variety of urban natural and Rural scenes making it more representative of the diversity of possible use scenarios than comparable data sets so train on different scenes we we show that our model can learn a wide variety of prototypical shapes such as Boat Boat Mast win turbines or or green houses these prototypes provide an immediate Insight insight into the content of these Real World scenes the highly constrain nature of the deformations prevents slot from repurposing the same prototypes for different objects making the learn prototypes easy to identify to identify and manually annotate so after annotating the prototypes which is very easy because they are very few Point clouds uh we can use a reconstruction to to reproject U The annotation onto the input scene to perform convincing semantic segmentation of complex sces and by considering each each slot each each reconstructed slot as a different instance we can also perform instance segmentation of complex scenes such such as a marina here for example so we evaluated our model against others by comparing the semantic segmentation and the Reconstruction performances of different methods and for example despite its Simplicity a basic cing algorithms on the altitude and intensity of the 3D input points provide a strong semantic segmentation performance super quadric method that lears to approximate scenes with an adaptative number of Super quadric can adust the number of Super quadric it ches and to some extent their shape but because it ches a single parametric family for all prototypes it cannot provide us with semantic segmentation and fails to reconstru to reconstruct complex Real World scenes DTI Sprite nonsupervised image decomposition approach that decomposes elevation image into a set of 2.5 this price as a lower construction and segmentation quality which is to be expected because it doesn't leverage the full the full 3d of our Acquisitions uh atlet V2 uses a fixed number of L prototypical Point clouds to to reconstruct inputs it provides good reconstructions but semantic segmentation fails because because of its inability to adapt its prototypes usage to the input finally our method thanks to it thank to our probabilistic slot and prototype selection we are able to handle inputs with a varying number of objects using only a small set of learn of learned prototypical shapes so finally in this paper we showed that our novel un supervis method for passing complex real world a scans provides strong semantic segmentation results while maintaining interpretability of the prototypes so we also introduce the data set of seven annotated scenes acquired by a ler in various environments designed to evaluate our methods ability to pass complex re scenes and uh well to the best of our knowledge we are the first to demonstrate the possibility of Performing deep unsupervised 3D shape analysis on such challenging real world data sets uh thanks for your attention and I'm open for questions any questions right thank you so much thank you let's move to the third talk which is going to be e e okay hello everyone my name is Le I'll present our latest uh CPR work uh genz is the first zero shot approach to generating 3D humans in interactions J does not require any captured uh 3D interaction data or 3 learn given an arbitrary 3D scene our approach can synthesize 3D humans interacting with the SC from a short text description like sitting in a ritual walking on a bridge or sitting on the ground pick and thinking generating realistic humans interactions is a changing task it requires holistic semantic understanding of both the environment and possible human actions therefore existing approaches uh rely on carefully captured data of real people interacting in 3D scenes for supervision however collecting such data requires expensive complex setup leading to a limited scene and action captures in existing 3D interaction data sets this further limit existing approaches to indorsing interaction synthesis we thus consider an alternative perspective and POS the following question can we achieve plausible 3D interaction synthesis without using any capture 3D interaction data we propose to distill interaction priors from large region language models which show powerful capabilities in synthesizing 2D human imagery given as input a 3D SS and approximate Point location P for the interaction and this uh short text prompt we optimize for 3D human body model that is uh performing the action performing the action in the SC Guided by a large region language model we first render the SC into multiple views to capture the local context around p and leverage latent diffusion implanting to imagine possible 2D human interactions in each view based on the input hex promp here we develop a fully auto automated implenting process by dynamically generating uh human impending masks based on Cross attention maps of lat diffusion to find the region of Interest given the 2D interaction hypothesis after the implenting we use an off the shell post estimator to detect 2D poses for the implanted humans we then leave these 2D interaction hypothesis to 3D we formulate a robust optimization optimizing for a 3D human body model that is most consistent with the 2D post guidance here the blue bars represent our optimizable view consistency scores more concretely our optimization objective is formulated with the following terms a post fitting term for the detected 2D poses and the 3D post projection a view selection term to make sure multiv view uh Supervision in the meantime we regularize the 3D body pose and shape to promote natural and we also penalize uh sin penetration and self uh penetration for more details please refer to our paper to evaluate the effectiveness and generosity of our approach we collected eight large scale 3D sces including various indoor and outdoor uh scenes with diverse geometric uh structures for for example a cartoon style uh food truck a gym and a realistic veness city as shown here our approach is applicable to diverse SC types including both indoor and outdoor environment our approach circumvents the conventional need for capture 3D interaction data and allows for flexible control of the 3D interaction synthesis with easy to use text prompt here are more generation results our approach synthesize more realistic to the human interactions and generaliz better across Sy types when compared to baselines that are either trained on the inter interaction data or based on 3D human estimation from a single RGB image in a binary perceptual study participants show strong preference for Our Generations more than 87% of the time in a un perceptual study Our Generations have a have the highest realism breing compared to baselines further quantitative evaluation shows that our approach achieves the best semantic consistency score echoing the draw user preference for our approach in the previous uh perceptual studies our approach can generate various plausible 3D human SC interactions when given the same 3D scene text prompt and 3D location input we further perform comparisons on a recent indoors in data set pro-s our zero sh approach achieves competitive generation performance with compare to the baselines that are specifically trained on this annotated indor data set at last our code and data are available on this website thank you for your attention any questions right thank you so thank you uh so the next presentation is going to be about diff Assemble UniFi e okay so um hello everyone I'm stefanini I'm presenting today the paper def assemble a UniFi graph diffusion model for 2D andd assembly uh in this paper uh we try to solve the problem of placing each individual component in their correct position orientation to form a current structure why would decided to focus on this problem because like since we are young the reassembly object is one of the skill that we learn so if we want to believe and to create a general AI this is one of the key component we should develop we also find that this application can be really useful for example in the Heritage and cultural field where for example we can reassemble frescos and other object that we found broken for this reason we Design This model that is called diff assemble def assemble is a general fail mod for solving task using a graph representation and a diffusion model formulation the idea here is that we can work both in the 2D and 3D without any problem because one of the key component is the use of a backbone that in case is an equivalent backbone the other key component is the graph formulation as you can see here our graph is a at starting point is a complete graph where all the element are connected to each other we have three elements the features so the features are the element generated by our backbone that depends on the task on the dimension of the task we are working in for example if we are working on the 2D we have convolution NE network if we are working we have point point Cloud then we give the position the position is the representation of the dimensionality of the continue equ space and then we have the rotation the information on the rotation of the of the pie to do the process of reassembly the elements we decide to rely on the use of diffusion why because like we create starting from a random scenario we learn how to reverse the process step by step so as you did they we put step by step the right in the right position the all the pieces as I said we our approaches is able to work both in the 3D and in the 2D scenarios for the 3D we use um we use as data set the the breaking B data set this data set is composed of around 10K meshes from partnet and F and K uh in this specific data set we have for each object we have 20 fractures that are designed and then there are all simulated also 80 fractures so at the end you have 100 fracture for each object in this as you can see here we reported some of the results we obtain we are state-ofthe-art in this specific task we are there is no trade accuracy between rotation and translation and we also as you can see here from the uh results we also Ry a lot on the use of diffusion process because and also to the use of no equivalent on the use of equivalent backp because thanks to these two uh other component we are able to obain better performance we also apply some uh other tests to see if our approaches is working um if the method we are using is working better or not as you can see here we saw that the use of the diffusion process on the two uh um on the 3D rotation without the use of specific distribution is working better we also see like as I saw like the use of diffusion approaches is bringing us a lot of better performance we also try to see if the use of equivalent backbone to rely on the rotation of each pieces is something that can give us a really um better performance as you can see like using an invariant encoder or non equivalent encoder we obtain better results and finally also we try to see if the attention mechanism we use to define which are the neighborhood of each pieces can help us to obtain better results and here we use a standard gcn as you can see we didn't um we obtain better results in in our cases these are some qualitative results on the 3D the hours is better than the all the state of the art3 variant these are some other qualitative results as you can see like the uh diffusion process step by step try to recover the right position so instead of having one shot step by step we move the pieces we also test our approaches on the 2D assembly task we use C that is a um data set contains only uh faces and then we have also Wiki art that is a data set well you can find different paintings with different contents such as portrait landscape in these cases we compare our approaches against other optimization based method since our approaches is the first one is the first learning based approach that work with pieces that are they have a different orientation so as you can see here in puzzle C Bay we obtain the State of the State of thee art results while in Wiki art the results are worse due to the fact that Wiki art is a harder data set and it's harder for our approaches to be the optimization based method however when we test and another scenario where you have missing pieces as you can see from the results the op optimization based method they drop a lot their performance while our approaches is becoming the best so the idea is like here the idea is like in a real scenario where for example you don't have all the pieces of your puzzle our approaches can rely not only on the visual appearances but also on the semantic contents and we can obtain better results so at the end we are more robust in this kind of scenarios these are some qualitative results as you can see we are still relying on step by step um definition of the right position of the PC so starting from a random scenario we recover the right position we also try to see if we can scale up our approaches as I told you at the beginning we were using an attention based gcn so basically our approaches as um so when we increase the number of elements we have also an exponential increasing of our um of the memory required so we apply a sparcity attention mechanism and we were able to up arrive up to 900 pieces with a standard GPU with 24 GP on the other side on the right right side we also test our approach is inference compared to the optimizations based method as you can see the optimization based method they grow exponentially when the number of pieces increase on our case our approaches is scaling linearly like but with a few uh increase really uh with a really low increase and we also see that there was no reduction in the accuracy these are these are some qualitative results on puzzle with 900 pieces so in conclusion we uh introduced def assembled it's a general framework for uh taking their assembly task we are graph diffusion approaches we demonstrate the effectiveness of our method both in the 3D and in the 2D uh puzzle we are we also demonstrate that we are robust compared to optimization B method in real scenario such as missing pieces we obtain state of the results in the 3D domain and we also show that with our sparity attention mechanism we we can also increase the number of pieces we can uh deal with so um if you want to visit us we will have a poster on Friday uh and after this presentation so thank you very much for your attention any questions um no well uh no for this work we already focus on 2D and 3D uh we did another work previously where we try to focus also on the 1D so like the we have a shuffle sentences and we try to put the words in the right position but till now we focus only on these three uh on in particular in this St on the two uh this modalities like well at the maximum we can assemble one 100 pieces that is the number the maximum number of pieces that they are given in the data set but um this oh there are some details I think one sorry here oh sh um so the data set here uh contains it's here they have one million of breakbone pters so we have 1 million of objects okay thank you very much G be thanks okay uh first I'd like to thank everyone here for coming to my talk my name is Thomas Mitchell I'm a senior research scientist at PlayStation uh today I'm going to discuss our cvpr paper uh titled single Mees Fusion models with field lat for texture generation uh which was done in uh so I'm sure here arear textur well immediately two problems the first is the question of and what domain does it live on how you build the second problem ISSC sty upon which large model condition so the benefit model be applied on top ofation scales of uh uh daily say of fine details in the features and uh methods that the noise and project must handle uh artifacts viewing consistencies right furthermore using a pre-trained image diffusion model to circumvent the D data scarcity problem produces synthesized textures that may contain a natural tones or light so here we present original framework for latent diffusion models that operate directly on the surfaces of shapes enabling the generation of high quality textures so our first me our first uh sorry our method consists of two distinct novel components uh field uh the first is a lat representation called field latence uh which Maps textures to tangent Vector features at the vertices so field latence offer an effective form of perceptual compression in which a high RIS texture analogous to almost a continuous signal defined over the surface is mapped to a collection of discrete Vector Fields taking values to the vertices so the choice of vector value features over scalar features is critical as they capture directional information related to the local texture which we show enables high quality reconstructions the second component is a field lat and diffusion model uh which lends into noising diffusion process in the field Laten space so although a framework is general and can handle standard large scale diffusion paradigms due to this uh due to the data scarcity problem we circumvent this uh by training our models in a single mesh Paradigm uh where we aim to create new variations of uh texture from a single object so our approach enables geometry processing applications Beyond unconditional generation including a straightforward notion of gender detection transfer by simply sampling a pre-train model on a new mesh so latent diffusion trains a model in latent space what does this mean well with images uh a highres uh image is mapped to a lowed latent space using a noral network called an encoder a second Nal Network called the decoder Maps back from the latent space to the image so the key idea is that the encoder acts as a form of perceptual compression for example a 1024 x 1024 RGB image May map to features on a 64x 64 grid so latent diffusion uses this to do diffusion in the latent space images are encoded to latent space where Theus model is trained and samples are decoded to get the generated images uh this is very efficient because instead of operating over the highr images directly the model Works in a much smaller lat in space so can we do something similar with a high RIS texured object so typically highres textured mesh or highres Texture itself might be a you know 4K image with millions of pixels but the mhes to Def find them may only be like 5K to 10K verticies so our observation is that because of this difference in resolution the mesh is a natural low dimensional Laten space specifically we can D design an encoder that will map textures to Features at the vertices we can use surface convolutions to build a diffusion model directly on the surface of the mesh to generate textures so to this end we introduce an auto encoder called the field Laten VA which Maps textures to tangent Vector fields on the mesh so each vertex the encoder is trained to map the texture uh to in the nearby triangles to tangent Vector features at the vertex similarly the decoder is train to map back from the latent Vector features the vertex that texture on the surrounding triangles so why are we using Vector features well it's because they allow us to continuously extend latent features across the mesh in a directionally sensitive way so to see how this works we'll look at how the decoder gets the value of the texture at a nearby Point Q so using the relative position of Q and the latent vectors we build coordinate features which the decoder uses to get the texture value specifically we compute the do product and determinant of the latent vectors with the relative position of Q to get a collection of coordin Fe features which describe a q is position with respect to the vertex so these features are then passed to decoder which use them which uses them to get the value of the texture at Q so critically we demonstrate through experiments that that this approach is far more effective than bartic interpolation H and is key to enabling High Fidelity reconstructions okay so uh for comparisons we quantitatively evaluate the representational capacity of our flva against two other approaches the previously state-of-the-art intrinsic neural fields and a modified version of fvas serving as an ablation so INF trains a neural field which predicts texture values from the pl and aen functions extended over the triangles using baric interpolation we evaluate the methods on 16 objects from the Google scan object data set which exhibit uh complex tetral details uh with High Fidelity and high frequency uh information so the quality of the reconstructions is measured using the average Peak noise ratio structural dissimilarity index and L Pip's perceptual similarity metric all computed in the texture atas so two sets of valuations performed on high resed and low red versions of the meshes with 30k and 5K vertices respectively and the lot of results are shown in parentheses so our proposed FV achieves the best results across all metrics B and this is really because btic interpretation is linear and so it's inherently low frequency right this makes it nearly impossible for it to capture high frequency changes in the texture across each triangle regardless of the dimensionality of the interpolated latens so in contrast our proposed coordinate based interpolation is nonlinear and directionally sensitive so it's significant increases its representational capacity uh fovs allow us to Define field lighten diffusion models or fodm for short simply put we can encode the textures to Vector fields and the meshes and you fi use field convolutions the state-of-the-art surface Network designed to operate over Vector Fields uh to build it a fusion model which will generate new latent features which we decode to get high quality textures so we quantitatively evaluate the quality and diversity of generated textures by comparing against sin 3dm which rizes meses to 3D grids and encodes them to 2D triplan features to perform the models are trained to generate new textual variations of the assets of assets from the obts and scaned object data sets and these assets were chosen uh to exhibit complex high frequency textures so Fidelity is measured by the mean single image foret in Inception distance between renderings of the original and synthesized textures and diversity is measured uh by the mean l Pips between renderings of different samples so compared this in 3dm fldm are able to synthesize higher Fidelity texures which is likely due to the former rization of texure geomet to a 3D grid before encoding which potentially ales the fine details more generally we observe that operating that you know uh by operating over a extrinsic domain extrinsic representation of the geometry sin 3dm intertwin texture with the meses 3D embedding and so this means that texture and position are so strongly interlined that textual details appear as repetitions or extrusions of patterns along the major axes in synm is unable to produce ific textural variations without modifying the geometry so in contrast fldm and FV are intrinsic and aware only the mesh's remani and structure this allows them to effectively replicate texal details across areas that are only locally similar regardless of the relative configuration of these regions in the 3D embedding so uh in practice textured objects are often often exhibit specific content uh that is uh locally attenuated to certain regions of the mesh uh which may or may not be geometrically unique so our FMS can be easily extended uh to condition on user specified labels that reflect a subjective distribution of content on the mesh in the simplest case a user can paint a core semantic labeling which generated textures will reflect such as the flippers head and rock base of the seal sculpture hair a similar approach can also be used for in painting where a user can paint Regional MK delineating portions of the texture sh ring constant uh during sampling the noise latents in the Mast region are replaced with the original latent features this encourages the N the uh in pain and texture to agree the original mask at the boundaries due to the convolutional structure of the noising network so last I'll finish up by talking about my favorite application which is G detector transfer uh and as we discussed in the paper fvs and fdms are are locally aset acarian meaning that they're agnostic to shape deformations that preserve distances Loosely speaking this allows them to replicate textual details on locally similar regions not only the same mesh but across different meshes as well so a key benefit is that we get a notion of gender detection transfer essentially for free that a model trained to one mesh can be sampled on a second mesh to texture it and style of the first in general texture transfer is a really tough problem that even State of-the-art correspondence networks struggle with as shown in the bottom right uh however we get great results without any fine tuning we make it work very well uh in practice we find that FMS uh we can use FMS to actually effectively transfer textures even between highly dissimilar shapes such as uh from the skull mesh if you remember on one of the first slides these dinosaurs here uh even though this sort of these meshes aren't really isometric uh so thank you that's the end of my talk I appreciate everybody coming here thank you so much to the organizers putting this together it's wonderful it's been a privilege to speak here and I really appreciate it uh thank you and come see our paper tomorrow in the PM session forget the number any questions yes sir you said use the Google scan object yeah did you experiment all with object Exel yeah so the actually most of the messes for the diffusion are from object verse Excel there's only like one we use from like Google scan object which is kind of cool but actually so for the first set of experience with the FL uh with the VES we use objects um models and then for the second set we basically use object first models um there's not really a reason we just found cool models from both and ended up organizing that way did you uh so that's that's a great question so the pre so make this look ah great but the pre-processing is actually kind of tough right because uh you know this Fu fuel convolutions uh is essentially a message passing Network it's effective but it's also sensitive to connectivity so basically when you take a mesh right the Gory details are that you really need to take this mesh and during pre pre-processing make like you know 50 or 100 different uniform Reet triangulations of this mesh essentially because you don't want it to overfit to the connectivity otherwise it's you know this network is just going to learn to say aha this one ring is kind of unique connectivity I'm just going to match this latent feature and you'll get no diversity textures so you have to do the three measuring it's not ideal I mean ideally you'd like to use something like diffusion net which is connectivity agnostic the problem is diffusion net band limits features so if you try and generalize it to slate representation which is discontinuous and could be high frequency it's going to ban limited textual details so there isn't at the moment a great solution for actually doing convolutional de noising on a mesh right this is a separate question but what like you know it's unanswered in geometry what is really the a grid representation that both uh can handle High Fidelity information and not band limit will also be being efficient and fast and connectiv agnostic so that's an open question um that uh could be you know if someone solves that you can plug this in this Paradigm I think the real thing I'm proud of is the field latens I think they're inventive representation but the but the that convolutional model itself can be improved and the field lat probably be improved too any other questions yes sir oh we didn't that's a cool idea um no I did that in earlier paper but like using TSN or something to yeah that would be cool I should do that uh there's there's the code is available now on the website feel free to take it there is a pre-train model um that's a great idea we didn't we didn't do that it's a little bit tricky because the features are equivariant right so you have to pick some ordinate frame right maybe if you look at the invariant components of that like uh you'll see the paper we compute equivariant and invariant features maybe there's some structure there that's a cool idea thank you okay last call the questions thank thank you again it's a privilege great so the next talk is going to be CNS edit fre shaped editing bya coupled neural shape imization and the speaker is Jen e e e I think I need to see the see the and not I help well okay is it okay okay show so so you okay okay SU just yeah change you sub okay uh hi everyone welcome to my presentation uh today I'm going to introduce our work see as edit I'm Jun from the Chinese University of Hong Kong and Sam and this this is a joint work with J uh Edward Richard John and Philip let's start with the motivation of this work so as one of the most important applications in computer Graphics a 3D shape editing has been widely used in many applications including uh cat design met verse uh 3D games and V traditional editing and manipulation approaches typically relies on editing handles such as points the sketches and cages however these techniques typically like semantic understanding on control for example when we addit the H legs of this horse the front legs does not adjust accordingly which is UN natural to enhance a sematic control in 3D shape editing existing methods cou the implicit functions with with geometric Primitives or to connect a shape latent space with a CLE fure space in this way they allows to addit 3D shapes by modifying the Primitive parameters or to trace along clip space however they cannot maintain uned region and their qualities are far from satisfactory for example to edit the pillow of this so far artifacts are induced and in the right example when we try to use text to edit the back of this chair the legs are also modified unexpectedly and motivated by the above works the desired 3 shade Ting should have the following features first it's highly desired to have various operators for the flexible editing and second it's essential to have the shape understanding and shape semantics to preserve the validness of shape in the editing process and also the unrated regions should be kept untouched in editing process next we will introduce our work given an input shape we first adopt encoder to generate with a latent code and then we use a diffusion unit to reconstruct the input shape and extract the intermediate feature volume and next we take both the latent code and neuro volume as a couple neuros shape Rec representation and in other words these two components together will represent the input shape the key insights behind this design are in the following two aspects first the latent code is a compact representation which can provide a rich semantic but it Lakes special context and to com to compensate with it we also introduce a neural volume to provide Rich spal contacts for precise editing not that these two components are closely related and correlated on one hand we can generate the neur volume given a shape L code and on the other hand we can Pro Gates loss through neural volume to optimize the latent code by considering both the latent code and neural volume or new SC as representation contains both semantics and spatial context which is suitable for semantic editing with precise uh special control next we aim to make use of C as representation to deliver a variety of Ting operators and taking a copy operator as an example we can formulate it as object function to be applied on our neural volume the object function contains two parts the first part is a list of spatial coordinate in the neuro ficial volume specifying region to be addited and for the copy operator this is the blue region and the second part is a list of Target feature values asso with the spatial coordinate as shown here the target values of this copy operator is shown in red region and using the two parts we can formulate our loss function as follows and for more details about this uh and about how we obtain the two parts for different operators so please refer to our paper for more details and then we can back back propagate the gradients to update lat code and after end iteration code optimization uh we can obtain the updated latent code and optain the final Ed shape like this chair and by formulating the objective functions as different form we further provide various operators for user to edit the input 3D shapes interactively uh with more flexibility uh the first one is a copy operator which enables users to select a portion of the input shape and copy it into some other location in this shap notice that we only copy a bar of this chair but the other bar in in the other side has also copied automatically next we introduce a Reise operator which enables users to specify the selected region together with the Anchor Point for resizing and we only resize one side of this cat model and the other side has also been resized automatically besides we introduce the delete operator to allow users to remove a selected part of the input shape notice that after the removal the rest back of the S far is still smooth and coherent without a hole finally we propose a drag operator which enables users uh to addit the input shape by dragging a single Point as sh here users can easily edit the size of this hole by dragging one point which is very challenging for existing approaches all these results are exciting and high quality which are not achievable for existing neur ship editing approaches besides B operators we allow users to combine existing ones to form new equat for example combining the copy and delete operators can result in a cut paste operator finally we will introduce our experiment here we present some real results of our method for example by simply dragging a source purple Point towards uh this Target red Point users can resize the Cinders hole which is unachievable for uh existing Works besides for the delete oper when users delete a hole in one armrest another hole will appear on another armrest automatically also for the copy operator we allow users to copy this host and paste them to other locations additionally when we uze one uh L here another uh legs will also be resized automatically uh here will provide some visual comparisons between our method and State ofthe art approaches our method better maintains shape semantics during the editing process for example uh in this bottom row shows when one armrest of the sof far is checked the other armrest would adjust automatically and our method can better follow the desired operation with higher fidelity a quantitive comparisons between our method and other state of art methods further show that our an shapes are are have the best quality for all the matrics and here are more results for drag operator it can be applied for 3D AV generation and the can mod design and here are more video results of the Reise operator and cut paste operator and here are more visual results for the copy operator our method enables one to choose a source region which is in red and in a shape and pce it to a Target region which is in blue a visual results for the delete operator our method allows one to choose a part of a shape and remove it to conclude in this paper we propose a couple CNS representation next we propose a no CNS code optimization procedure a finally we provide four operators for users to addit input shape interactively thanks for your listening thank you any questions all right thanks again then let's go to the next talk and this is going to be Dae net the forming aut for R shade for segmentation and this is shit one something you therey everyone uh my name is shimin and unfortunately Jing is not able to present today uh due to some personal reasons so and I'll instead present our work on deforming aut encoder for f gr shap Co segmentation so um segmentation uh when talking about segmentation um people usually think of uh semantic shape segmentation where uh shapes are segmented into predefined uh meaningful U semantic parts and one of the most famous work is point that and it's trained with u full supervision with respect to the to the ground true segmented data and some other works also explore you know weekly supervised semantic uh segmentation such as um having uh one semantic label per part per shape for uh training and we also have uh zero shot so zero shot models can uh segment the shape with a user provided part labels and they typically relied on uh pre- trained large Vision or language model to uh segment the rendered image of the 3D shapes and then aggregate the image segmentation into uh 3D segmentation so um basically they all require uh ground true segmentation labels for training and uh you know either uh Dan segmentation labels or just a little bit of uh labels or you know prior from uh pre- chain large Foundation models and on the other hand when there's no ground Truth uh segmentation as reference we can still uh segment these shapes uh like lots of classic shape segmentation method do because um most of most of the time we are not guaranteed to have access to uh stic information and uh there's also co- segmentation where given the group of shapes um we are not we not only want to segment the shapes into Parts but we also want uh to have the correspondence between them and the parts obtained through uh shape Cod segmentation often seems to be uh semantic aware and imagine that um if a has many corresponding parts in other shapes the part itself must be you know important enough to have uh semantic meanings and also since we don't have any uh sort of supervisions uh classic shape segmentation algorithm usually usually require lots of U manually designed uh priors and even more such priors are needed for uh Co segmentation so um in this we uh in this work we essentially introduce a new prior for uh shape Co segmentation um which is made possible by recent progress in machine learning and we observe that um Parts in different shapes often have approximately the same shape uh which makes sense you know because of uh form uh follows function meaning that um the shape of an object should primarily relate to its intended uh functions or purposes and for example if you want to design uh let's say a swi chair or a chair that can move uh the first thing we can think of probably to add a Wheels right but you definitely don't want to put wheels like this one because um it's unstable and you probably need something like this one because the chair now is stable enough for us to sit so um the prior we assume in this work is that um Parts in different shapes should have approximately the same shape and now the question becomes how to design an algorithm to use this prior and then here comes the second observation uh capsule Network and we are especially interested in its part capsule Auto encoder uh which is essentially um the transforming Auto encoder and this is introduced to to learn the first level of the capsules and for example in 2D um we have a neuron Network to predict a set of part attributes and poses from uh an input image for example the a here and we also learn a set of part templates and then we can use the learn poses to sort of Select some parts from all the templates and then these selected temp candidates are then aine transformed um to assemble the shape in the input image and the chaining can be supervised by uh reconstruction laws between the reassembled image and the input image and then the question now becomes what if we apply transforming Auto encoder on 3D shapes will still work and most importantly uh can it give us you know reasonable part segmentations so then here's an overview of the idea we just discussed and the goal is to uh co- segment agreable shapes and to achieve this uh we want to learn a set of part templates and on aun transform them to approximate uh the ground CH shapes then we can segment the ground CH shapes with respect to the to the parts after uh fi transformation for example using uh near neighor and uh here's the network design so um it's pretty simple we have a CN encoder to predict fine matrices and part existing probabilities for each shape and then for each 3D Point uh 3 query points we apply uh the F uh Matrix so that um the coordinates are uh sort of transformed from the global frame uh to the parts local frame and for each part and an MLP is used to predict the occupancy value of the input points and then at the end the occupancy values are multipli with the existing scores and then sum together to produce the occupancy of the output shape and um transforming order encoder actually uh perform pretty well um we can get like very nice segmentations on this airplane model and the segmentations on this chair and guitar are uh know both reasonable but we notice that there are uh small uh fractured pieces in the chair and guitar and if you recall that um ground tooth chairs even for the same part let's say the rrest uh there are different variations in different shapes so it it's not possible to F1 transform a template part into all these different uh variations since transforming Auto incoder we try to approximate the ground tooth uh shape uh these small pieces are uh naturally generated to minimize the Reconstruction loss but we what we want is more a more compact shape segmentation so we add one additional step uh we um after fi uh transformation each part is further def uh deformed to better approximate the ground to shapes and to this end we call our method deforming Auto incoder and we only need to uh do a little bit adjustment uh to the transforming AUD coder Network um we by adding these uh deformation networks to sort of a wrap Point coordinate and then by predicting the latent code to condition on the uh deformation and um training our network is very intuitive we only need um some reconstruction laws to measure the error between the predicted occupancy values and the ground choose occupancy and the max reconstruction to sort of encourage non over overlapping between parts and a regular regularization laws to encourage as little defamation as possible and then uh a sparse loss to uh encourage as few parts as possible so the network can actually generate compact uh segmentation and um we also have a uh training trick to overcome the local Minima so uh sometimes our network will uh converge into a local minimal like this one where you don't you you don't get a good constructions like uh the curve leg and we want the lag and the seat to be separated so uh what we do is that we re initialize some parts so that the network has a chance to uh converge to a better uh local Minima and this also works if we have redundant parts for example uh we want to merge these two seats into uh one and then we can simply reinitialize this part and then hopefully Network can reconstruct the shape with one seat and now uh I'm going to show some results and compare with other works these are the results on the uh shitet part data set and we can achieve uh cleaner and more fine grin segmentations compared to other works especially in this motorbike examples and uh the airplanes so um AR meth also outperform the other methods by almost 20 points in segmentation accuracy and um here are some results on the animal subset observers and default and since our segmentation is clean enough we can actually uh construct core scalant from uh the segment uh segmentation very easily and uh remember that um for each shape we predict a part existing uh scores for each part templates and we can uh treat the the part existence score as a binary vector and use this Vector as you know key to uh group The Shape into subcategories for example uh shapes that have the same binary vector can be clustered into the same category so these are some grouping results each block is a group and then um we Show an example shape from the group and the number of shapes in uh this group contained so for example about 1,700 shares in the ship net part data set just have you know four LS and then about 400 have additional omor Etc and these are some example shape in each uh cluster group and um of course our method has a few limitations and first of all you can only segment uh volumetric parts so uh areas for example surface parts for example the windshield uh or the roof the car cannot uh be segmented and also we uh require uh the shap to be to be solid so not hollow inside like the the last example here and um that's all about da net if you have any uh questions uh please feel free to to reach out to me or Jing and thank you questions yes sir right um let's go back to let me go back to network very quick so we have a we have a the F trans the F me is what uh sort of bring uh all the uh input coordinate into like the local frame so they're basically in the rough position and we have to some uh reconstruction laws to make sure that they are uh they can connect with each other and the the final output uh is as close to the ground sh occupancy as possible and then uh that's how you know we make sure that each part is uh connected with each other and then we don't want them to overlap because uh that doesn't give us you know clean uh uh segmentation so we have Max reconstruction loss to sort of uh encourage nonoverlapping so you're saying that how do I make sure that for example if the arms it doesn't have a hole inside when do oh oh okay um that's a question um that's this would consider a local Minima where like we want one part to represent the entire forarm instead of two parts right so um uh the way we did is that um we computed the fractions of all the shapes that um have let's say part I and uh we have a thres hole and if the fraction is less than you know let's say 10% that mean that means that this part this part template is not used only only been used in 10% of shapes and that could possibly there higher chance that they are uh you know small pieces so we reinitialize the branches so they uh have better chance to uh generate the entire uh part instead of combining two con uh we have to keep doing it yeah no problem any questions right thank you this concludes the um the part of the part presentation the next e hello hello can you hear me hello hello can okay okay nice just a second chck sharing my screen yes yes please okay can you see my screen just a second okay yes okay nice hello everyone my name is shanii from Professor Muhammad a hosis group today I will presenting a summary of the 3D compact challenge H at this Workshop our challenge is inspired by a long-standing question how can we build models that possess a composition understanding of 3D shapes when we talk about composition understanding we focus on how different parts and the materials come together to form a 3D shape here I show an example of a chair as you can see the chair is made of of multiple parts of different materials for example the back rest is made of fabric and the state is made of leather as you can see human can easily understand part material compositions of this chair how can we equip computer vision system with the ability to understand compositions in an automatic way to do this is crucial that we create a data set that include labors or part material compositions to meet this lead we have developed the 3D compact data set and it sucessors 3D compact Plus+ these dat set are specifically designed to Foster compositional 3D understanding first let's take a look at some statistics from the 3D compact data set when we compare it to shipet data sets you will notice that 3D compact features more more vertex edges faces per shape this rich list in detail is crucial for our goals in compositional understanding unlike previous 3D data sets that mostly focus on geometri understanding our 3D compact data set provides huge amount of sterilized shapes each shape contains on average 1,000 human verified stair compositions for each ster shape we Prov RGB Point Cloud part labers material label and the texture mesh we also render eight different views for each sterlized shape four from Clinical views and four from random views for each render image we provide part mask material mask and depth map to collect this data set we have human orators to collect 3D shapes and part material compositions the whole process is Guided by 3D experts with detailed annotation guidelines to ensure the quality of annotations The annotation process includes shape collection shape editing shape alignment part material annotation shape sterilzation and image rendering we repeat this process for each shape to get a collection of sterilized shapes and rendered images here I should watch for examples of sterilized shapes from the 3D compact Plus+ data set to understand advantage of our data set with previous 3D sh data sets in this table we show the features of commonly used 3D data sets such as mod net shap net Omni object 3D object verse and so on we also compare with data sets with part lever annotations such as shap net part and the part net in comp comparison our 3D Compact Plus Plus data set contains a large number of 3D shapes with fine grain part level annotations including both part label and material label we also provide 160 million rendered images which can be used for large scale compositional image understanding based on 3D compact data set we Benchmark several task using SOA methods here I show the fine grain and cross grain part segmentation performance using several SOA methods we further introduce a new task called grounded composition recognition for short GCR for compositional 3D recognition we introduce several metrics to evaluate the performance of this task at both fine and crossgrain Nevers we Benchmark this task using different method designs including methods based on separate 2D 3D training joint 2D 3D training and 3D based methods we provide a userfriendly web tools for users to explore the 3D compact dat set we also provide apis for easy data access and usage more details about the 3D compact challenge can be found at the workshop website going Beyond 3D compact Plus+ we recently completed another project that aim to collect more 3D shapes for more shape categories and more fine grein part material compositions we are excited to share the 3D compact 200 data set which features 19,000 3D shapes across 200 shape categories each elated with part fine green and cross grain part and the material label this data set contains 19 million Starlight shapes and 300 million rendered images we show visual comparison between 3D Compact and 3 compa to 200 in this figure we show the distributions of part Occurrence at both fine and cross granularity levels as you can see the part occurrence follow long PA distributions which pose F further challenges for compositional 3D recognition we also collect 3D captions from part and material label and build a new Benchmark for compositional test to shape retriever now let's summary what you can do with 3D compact data set with this data set you can explore diverse 3D tasks including 3D shape classification part segmentation grounded composition recognition test based 3D generation and editing grounding grounded 3D shape captioning and test to 3D retriever this is a recent paper about test based 3D shape editing published in cvpr 2024 this work is built upon our 3D compact Plus+ data set here's another work about grounded 3D shape captioning this work is also built upon our 3D compact Plus+ data set finally our data set can be accessed at this link thanks for your attention now I will hand it over to Adia to continue with another part of the challenge um hello can you guys hear me can anyone hear me yes we can hear you but we yeah and we can see the screen now too all right wonderful okay so hello everyone I hope you are all enjoying cvpr so uh today I'm going to introduce the visual shape and frence challenge so 3D objects are highly structured right this structure actually arises from various aspects such as interp part symmetry groups or even from construction plans and programs are very natural way to model them they enforce stct constraints expose high level control and also Aid interpretable editing and I feel like in this era of large language models it's worth mentioning that they are llm interpretable and also llm editable so the main challenge comes from the fact that manually crafting these programs is expensive as it often requires human experts who require years of training so visual program inference is a solution where given a shape as input a system automatically infers the corresponding program which will reconstruct this shape so in this challenge we are hoping to compare different approaches for program inference on the 3D compact data set basically VPI for 3D compact objects participants will be tasked with creating a system which infers programs that are accurate as well as par mons so uh in terms of the input of this system uh participants can create systems which take many different inputs they can take mesh voel Point cloud or even multiv view images we don't restrict that but the system should give output in a fixed format which is going to be a program in a fixed DSL which we are calling CSG light and essentially CSG light allows creation of parameterized perit right for example example uh these different Primitives including super quadrics perform Boolean operations between them and also construct symmetry groups such as translational or rotational groups so how will these systems be evaluated the evaluation metric is going to combine two aspects namely reconstruction accuracy and program conciseness essentially we want to have lower deconstruction uh ERS and have a smaller program uh a system which gives both of these would be a better system we'll release the exact formulation of the evaluation metric closer to the challenge now there are many different approaches to info programs for these data sets uh popular approaches include per shape optimization such as inverse CSG learned neural search such as plad and methods which combine learning with per shape optimization such as capret we'll provide a starter kit to get started with some of these approaches the challenge will be conducted in two tracks namely a under 30C track and a under 300 second track the first track is geared to its methods that may eventually be used for TAS such as realtime shape abstraction scen abstraction and the second track is geared to its method that Aid discovery of Highly parsimonious programs do at a higher computational cost finally let's look at the final piece of information first the timeline so around November of this year many folks in this room might be working tirelessly for their submissions to CV cvpr 2025 after which you can finally have some rest but wait there's Sig graph deadline as well in January so after that perhaps people get their rest um so we have decided to have the challenge start from early February and then have it continue till the end of May and thanks to the sponsors of this challenge the prize money allotted for this challenge is $2,000 uh thank you for your attention please send your questions to my email ID and if you'd like to sign up for a reminder email please use this QR code uh that's it I'm happy to take any questions if people have some questions any questions okay I don't know if you can hear me but no we don't have questions so thank you very much all right and great uh so now we're going to have a break for coffee imposters that's unfortunately in the other building as always uh but we're going to regroup at 4M here for more invited talk uh talks and the panel thank you for e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e sub do I need to un mute myself um question I think just leave it like this okay okay okay okay uh welcome back to the last session of the day so we're going to start with the invited talk from Dr Chan king sorry if I mispronounced that she's an assistant professor in the Department of electrical and electronic engineering at the University of Hong Kong she received her PhD from the Chinese University of Hong Kong and has worked in exchange at the University of Toronto Oxford University and Intel visual Computing group and she's committed to empowering machines with the ability to perceive understand and reconstruct the visual world in the open world and pushing their deel deployments embodied agents please over to you okay okay uh thank you so okay so let's uh start so so I'm from the University of Hong Kong so I'm going to talk about some research on trying to simulate the 3D environment from videos uh so before I go into the details let's first look at where we the this uh so uh in the past in the past decade we have seen a lot of success uh by models learning from internet data R so we have different develop different architectures we have have different models with high capability and like a GPT forway and gini and this sze can be all attributed to learning from internet data but this this type of learning Paradigm has some drawbacks first it's a passive right uh so the model uh learn from a static data set this is different from our humans learn right a human will learn during about curiosity dur about Our intention to accomplish a task and also uh in terms of inference the existing Paradigm is of is also open loop and L interaction with our 3D physical world this is also fundamentally different from how humans leverage our intelligence to interact with the world we we we actually will have Clos Loop interaction right we interact with the environment and the environment will give us feedback to help us refund predictions and so on so uh uh like General uh agents in uh agents uh in in you our physical world actually learn by interacting with the S physical world instead of learning from a static data set we learn acely and understand acually in in the 3D world uh we learn through interacting with it then the learn knowledge further utilized for us to better interact with the environment uh so so but how how how can we rate this this kind of learning Paradigm within machines if directly expose machines to the real world it will will will cause a lot of mistakes right it's unsafe and they also suffer from scalability issues if we directly let machines to learn from interacting with the real world it's it will it is very expensive not able and unsafe so this is the reason that we that we need a 3D environmental creation we need to use some approaches to recreate a 3D world within machines so that machines can learn from virtual 3D watch so this is our motivation uh so so although it is very promising that if we can allow machines to learn in the virtual world uh we we can have high skillability we can be more we can the learning can become more safe uh but uh the interactive 3D environment are one rare it's very difficult to scale up how can we acquire a 3D interactive environments this is different from how existing models are TR on image data set uh but the image data are very easy to acquire through internet right however for the 3 environment we can only afford or afford to capture thousand of 3 environment and they also have a very low diversity so you can see in this examples they are typically household environment and the diver diversity lighting environment and object arrangements are also have limited variabilities this is because in order to collect to build a 3D interactive environment we actually need specializ the sensors like such as connect and data to capture data and then go through reconstruction and the posst processing so this Paradigm this this this kind of data capture on process is not scalable So currently we can only have limited number of s environment for agents to learn from the this environmental activity AC activity so in my research so we would like to uh uh to uh seek another Paradigm want to uh seek solution that where either we can turn a video into a interactive with 3 water environment so so the video data can be easily cross from the internet this can offer a lot of benefits so firstly is the diversity can be significantly enhanced because videos can be CR soured from the Internet or in the future it can be generated by diffusion models and uh the second uh benefit is scalability with videos with only monocolor videos we do not need additional sensors for data capturing and also contabil if we can build a interactive environment we can make it be more controllable and we can rearrange the objects to create a more diversifi 3D environments but this is definitely suffer from a lot of challenges so the first challenge is you post the data uh so visual close alone can be very ambiguous if we want you leverage it uh for recovering the 3D uh the 3D geometry and secondly uh so the representation uh so what what's the best representation that we can that can be utilized to represent a 3D SC in order to facilitate optimization for learning from a video and the third challenge is that is the natural Dynamics in videos so in videos there are a lot of dynamic objects so how can we recover the 3D uh the 3D World and at the same time to recover the Dynamics of the objects to create a dynamic 3D World so these are the challenges so uh in order to uh tackle the challenges uh so we we have divided this problem into several steps uh the first step is uh is we can develop some uh models that can convert RGB videos into rgbd videos so with this deps L the the we can bypass the uh the OST data problem and have a better chance to uh have better chance to recover the 3D geometry from videos and given the depth information from images as a clue we can we can develop some 3D surface Rec conion method that allows the model to learn from videos and the depth information we have acquired in the first stage to help us better resol ambiguities and do better 3D suf contraction to obtain a 3D a 3D environment and uh and if you want so the third the third part is to make this environment to be interactable so we care about how can we decompose different objects after reconstruction so that the objects can be rearranged and can be we can create a new environments and lastly is is to handle the Dynamics of real world real world things that we would like effective methods that can effectively handle real world Dynamics in videos uh so I will go through the uh some of the works in my group following this uh TR dig research uh so the first is deps from monocolor stero images uh so the task is that given the input images we would like the model to Output to estimate the depth uh or depth given this this images so given this task a natural solution may be if we can plug in a de new network and treat this as a simple regression task to Output the depth values right but the depth is actually a projection of the structure the 3D surface onto the image plan right so the depth uh values they are constrained by the underlying 3D surface that are gener generating this depth values so in this uh in this work we would like to incorporate the regularization from surface normal that is another attribute uh that reflects the 3D the structure of 3D World to regulatee the estimation of depth so that it can more can Fally reflect the uh the our real world surface so in this uh so deps and sub normal they are actually geometrically connected so let like if we have the depth we can convert it into 3D pwn cloud and then we can estivate the normal from the pwn cloud and if we have normal the normal determines the the plan information and it can constraint uh the distribution of deps so in this work we incorporate the uh geomet geometrical con between depths and surface normal to have a better model that can help us to obtain depths that can more Pressly follow the structure of 3D work uh so we incorporate this dep to normal and normal to depth uh uh constraint into the model design and and then CH this model supervised by a depth and normal ground Toth data Al so this is this is these are the some results so you can see with this normal con we can we can have better reconduction results if the depth is utilized as a p Cloud so you can see here the pl a flat and we uh we compared with the Baseline approach okay and this also we can have better P Cloud reconstruction if we have this kind of regularization on dep prediction and this uh this is the result on the o do uh so you can see still our approach can better recover the recover the the s p Cloud that has a has a better quality uh than the Baseline approach okay uh so so this is one this is one work so but when we do this work we check the data but we find there's a issue when we check the ground trth data to Super Ching the model we find that the ground truth data are not accurate for example in this data set this is the K data set uh the data are captured using lat sensors um it is a time is is not only time consuming but takes a lot of resources but even after uh very complicated post processing steps we find that the ground truths cannot alar with the original image and the ground truth had a lot of mistakes so dep ground truth is really hard to acquire so even for using connect there are a lot of mistakes and a lot of areas that do not have accurate measurements but at the same time we have the computer game engines that can produce synthetic data without relying on human labors so you can see the uh the synthetic the quality of the synthetic data that can provide Dan uh ground to uh depth information with one accurate boundaries that are precisely aligned with the images so in in this work we explore whether this kind of synthetic data can help us to uh to do depth estimation so in this work uh we study this in the stero image setting the input is a pair of stero images and and we will as model to Output the disparity map that's similar as depth is a inversal depth and and the core idea is to use synthetic data because synthetic data have high quality and also it is scalable and we can collect it without so much P investment and efforts so our initial expirement using synthetic data already achieve surprisingly well result we can get a very accurate uh uh depth estimation uh using only synthetic data to further enhance the result because the synthetic domain and real domain has a gap we further develop some strategies to address the domain difference between synthetic and real data so the first uh the first strategy is to do domain normalization so we normalize at both the Image level and pixel level to remove the Stell and texture texture differences between synthetic and real Doms and another uh another modification of the architecture is to develop a structure preserving graph based filter to encourage the model to extract Global level information for prediction instead of rely relying on local information this is because when we look at the synthetic and real data globally they are very similar but locally for example the TCH Char they may be very different so we would like a model to try to attract Global Information instead of relying on a local information that has a large Dom differences so let's see the result uh so this the model is trained purely on synthetic data and we directly test data on real data so this is a video from citycape data set we have we don't use data for PR and this is the result from a classic method sgbm and this is the model that train on K data set using human habart ground and this is a model trained purely using synthetic data so let's look at the result so uh so it's OB uh that although we only use synthetic data the quality of the dep estimate the result is much higher than than the approach that use uh real real real ground to real lat capture data and you can see the details and the boundaries of the depth information is much more clear than PR with Baseline approaches that human captur ground to sta for learning So This research do tell us that the synthe synthetic data is especially valuable for geometry estimation tasks because capure accurate depth is is a is a invasible problem in if we you on sensor like connect connect of L and also it often you on warious also processing steps every step of the data capture is very is PR to making mistakes so sthetic data do help a lot so so for depth estimation recently we have also seen a lot of advancements like in this cvpr we have the marod paper uh this paper this work actually the model is also trained purely on synthetic data but although they they produce very very sharp results when when we look at a single frame the result quality is one high but if we look at the result across different frames the scills are in consistent so you can see there is is a significant fle this is because the skill will drift frame by frame and the model is only capable of producing relative depths that is not not what we need for conjunction right we need accurate metric depths so in comparison like there are also some works also in this cvpr the UNS work let we can say although the the quality is not as good as like a maral but it's consistency is much hair so actually this is very important if we really want to turn RGB video into rgbd video we want a consistency across different frames so the next step for depth estimation so of future work will to explore whether we can do we can obtain video consistent magic deps so this definit definitely need us to understand the camera models because the intrinsics of the cameras will determine and the image and the depth you your your data will have okay but uh in this part would like to emphasize that synthetic data are very important resource if we want to really want to recover depths from images or from videos okay so this is the first part so any problems Okay cool so uh so with this depth information from images from from wios they actually can provide very good regularization for S surface Rec construction so let's go to the second stage that is to to make to to turn rgbg into a 3D surface so so the first problem problem is about how can we represent a 3D surface we definitely can have different approaches right we can use the P clouds or work source and M so uh and also is another representation implic representation it represent surface as an implied function it offer the high resolution and high scalability and importantly is continuous so this is especially beneficial uh if for for us because we wanted to get a 3D environment so we wanted the rep pration to be continuous uh to be more closer to what the real world geometry look like uh so in this impl representation it will use a function to represent the surface uh so this function value is negative if the point if the location is inside the surface and is lger than zero if it's outside the surface then the surface can be uted by applying the by obtaining the Z level St for this function okay so this is the representation that we use for 3D surface construction uh let's look at uh the approach so so given this representation so the goal is to learn and implicit function to represent 3 from multiple post images so we have RP data video as input and we would like to turn and place the neural uh imp place the uh MLP function that can output either volum density occupancy or SDF value to represent the surface so PR resar has shown that using the SDF value as the output will give us a better surface reconduction quality so we also use this as the prediction so uh another paper mod SDF has also demonstrated if we incorporate depths and normal from monard deps exm approaches to S TR uh this impl function will it will gain a lot of advantages for surface con so this this monocolor depths and the surface add a strong regularization for learning a surface especially for same level withd conjuction but if we look closer into the prediction results we will find that say so there are a lot of missing detail the missing of detail structure here so this the detail of this chair is missing and also there is a black punch here is also missing from the rec conjunction so this m is us to investigate why why this happens so we we find this SDF based representation suffer from two programs if it is applied to S level surface construction the first limitation is the color Bas problem uh so we we find that the op optimization is based toward the high intensity areas and make the low intensity areas like the black Aras to be under optimized so for example you see like let's see in this case so for the brighter areas they will typically have a large gradient magnitude but for for darker areas the gadient magnitude is very small so this makes the model uh prediction will miss the Mis reconstruction of the areas that have the low intensity values so this is why this bench is missed from this reconstruction and another problem is the bi of SDF SDF based formulation if it is applied to S level 3D conjuction so we have done a simulation experiment we use the ground to SDF field or of this SC that consider multiple objects and use this ground to SDF to render depths so this this render depths is div Al also divice from the gr Sho deps but we depth as superation to change the model so this C candle B will make the optimization to be bed toward the large objects because the large objects will occupy most of the gradients so this is the reason why some small structure of the rec conjuction is missing so in order to uh solve this problem we propose two strategies to to address the two problems the first is that uh to avoid the B to low int to high intensity areas instead of using Color R loss uh we we use this feature rer loss we render features because using features will not have the low value issue and we bring back the optimization signal to superise CH or the low intensity areas and to address the limitations of the SDF based representation uh on same levels jion we incorporate occupancy based supervision to again bring back supervision signals for detailed areas uh so let's see the result uh the left one is the result from Mod SDF and the red one is from the Reconstruction from all approaches so you can see man of the missing uh let's let many of the small many of the objects are missing uh with this more SDF Bas reconstruction but all approach can recover this uh recover the Reconstruction of these areas and can have a more complic complete complete reconstruction you can see the result you can see a lot of small objects are missing from the original Bas line reconstruction but our approach can recovered and obtain of a more complete reconstruction uh but honestly speaking so although we have a littleit this issue a lot but this is still a problem for level 3 contraction so this we believe this is because of the representation we need a new representations to represent the surface if we want to Model S level uh Sev s level for S recr okay any problem so far okay let's move on so with the 3D surface uh you can say all the object are still not interactable right it's still a SC that represented by surface and all objects are embedded together with the whole thing is not interactable so the next problem we tackle is how can we make this uh reconstruct the environment to be interactable by decomposing the objects from the environment uh so so the goal is to want to exract the individual inst instances of objects from the Reconstruction so that objects can move and can be interactable so however if we want to achieve this uh this goal we need to an notate the object masks across different wheels in the whole video this is a very challenging task and is also consuming but the fortunately now we have lot Foundation models like the segment and model this can be leveraged as a tool for us to do better object exraction without rely on human annotations uh so this is what we do so uh so given the 3D surface conjuction we will exract individual objects these objects can be further processed with some Reet reector Tech techniques to get Diversified 3D asset and we can rearrange the objects to create new 3D environments so let's look at the technical details so the first stage is similarly as what we do in our previous work we do in place the contraction so two differences happen here the first difference is that we also separate the background from foreground and in place the Sur construction Stace and another one is that we will distill the segmented the featur from seg the segmented ening model to the impl feature field this is two uh uh uh two modification or PR approach and and after this stage so we have the we will we will have the 3D surface but the objects are still not separate they still model together so we have the uh second Stace that's to extract individual objects so here as we already distilled the same feature here at the feature field so a very simple not very simple idea may be we can do feature grouping right with group of features together to exract objects but we find that the distilled features are not sufficient for us to separate individual objects because you can see here although there there are different object instances they actually have one similar St features Al so motivated by this we introduce the second stage we introduce the second stage that is to uh okay okay so uh we use the second stage the second stage is is plac the stage given the surface conion from the first stage we will first exract uh m you Tred the surface mesh here so the mes has a good topology it has a you the Vex are connected in uh Along regul by the surface and we also will render the features from Sam from segment animation model and also we will ask users to click on one object and by combining the this click that is from users and the uh render the F from Sam we can fit them into the Sam decoder and then it will output the mask for the object but it should be noted that this mask is for only for one wheel of the object right so but we want to exract the whole object from 3D so the next step we do is to uh lift this mask into the 3D mice uh to use this mask as as St for the for the object then we we do surface growing on the 3D MH to you trct this individual MCH for the for the single object okay let's move on to the result so this is the colored MCH this is from the first stage the MH reconstruction and this is the mes result and this is the distilled same features so you can say if we only do grouping we cannot separate objects so this is the de composed result from from all approach you can see individual objects are decomposed we can move these objects and adding new objects into this environment to create a new environment and then uh add a new Texs uh add Texs on on on the mes to create a new environment with Diversified textures okay so any problems Al so in this uh in in this part so we actually have shown that in place the subf contraction can help us get a good surface uh but it often will will sacrifice the rending quality if we have a good surface it often suffer from the Rend quality so for example in this example the big is a new imp surface contraction approach it can have very high quality surface construction but the rendering quality is much lower than uh 3dgs a new representation for new for nor whe synthesize so the next question may be how can we simultaneously achieve how quality surf conjuction and Ne render like you see cion spting it can deliver high quality surface high quality rending but the surf quality is one low so how can we combine the best of both words so as we think this is the the next question and also we have some pre preliminary observations that there is a contradiction between the low frequency surface and the high frequency appearance for rendering so like in this example uh this this is this is a ground to surface mass and this is the mass You traed by go opacity field so you can say this is from gion so you can say in order to reflect this high frequency details there will be some noisy reconstruction so there there is a natural contradiction between the low frequency surface and high frequency appearance so the next step in the future we made explore is what the new representation for to achieve a good Tre off between reconstruction and R that we can simultaneously obtain high quality reconstruction and high quality R so so for this problem there are there is a lot of research that works doing that how can we decou geometry and appearance or maybe we can have some hybrid designs in the fure so we actually have some preliminary results using by combining in place the conjunction with go splatting yeah that achiev a reasonably good results we still working on this direction Okay cool so so the above is about surface reconduction but they all from static images a static video so there's no moving objects so the next problem is as the videos often contain a lot of dynamic objects the the next question is how can we handle the Dynamics in real world s to reconstruct the Dynamics of real world s so uh in this problem actually this Goen spting du to its explicit nature it is one suitable for modeling Dynamic things because we can DW the gions to move in order to model the Dynamics of the real world uh initially cion spling is not designed to handle Dynamic things so in our work I will introduce later we develop new approach one simple approach to make a go splatting capable of handling Dynamic sense so uh so the question is how can we make a 3dgs be able to handle Dynamic things so one natural idea might be we can for each of the cions we can let it to to have a Time war in six St transformation to model the movement of cions across different time steps however as we use only monocolor supervision we find that if we let allow ER CS to have independent six St Transformations for model movement there will be a lot of noisey aulion trajectory and as a result the rending quality is also lower you can say it's a blur rending result uh so so motivated by a simple observation that are motion patterns in your real world are often SP they are specially continuous and locally rid so we ask whether we can use SP representations to D gos to move so in this work we introduce SP points for modeling the Dynamics of cions uh so the idea is pretty simple so we have 3 cions we will firstly introduce control points that's one spot one a few control points and for the control points uh we will have MLP that takes the position of the control points as input and the conditional and the time information to provide to prict a six St transformation for each of the control points uh so this this MLP will model the Dynamics of the control points and then to dra the MS of the cion for each of the cion it will look at it K neighbors and the uh the six St transformation of the K neighbor will be interpret to generate the six St transformation of the cion to to make it move and then all the cions will have related six St transformation that will change across time we will uh we will do rendering to render images and this image will be compared with the BR video data for learning so at the same time we will also uh we also design some ping and densify strategies to uh control the control points to make it capable of handling then different levels of motion complexities so finally after train we find that we can use around 500 control points to DW 100 100 100,000 3D cions so the underlying motion patterns are essentially spots so these control points are learning the bases of the motions and they will interplate to generate the Motions of the Cs to model the real world Dynamics okay let's see some results okay so different training we also add as rigid as possible constraint to uh to encourage the local motion to be rigid this is also a strateg and utilized in PR approach okay let's see some uh so uh so the the left one is the L go trajectory without this SP control points and the right one is the allo you can see we can have more smooth we can have smooth ction trajectories and also it also benefits the r quality we can have one sharp qualities High qualities this is another example so this is another example this is the effect of the as Vis as possible conents it also help us to make the gotion trajectories to be smooth to follow real world motion Dynamics okay so this is some qualitative results I will so this is the recr result about this Dynamic sense you can see the r quality is high and I would like to highlight another feature enabled by this SP representation is that we can uh enable motion editing we can create new motions by just replacing this prediction using MLP with user controls we can allow user to modify the motions and to create a new motion patterns let's look at the result uh so the first row the first row is the results from the from original motion pattern used for Trum and the bottom one is the is the motion editing result that we let users to control the control points and to create a new motion patterns this is so in the bottom so you can say although these motions do not exist dur TR so the quality of this the motion sequence is still very high also the more results go fast this is the more result okay so I I skip part so this is use this 3D Dynamic CS to model casual videos so we can lift C video into Dynamics 3 CS enabl a lot of applications for example checking and will video cons deps and also video applications are okay okay okay so besides this Rec conjuction we also do some research Works relevant to generation with generate 3D objects in order to make the 3D environment to be Diversified so this is one work I will skip the details so this is another work that we can compose since since object see since this this the texture of this object are all created and we can PL uh add a new virtual objects into real world environment so let's uh to the conclusion part so so in this talk I have introduced our works that's trying to convert a casually captur video into 3D interactive environment so we have firstly we will trct deps from videos and then we do Sur conjuction and modeling that dynamics of videos and in the future there are still a lot of challenges in order to for us to convert how capture video inut and 3D interactive environment so this includes that uh for dep estimation we would like to have video consistent monar deps uh and this requires new data and new model for us to have video consistent RGB to rgbd conversion and secondly is still representation currently we think we still not have a good representation to model lar large scale s level recr suff Rec conduction tasks and also how to simultaneously handle Dynamics and and static background your uni unified representation still a problem uh certainly the rendering and geometry conjuction their Tre off should also be considered in the future and also for the Casual capture video the data are imperfect uh and uh the intrinsics and the fixic properties are also unknown so in future research may maybe there are a lot of research that can be done to uncover material intrinsic information and even physics from videos but we really to build a 3D uh simulation environment from videos and fortunately we now also have a lot of opportunities we have lot skill 2D 3D video gentic models and we have a lot of Foundation models that have a good feature uing capabilities that may offer a lot of opportunities for us to advance this area Okay uh this is the end of my presentations thank you okay maybe we are joining us in the panel yeah yeah yeah that will be good will take you to the interested time we'll take the questions in thank you understanding of the world around us capturing and constructing semantically informed 3D models of the real world including 3D reconstruction semantic understanding of community rbb sensors e e e e okay so I think that shows now the audio on on Zoom um you know in terms of just applications um we need to to understand spatial environments in order to allow machines to be able to also understand these environments um so uh we've been working a lot in terms of 3D reconstruction and then semantic understanding of these kinds of indoor environments um and uh this works I think uh to to a nice degree in terms of understanding the 3D scenes um but but what we see when we think about you know what humans potentially see when they they look at 3D is they're capable of looking at something like this image and understanding the general 3D structure um and that means that we have actually very strong 3D priors baked into our perception system because there is not um any explicit geometric signal in this in this image uh we have already a general notion of of what these kinds of stools potentially look like without seeing the other side or the other side of of the kitchen island um and so if we want to to to help machines be able to perceive this I want to talk about a slightly different kind of Paradigm then probably um you've seen a lot of work in terms of computer vision going which is towards modelbased recognition right so this is taking a look at okay in in early kind of Robotics if you want to be able to recognize an object um can you match it from a set of templates that you have available um and use that in order to perceive um so that we can of course then uh lift to this kind of application we have data sets like shapenet obverse and such um can we then retrieve and align uh a similar object from these kinds of databases and use that to represent the 3D and the scene and this is also particularly appealing because the objects in these databases are typically um human designed and so they're much more Compact and efficient and they fit well with modern sort of rendering applications Graphics applications to be able to use these representations so um a very easy way to think about this is to say okay if we have correlated pairs between regions of an image and the corresponding 3D shape we can just learn to match them uh together with this supervision information so given some kind of segmentation on the image um and the associated 3D shape we can use a contrasted learning formulation in order to make sure that the similarly structured objects lie close together in a joint embedding space and different structured objects lie further away despite them coming from uh these different domains and then you can also jointly predict the pose of the objects so that you get the right representation in the space so when you see a new image you can just go ahead and do a nearest neighbor look up in the embedding space uh and then find a representative object so this embedding space looks a bit like this you see the cluster between different types of structures of of of objects like bookshelves and round tables um are actually clustered together differently uh based on on the structure and that means that we can actually start to do this kind of retrieval based reconstruction um and if you have a good models in your database you can overcome some of these challenging scenarios like when you have a lot of occlusions for instance in front of this table or very thin structures in terms of the chair back or or the bed structure can be represented just by the nature of the Fidelity of these objects so this is on the Pix 3D data set it's um a a simpler retrieval scenario because in the database of 3D shapes associated with fix 3D you do actually get effective l a ground truth shape that's associated with the observation um it's not a very realistic assumption to make in practice when we want to go ahead and deploy this in terms of 3D perception since uh generally speaking the 3D shapes that we have available like shape net observers they're not going to match exactly the kind of stuff that you see in general settings maybe unless you are Ikea and then have that particular database available um it's not something that we can uh operate on so in order to make this more robust and not necessarily overfit to Global kind of shape characteristics that are unique to exactly matching settings um you can operate on a patch based level so figure out what regions of the image best match what regions of a local 3D shape and use that in a voting based strategy to get um the most similar kind of shape that represents things and this also has the side benefit of being much more robust here on the bottom to scenarios where you have occluded objects or objects that go a little bit out of the view of the image if you operate on a a more local patch basis um you can represent those better as well as the general structure for instance on these chairs um which interestingly um I think this does quite well for independent predictions on a per object basis but clearly when you we see these results you notice that the uh input Shares are actually all the same um and they're not being represented as the same shape but that's because they're being um independently predicted at the end of this pipeline here uh with no knowledge of any of the other objects in the scene at that point so that's something that can certainly be improved in that that particular kind of setting but the nice thing about this patch basis is that you get a much better structured embedding space if you look at the top eight nearest neighbors instead of just the top one you see that there's a much more similar structure in terms of the shape structure to the input query then if you actually do this mtic cat approach of associating shape by shape and that typically tries to memorize one very similar object if it can find one uh for instance the sofa and then it starts to produce neighbors that are actually very structurally different um whereas when you do a more patch based reasoning in terms of finding a similar shape to represent that image um you can get a much better structure in terms of of the queries here in the local neighborhood um the thing uh that uh I wanted to address a bit more here is that this is actually a monocular perception task right input image to a three representation and that is inherently actually a very ambiguous t ask in in in many different kind of ways so actually we would much rather have a probabilistic formulation of how to do this CAD reconstruction um in particular uh there are depth scale ambiguities in monocular perception right the scene can be a little bit larger and a little bit further away or a little bit smaller and a little bit closer to the camera um and it's inherently ambiguous based on just a single image um the other thing that's nice about having this probabilistic formulation it helps you with some logistical things like for instance you know that you don't necessarily see the complete object here in the frame of the view so there's this ambiguity here in terms of the object nature and also in terms of how to potentially match up objects that don't exactly match right so we want a probabilistic formulation to help generalization when we train only on synthetic data where we have actually good matches and the ability to synthesize a lot of um augmentations uh in terms of data and then just apply it directly to the real setting uh and having this prob istic model helps to bridge this domain Gap here so our approach uh here is basically taking um uh an image and producing several different hypotheses of what would potentially explain the cad reconstruction of the objects that are seen here so here you see there is a variability in terms of the length of the the particular sofa since we don't see um the last bit of it in terms of the image um and if you compose these together you see that there's um some kind of model of the the depth scale ambiguity as some of these explanations are a little bit smaller closer to the camera and a little bit further away so the way that this is captured in a probabilistic setting we have um a decomposition of these different potential um ambiguity modalities so one big thing is is depth scale um we operate this on a machine generated depth estimate So based on that depth estimate we're trying to predict the scale that scales it back to its correct true scale um and so that is being modeled probabilistically inherently with a diffusion process so that allows capturing these different potential kind of modes in terms of the the scale offsets that would correct um the depth estimate to something that is uh a correct explanation and so uh then we can sample a bunch of different kinds of scales to explain different potential versions of that particular scene being seen through the camera um so once we model the scale then we can go ahead and and and model the pose so this is the machine generated Jeff machine generated instance mask um if you put these together you get um a partial Point cloud of an object that we then use to inform a diffusion model that models the normalized object coordinates associated with these particular um object and so uh you can also directly just model the um the object pose as rotation and and translation um but it's a much harder distribution to learn correctly whereas the normalized object coordinates um is a much denser information source that then allows us to also solve for the object pose based on the the correlation between the the normalized object coordinates and the original um input Point Cloud here um and so this helps us to to model the potential object poses uh and then finally we want to um encode the distribution of uh shapes in terms of retrieval so the shapes we can encode through um a pre-training process to extract feature representations for all of these shapes and then based on the condition of the the nox we can then learn a diffusion process that predicts the actual object latent code here um and this we can then sample and do a nearest neighbor lookup into the the train database in order to get the actual uh CAD model that represents that shape so it time this is then going through all of these um diffusion models in a cascaded setting so we take an input RGB image this is now a real image from from scannet and get a machine generated estimate of the depth and the instance semantics um and so that depth image is going to be uh fed into the diffusion model that we can then sample different scale estimates from um and then we go ahead and use those um scale depth maps in order to uh take as input into estimating the object post and getting out different samples of normalized object cordinates here um and then finally we can go ahead and estimate the uh Laden codes of the objects for nearest neighbor retrieval um and that produces the final set of object hypothesis that explain um this image and so the the interesting thing here is that actually then when we train only on Sye data so synthetic 3D front data um we can become more robust than uh models that have been trained on IND domain data like Roa and and Spark um especially in these challenging settings where you see here like the the sofa is cut off out of frame um and so there's um a much more robust formulation here in in our model uh the table is um a little bit due to to data bias in the sense that a lot of tables are um happen to be um rectangular kind of tables in shape n so here this lets us more accurately model the round table when when we formulate it in a probabilistic kind of fashion and so here you can see just the alternative 3D um uh representation here so this is nice in terms of having a really strong prior in terms of the uh CAD mesh representation but certainly um just retrieving means that you're limited to the set of objects that are actually in the database itself so we might want to also be able to generate new 3D meshes that would better fit to observations right in practice um you know we we talk about these CAD models and these meshes they are the predominant 3D representation in Practical applications in in real world settings um because uh you know they are actually very compact they're very efficient to render um much faster than pretty much all other uh representations um from a learning perspective though they're very irregular which makes it extremely challenging to learn um so you know we want to be able to actually directly generate 3D meshes because if you go the route of generating a a neural implicit field which is super nice in terms of you know like optimization because you get gradients through the whole volume um you end up having to run marching cubes you get out this kind of uh messy triangulation like this where you have way too many triangles and also because of marching cubes you lose the ability to represent sharp edges actually um so of course you can go through run that through a mesh decimation process you lose a little bit of of the quality there and you don't quite get the the correct um tessellation that would intuitively make sense in in this setting which is that the table is actually just a FL Ling so you only really need to represent that as simply as possible um and that's something that you get when you actually learn directly from uh meshes that have been designed um by people rather than by uh marching cubes so how can we represent these these 3D meshes I mean we've all seen the success of sequence modeling and um Transformers for language these GPT models are extremely good at at modeling text various different kinds of languages even code um so we want to be able to leverage that for the ability to generate a MH which is can be interpreted as a sequence of of triangles um it sounds maybe a little bit unintuitive uh but um it it fits into the sequence modeling Paradigm and then we can try to model this with with a Transformer so um here when you have a mesh as a sequence of of triangles what we want to learn first is a vocabulary of these triangles kind of like a vocabulary of words in text Generation Um and that's being learned on uh shape net in particular a shape data set that has these kinds of human design kind of meshes so we want to encode all of the mesh faces um into uh a dictionary um of uh uh different description of what mesh faces would look like um so then once we have this trained in terms of a code book of uh face descriptors we can go ahead and put this into a GPT style Transformer and train it for uh you know next token prediction or next mesh Bas prediction Styles um and that allows us to do this autor regressive generation of a MH as a sequence of triangles the sequence in this case is inspired by bip polyon which shows they actually so you can just apply very simple sequence ordering just in zyx order you end up generating a shape um in this kind of bottomup fish fashion um triangle by triangle uh and that can actually produce these nice sort of compact shapes with very sharp um edges and and features here so this um Transformer vocabulary is is learned as an encoding on these mesh face features this is interpreted from um a graph convolution um and the features should represent uh the the nodes on or sorry the faces should be nodes on this particular graph representation um and then we can extract out these face features they're going to go through this quantization module to help um with the learning process so that faces can learn to kind of snap to each other uh since faces need to fit each other in order to make a mesh look actually coherent um the actual uh tokenization of face features you can think about the design in a few different ways um you can potentially have just one Global descriptor for that particular face um and then try to generate this um the thing that's interesting about meshes is that they they need to have faces that actually stick together otherwise it just just a complete mess right um and so there's this very uh sort of strict structure that vertices do actually get repeated um if you're generating faces as uh you know a set of of triangles over time and so that structure we can encode directly into the vocabulary as well by saying okay actually um faces are going to be encoded as uh tokens on a per vertex basis in this case we have two tokens on per vertex um and the whole face feature is just going to be the six tokens corresponding to all of these uh vertices and that means that when you generate a new face that should actually match with this face um we will see these tokens repeat themselves in the sequence uh and that's actually pretty important to get the faces to actually adhere to each other in the generation process is to structure that vocabulary so that you can easily learn these rep repetition structures um and so this is in the end the face encoding is um just the three vertices concatenated together into a series of of tokens that represent that particular pH here um so once these are encoded we make sure that the decoding can actually reconstruct the original uh faces um so that they encode the sufficient information here um what's interesting here is if you do this uh without um this discretization the quantization you get kind of what looks a bit like the face structure of a mesh but nothing quite fits each other and so this this quantization is is important to make sure that we can more easily learn uh the setting where faces are actually fitting and slutting in with each other so once that vocabulary has been learned we can go ahead and just train it with GPT style Transformer to predict what's the next phase in a sequence so here you see the start token of the sequence the end token of the sequence we have all of the face en codings um in between and then we just predict what's the the next phas um in terms of that that particular sequence uh and so when you go through an inference it you get this this nice generation on a triangle by triangle basis um that learns how to generate these kinds of interesting irregular structures that compose um a an actual match representation here and so if you look at that in comparison also to other works that generate 3D shapes um here you see uh polygen is also a mesh generation approach but it's not an an endtoend um learned uh U method um get 3D takes more of the prent style of learning a neural field and then extracting it with marching cubes um and you also have a couple of other alternative representations for how to represent and a 3D mesh uh in terms of an atlas or in terms of a binary space partition and and here we can generate much more coherent um structures in terms of these particular shapes I think this particular example is nice a bunch of generated tables and and here we can spend um a lot more detail in terms of the table legs versus the the top part of the the table here as well um I think what's quite nice about this is that actually this uh Transformer based approach my zoom just restarted so I'm going to just share the screen again so hopefully that's okay um and so this is I think Super interesting um and you should check out uh yall were at um uh presenting this particular work of of here at cbpr um H and it allows us to actually start to generate new interesting um 3D shapes uh I think there's still um an additional question in terms of how we can potentially model really the complexity of these real scenes right if we train on databases like shape net or even on observers that has tons and tons of of objects it's not quite the same as modeling the complexity of the scene distribution of what you see in in 3D scenes where you have all this kind of little clutter um that makes a real scene look kind of lived in versus when you see these kinds of real estate kind of post settings or synthetic data um so there's something I've been missing to really understand the the complexity of a of a real object distribution here um there's certainly uh a lot I think that is to be done for like then saying okay well these are all static scenes how can we actually model Dynamics um in indoor scenes Dynamics usually means what does a person do in terms of an indoor scene because the people are usually the the moving Factor behind um objects and so on and so we can try to start modeling uh that the dynamic motion of um you know a person or even stick that this is this is on a per object basis um if the video please maybe yes okay so we can do on a per object basis and then just stick that on all the objects in the scene and approximate some kind of um interaction with human scene obviously that's that's missing this complexity kind of notion of of the actual scene configuration and it it won't get you that far in terms of of of true Dynamics um but a lot of the challenge here is actually in terms of of looking for data um we don't have a lot of of data in the 3D domain and when you move to the 4D Dynamic domain uh it becomes even more challenging um and that's where uh you know we can start to take inspiration from uh these very powerful generative image and and video kind of models you know learn from that in order to zero shot uh interaction generation so given this text prompt generate the person on the scene performing that interaction without actually training on any 3D interaction data um and that's purely by leveraging this imaginative power of uh you know image generative models in this case um we're taking a look at stable diffusion in painting models uh they're very good at imagining what kind of interactions could occur in an environment um you know you can ask them to generate various kinds of images of of people uh interacting with their environment and and use that to distill interaction um knowledge to produce a 3D version of an interaction in an environment um and what we do there is is really just leverage uh this imaginative capacity and then try to lift it into a 3D consistent setting so the input is a 3D scene um you have an approximate Point location in that scene uh and the text description for the interaction you want to generate you can just render a bunch of images from that location uh for you know where that interaction should occur and then ask the Imp painting model to synthesize that interaction right um so there are a few things to take care for here um if you just do this directly uh the Imp paint model will rewrite the whole background which is not great because we want the interaction to be actually consistent with the background so in order to keep it the background context consistent we actually take the attention according to the Token of the person in this case the word woman um and use that as a soft guidance for an imp painting Mass um this is what this looks like over time of the attention and the Imp painting process in order to localize the person such that it's imp painted with the most consistent uh representation of the background still available so that gives a bunch of 2D interaction hypothesis um ideally we can then optimize for some kind of uh consistent 3D representation here um one thing to note is that the appearance is still of course different these are generated on an independent basis we can't expect that the appearance is the same so we can abstract that information out by just saying okay we only want to match the 2D poses we want our 3D model to match the 2D poses obviously again these are all generated independently of each other so the 2D poses are not necessarily view consistent uh from a 3D setting however if we generate a bunch of them we can expect that actually some of them are going to be consistent with each other in terms of the the 3D representation and that's exactly what we optimize for so here we're optimizing for a simple body model of the person these are the pose and shape parameters and we want to optimize such that it fits with the 2D poses um while allowing for some flexibility in terms of which ones are actually consistent we have a variable associated with each hypothesis um that will help to down we inconsistent hypothesis uh in the optimization so that we're just left with the most consistent set to inform the actual interaction synthesis and that leaves us with this nice kind of sitting interaction here so this is very powerful in terms of just uh raw generality um so we can start to go ahead and imagine kind of things where we have no 3D interaction data available at all you know sitting on a cow picking up uh weights at a gym and and so on um there's no uh data set that that has that kind of realistic information and this allows us to extend the knowledge of the information out here um certainly there are a lot of limitations nonetheless uh if you look at this C certainly the person's fine grain motion uh or action here is not quite correct um and that's because we have these different levels of abstraction towards saying okay we only want an approximate consistency with the extracted 2D post um it's a very coarse representation of the interaction uh and that's what we can extract out here um but I think this is super promising in terms of starting to inject more generality um and hopefully also bridging this with models that have then been trained on this explicit 3D representation that tells us you know what should um a good 3D uh mesh or or scene or interaction actually look like so um hopefully that's also uh interesting in terms of this uncomposed um General prayers uh along with um uh sort of true High Fidelity kind of 3D data um with that I would like to conclude um and uh also um you know uh can take a few questions if there's actually time sure body but body to of Capt the en of or you model the en and then sure I think in general it makes sense to um have something that allows you to get as close as possible to the original image prior which was trained you know on appearance uh in this case um we explicitly abstracted that out because uh we like with just a pure image generative model you aren't able to generate consistent appearance over multiple different independent images um so it's a lot easier to abstract that out perhaps um I don't think it's an impossible thing to uh to modify the formulation such that you could encourage a much more consistent generation of the appearance o over these kinds of images there are some small kind of logistical things to fight about so like the pose abstraction helps also with um biases that are still baked into image narrative models which is that often it generates the person with a front-facing head um which kind of sucks when you want to generate lots of different uh views um of a person performing an action and it tries to to force the head into the front facing configuration it's it's something that is sort of easy to bypass by um just saying okay let's just deal with pose instead and but I think that is what is necessary if you want to get something that's much finer grain in terms of the interaction understand okay so the first question how to handle let's say bad meshes right with have self intersections or stuff like this we don't because shapenet inherently has in its training data lots and lots of self intersections um so we're just trying to model the data that has been available in terms of what what humans approximately have modeled um and that means that yeah uh there are already self- intersections so it is entirely possible to to generate U meshes with self- intersections I think this is another challenge as to figure out what are the right kind of loss configurations or supervision signal when you are training on imperfect data in this regard and and trying to encourage you know what is um really a good mesh um because there are a lot of things that we can say ideally it should you know be manifold not self- intersecting and all this kind of stuff um but if you fit together all of these lost configurations into the the training Paradigm there usually still a few edge cases of what can occur um that doesn't look quite right in terms of of what you want and and I don't I don't know what exactly the right solution is but it's still um certainly important but I mean uh one way to potentially bypass that is just to get a lot of clean data I don't I don't know how how feasible that is at the moment um but it is certainly a lot more difficult to learn when you're learning from already a corrupted set of data um so in this case the goal was more to say okay this is stuff that you know people have generated so whatever kinds of characteristics that occur in those meshes are characteristics that we will imitate For Better or For Worse um in terms of sequence length I think this is something that also um uh it's just an inherent thing towards you know Transformers llms and stuff like this in terms of of understanding better context um there are now some uh you know um approaches that have been proposed just for the the general language modeling um Transformer domain uh like that potentially allow for a much greater context length um so certainly I think that would probably be applicable in this domain as well um you might imagine maybe also reparameterizing the setting such that it's a bit more semantically informed because this kind of zyx sequence generation is not the most intuitive one in terms of how a person would generate an object um but uh it is I think still an important question than how you would Define that particular um setting because we don't really have any ground truth information as to how to really separate that out in maybe some kind of better hierarchical semantically informed um approach okay so um in terms of modeling Dynamic objects or modeling humans I think the main thing is just is their data so for humans that's definitely the case in fact um then it maybe it depends a little bit on on what kind of data so there's a lot of data um that is uh let's say represented by a simple model which is almost too simple to generate because it's the same mesh topology anyways right um so ideally you'd like to to generate something that is uh humans that have been modeled by you know digital artists that actually have that because otherwise if you want to use this to model like Marsh Cube smashes I think you may as well actually just model the implicit feels first because they're actually very nice to optimize and there's no reason to try to to model like marching cubes triangles from from scratch um but uh but it's it's mainly I think about the data if you have a sufficient quantity of data you can also um bake in the dynamic information here right but then like the question is what data is that coming in if you have a lot of artists created meshes of that style um I think this can be directly transferred to that setting uh the thing is I think a lot of the data that we have of of humans encloses like captured of real people which means it's usually like you know implicit field marching Cube style kind of meshes in which case I if you're going to work on that data it's going to be better to just model the implicit Fields themselves because there's no reason to to do this weird roundabout thing of of generating the marching cubes meeses in in that setting uh by the way I uh completely my bad left out y on this thank you slide because I copy pasted it from a different presentation but you should go check out y's um best GP to work uh here at cvpr and and also lay for the interaction Sy um yeah so you can definitely add in an exra condition I think there's been uh work that other people have already done to produce a shape conditional version of a mes GPT style autor regressive Transformer and it looks like it works quite well it helps them train on obers um so I mean there's a couple of different ways you can input in in that potential kind of condition in terms of whether it's directly in terms of encoded in the sequence or injected in um but I think there's a lot that can be done then in terms of a conditional mesh model um so that's what these guys sorry I blanked a little bit on the name I think it's um uh there's already an approach that because of the shape condition allows them to train on um a lot of different object class categories I think here you you could do it as as well it's just a more complex learning Paradigm you need a lot of examples of the different object classes um of which shapenet has really a lot of like chairs and tables and a lot less of everything else um so that makes it a little bit of a logistical challenge if you operate on on only shape net kind of objects i' like to ask the panelist please come to the panel table and also J

