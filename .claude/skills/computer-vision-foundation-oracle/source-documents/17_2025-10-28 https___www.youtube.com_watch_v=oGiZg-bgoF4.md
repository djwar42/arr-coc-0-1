---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=oGiZg-bgoF4"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:04.168Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=oGiZg-bgoF4

bc1e7907-3a53-4db7-9fd9-944bdea2b311

2025-10-28 https://www.youtube.com/watch?v=oGiZg-bgoF4

c20956a2-681a-4266-9956-83069d58866d

https://www.youtube.com/watch?v=oGiZg-bgoF4

oGiZg-bgoF4

## ComputerVisionFoundation Videos

hello everyone this is T Dream from AWS aai thanks for joining us in the object Centric representations for computer vision tutorial in this session we'll talk about the topic of breaching the Gap to real world object Centric learning this is a catalog of my talk we take a retrospection on the motivation of object Centric learning then we are check the gap between where object Centric learning starts and how Realo data requires it to work then we'll introduce how to upgrade the object Centric model from three perspectives learning objective encoder and decoder at last we briefly introduce our open- source object Centric framework ocf object Centric representation is a compositional structured representation compositionality might bring systematic generalization compared with image patch token object tokens is a more comprehensive unit that we can find its natural language counterpart and what physics applies on as Francesco just mentioned this is the foundation of Cal representation learning we hope object Centric can run in a self-supervised manner then we can learn the knowledge of the World by just watching video the third motivation this is an endtoend differentiable architecture object tokens can be passed to various Downstream tasks and to be optimized together such Spirits can be found in recent endtoend autonomous driving and robotic system we St diving into details from observing the gaps between where object Centric learning starts and what what we expect it to work the slot attention model published in eures 2020 is a representative early work of object Centric learning slot attention is an iterative competitive attention that can be considered as a differentiable interface between distributed representation and a set represent ation which we call slot we have three facilities to support the arism to run encoder learning objective and decoder for the original slot attention paper the encoder is a small size CNN the learning objective is to reconstruct image RGB pixels from slots the decoder first broadcast each slot into a spal t tensor then use deconvolution operations to up sample to image resolution along with RGB pixels it will also produce a alpha Channel each slot is decoded separately and use the alpha channel to compete to explain the image with the architecture described in the previous slides this are the data set the original slot attention model can work welome clever multi-d Sprites determiners all the object with simple shape and texture and lay down simple background then also on theic image not video These are the data sets recent object Centric model can work on most of them are real world imag and videos with much more complex texture shape lightening rigid and non-rigid motion in this tutorial session we'll mayorly check how these upgrades happen firstly we're look into the learning objectives the original slot attention model lears by reconstructing RGB pixels from slots however RGB pixel level Reconstruction from Real World complexing is very challenging especially for a simple mixture decoder instead researchers tend to reconstruct signals that are representative object needs less hetrogeneous vising object and can be obtained in a self-supervised or other scalable way the first work we introduce is SL attention video one of the major upgrades is video level structure however this is not the focus of this tutorial session we're describing more in the next session what's interesting here is instead of reconstructing the RGB pixels it reconstruct an optical flow map Optical flow has two advantages first it is homogeneous within one object if we're considering the common fate principle second motion is a strong indicator for object so Optical flow is easier to reconstruct and it also powerful to capture object s attention video can now work on this movie or movie Plus+ data set this data set is also a synthetic one but the object are scanned from Real World with complex looks there's a physix simulation engine behind th we can mimic real world motion collision and gravity the data set is generated using the kubric engine Open Source by graphs group it can generate video Optical flow depths and segmentation labels or at once though synthetic this is a big step towards real world here's another object Centric work utilizes the optical flow reconstruction idea those Ste using a CN encoder and a mixture decoder by taking Optical flow input and Optical flow reconstruction objective the model can now work on some real world data sets with moving objects including Davis 2016 zra V2 fbms 59 and mocha while Optical flow works well for dynamic objects It suffers on static objects a follow-up work from the same group inan Savi takes Inspirations from human development and hypothesis that information about s geometry in the form of depth signal can facilitate OB Centric learning the learning objective is to reconstruct the depth information captured by Lia sensor this model works for both static and dynamic objects though depths can be considered as as a supervision the signal can be collected in a scalable way like installing a lighter sensor on top of a vehicle and just a drive around you can collect a large amount of deps data the model now works on way more open data set which is famous for real world automas driving the work we just described scale to real world data set without touching too much on the encoder and decoder this is good but also has trade offs we make the learning objective simpler by utilizing a pre-trained flow net and a pre-collected depth data set in this session we check upgrading on the encoder Vision Transformer is a strong Vision encoder that meets success in a lot of vision tasks why not simply introduce that in the obio Centric model we describe the reason here encoder and decoder should work together to make an object Centric model work the larger and deeper encoder we use the larger gap between the final feature where slot attention is executed are and the raw RGB signal while using weight encoder is well accepted Choice the decoder to RGB space is much harder to design before we moveing to the decoder part another way to make things work is to just reconstruct in encoder hidden feature space this is a thought mentioned by various work on world model learning it also applies in object Centric context we introduced dinosaur architecture we worked on in AWS AI labate the core idea is to upgrade the encoder into self-supervised train Vision Transformer like dyo or Mae and the Reconstruction objective is some Vision Transformer output feature in this way we still keep the decoder relatively small it's either an MLP mixture decoder or a f layer Transformer Auto regressive decoder though the idea is simple the dinosaur model can work on real world data set like VC 2012 and Ms Coco and compared to previous object Centric architecture we could tell the object Discovery performance is better okay this is the first page we show performance comparison table and diagrams as common in object Centric literature we evaluate this task using foreground adjusted rent index AKA FG AI a metric mattering cluster similarity additionally we compute a metric based on interaction over Union the M best overlap AKA MBO MBO is computed by assigning each ground shoes mask and the predicted mask with the largest overlap and then averaging the I of the assigned mask pairs in contrast to AI MB takes background pixels into account that's also measuring how close masks fit to objects on data set we object have a semantic label attached for example Ms Coco we can evaluate a metric with instance level masks and semantic level masks this allows us to find model performance towards instance or sematic level grouping we use this metric we can observe dinosaur works better than previous clustering and object Centric method on both synthetic and real world data set we also compare our method to other unsupervised object localization segmentation model in this comparison we use the well used segmentation or localization metrix such as kog and mou our conclusion is that our method reaches comparable result to what has been previously reported the self-supervised method such as token card and a stago we also introduce two follow-up works of dinosaur the FR first one is adaptive slot attention a mayor drawback of most OB Centric models including slot attention is there Reliance on pretty defining the number of slots this not only necessitate prior knowledge of the data set but also overlooks inherent variability in the number of object present in each instance to overcome this fundamental limitation we present a noal complexity of their object AO encoder framework within this framework we introduce an Adaptive slop attention mechanism that dynamically determin the optimal number of slots based on the content of the data this is achieved by proposing a discrete slot sampling module that is responsible for selecting an appropriate number of slots from a candidate list our framework tested extensively on object Discovery task with various data sets shows performance matching are existing top fix slocked attention method this work is accepted by cvpr 2024 if you're interested in let's chat on the poster session okay here now people may ask if object Centric model just another unsupervised detection and segmentation model well we'll emphasize that our go is not pixel level accuracy on segmentation but to get a good structure to the scene representation where each object has a token and a representation this structured s representation is aimed for systematic compositional generalization to demonstrate compositional generalization we borrow one slide from Eric JN block there are different types of generalization for example systematicity re combining constituents that have not been seen together during training productivity test sequences longer than once seen during training for vision we'll see by constructing a scene with more object than in training we can still get a valid representation substitutivity an expression is unchanged if a component is replaced with something of the same meaning for vision it means we can replace an object in a in in a scene we still get a a valid representation finally localism this means of local parts are unchanged by the global context in the next session talking about decoder we'll start to see how object Centric representation makes a difference difference and in later talks we'll also see how it works well on Downstream tasks finally we'll go through the decoder part this comes at last but actually the most interesting one we'll start this session by describing the slot decoding Dilemma to make a slot focus on Pixel area corresponding to an object it is usually required to employ a wig decoder to make object concept emerge in the slots wicker decoder prevent one slot from modeling multiple object introducing a statistical regularity in the Slate paper presented in I clear 2022 they had an experiment showing that if you're just using a stronger decoder the SC cannot be decomposed in two slots however weaker decoder can make the generated image blur and Miss details this is a so-called slot decoding dilemma in this section researchers have different solutions on this for example they will introduce more s inductive bias to keep the decoder in moderated SES while still providing high quality image generation the second and the more important one is people decoupling the decoding process make it possible to get to real world a real really good looking reconstruction different performance we'll start by checking how to utilize 3D inductive bias to help decoding recently a powerful nov view synthesis architecture Nerf draws a lot of search ATT tension in this model observe the grouping architecture is still slot attension but the learning objective is turned into normal view synthesis like how the EUR nerve model is trained in the decoding part each slot is sent to the Nerf decoder together with some special coordinates each slot is decoded separately and in later phase transmit and density are combined across objects though this paper only experiment a mul clever synthetic data set the idea it introduced is inspiring another work using nov synthesis as the objective is osr object Sy representation Transformer published in NE ribs 2022 the encoder for OS s it is called s representation Transformer that is a geometry free but a Transformer architecture TR by normal view synthesis object representation is obtained by slot tension and the decoder is a modified aray based decoder the decoder is faster by avoiding decoding object one by one as what uh the model did in the previous slides this model works better and faster uh compared with the observe model in benchmarks like clever 3D and the multi-shape object net okay after checking the method utilizing 3D inductive bias we check another method decoupling decoding the work we introduce is slate and its video version Steve in the decoding phase instead of decoding into pixel Space by one shot slate decodes the object slot first into VQ code space then apply another pre-trained decoder to turn VQ code into images in this way we can keep the trainable slot decoder relatively small but also powerful in slate they are using a moderated SES autor regressive Transformer as the decoder this aluto so introducing some nonlinear interaction between slots we get a VQ code by training a VQ vae a discrete vae such idea is very adopted in famous models like d and stable diffusion model now we're talking about VQ space and diffusion model why not just introducing diffusion model to object Centric the object slot diffusion model published at NE 2023 achieved this in the original stable diffusion model feature were conditioned on text feature through a cross attention layer to achieve text based image generation in RSD they replace the text condition into object object lock condition the RSD model takes a pre-rain out encoder as the encoder a prrint stable diffusion model as the decoder the learning objective is to predict the diffusion noise such model works on more movie Plus+ data set as well as real world face data set ffq as we discussed before op Centric is not just another segmentation model what's more important is to get the object level representation and the ability to systematically generalize in RSD we can get we can edit the SC by adding removing slots this is called slot-based image editing the edited the scene is still real looking which means the structure representation generalized real we can also do the same with faces in face the slot attention model vertical compos facing components such as hair nose mouth and Eyes by combining face element from different person we are actually getting new person ID like the image demonstrated on the right side which is interesting another model utilize diffusion model is dorsal this model is from the same group invented osrt they first TR an osrt model to get slot representation while considering the r based decode is there not strong enough they tune another diffusion based decoder to get more real looking scenes and videos this model works on multi-shaped net and street view data set the doo model is also a good example to show the combinatorial benefit from OB Centric representation we get the object slot in s one and S two by just combining the slot together and use the diffusion model to decode again we will get a s combining object from both SS you'll see the exact object from scene one and two appeared in the combined scene we know that there are powerful image editing work that can stitch an entity into the target image but it is not that simple and straightforward like the slot based editing the objects are tokenized and can be used as assets and combined with diffusion model as text token this is really interesting okay besides the mainline skeleton of this talk there are also important works in the object Centric learning topic that I interesting and play important rule to make it work on real world image and videos I listed a few topics besides training alone object Centric architecture can be trained together with other objectives for example in this OBS segmentation work slot attention is built in to be trained with image text pair contrastive law and entity predicting law OB segmentor tends to be a strong open vocabulary object segmentation model that works on real data set like Pascal Vos Coco and AD 20K data set another important topic for object Centric learning is disentangled representation we consider decomposing things into object is still not fundamental enough when object also has attributes like shape color position Etc in this neuros systematic BND work slots are further decomposed into blocks where certain dimension of a slot has certain meaning this enables fine green slot based image editing other important topics are about videos we are signals in video placing important rules on describing object this will be discussed in detail in the next tutorial talk by Tom and we also check how OB Centric architectures are applied in various Downstream tasks by the first talk in this tutorial by Professor Fu finally we'll take a few minutes to introduce the open-sourced object Centric learning framework ocf released by a AI labate ocf or obio Centric learning framework is a framework designed to ease running experiment for object Centric learning research yet is not limited to this use case at its heart lies the idea that why code is not typically composable many experiment in machine learning very similar with minor changes and only represent minor changes one such example is multitask training where a model might might be trained to S multitasks at the same time different appliation of set model then contain different model components but largely remain the same Olf allows for such ablation without creating duplicated code by defining models and experiment in configuration files and allowing the their composition in configuration space VI Hydra we put a link of this framework in these slides in this framework we pre-implemented different modules for each components to let us exercise what has been covered in this talk bridging to rear World object Centric LLY and encoder we implemented several CN encoder and Transformer encoder decoder we implemented the mixture decoder and auto auto regressive Transformer decoder for the learning objectives we implemented the Reconstruction on RGB value on dyo feature on VQ and the optical flow we also implemented several tricks to make obric model works well such as the slot initialization trick and the latent duplic suppression trick we also have some pre-implemented models in the motor Zoo such as slot attention savy slate dinosaur ocot and adaptive slot attention feel free to have a try and Report issue to us thanks very much for your time joining us why um and we are going to break our presentation into two parts one part which directly depends on this other variable Z which U can still have some high information with the labels because of this selected vable which opens this VOR um but um this B is assumed to be potentially changing between training and test time um while the features that are directly posed by Y and not by Z will remain invariant so now if we have this separation um you know we can flexibly decide which information we want to use uh when we make predictions um of course uh having just in variance may not be optimal in the sense that uh when we are in distribution this these features uh still contain information about why and so we we may very well want to actually use them um but then you know when we are distribution maybe we want to fle the side that that this feature should not be used um and and similar ideas have popped up both from uh this Auto distribution perspective but also in In fairness where uh instead of talking about perhaps you know new environments we talk about uh accuracy across uh different subpopulations or or or groups uh and again having an explicit representation of your groups can allow you to you know decide flexibly um how you want to make a prediction order to satisfy whatever F metric you're interested in and uh this is a slide that I had in my uh phc defense at the time I was I was working on on this entanglement and uh I wanted to prove to my committee that the the structur representations were broadly used so what you see here um is um three uh very different tasks um and you see the Brank correlation between uh this different performance matri uh on fairness task and reasoning task and how to distribution generalization task um and and you see here the rank cor relation and this looks good um you know everything is working as expected um and uh whatever classifier we train on our thisle presentation is going to you know simultaneously achieve all properties um um you know um without actually uh doing anything special of course the the reality is a bit more SLE than that um and uh this is perhaps through when your representation is actually uh robust to uh these changes but it's not necessarily true that the the representation itself will be more robust when it is dis uh I think it's very common misconception which I also had when I was working on this topic um so here you see um an example of a simple um experiment U where we consider the different types of systematic splits uh for training and test data which could be you know random splits composition interation extrapolation uh and this is a simple visualization this Price look like and and what you can see from from this plot here is that uh you know this unsupervised and weakly supervis taking away structural methods and some of these even have identifiability guarantees which later uh don't necessarily do better than the unstructured representations and and the more data approach IM that still seems to win in the uh most more complicated reges um so really the take message here is that the structure may not be universally useful um for but it is certainly true that if your Downstream task does align with the structure and then your actuation robust then then uh you uh you get a benefit the second example which um was uh um also very common when when thinking about the learning was in reusing Knowledge and Skills to solve new tasks in reinforcement learning and particular points um so again let's say we train a policy um on a certain platform and uh this policy learns to manipulate one Cube then we would expect that this policy would work well when our Downstream task perhaps uh involves more than one Cube um but still we're going to manipulate one Cube at the time so at least in terms of like skills and mot skills that the robot needs to learn um we would hope that that our policy will transfer um of course we do this from from pixels um directly this may not necessarily be the case and so um again unsurprisingly what people started doing is to uh when you have object relational task like trying to introduce um a suitable like mirroring structure in in the mod so um if you're task involves you know predicting the um relations between different objects and interaction with the environment then if you structure your architecture exactly in this way where you you know first compute uh objects uh relations and interactions and then the interaction between the objects and external effects does not only allow you to train faster but then also will allow you better generalization for example um predicting Dynamics which are out distribution because you have more op than tra and again this this has been used uh in uh reinforcement learning this is a simple example where we want to push a cube to a specific location and we have a certain number of destructors um and this number of destructors can change dramatically between training and uh deployment time and uh again you know if you embed the assumption that the interuptions are only pairwise um between you know the robot finger is pushing typically one object at a time um and um you know uh one object is almost never affecting all the other objects at the same time then you know incling this assumption actually help you to uh Le faster and generalize better because you know in a way your assumption mates reality and so um at some point people started u training like interesting like uh policies directly from pixels also in in a in a sised but again um the structure will perhaps not be universally U useful so here is the result from from a study by and Al um last year's icml where they looked when do object representations actually help for this reinforcement and uh you know perhaps not surprising uh the um result was that they helped the most whenever you have tasks which are well described at the object level so if your task involves comparing different objects or comparing the properties of different objects then you get a lot of benefits to structure your representation in an object centc way um if um your goal is to Simply push an object to location then then you as much and I'm going to talk a bit more later on the actual subject so really the uh take message for uh this part is that the the structure helps on specific task which are aligned with the structure it's not it's not something which is you know uh uh in practice at least in generally useful um but uh if you want for example to ignore certain Nuance variabilities or you want to flexibly decide to include this variabilities or not in your D test and then you know having a representation which is aligned ands um so really the the underlying issue that we're describing is uh when we when we giving a problem how do we how to we model um so let's stick with this robotic example and uh um our task is given initial conditions predict where the cube will land now um there are many ways to answer this question correctly um you know one extreme could be well we control the robot so we know you know what the cube is initially we know how much force we are applying to it so perhaps you know we also know physics uh and so we actually can just compute where the cube will end maybe it's not super easy but but you know like uh I'm sure like um you could do this the other extreme is to say well we don't know anything about the world and we're just going to hit the cube a million times take a bunch of videos and trying to train a big more uh now both these options are are valid and you can imagine there is a whole spectrum of Solutions in between but the important consideration is is how we model the problem and how we represent the state of the environment uh for the model can hide or rever certain structural properties and this could be more or less relevant for different tasks so uh for example you know if we um only train a predictive model um to predict the cube will land initial conditions we may or may not be able to um predict uh which force we need to apply to land it in a specific place or what would have happened U if the cube was heavier than the one we actually so then uh people started the exploring this idea of having the structure emerge from pixels and um trying to train a structure War model jointly uh with this obstruction St and I think this paper was was pretty nice in this refer mechanisms uh is again for reinforcement learning and they have uh they want to train policies from from pixels and so they have this this obstruction step which is in a way very similar to to slot attention um and uh as they get the structure presentation they can then match them with the um default latent mechanisms that just Express the Dynamics uh and model spse interaction between the objects and again embeding these assumptions at interactions of sparse um then you can you know improve the success rate uh and efficiency in otherwise problem polic fixes if we go back a few years um this idea perhaps uh can be traced back to this ban perspectives on on C interpretation where we can think of the different objects as lat variables and um our image is uh you know sampling of the statement variables and then M down to our observation space uh and then a lot of Works uh take this perspective perhaps complicate the structure of the generative Model A little bit but really the the game was how can we do this posterior inference efficiently so we started with the um Auto regressive uh model trying to explain one object at the time to doing um expectation maximization assuming uh slightly more sophisticated spatial mixture model to do then uh posterior approximation with the with variational INF and of course there were also architectural Innovations in between but but um at least a big part of the game was how can we do this posterior sence and Sol this inverse problem accurately um and before then um people also explored you know actually trying to keep the generative model fixed uh and uh uh simply do posterior inference on on the paramet factors uh and then taking the input parameters so as the representation this inverse graphs and what I wanted to mention here is that this idea are actually still used um in particular in in Neuroscience um where this simulation based inference still quite popular U because there um they have this uh mechanistic models which should approximate you know how uh different neurons uh work and and communicate with each other other and these are typically not really differentiable so what people do is to use this mechanistic model and whatever priv information they have to generate data and then uh using a neural this density estimator trying to compute U these posterior distributions over the parameters of the that goes as into the me model and then they can do this iteratively until they fit the data uh so uh methodologically um a lot of this works uh formulate this problem of learning OB structions as a probabilistic graphical model and then the um task is to do posterior inference um efficiently so this is where I would like to then connect it with the um caal models so um I'm going to give you a a very brief overview of what C models are about and I think like an easy way to understand it is is from the FL principle which states that if You observe two random variables and they're correlated with each other either one is the close of the other or there is some third variable which is cling both of them so if you see the number of babies and the number of stars being correlated either the babies attract stalks or the stalks bring babies or uh you know the economic development of a country C influence and um this type of model give you uh strictly more information than a simple uh meion Network so here you have a simple example U where you have um three different uh vas networks that still encode exactly the same um conditional independencies uh but this would be three distinct caal models in the sense of the intervention distributions of dat there so um the key difference between a statistical and a causal model is that a statistical model describes a physical system using a single probability distribution so we get independent samples from this distribution and then after we train our model we'll be able to generalize to new samples that are independently drawn from the very same distribution with C models instead we try to describe a set of distributions which um are over all the possible configurations that the system can take and so this is a complete description of the physical system once you have this you can do caal INF reasoning to draw conclusions on on the effect of this interventions contact so um to establish a little bit of a language um we encode um caal relations in the structural form uh in the graphical form where we have a graph the variables are the nodes and the caal relations are uh the edges this is similar to the normal V network but we here we also have this uh Interventional perspective um so um whenever whenever U you know people get exposed for the first time to this causality ideas uh um there is always this confusion of of uh you know where does this graph actually come from and can we really establish in a way a universal um causal graph that describes the AR and there is this nice quote U which I think uh explains really the the cracks of the problem here so uh this quotes reads as follows uh in this section we clarify what we mean by a two causal graph in short we use this term if the results of run of my studies are determined by the graph and the observation joint distribution this means that the graph and the observational joint distribution lead to causal effects that one observes in practice so in a way this means that um your graph contain information about the system and given your assumptions which are encoded in the graph and some data that you observe you are able to predict what would happen if you did an experiment in the real one so in this sense uh the true caal graph doesn't necessarily have to be unique what we really are after is having consistency between the predictions made by the models and the predictions and what that you actually do an experiment so more formally um we typically use uh structural C models um so here we uh further specify the relation between the ROM variable in a in an equation form and so we say that the variable XI is a function of its parents and some other um unexplained noise variables for U um and these variables this U variables contains all sources of Randomness in the system and so then they entail a joint distribution P of X1 to xn which factorizes according to the graph as a normal bwork uh the key with the Bion network is that U this class also describes and and the structural equations describe intervention distributions so um this is an example for a hard intervention um what we do is fix the value of one variable to say x we're going to cut the incoming edges so now this variable doesn't depend on the parents anymore and uh this will have a very well defined change in the in our joint distribution where everything is going to stay the same except for uh this conditional here which is not a conditional anymore it's replaced with a so now the distribution could be could be very different if you just look at the Joint probability table but when you consider the structure only one me change and now uh with this notation is is nice to go back to the beginning of the talk and um describe CA of point on distribution shifts so of course if our test distribution can be completely arbitrary um then we we cannot have any hope that the machine Lear model be able to generalize um if suddenly you know all cats can become dogs and vice versa uh there there's nothing we can do to prevent that uh but if you are comfortable assuming that the uh our problem has some underlying structure and the structure will roughly remain the same between uh our trading and test time um except for perhaps one conditional um or or few conditionals that have changed then you know having this additional assumption that the structure of the problem will not change you know could hopefully lead to more generalized models and um in computer vision um this idea of um representing scenes in in graphs uh is actually quite established so there was this uh nice picture that I stole after seeing I clear keynot a couple of months ago and so here the idea is that um given many images of a scene we can you know learn uh specific relations between the objects and then group them hierarchically so for example the keyboard is is always in front of the monitor so it's like a computer era area um the computer and the phone are typically next to each other this was 2007 and people still have phones on their uh desk um and then hierarchically they they they Define the scene and represent the scene in in this graph now this would not be um a causal model because it doesn't necessarily have a well- defined Notions of interventions um so uh but but likewise you you could describe this imagination process as a as a causal model and the key difference is that uh we want our interventions in uh these lat variables to then still produce uh meaningful images and so our lat variables could be the object appearance and specific features about individual objects but also Global um uh information about the scene such as the lighting or or other things like um and ideally what you want out of this type of model is to be able to do interventions also outside of the data that you have observed for training um so here you can see some examples of traversal where it's an aquarium seene and you can change the number of fish the scale the background and the location and uh now if you have a c model you can you know just uh change for example number of objects to to uh uh something that you have not seain and interestingly you can also interpret options as changes in uh this scene graph um assuming that actions only affects uh some of the attributes of one of the objects uh but not you know densely the uh or scene so what I hope you um take away from from this introduction is that what Kaz gives you is a language to talk about the structures in a formal way and um the structural changes now this leads completely open how do we actually learn and identify um these abstract and variables which so uh what I described so far assumes that you know uh or this C Mod assume that you get the variables to begin with um and this varable should be sufficient to describe the relations that among them but of course in comp Vision we don't get variables we get pixels um and so the question is well under which condition if this uh task is even possible to solve at all we can actually learn representations that support meaningful interventions reasoning and planning in this CA s so the one of the early papers where where I I found this idea is this paper uh visual caal feature learning which in a way very similar to Spirit uh that we described earlier we have an image uh and we have a prediction Target which is seem to be uh an effect of the image and the problem is that we have many unseen confounders that canly correlate the visual features with the prediction targets and so the idea is that if we can cor the observations the pixel and separate the scuse and caal features then we can only consider the caor features and train prors again very S as way this is um read the game for um C representation learning how can we learn a representation uh that supports a CA of statements that are useful for a set of tasks that we actually care about of course uh now we cannot really be fully general and this problem is going to be severely defined because uh you know we can describe scen multiple levels of granularity uh and so um the type of uh statements that that we can keep we're going to depend on how rich our data is and what are the classes that we uh want to actually solve uh but if we have to write it as a as a wish list what we want is that our representation behaves in a way that is similar to the causal factorization as opposed to a non factorization um so we want our presentation to recover um some meaningful obstructions and allow for through tasks uh we want that the result of an intervention a sparse effect on the representation um we want that the variables and the mechanisms are to be transferable and be repurpose for new tasks that share a similar structure and uh uh we want to support meaningful planning and reasoning in the representation and as I I've alluded to before uh here we have a course training issue because Cal effects can be described consistently across different granularity levels um so uh depending on um your system you know you can cor grain it and describe an intervention if you could do in the fine grain system also as an intervention um in the um for grain system perhaps not VI Versa but then deciding at which granularity level you want to be is is clearly a non-rival problem which weely so from the methodological perspective U this idea dates back to Independent component analysis where we assume a deterministic generative model uh with some latent sources and these are mixed into our observations and uh um already um more than uh 25 years ago uh people had already proven that the linear case you could actually probably recover uh the latent sources up to nonlinear transformation and permutations but this will not be identifiable in the um regardless in the Deep learning age people uh formulated this instead of as a deterministic process as a probabilistic process U and try to recover a posterior distribution that factorizes in a way that is according to um some this en variables and um um the ca of connection between this entanglement and um like this caal features has been first proposing in these two works and the idea is really that uh you want to match your representation to uh your lat variables in an Interventional sense which means that you know if I if I do an intervention in the real world then I want to have a corresponding intervention in in my representation and vice versa when I have a decoder attached I can do an intervention in the representation which M which matches what I would get if I were to actually do an intervention in the real world uh now without going into lot of details um if you don't have access supervision this this task is deposed clearly um and uh the inition is very simple um if you have observations and you want to learn U such a representation uh this is a specific representation we want to learn because we we have generated the data and so we actually know what the representation is um but we could generate the same data using a different set of fact variation which would be entangled with the first one but still gives you the same observations and so now this is a problem for unsupervised learning algorithms because if you only see this part then of course you cannot tell uh where the observation come from of course this you can um completely uh solve if you if you're willing to to add uh more assumption and um um a lot of Works have explor this kind of like hierarchical U representations where you can put some constraints on the degrees of these lat variables and X would be the pixels That You observe um and you can still get um identifiability guarantees in this hierarchical settings uh even allowing uh you know uh cause relations between Las variables which is pretty nice another way to get um identifiability guarantees is to uh use some of the assumptions that areal in causality and one that U I also work was this independent mechanisms and Spar mechanisms shift so this principle state that if you change or have any knowledge on one causal mechanism this does not affect any of the other mechanisms and conversely When You observe a small distribution change um this is the result of a sparse intervention um if you have the appropriate factorization so if you want to apply this idea to this entanglement this is kind of like how it looks like um where you have multiple observations which could have perhaps even very dense pixel changes in between um but if you have the appropriate representation then the changes will be sparse and this is something that you can enforce during the training and and this can gives you this identifiability and the equion is very simple so um if you if you only get the observations uh of course you cannot tell from which generative models they come from but if you group The observations like this and you are told that uh the two images that belong together only are different from sparse change in the r factors then clearly uh you can R out uh some of these U generative Mars and and since um pretty a lot of Works have been have been done uh in this area and I canot do justice really to to the works that people have done um but uh really I think that we we can U cluster them into two different types of violations of the IID assumption um one which is using nonidentically distributed data for example you have multiple groups uh and one which you have non-independent data for example you have this part peration and D and this is kind of like where objects come into the picture because um let's go back to the example of of this robotic transform and let's say um you know uh by taking nearby frames we we can assume these parts changes and we can train a model which you know ID identifies the SP variation now this is still uh normal neural network in some sense so our representation is just going to be a vector um and so now what happens when we change the number of objects now clearly our our model doesn't have capacity to store a presentation for three objects during training it only has seen a single object so really the problem is that we try to store a c representation in a in a distributed form and so at least for myself this is this is why I um wanted to work on this lot attention problem um because lot attention is is really this differential interface between a distributed representation and um a set structur representation where you have permutation Symmetry and um variables and the granularity of the variables depends on the task um in a way similar to discuss neur networks um but uh the slots here are fully General and combine to any objects uh without specializing globally and this binding is is you can really think of it as a metal learned um cluster so uh I'm sure you seen it before uh but when you contrast it with the soft cluster in pseudo cold you can see that the Z tension is is almost identical um we um just do the same thing with slightly different functions so um instead of computing scores Clan distance as you would do in clustering uh we use do products and projection matri this allows it in a way so learn the uh cost because you have this projection mates uh the weights are identical um and the updates are are also identical but then how we actually update the SIDS is slightly different but really you can interpret it as as a um a feature clustering and so um really like you can think of attention as combining uh two ideas um which um also other words have considered one is this slot based structure um and the second is uh this this grouping of U features and um um these are still relevant today and uh some of the state-ofthe-art uh unsupervised uh segmentation still use um some similar ideas that you know you have very good uh feature representations typically from pre-rain dyo uh and then you group uh your features in uh however uh whatever is your favorite you know grouping method and of course you can also use uh attention for this because in a way we're just doing stuff clustering and I will leave this describe uh more in details later so now in which sense are the objects representation of attention caus so here you can see an example of the composition of a simple scene um using slot attention and now if we take one of these objects out of distribution um I think it's very hard to see in this image but we are applying some um very weird texture to to this object um which is very far from the normal texture of of clever then what you see is that uh the representation of the different objects Remains the Same these are what is in the green boxes uh of course mod permutation because uh this permutation symmetric um the representation of the object that um has been interb on of course is is not good um and the encoder is going to have a very hard time you know uh mapping the new texture into something that has seen during training and so what actually happens is that it splits the object into two uh which uh are kind of close in terms of color and and shape to the object that out distributions it kind way finds whatever was close to the training set and reconstruct it that way but the important thing here is that U although the pixel change is is dense uh most of the slots representation actually stays identical only the slots that bind to the AO distribution objects have a PO representation but then everything else remains exactly the same meaning that whatever Downstream model you have trained on your slot representation of course is poorly on on this subject but can still be accurate on which keeps High CH well and um perh a surprise surprisingly um people have extended this identifiability results um from the cost presentation learning setting to uh the object Discovery setting um and the idea here is again that you formulate the problem as a probabilistic model where you have ground trth latens which are now having this like loot based structure and then you assume that there is an unknown generator um which gives you your uh multiobject SE uh and then you have um your whatever Cod decoder model with a slot structure representation and under some conditions you can actually get uh proably a nonlinear mapping of the original slots but the problem with this type of uh analysis is that the assumption that we put on the uh generator are bit too strong uh so we want invertibility we want that preserve positional structure and is irreducible um and the latter two maybe are not so bad the invertibility is a bit problematic here uh because it also means that we're not allowing occlusions uh which of course um is is an important challenge Vision um I think there are approaches to U Get r of this issue uh but I don't think that's been been so yet expl this later so um really I think you can think of these object Centric methods as this kind of like modular architectures that achieve this mechanism reuse at the at the object level uh now I want to use the remaining few minutes from my presentation to discuss some some future outlook of things that I think are exciting in this space um one um it's a new type of works that I've seen happening which likey to still learn object Centric representations um but without having this Lo based structure U so the idea here is to take inspiration from spiking signals in the brain and neurons will find um to different concepts and objects by essentially timing their finding patterns um and of course like we don't want to exactly do this in neur network because this by neur ns are not so nice to train um but we can um make our representation complex and then we have a magnitude and a phase and we can process together features that have similar phases and features that areal to each other in this face space you know are completely and so um Cindy especially from University of Amsterdam developed an out version of this idea um and uh it's nice because uh you can still obtain an object cented representation in the sense that objects are processed separately within the architecture but it's completely without this slot based structure so the representation is still um fully distributed and if you use the same tricks as before of in a way clustering um a very expressive um feature representation then then still getent results on um on the C representation learning side I'm uh excited about uh method that really try to go beyond this identifiability perspective um because I I I feel like it's very difficult to talk about what assumptions are reasonable or not to get theability results if we don't explicit talk about what tasks we want to solve and um uh out of distribution or robustness is perhaps an application but I'm wondering whether it's the only application or perhaps the right one um in fact I think one of the most exciting um no new capabilities uh that are enabled by these object Centric models are really this set scene controllability and and um this is a recent work um from uh thas um which is again in a way similar to to sension uh with an encoder that extracts neural assets but now these are rendered in 3D and allow to do the this uh manipulation in the uh space think quite nice um one interesting approach to also get rid of this problem of um conclusions uh um is is um what we call partial observability so let's say you have a latent causal graph that looks like this which you would like to uh recover uh one assumption you can make is that uh you never get observation that really tell you everything about the graph for example to conclusions but you can get multiple partial observations so your observation uh will tell you something about of the variables and you know independently you will not be able to get all the variables out of single observation but then you get you know multiple images for example of the same scene and you know ideally then you can actually uh you know complete whatever is missing from any observations um although this I think can be applied to object learning I I I don't believe anyone Has Done Yet U what I'm even more excited about is the applications of these ideas to train um controllable um dynamical systems where you train um still like structure your networks but now U the structure is an explicit differential equation in the l space and you use this CA representation learning construct to identify the parameters of this differential equation which then allows you to uh again do the similar like intervention things like for example reconstruct the sea water temperature if you know everything was for example um another nice example for um caal Downstream tasks um is a project that we did recently together with experimental ecologists so they are interested in understanding how ends uh behave under exposure to different substances and they want to understand you know how changes in the ecosystem can affect um The Colony and so what they do is uh synthesize a chemical expose one of the ends uh and then take lots of videos and then visually and manually actually looking at them and perform what they call this behavioral analysis of course like we a computer Vision conference so this this step could be entirely automated and and done uh with NE networks um so for this we recorded um this data set um which to the best of my knowledge is the first data set for treatment effect estimation in which the observations are actually high dimension and um so the task here is of course to predict whether ends are grooming each other or not but the goal is not just to maximize accuracy the goal is to get uh the same answer that an ecologist would give you on whether the chemical is statically affecting the behavior of the ends or not and if you just take State ofthe art Vision model this is what you see uh on the x-axis you can see the balanced accuracy of course this a very highly imbalanced data set but stateoftheart models are pretty good uh however on the Y AIS you see the treatment effect relative bias and the score of zero means that the um output of the model is the same as what an ecologist would tell you and so as you can see now again we can see a very large spread U of models all achieving you know very good accuracy uh but sometimes very poor performance on this Con task and so here I I think that you know having the structure presentation it would be very useful um because uh uh if we can you know separate the information about which and got the and so not we can try to balance the error rates and uh get something which is close to so with this I want to conclude this is my last slide um and I want to conclude a bit with the pro question which is why can't we just train big networks because after all if the structure is any useful it will emerge uh and uh I hope I convinced you in in this presentation that uh structure is is useful for T that are aligned you know with the structure itself um theological way um but interestingly like like we see this concept coming up again um with this linear representation hypothesis and mechanistic interpretability um of large language models where people have seen that you know specific uh concepts are encoded linearly in the representation of language model and then you can you know uh hopefully control them and this is useful for alignment so again you can see that um having the struction presentation can help you with I hope I convince you that cality is an interesting uh research area um it gives a language to talk about many of the things that we are actually interested in in machine learning uh and I think uh there are uh very interesting open problems in CL representation learning Beyond uh just this theoretical identification one of the biggest botton have been the data U but this is also you know an exciting opportunity in developing new benchmarks um so really I think a lot can be done here um and uh a problem which I'm very excited about is to actually how to me the discovery of the structural Insight with statistical representations and um I I couldn't show this but for C of Discovery we have seen that that um extracting a caal graph from a diffusion model can give you uh very good performance not only terms of accuracy but also scalability uh and uh so really having principal way to do that in the C presentation and very exciting and uh with this I would like to thank all of you U and also use the opportunity to say that I'm hiring post of phc students in my lab at pistan uh so if you're interested in in some of the things that I um shown you today feel free to send me thank you okay cool thank thank you

