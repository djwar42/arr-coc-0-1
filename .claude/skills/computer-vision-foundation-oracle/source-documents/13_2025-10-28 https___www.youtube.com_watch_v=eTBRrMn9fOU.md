---
sourceFile: "2025-10-28 https://www.youtube.com/watch?v=eTBRrMn9fOU"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:37:03.288Z"
---

# 2025-10-28 https://www.youtube.com/watch?v=eTBRrMn9fOU

5d068de0-71b6-4c93-a040-73ccd28e1c96

2025-10-28 https://www.youtube.com/watch?v=eTBRrMn9fOU

d02cd47b-5dc4-48ea-bc4f-49db7517f600

https://www.youtube.com/watch?v=eTBRrMn9fOU

eTBRrMn9fOU

## ComputerVisionFoundation Videos

all right uh hello everyone Welcome to our Workshop that is the fourth Workshop of open World um namely visual perception and learning the open world and my name is sh I'm the one of the organizer um in this Workshop so first I want to welcome you to this Workshop I hope everyone is safe and healthy the goals of the workshop is to connect people and exchange ideas about the open World Vision and we should discuss new opportunities and challenges about the open world uh this is a hybrid a hybrid Workshop uh we have onsite um participants and so please en join and involve by asking questions and we also have online Zoom link provided by cvpr conference and the schedule is like this you can search open World Learning on the website and to find the link uh to the to the workshop website and we have a for ch es uh list as below uh you can uh refer to the agenda and come come over whenever you like I want to thank all the speakers organizers uh advisors coordinators uh we have a smiling faces of them so you can say hello to them and they will greet you as well uh first I want to introduce the open world a little bit um firstly we emphasize testing in the open world but we still train models in a closed world for example we have a open set recognition which means that we train models on some closed set and we emphasize that we have uh never before seen Noble examples in the in testing scenario U that's why we have to detect the no stuff and later we have a open world recognition uh which emphasize oh we should not only um detect the no species but also do some continual learning for the noble species with human annotation the loop um this is the old days but now I want to make a an argument that we nowaday train the open world for example we sample data from the open world for better uh open set recognition where we sample outlier data or outlier exposure to uh train better models to detect never before seen um no species and uh I want to claim that the foundation models are actually pre-trained in the open world for example clip with the open world models or Foundation models we can do open vocabulary recognition we can do zero shot recognition so and so forth um but uh here we want to think okay with train finition models in the open world can we predict test testing performance with the distribution shift uh given that we train in the open world the open world represent some data distribution but in the uh in a downstream task we have some distribution ship can we predict the performance I want to remind people that uh this is a actually an established problem uh Which Way call okay uh we train in some in distribution data for example image net but we test on some um domain shift data or out of distribution data uh image net V2 so can we do can we predict the performance actually um people report that we can we report accuracy on the line observation meaning that empirically we see o performance is strongly correlated with the IND distribution performance for wide range of models and distribution Shi um but here I want I want to remind people that the models tested in this scenario are trained in the closed world and we wonder um does this um conclusion still hold if we test the open world data open world models and here we tested 75 models including Vision models Vision language models train in nether the closer world I IE image net or the open world uh which uh represent data from the internet and we if we draw this line again and we we we do this scatter plot we see that okay accuracy on the line does not hold anymore um but we see another interesting metric RC or at least a common ancestor if we use that metric to draw uh the scatter plot we see okay RC is on the line no matter what model we train or in What scenario open world or closed World um but here and I want to quickly and explain why the intuition is that if a model can make better mistakes measured by RCA it can mitigate a spous correlation leading to better generalization um this is a work published by our group uh at the icml okay uh here we see that oh this is a a very exciting scenario very exciting conclusion that we can predict open world models on any down task um but if we dive into some fer cases we see some interesting examples for example uh I work on species recognition I uh threw in some uh some uh species names like this one panana citrina and I asked the GPT 4 to take uh to describe the visual features of this species but GPD 4 fails this is surprising because uh we can sortly believe that the GPD 4 is already trained in the open world using all the internet data um which open eye can obtain right but why it feels you know if you search Google you actually can see some examples and some some descriptions in this website um why it is still fails my hypothesis that some Concepts especially scientific names WR in Latin are too few in the open world to train models but how to justify this uh we count how we count we use some open source preining data set like Li 2 billion Li 400 million if we count we see that okay the Latin scientific names are actually very few and sampled in the open World um this is not the end of the world not a um oh we have a some uh some uh uh tricky remedy like this um given a scientific name for zero shot recognition we translate the scientific Latin name to English common name and we use the English common name in the prompts to prompt the pre-train foundation model or Vision language model to do zero shot recognition if we do so we can significantly boost the zero shot recognition performance uh that is zero shot accuracy on two data sets uh as name is scientific name prompt and common name this is the same name right U this is interesting and uh we we want to test more uh data more data in with the diverse Foundation models for example I threw in this image which is a n snake n snake is a category listed in image net and we ask the question ask the ask a gp4 and uh some other Foundation models what is the species name of this animal in the photo and none of the foundation models can answer this question well correctly and we also ask d three to generate a photo for night snake and uh they fail this is uh again very surprising um my hypothesis is that certain concepts are insufficiently presented in the open world examples relate to this those kind of a these kind of concepts are very few to train a good foundation model and so we have some evidence to show this uh we have a way to measure the concept of frequency in the open world and here we we use the measure I will describe how how to measure the concept concept of frequency um just imagine that we have the measure and we can draw this curve which is a concept of frequency in the image net uh we sort them from a from low to to High um this is the um imbalance distribution of uh very obviously and we also draw this kind of a curve which is a u zero shot accuracy for uh the ranked um class names and we see a strong correlation between the concept of frequency and a per concept accuracy uh this is a kind of evidence showing that certain concepts are just a few in the open world to to uh to train reliable Foundation models U okay now I want to show you how to measure concept of frequency um of course we still count intuitively we can count the occurrence of pre-train test relate to the concept of interest but here we have a challenge U pre-training data is a large scale we have a billions of training examples uh so how to do a reasonable measure here we use stream matching uh stream matching is a quick is a efficient but we have two more um um limited furos uh the first one is the lexical variation I.E there are some synonyms for example for the running shoes we have other names like sneakers trainer shoes so and so forth a second C is that is the linguistic ambiguity and for uh just remember that we use stream matching we want to find Tiger but we still have other sort of non- tiger tiger uh examples like tiger shark tiger wo obviously the l two are not a tiger anyal so how to address this issue we come up with this flowchart this pipeline um to deal with the lexical variation we ask a large language model to list the synonyms for concept of interesting of interest uh this one is the tiger we ask the question what are the homonyms or synonyms referring to tiger and the large language model can give us some some uh candidates and we use all the candidates to search the relevant pring text using stream matching is here and later on we use a um another uh the same language model to to filter out irrelevant captions for example we ask this question does this concept in the caption referring to tiger and we can figed out some uh um ambiguous irrelevant captions um this this enables enables us um to reab re reliably measure the concept of frequency uh by doing so we can draw the curves and show the strong correlation between the concept frequency and per concept zero shot accuracy just defying that some certain or certain concepts are just a few in the open world to train Foundation models um and there are some uh interesting insights for example if we do zero sh recognition we can translate the given concept name to its most frequent synonym in the prompt if we do so we can significantly boost the zero shot accuracy and another inside is that okay we can do retrieval augmented learning uh I just want to give people a little background um there's another paper published last year called react which is the state state of the art retrieval augmented learning method for zero shot recognition basically if we care about one concept we can search relevant examples in the in in on the Internet or in the preining data set of a VAR language model and after we retrieve a bunch of examples we can train a new model for recognition and here um people just use the original concept name but now we know there are some synonyms or other common names if we use all the common names or synonymes to do the do the retrieval we can retrieve more relevant data and train better models that is our uh method now we use other synonyms for retrieval augmented learning we can see we can outperform the St are react and by a large margin and um I want to remind people that when they do uh retrieval the computer image features or text features but we now use uh stream matching which is pretty fast uh by doing so we can reduce the computation cost significantly by orders right um another Insight is that we can use the most frequency name image generation for example here if we want to generate an image for bank swallow we can of course ask uh D 3 to generate a photo of a bank swallow but D 3 can fail because um we know the reason bank swallow is not commonly represented or presented in the open world that's why d three fails but now we can find it most frequent synonym which is the San Martin if we replace bank swallow with San Martin in the prompt we can let data three generate the correct image um this is our paper um we will present the paper uh later in the conference so please uh come over and discuss with us all right uh I want to conclude my um very short talk with three remarks the first one that Foundation models are open world models and we train models in the open world and uh yeah that's pretty cool and the second remark is that open world models have open issues and the open issues are mostly related to open world data for example the data is long till distributed uh it has some domain shaped so and so forth all right um that's all about uh my introduction so I hope everyone enjoy the conference enjoy the workshop uh so Niar please take over right thanks so much here uh all right uh W come up uh our first talk today will be from Professor Walter Shear who is a professor at Notre Dame he's been uh doing many pioneering Works in in open set recognition and he's been leading the field for more than a decade so today he'll be uh presenting his talk open issues in open World Learning and I think this will be a very exciting talk so please Walter take it away learning um so I want to talk about to continue working on so I'm going to talk about some of these that are sort of the most Salient things things I think we should all be thinking about um but to begin let me talk about why this is such an important problem um and a little bit of how the computer vision Community has underrated this problem historically um asking a question here why don't we have selfs safe driving cars in 2024 uh those of you whove been attending cdpr for uh you know the past several years know this was a pretty hyped up area right a hyped up application area um know Elon Musk even before this was like reaching its apex in like 2019 in our community was saying you know uh Teslas are going to be like level five autonomy you know next year and he said this over a series of years um and we never quite got there now of course again if you look at our Expo uh which is opening up tomorrow um almost all of those self-driving car companies have disappeared they've gone out of business they're being wound down um what happened right um I think a big piece of the puzzle when it comes comes to autonomous vehicles in general is in fact the novelty problem um driving is really tough um there's lots of stuff that can appear on the roads um for instance um think about new objects um in uh much of the history of the automobile the smartphone did not exist right um not only is this a new visual object right that could be recognized in the scene it also changed user Behavior right uh interactions between users changed interactions between users uh and the road environment change right all these things were new right things in terms of the behavior of pedestrians that were simply not observed right and things like this of course will happen time and time again another interesting thing which is also terrible um if you dig into recent accident reports for Tesla's autopilot system um some strange things have been happening uh for instance uh the car with autopilot engaged will approach the scene of an existing accident and drive into it why is that happening well uh these are rare instances that aren't reflected well in the training data and also i' point out um accidents aren't well characterized right there isn't like a stereotypical accidency right the configurations of the objects are going to be different um there's going to be right different numbers of emergency responders and emergency vehicles all these different things um so that's something that's really hard to learn um and right it's a novelty that the car is simply not managing well right you get a secondary crash which in some cases right has been catastrophic so like all of that's pretty bad um and I want to point out the fundamental shortcoming here is one that's related to this big data strategy we've been pursuing uh machine learning for a number of years um right it's always going to necessarily under sample the physical world and often drastically I think that's surprising to a lot of computer vision researchers um because we've been brute forcing a lot of other problems with webscale data for a long time um but again physical world there's just so much that can happen um Neil Lawrence actually pointed this out a number of years ago just as again like the self-driving hip was reaching um its apex on X he noted that there are countably infinite things that can possibly happen on the road um so if you if you acknowledge this you know then that you're going to have to do something else uh when it comes to novelty right um there's no way to reforce this you're going to have to come up with a scheme to basically identify it to characterize it and then to manage it in some way uh and so all of those were objectives of the the Dara Salon program right is the sort of like government folks had known this for a number of years right um that that this was you know something that was inhibiting like really good mobile robotics right autonomous agents and other settings um another thing again perhaps again if people working on this problem this isn't surprising um but researchers have been looking at really simple environments um and adding Novelties to them and noticing again a drastic impact on the performance of agents and recognition algorithms in these environments um environments as simple as cart Pole right it's like one of the simplest AI tasks right like balance the beam right on the cart um there there are fun ways to sort of add Novelties to even these very simple environments like carple for instance changing the physics of the environment uh perhaps varying the force right the complexity of the tab then goes up by quite a bit um this was actually one of the environments that was being studied in the salon program um really simple data sets uh in computer vision like like CFR 10 um hold out some of those classes right they become Novelties um and the tests becomes more difficult right and folks have done this right with all sorts of different simple data sets M this uh letter right all all these really basic things we've assumed to be solved right you had the novelty and all of a sudden right a lot of our best approaches are starting to struggle like why does that happen um as was noted in the introduction for many years I've been studying uh this problem uh and a long time ago now uh we sort of came up with some uh reasons for why this is occurring in a machine learning setting uh for instance right if we look at the training data we're always going to have incomplete knowledge of the world at training time for these open world domains um because again we simply can't root Force this at training time there's always going to be new things um but why does it fall flat right in a classification sense um it comes down to to um the basis for making a decision uh there's no basis from which to generalize to novel data from known data both of which are typically far away from each other in some future space um and if you're dealing with things like linear classifiers this is really dangerous because you get these half uh infinite halfspace effects um where again you have a nice clear decision boundary but this novel thing is appearing way out there in the future space far from the support of the known training data uh and of course right you get U this classification um the same thing happens too in a slightly different way in a nonlinear context right so if I have a really tight boundary around my my known features um I still right then have problems generalizing to new samples of the known data because I don't get good generalization and in some cases again the feature space has a lot of aliasing and it's hard to make those distinctions um with respect to the Novelties themselves um new things as I pointed out are always going to appear in an open environment right this is a fundamental um aspect ECT of the environment um and it's not restricted to object classes right this is something I think that's also underappreciated by a lot of folks working on these problems mainly these the literature has been focused on right object classes thus far and again that's that's part of the the puzzle but it's not the only thing I want to point out that novel activities and interactions are also really significant compounds that you're going to have to think about all right so again if you're in machine learning you may be thinking well wait a minute right like we can probably manage novel using reinforcement learning right isn't that the point uh we're going to go through some iterative process right we'll get some new information right we'll learn that information incorporate it back into the model perhaps with some uh human feedback right no problem well think about this right sort of in the limit right you can't number one have somebody in the loop doing this all of the time saying this is a novelty this is a novelty this is a novelty that means you need a really good automatic novelty detector right to automatically figure out that this is something that's new now that's potentially dangerous if you just blindly learn it because there are many Novelties that we don't care about uh in the salon program one thing we recognized quickly uh was the importance of making this the distinction between Novelties you care about and novelties that you don't uh and we called those Novelties that you don't really care about Nuance Novelties um so we all know it like uh the area of anomaly detection right uh has been pretty well developed over the years there are lots of different anomaly detectors um but they're really not good at characterizing novelty in fact almost all of them don't do that um and so if again if you think about the world there are plenty of things that change that we simply just don't worry about uh for instance right this room if I have an autonomous agent traversing it um it has a visual recognition system if it turns out the color of this carpet is a novel shade of beige one that has never been seen before by the system that's a novelty but does the agent care about it no right it's it's totally irrelevant um but if that's being flagged by some anomaly detector uh as something to consider right and especially in a reinforcement learning regime all of a sudden you're ballooning your model right because you're ingesting all this irrelevant information um so out of the box reinforcement learning um is not going to help you inherently and again this comes up quite a bit which is surprising to me right it's like a lot of researchers just think oh this is it you know Works in so many other domains don't worry about it uh that's not true okay so if you're going to build a really good open world agent what would it look like um probably something like this right it's going to have these three pieces that I mentioned uh novelty detection novelty characterization and then some form of incremental learning to incorporate the non- Nuance Novelties the important Novelties that impact agent performance back into the system right so the input is going to be some sort of image or video um of course you're going to extract features right or learn features from that data um you're still still going to have a classification component to this right um which will probably be task specific for whatever you're doing um and then you're going to move on to this novelty recognition process you're going to detect novelty characterize it um and then figure out you know is it important is it not you could have rate external feedback there could be a human in the loop um but again that doesn't scale as we all know um so that's going to be a factor um so a few notes to this basic template number one novelty detection and characterization turn out to be really really hard and again if you've been working on this problem uh you know that uh number two efficient incremental learning is necessary um there aren't that many really good incremental learning classifiers out there um a number of different groups are working on them uh I've I've done this uh uh in the past but there isn't something that I'd say is like off the shelf great at this point uh and a related note to that one classify research is just not very popular right now you know I think like part of that just the trends of the field right it's like representation learning became hot and then we heard you know if you have really good representations you'll get great separation in the future space use any classifier any basic linear discriminate will work great um that turns out not to be true and again we figing about more complex features to that like um adding incremental learning that's an issue um everybody's excited about generative models generative AI right so I feel like more folks need to come back to to this problem because um there's some big open questions that we're left with which are important okay so let me me talk more about DARPA Salon uh before um we get into some of the big open issues that came out of the program um so DARPA Salon was really interesting um because again I think it tried uh in in a rigorous sense to find a bunch of different domains that varied quite a bit uh in different modes not just computer vision but also thinking about interactive domains um like games um thinking about Vision in in different instantiations activity recognition object recognition uh handwriting recognition right lots of different data domains um and again pushing this point that we should have some kind of domain Independence right if we're building these systems right um the ideal would be the agent is moving seamlessly across the different domains um and that's something I think is kind of shocking to the computer vision Community is typically you're working in one domain you have a very specialized approach specifically for that domain you don't think about right it generalizing to perhaps r different uh domains with with very different data uh interactive in some cases right not just static images but but the program folded all these things together the program also had a very specific uh notion of novelty uh and how it's sort of uh introduced into an environment for evaluation purposes um this was called the big red button so you imagine there's an agent it's in some domain right it's operating you know as it's been trained right or as it's been programmed if it's procedural um a button is pushed somewhere in the universe and something fundamental changes in that domain the agent doesn't know this has happened but it's expected to detect that change uh and and be able to manage whatever novelty has been injected right into into the environment um so right something if you're think about this in like a distributional sense right like the distribution is now different in terms of the data that are being ingested by the agent the agent's got to figure that out on its own and then adjust accordingly um and that's kind of how each evaluation proceeded throughout this program regardless of the domain right the domains were were quite different again it was like everything from like cart Pole to like visual object recognition um viz Doom Carla like the autonomous vehicle simulator like all these different environments but they would all sort of be structured around the same the same idea um what was also interesting again moving beyond just this notion of object classes uh when it came to novelty um the saleon program organized things like most DARPA programs uh in different phases they had three specific uh phases that they carved out um where different aspects of novelty uh would be analyzed um so in phase one we indeed did start with just objects right so this is like new classes attributes um and and representations uh but in phase two uh we were looking at novel agents right novel things interacting with the agent that like we sort of designed right in the environment um actions right like how are these things behaving right if it's an interactive um setting or even in computer vision right novel actions reflected in a recorded video sequence relations right if there are different objects uh in a scene how are they related right do those relationships change again something that we typically don't think about uh in computer vision but that's something that's quite consequential even in still images um interactions um again different ways of of objects or agents interacting in some environment right I already pointed that out as a confound for self-driving cars these like pedestrian interactions change as we introduced smartphone um and then in phase three we looked at more complex phenomena uh rules all right especially for game playing right um what happens if you change the rules of a game like Monopoly which was another one of the domains uh that we considered in the program um Can an agent figure out that something is shifted because the points being assigned right are not the same um in in terms of the gameplay uh goals um can you change right the the goals of perhaps winning a game without telling the agent does the agent figure it out um and then events right like big big things right that are sort of introduced into an environment that the agent will have to negotiating again an event could be uh compositional right a lot of different things have changed that's all building up into uh one big event right that has to be negotiated um and so again I think this is interesting but when we're thinking about open ISS issues right um this hierarchy is pretty arbitrary right it's just um the the organizers of this DARPA challenge came up with this right and so we could probably think of more things to add to this um we could probably fold some of these things together right this was sort of an initial cut um at coming up with right some structur challenges for the program okay another big thing the metrics how do we assess open world recognition again I see a lot of heads nodding here we know in the literature that many things have been tried um right everything from like very basic classification measures right is ultimately if you're thinking about a task like this open set recognition and object recognition um you're thinking about like the accuracy of the classifier in some sense right uh and of course right accuracy is not a great metric if um the distribution is not balanced in terms of like novel things versus known things right so we sort of know that you know things can get very um you know if it's like um uh some sort of classifier right that always says everything is novel and you show it mostly novel things right the accuracy is going to be high right so there's some ways to gain the metrics there which isn't great um so in in literature including my own papers we looked at other measures like like an F1 score uh Precision recall right things from information retrieval but all of those again sort of have uh different pitfalls um that said you need some metrics right to figure out is the detection part of this working um and so the salon program looked at different uh specific measures for detection right coming you know down to basically um this measure of like correct detections and also false positives um is the detector working in some rigorous sense um but that's only one piece of the puzzle as I pointed out right the program and I think researchers overall really interested in these three things right detection characterization accommodation right or you know that's the incremental learning piece um weirdly characterization nobody could come up with any good formal metrics that remains an open issue um so the program skipped ahead to accommodation right how well is the agent uh responding to um changes in the environment um so there were metrics that were formulated sort of like pre and post uh red button push pre and post uh introduction of novelty what happens to the agent's performance um and so these various metrics were sort of proposed looking at like different ways of uh assessing um the the of the agent to sort of like perform in the environment to learn these new things right and perform uh different temporal sort of aspects of that as well um and um you know this was sort of a good first cut but again we need more we need more work there um so what did we learn out of all this as I mentioned right this talk is really about the open issues and I want to like spend the rest of the time talking about the eight big ones which cover these aspects and some other ones which I think are really interesting um we we organize these um in terms of uh three different categor ories related to novelty uh number one theory of novelty um as it turns out there is a lot of good theoretical work out there in the computer vision Community especially on like what what is novelty right are there other communities thinking about this yes stand by we'll talk about that um design of Agents that's mostly what we spend our time doing right building algorithms right to tackle this problem um but as I mentioned right there's some really interesting big open issues there um and then finally what I was just mentioning just introducing evaluation um are we there yet when it comes to the metrics uh Are We There Yet when it comes to actually organizing uh benchmarks right for us to assess whether or not um this is actually working um and It's Tricky especially you know right if this is open World Learning um information leakage right that's really dangerous right so trying to mitigate that is going to be a big thing um so as mentioned yeah let's talk about these eight right the need to for better developed theories of novelty uh differences between activity and perceptual domains domain Independence better representation for novelty learning robustness to novelty versus novelty detection and characterization risk-based reasoning uh spectrum of partial knowledge the system designer has about Novelties and the lack of measures that are really specific to open World Learning yeah we need time here to wrap this up okay um so number one the need for better developed theories of novelty um as we embarked on the salon program it became apparent um what were we working from like what are the what are the assumptions like what are the first principles um turning to the literature in pattern recognition and artificial intelligence and computer vision and natural language processing there wasn't a heck of a lot right and so that meant we had to go back in time all the way to Plato right you don't typically get uh a talk on Greek philosophy here at cvpr um but it turns out Plato in one of his Socratic dialogues actually considered this question of what does it mean for something to be novel um and it actually has some interesting advice for us today uh for artificial intelligence Plato claims that new things are generated by reconfiguring existing material into a new form through the guidance of set patterns interesting um so you can think again about some kind of generative Universe right where some things are kind of fixed like say the particles in the universe the forms of matter Etc um there are great rules of physics that Universe needs to conform to to create things that we uh assume to be new um that's kind of interesting um but perhaps not entirely practical for our purposes right we want to figure out you know is there an algorithmic way we can sort of you know uh uh reason about all this so flashing forward uh to another philosopher uh writing in the 20th century Thomas Coon who was also thinking about this Coon argue that novelty reduces to the perception of things in an environment that are new to the obser server this one is very interesting um because as we know uh we won't have comprehensive knowledge built into any agent that we program that we train Etc um and some things that are perhaps known to us humans may not be known to the agent but the agent is going to have to figure them out eventually right especially if they impact some task performance um so this really becomes a matter of is it new to me versus is it new uh to the itself um that said um folks within the DARPA Salon program uh were really interested in the environment not necessarily just this agent forward perspective for instance uh Pat Langley uh uh who some of us probably uh know is right a really like sort of old school AI researcher um who knows a heck of a lot about machine learning too um he was really interested again in this sort of generative mode that the environment operates in and argued that um environmental change um in a strictly generative mode is the basis of novelty right you have to sort of reason about novelty uh from this environment first perspective right so there was a lot of debate within the DARPA Salon program about these two perspectives right is is it the agents observations or is it really something fundamental in in the environment um and there's arguments you know for and against all these things um I landed uh I am some of my colleagues uh landed um on Coon's sort of you know perspective here um just because it's more practical when you're sort of building these agents right I can't comprehensively account for everything in an environment right that's really really difficult but I can like in a in an easier sense like figure out this is new to me right um I can formulate um uh an observation sort of uh uh way to reason about these environments um that said so in a paper we had at triple AI in 21 we did sort of break this out into um things that do change in the world but again in a dissimilarity measurement so that still conforms to what Coon is saying um from the agent's perspective in an observation sense so I'm sensing information from that environment where something may have changed or maybe nothing has changed I've just encountered something new um and I'm again performing a dissimilarity assessment um and I'm also factoring in my regret right is I have to solve some task like will this actually impact me or not um and that's really consequential as I pointed out for managing nuisance novelty um and that that I think is probably the the more tractable approach right instead of accounting for everything in the universe um but still there are open questions when we think about these theories um given the two overarching framings of environmental novelty and agent Centric novelty is it possible to reconcile them into a single Theory um is there any theoretical basis for a novelty hierarchy as I mentioned um Dara kind of came up with that right the the creators of the program came up with that but we can probably think of more things to add to a hierarchy we can probably think about rearranging that hierarchy in other ways eliminating some things adding some things right all these things are possibilities um I think again all that's sort of built out of assumptions we make about our task right and that ties it then to some domain which could be dangerous or maybe not and does any exent theory help make predictions about agents and their interactions with the environment that can help guide agent design is what we want are better autonomous agents right that handle all these things okay a second theoretical concern that came differences between activity and perceptual domains as I mentioned the DARPA Salon program had interactive game environments in some cases that didn't have any Vision component um and they also had just straight up computer vision data sets right all these things uh were seen as being places where Novelties could occur but what the real goal was was to have some sort of right uh agent that could cross them seamlessly that's really really tough um right because again if we look at um activity domains the artificial intelligence Community has like a totally different set of like planning based techniques um in some cases there may be a vision component usually not uh on the flip side right we typically don't do planning for static image sets right for object recognition because there's no temporal Dimension right there's nothing to plan um this makes this really really hard to to sort of you know unpack um but again if if people can seamlessly cross these domains why don't we have agents that can do it right that's a big question um can the two be reconciled I mean there are some hints that this is possible mobile robotics right this happens all the time right it's like I need I need a vision system in many cases right um some piece of it is trained on static images but it's on an agent that is operating in the environment now of course I hear complaints from my roboticist colleagues all the time computer vision you folks created these highly accurate object recognition systems right uh these great neural networks trained on these data sets from the the web scale data right the papers look amazing the The Benchmark numbers are so high I put this on my mobile robot and it's just misidentifying everything it's crashing it's exploding right the Magic Smoke is pouring out of it like why is that happening um you know good question um right because it's it's a domain shift right there's a problem there um something though I think perhaps is a little bit more useful for thinking about this um like remember like like Deep Mind Style video game play right remember like like 10 years ago it was like really like exciting to and open AI was doing this too right it's like you have you have a vision system that is playing a video game like a person is playing it right so it's it's doing all of the visual recognition and then it has like the planning pieces right and and playing the game like a right like a human player is playing it um you know in the sort of back end as well um that's really really interesting uh predictive coding is a classic idea in computer vision which also has a temporal Dimension um but it works for fixed images I think that's another way to sort of bridge this Gap um and then U this notion of dissimilarity assessment through agent observation right um that's another way you can sort of reason about this um so again I think there's some Clues right and some also prior work uh that fits this regime really well okay uh the third thing so going into the design of Agents domain Independence um we've heard all these bold claims about deep learning generalization but they've really never been true right um if you think about deep Nets they're basically Ally limited to just a single domain the one associated with the training data um and you know when we think about generalization it's usually like generalizing to do instances usually objects if it's object recognition uh within that one data domain um but the salon program had this much broader notion of domain uh dependence right where it's like I should be able to move from object recognition uh to say an interactive uh car simulation environment or a video game environment or anything um planning of course suffers from a similar problem um if an agent moves to a new environment the old plan May provide it no useful information right everything has been intentionally designed for that other other domain which is an issue um so the big challenge is crossing between activity and perceptual domains um is it possible to have a feature representation that applies in a universal way um no such feature representation currently exists right so that's a big constraint um one possibility is though to pick a universal represent ation perhaps something like a spectrogram which has been suggested in the machine learning literature and transform all the data into it um so maybe that works um one then would be left minimizing domain dependence rather than achieving strict domain Independence these ultimately again you're going to need some sort of domain uh dependent information for instance like the sensing right there's got to be some piece of this system that interfaces with the domain in intentional way all right a related issue to better representation for novelty learning right so a few more thoughts on this idea of representation there is an intrinsic link between the goodness of a representation and the ability to detect novelty uh if known information can be clearly represented then what is different from it can be discerned without significant effort um but of course as we know again a lot of us been training systems to do this right there's too much aliasing between the known and unknown information you you get a lot of false positives and false negatives right that's that's bad um so like what can we do um one thing that's surface which I think is kind of interesting is the notion of a representation edit distance this is suggested by Jos all Spectre recently um and um it actually builds off of some earlier Works suggested by uh fris cholle of of Kira fame um who's been out there more recently like with these Grand challenges on like right like human level like uh cognition right um but he he has some interesting things to say about representation too when it comes to these these problems um uh perhaps right we need a measure of novelty uh that can be used by agents to adapt to it um and a change in information content in bit strings if you think about this in an information theoretical sense uh could be measured by comparing pre- and post novelty skill programs um what's nice about a bit string representation again I was just talking about like is there's some common representation that's useful across radically different domains this is one of them that could potentially work and also across different algorithmic approaches um right if you think about AI more broadly it's not just Neal networks and I I realize that's that's shocking to the audience here but there are other things right knowledge graphs other regression algorithms even intuitive representations of knowledge if you're going to be very very old school um some of that stuff still works um this is a representation that will cross all those different algorithms and of course neural networks um there is a a constraint though if we think about information Theory um you need good approximations to what should be formally be optimal uh in in terms of the framework we think about information Theory it's usually about formal coding in formal systems uh these real environments are a lot Messier right so you're always going to be dealing with approximations and some of your assumptions may not hold if you're trying to prove things in a rigorous sense about what this representation uh can actually do okay number five robustness to novelty versus novelty detection and characterization as I pointed out there is a huge literature on novelty detection through anomaly detection um but it's not inherently useful by itself right it's like fine we detected it but as I pointed out if you're always not always but most of the time you're detecting uh nuisance novelty Novelties that are true Novelties but don't impact agent performance that's not great so that means you need some way to characterize these Novelties um to sort all of this out um and that means you need some regret calculation in your agent to do this um novelty adaptation then means an agent uses novel information to adjust its decision-making process or as a conditioning agent to ignore it but not factoring in those Nuance Novelties right throw those away you found the Salient Novelties you're actually interested in put that back into your agent um there are different strategies for doing this and again this is where you sort of break domain Independence or at least this is what we found to be useful in the DARPA Salon program but again there was there were questions about the domain Independence aspect of this so the strategy for perceptual domains number one you're going to have a future extraction via a neural network number two novelty detection using extracted features three you're going to use a clustering algorithm over novel features to identify new categories of novelty that's your characterization process four you're going to incrementally learn to incorporate this new type of novelty back into the model um now the process isn't all together dissimilar for um interactive agents in some activity domain but there are some differences number one the agent is going to sense information from the environment number two the novelty detection using sensed information is going to take place number three that detected novelty has to be characterized in some way and then four is a key distinction rate the agent's plan is revised to incorporate the novelty all right another big aspect of this risk-based reasoning uh novelty detection carries the risk of being too insensitive to hard to detect instances of novelty or being overly sensitive to all instances of novelty leading to this problem like keep harping on right Nuance novelty um so I I think this notion of regret is a powerful one right especially right the training process if you're trying to anticipate right um the impact of this novelty on future States um and then finally like checking it you know through like a loss formulation that's going to be really really insightful right as you try to manage these Novelties um you know if if the regret is quite low you're going to learn what these nuisance uh Novelties are quite quickly right um for whatever your task is because they're not really impacting the the agent in the future if the regret is extremely high oh I need to incrementally learn this novelty because it's really impacting agent performance in the future um so so building that in I think it's something that we typically don't do in these algorithms but it's something that I think is really key uh for for more successful Asian Behavior okay to wrap up let's just talk uh briefly about the evaluation of these agents um there two open issues here number one the spectrum of partial knowledge the system designer has about novelty um this is a general problem in AI research right evaluations uh May leak information um if you're training you're trying to Brute Force the problem you may have sucked up some Novelties right that are in the evaluation unintentionally or shame on you if you're cheating intentionally um this comes up quite a bit uh in fact we saw this within the salon program with like uh large Vision language models like clip right um in some cases right like we can't we can't go and look at all of the training data by hand and in some cases these Novelties right are very subtle right um and so they're going to be missed even if you did a large scale uh manual inspection um so that's make that that makes this really dangerous right because you may think you have a really like uh good agent in terms of its performance but it's really cheating on all this stuff right that we thought uh was novel at evaluation time that was actually in the training data set right um so you need a really strict firewall um one thing I I think which is nice if you're doing these um activity domains that are like in simulation you have a lot more control over the environment right um so the chance of unintentional leakage is pretty low um that is not the case with uh traditional computer vision data sets right where we're sort of blindly uh um collecting data having people label them right we know there's some inherent noise in there and again if we're thinking about latent Novelties especially nuisance Novelties right there could be a whole lot going on there which is an issue um so there are some leak mitigation strategies um number one have a sufficiently large validation set of data for the novelty detector to be trained with with with Novelties that do not occur in the testing set for debugging purposes two clearly describe everything considered to be known this can be especially difficult with perception domains but is crucial since the problem becomes underdefined otherwise making it impossible to tell the difference between nuisance novelty and manage novelty right the Novelties we care about during the creation of the novelty detector ensure that the system is defined by looking for novelty rather than guessing a predefined set of potential novel States so that is a risk in these interactive domains I will just anticipate all the possible States and root Force the problem you can actually do that with really simple domains like cple um more expansive virtual environments though right think like like guess self-driving car simulator that's not going to be as easy and then finally ensure the problem space potential Novelties is large enough that it would be impossible to hand code most of the novel States again another that's sort of like the human brute force uh approach all right one more here uh in terms of open issues the lack of measures specific to open World Learning um I pointed out earlier that the metrics that we find most commonly in the literature are pretty in inadequate alone to assess the success of open World Learning right accuracy precision recall F1 Au uh uh MCC um these are basically all limited to classification performance right you said there's this characterization piece that's really hard to figure out metrics wise um know there's more when we think about the performance of uh incremental learning all these different things um also I think really important something we don't do enough uh in computer vision measuring the uncertainty of these systems um what's really fascinating when you think about uncertainty and Novelty um in many cases the uncertainty of the model is quite High when it sees something new because it's something new right um that's a really like diagnostic uh measure right but we're not looking at that when it comes to evaluation um and then finally domain independent measures for estimating the complexity level of the domain also uh provides a way to compare different domains right which is pretty interesting as I mentioned right just you know sort of in an informal way some domains are really simple like carple some are really sophisticated right um like a self-driving car simulator right there's a range of that it'd be nice to actually put some numbers to those um so we do this in a more rigorous way all right I'll wrap up here I want to acknowledge um my collaborators on this work uh Steve Cruz uh who's A postto in my lab at Notre Dame uh Chris Funk at kitware uh who was on our team for the Dara Salon program and Karina doctor from the naval research laboratory uh who was running uh uh parts of the the DAR Salon program with respect to evaluation uh who helped us sort of formulate these open issues um if I have any time lap I'm happy to take a question or two looks like I do question is from the a surely there's a question I know it's early and I'm like the second talk in this Workshop everybody's still caffeinating yes yeah as you mentioned you know is trained on tons tons and tons of data don't know what's in it and your solution is have a sufficiently large delegation set so uh how does that actually solve the issue because we don't necessarily yeah yeah let me let me add another clarification I think the the warning I we give everybody don't use FP out of the box like you're going to have to retrain these things right with like a better curated data set um uh and again this is true of a lot of like the the sort of like Foundation model large model right it's just they're a huge headache to work with in in open world evaluation just because again you don't know what's in there um so I'd say you have a much better handle on this if you do it from scratch but there are still pitfalls right because there's just so much data and again especially if you're thinking about nuisance Novelties things you didn't anticipate that are in there um that's that's going to be a factor um again not as much of an issue when you're dealing with an interactive environment where you control like the game engine um but yeah this is this is a really important Point any else yes for your good presentation here my question will be um we jump out of the abstract destination of the noal we look at some real cases for examp the uh open world detection then we to figuring out the granularity of C because if you really take uh for example this building you could take it as a ho and you could take as a piece of this W this this Windows you could also take as a piece of the material and this kind of stuff you could always divided into the small pieces and what and if you always do the incremental learning then the capacity of the model is limited yeah this this is a great question it comes back to flip back all the way here um this idea of a hierarchy right um as I mentioned this was just one first cut of hierarchy I agree this could be an enormous hierarchy and much of what's in the hierarchy is Irrelevant for the task right so this comes back to my point on nuisance novelty I think this is one of again the more underappreciated aspects of this problem managing that I I think in most cases right you need an agent that knows that those really fine grain distinctions are not impacting task performance that could be through its regret formulation and it's simply ignoring them even even though they are truly novel and exist in the hierarchy in some sense right but that's that's not easy right that's that's a really tough tough situation to it out yeah yeah yeah yeah great all right turn it over to our next speaker thank you everybody let's thank the speaker again all right it looks like we need a bigger room for this all right I am very happy to introduce our next speaker de Ramadan uh who has won basically every computer vision award under the song including the mar prize B Pascal Vios Lifetime Achievement Award long nins prize best paper awards at cbpr 19cb 20 iccb 21 and over the years I've had the opportunity to have many interesting discussions with uh with daa about open world perception and he has a very unique perspective that we will see today in his talk open World Learning in the era of multimodal foundation models and with that I'll hand it off the table okay thank you sort of nice to get introduced by your own student okay so let me kind of motivate uh sort of my sort of Fay into this open world setting is when I arve at 2015 I had a conversation with Andrew Moore um who just recently returned at that point to Academia from Google and at that point I was writer on the Deep learning Revolution and I think he was probably the one person in the world training more machine learning models than anyone else um and he had this statement always stuck with me he said e that okay I'm going to say more hi okay so first off there's a continual learning Workshop happening right now um and so one of the things that frustrated us when we first looked at this is that the the data sets were all very toy sort of a continual learning mnus and if this is really such a big problem that you've you're sort of faced with it even if you don't want to in the real world we shouldn't have to really invent a setup for this that the data should just be screaming at us uh so the first thing we tried was we went to Yahoo flicker 100 million we assembled a data set of different categories of objects that we thought actually interestingly evolved over a span of 10 years so this data is timestamped um there's a wonderful talk in one of the workshops Yesterday by by dimma on the fact that uh 10 years ago we all had walkman's and now you don't see that anymore so even the radios that we've been using have changed from before to now um so we put together a benchmark the student the lead student involved with the zq and uh he sort of put a lot of Blood Sweat and Tears into this uh whole slew of car whole slew of categories I think we even talked about this in previous iterations of the workshop um and let me just tell you some some sort of take-home messages first was that the one of the reasons I got I guess became disillusioned in this space was that um the setup for continual learning felt a little bit artificial to me the traditional setup was really uh sort of on the left-hand side where you got data that evolved over time but your train and test data both evolved um and the problem with this is this doesn't address the the problem that Andrew mois said he dealt with which was domain shift so the right hand side is is more of a realistic setting where the new data comes the new data shifted because it it pertains to this year now you see um you know this year's models of iPhones versus before you just saw flip phones um and so what's kind of interesting about this is that once you uh test on this data um so today's test data next year will become your training set because you'll get a new Fresh fire hose of data arriving from this continual stream um and one nice uh sort of aspect of this is that you can you can sort of theoretically avoid overfitting because essentially once you test on a data set um at some level once you learn that Transformers work better than convolutional networks you've done model selection on this test set and you've somehow polluted it and what this is saying is that in a true continual learning setup that should be become your train set and you should get new new fresh data next year um but one more sort of take home from from the uh this this sort of large scale effort we we put is that um it be it felt even more important to emphasize the role of unlabeled data so this fire hose of data is coming in you can't really afford to label all of it you actually only afford to have a smidgen of this and so we found is that just like continual learning algorithms that really un leverage unlabelled data did did much better um and in fact there's something wrong with this picture because if I have a little if I can only afford to label a little bit data for training I can't really afford to test on all this data so in fact there's another problem um that we have is that even testing we should think about is we evaluate models with a little bit of labeled data and a whole bunch of unlabeled um and so just like we have this problem of like semi-supervised learning I think there's analogous difficulty of semi-supervised validation so we had this this old paper on this it turns out you can apply a lot of similar ideas from the semide learning but but but but there is some some wrinkles there okay but now I want to sort of zoom out and so this suggested that the role of unlabeled data uh is really important and that's no surprise we live in the foundational self-supervised era oh let me sort of okay yeah let me just sort of skip this then um and uh the thing that kind of cutt me up was this uh this this perspective of if the real challenge here is really about shifting kind of distributions then we actually look at the this as a community using different nomenclature sometimes we'll use the term domain adaptation in addition to continual learning and I think one of the the world experts on this is Kate Seno and we having this conversation with her at the workshop where she held a whole bunch of like domain adaptation benchmarks where uh sort of you know her group was working on Innovations of training models that' be easier to domain adapt but uh her her take-home statement was that it was sobering to see just how well scaled up models perform compared to models deliberately designed to to adapt um and so uh this is also not uh not surprising like in some sense the um open AI did not make a bet on domain adaptation did not make a bet on continual learning they made a bet on scale and it seemed like that was the right bet at that time um so the the perspective that I want to think about open World Learning is that um I I think it's important to think about how do we characterize open World Learning in the context of foundation models so uh is Walt still here okay well so I'm curious here to think about this so I would make the claim that like we need to have a formulation of the problem that lets these big bohemuth models enter into the conversation because if we say that they can't because you we don't know what you were trained on then um then it seems like somehow we're dead in the water because we know it feels like one of the biggest tools we have for generalization is scale and we need to be able to like ask about it there's wonderful change with barath and and jendra that that stuck with me as well so you brought up this question in one of these workshops like okay if we don't know what models are trained on how can we test them and gender's answer was perfect he said well that means we have to rethink the way that we we frame the problem um so my claim is we need to rethink 100% agree to rethink the way that we uh we frame we should not allow that we should allow it but we it's on us to sort of figure out a setting that allows those things to come into play so what is the right setting to think about open world F shot learning with foundational models um and and the starting point for us is well did these models really solve Vision so this is an autonomous vehicle data set where um what we did here is we ran um uh an open world detector uh uh glip um I think or dtic but here's a results of this and what's kind of interesting is that this is actually uh this does not score well according to the ground truth and it turns out that the way that this data was was annotated was um the bicycle bounding box included the person and the bicycle and the truck just included the front cab of this tractor trailer and not not the trailer proportion um so it feels like there's a disconnect between um uh the way these data sets were annotated and what uh I don't know GPT for vision or these Foundation models think of truck or bicycle should be um I iess said any any guess as to why this exists well uh what's kind of interesting is that you can pinpoint exactly why this exists and that is there's an instructions uh protocol for for this data set where the annotators were instructed to label bicycles as only containing a bounding box that only contains the bicycle and not the person um so this is very literally what what annotators were told to annotate um and what's kind of confusing is that this is actually a different definition than what weo is a bicycle so according to to waymo's definition you should not annotate a bicycle that has no person on it it's the exact opposite um so so at some level it seems like if we don't agree on the definition of bicycle how do we how do we expect uh foundational model to know what we mean when we say bicycle uh and in fact I think it gets even worse than this um there's been um sort of well-known analysis of like errors in uh in uh analysis of errors in data sets in particular high stakes data sets like a vehicle benchmarks where folks have analyzed oh these benchmarks are missing uh ground truth annotations and this is really dangerous this means that there's safety critical systems that are being trained with flawed data and there's sort of an En numerous like a sort of uh an interesting Twitter uh response that said actually no the labeling policy told annotators not to label cars that were um not on the drivable surface so if you're a parking lot don't annotate them um so so uh somehow it feels like we have to come to grips with this so clearly there's directives from on high in the white house so we need trustworthy artificial intelligence and one of the things about how we can build trust in what a system does is we have to understand what it was told to do um so uh something we've been looking about something we've been thinking about is thinking about instructions as a technical artifact and I think uh instructions are in incredibly interesting it's almost like how a stakeholder explains to an annotator what they want um and oftentimes it's done in a very deliberate multimodal way we give uh textual descriptions of what we mean by barrier these are the actual instructions for this data set um we give visual examples and the visual examples span different things sometimes we show prototypes this is a really nice clean barrier sometimes we show partially occluded barriers we show Corner cases because we want to make sure the answer knows you should also anate this and sometimes this is cool you show what a barrier is not because you know how get confused by that so um and sometimes you just get really weird descriptions of classes one of the classes you sanitat is debris by ask you to annotate debris in images I don't I think you'd have a really hard time you kind of need to see what examples mean uh and need to see this this uh it seems very helpful to see this description and also see these these visual examples um so what I claim is is that um I remember this time in uh in the self-driving space myself but often times anytime you're involved in a in a labeling effort there's this incredibly complex and laborious conversation that happens between the stakeholders and annotators where they keep saying um is a reflection of a person should I label that as a person um if I have a construction crane that's Ms to the ground is that a vehicle um and you keep refining a policy of what it means to be a person what it means to be a construction crane and here's here's a sort of maybe a controversial claim maybe that actual policy is more valuable than the annotations themselves um because it takes money to build that policy because you have to iterate you have to have a conversation um so uh the the claim that I want to make is is is one is that we should almost treat labeling instructions as a first class citizen it's part of a data set and it's a vital part of how a model gets trained and oriented and if you look at work from the fairness Community data sets for data sheets was a landmark paper it doesn't qu it falls a little bit short of saying we should also release in annotation instructions so in fact we've tried to get annotation instructions some of them have just been forgotten in the test through the the anals of time I don't know what the instructions for uh for imet actually were um and which is kind of a problem because you know there was a fantastic piece of work by Ben W where where he collected imag net V2 to kind of verify how well does it actually have we been overfitting or do we actually generalize and in order to collect V2 it would been really great if you just had the exact same instructions and you could just run them again so even for like policing ourselves and doing uh kind of of due diligence it's nice to have these things as historical artifacts um okay and and what I claim though is that this is actually uh uh maybe a really nice way to think about few shot recognition what I'm showing here are six examples uh that try to describe a class or a concept and there's positive examples negative examples um and in fact this is what Anish and Niar and Shu have put together as one of the The Benchmark competitions associated with the workshop a way of thinking about a way of one claim is you can think about view shot recognition in the foundational age as almost multimodal fine-tuning of you have uh a foundational model and it might think it has its own definition of what a truck or a bicycle is and I want to kind of adapt it to what I mean when I say truck or bicycle and I can actually use the exact same artifacts that we use to adapt each other when we explain to an an annotator um and now uh what's also I think kind of interesting is that you could think about this as almost just like multimodal prompting but rather than predicting rather than sort of picking random examples these these have been prompt engineered because that's how we're there been engineered through this laborious process of how we explain to each other what this concept means um so so this is one sort of hypothesis for how to think about open World Learning we really think about it in terms of adaptation or alignment of these models to the particular thing we want them to do we don't really have a good word for this welcome any ideas like right now we're calling this Benchmark foundational fuse shot and I don't know I almost feel like it should be called multimodal fuse shot um okay um and this is a work that uh uh Anish andar will talk about later today he guys uh and and I think there's been uh so I've been involved in in various Benchmark uh competitions and one Hallmark for me for success is that if the winning entry does much better than the Baseline and that's true here so something interesting happened when we set this up and challenge people to attack this problem they they came up with interesting Solutions so I look encourage you guys to to listen to that uh later on today um oh and so my my bet um now that we think about how should we kind of adapt a model to what we wanted to do we can do it via prompting we can do it via in context learning we can do retrieval augmented gener a we can do F tuning you know grading updates there's all these kind of new tools that we have um my my own bet that I've told my students a lot is that I think it's just going to be in context prompting um but I lost that bet so if you look at the winning results here they actually do some real grad based fine tuning um okay oh and I'll show like one uh do you know how much time I have okay um I'll show one one kind of fun example of uh of uh one particular approach of fine-tuning a multimodal model that's uh that's really simple and and kind of motivated by maybe I should first kind of even though I sort of motivated this this multimodal adaptation task from a practical perspective I think there's a lot of cognitive motivation for this so um in the cognitive development literature there's evidence to suggest that when you teach the concepts to to Children it really helps to give multimodal cues you show visual examples and you explain what you mean and it's remarkable to have both both of those different modalities in fact in Neuroscience there's a lot of evidence to suggest that you know when you say a particular word and when you see that object the same neuron fire so it it's sort of interesting to think about multimodal representations that are biologically motivated and of course we also we use them quite a bit things like clip and uh the intuition that I want to throw out is that here's a really simple multimodal fine-tuning algorithm that leverages this uh kind of cross modal space that clip has so importantly uh sorry here okay it says audio encoder it should be it should be text encoder um what I'm saying is pretend you have you're trying to solve a two shot problem you give them two examples of a class but you also have a class description and normally if you're going to train uh a few shot classify you would just take these two examples unless say you just do linear programing you would extract a five2 dimensional feature associated with the two examples and feed that into a classifier let me this okay feed that into a classif ifier um but since I'm trying to sort of emphasize this multimodal perspective we also have a textual description of the class at the very least we have a class name but maybe it's even more maybe it's one of these rich instructions but I can take any of that text push it through my clip text encoder I also get a 512 dimensional Vector so now I can think about doing my linear learning my linear proe classifier using three examples instead of two and as you might imagine this is going to help because three examples is way more than two um you know I've doubled or increased my data set by 50% and so uh just by training a linear classifier on top of these three examples you can do much better um and this is in contrast to think much more complicated prompting models and prompting techniques that sometimes take half a day to train versus this thing literally take seconds because you're just training a linear classifier on pre-extracted features but the features are actually using different modality encoders your text encoder from clip and your vision encoder from CL because they always map your data points to the same space you can just treat them as three examples so uh a kind of a fun artifact of this is that this works for any kind of modalities so we tried this on a data set of vision and audio example so something called imag net um with with sound so there you can have um image classes let's say of dog that's one class you also have audio clips of dogs barking and just as we showed before of training with three examples always better than two the same holds here you can train these three examples that are multi mod you get you get a linear classifier L linear plow classifier that works better just because it's been trained with more data and that linear classifier can be run on any one modality at test time it can be run on audio stuff audio clips and visual examples so what's what's cool about this is that what this means is that actually listening to a dog bark helps you do better at visual classification of dogs um which is sort of a cool result it's sort of really leading into this this multimodal uh encoding um okay all right let's say um this is just sort of a Shameless advertisement for zi will present a a poster this week on um if we're if we're leaning into this space of really leveraging text and visual examples to do adaptation we know that annoyingly these models are really sensitive to the textual that we give and so this is why I've heard this derived is we don't want to just resort to prompt engineering that's a hack um one of the things we've been looking at is can you actually automate prompt engineering by using another llm and it turns out you can so language models as blackb blackbox optimizers for vision language task so the idea is that you can actually give it maybe you have your fuse shot training set um and you can say um give me a prompt that will help me learn visual classifiers that will correctly classify five few shot examples and then you can give it feedback about what prompts worked well and what prompts did and it can actually do a form of hill climbing this is one of these wacky things that uh that that chat models can do but the the end result of this is we can learn prompts or not we the system can learn prompts that are actually better than prompt engineered results that we've seen and what's kind of neat is it actually learns prompts which we can evaluate this on imet or other Target data sets it can actually learn prompts that sort of describe what the data is uh it kind of makes sense in this reasonable way so if you're processing Caltech images that tend to have centered objects if you give it a textual prompt that's not just a dog but it's an image of a dog with a blurred background that emphasizes the subject you'll do better on Caltech um and so you can discover that automatically um you can apply sort of similar things for increasing the accuracy of Dolly three image generation or prompt inversion so so we actually are using this code quite bit to avoid prompt engineering ourselves for all of our projects um so I encourage folks to try this out okay uh in the last few minutes I want to talk about how to extend such uh foundational knowledge to multiple modalities so the argument that I that I want to make once again is that we kind of need to embrace this enormous massive knowledge source that that exists out there and figure out how to use it and and one of the ways that uh I want to exploit this is that I want to bring it to other modalities that we don't have all this massive knowledge for so I talked about one method already with this this cross modle linear probing um we have a a couple of other projects in collaboration with with folks as well that try to bring this knowledge to lar so in the space of autonomous vehicles lar is a very important sensor but we right now I don't think we'll ever have the amount of data uh in ladder that we have for images and so how can we best uh um kind of Leverage foundational models for images to help us improve processing of a different modality that we don't have foundational knowledge for um and it turns out there's actually a really kind of simple uh philosophy that we've we've seen work fairly well and that is assume you have unlabeled data that is across modalities so cameras and lar so this you can get just by mounting a camera next to a depth sensor um we all have one um right here a multimodal sensor this this the black thing is a lar so my claim is it's easy to get multimodal data we have lots of foundational image data and maybe I just say foundational image models that are encoded in the models but we don't have anything like this for Lars but how do we kind of transfer it and so one simple approach is we we take these foundational image models we can run Sam dtic or any of these other things on the images and then using the uh the Align lar we try to unpro into uh labels that we have for instances into the lar Point cloud and this is relatively straightforward because we have knowledge of where all the points leaves ladder points project to these image pixels um we can use extra knowledge of maps and the fact that 3D objects tend to be aligned to uh uh driving directions um but once we do that now we can get pseudo labels for 3D annotations and now we just interpret this as pseudo labeled multimodal data um and from this we can then train uh actually oh I should have started with uh we can train light our 3D detector that's the top one um we can train a multimodal 3D detector and what's also kind of interesting is we can actually distill this back to the image domain and train a RGB 3D detector uh now that you have this Align data um and so we have a couple of papers uh um including um segment anything in lar uh alos is there that's his baby and then we have some some variant of this it looks at 3D detection um and then segment anything in l does uh one one remarkable step further which is instead of actually using annotated cuboids it uh preserves a clip embedding um and what that allows you to do is learn a representation just for liar that allows you to type in any pits of text and then find objects that correspond to trash cans or debris or any of these other complex things that I hinted at before um um and there's uh some results from the detection side as well um where here we've also focused on uh RGB only detectors or even multimodal detectors where you have access to both modalities at test time okay let me skip this for the moment okay and actually want to maybe I'll I'll kind of uh call it here so this was sort of the uh let's say a take-home summary so this was all motivated by thinking about this problem of domain shift what happens when your test dat doesn't look like your training set and so while that really prompted the at least for me personally looking at domain adaptation and continual learning somehow it got a bit disolution from that perspective when when we saw all this incredible Innovation just by Brute Force scale and so I want to think about still this problem of how do we um adapt to our particular test scenario using these massively trained models and so I think these these multimodal instructions are a nice way to think about it um because at the very least that's way we talk to each other that's the way we instruct our kids what what these new Concepts are um and then finally one the the the aspects of doing sort of multimodal learning is that being able to transfer information across modalities and here I think the the assumption is always going to be that the elephant in the room is we have so much knowledge for images that and we that will always dwarf any amount of data that we'll have for lar for audio for tactile and we have to think about how to transfer this um this incredible knowledge source to the other modalities um okay and I will end there thanks yes I think regarding second mentioned instructions for actually creating good data sets this is a problem that we face in indust quite a bit we have aov we got a lot of domain as well as annotation instructions but in your approach what you're mentioning right basically treating it as a prompt engineering Problem by itself yeah uh I think one thing that we're missing here is the continuous feedback loop because uh for like the analy that you took right you have to how do you T about how to go about this the see like it's not one shot learning it is always a continuously continuous feedback process it maybe a few feedback steps or a long series of feedback depending on coaching versus kind of one set inspection but but do you think that for continuous or for more complex problems where real world scenarios captured uh this prompt engineering approach can be kind of created through feedback which can continuously of keep improving yeah you know totally I I agree 100% so so in fact um the analogy I want to make is like I definitely saw that as a as a conversation between stakeholders and annotators and that conversation was incredibly laborious it took a long time and it was backbreaking and then I almost feel like you know maybe in the future instead of having conversations with annotators we will have those conversations with chat Bots which that's what they enable they enable chatting with history so so I actually think that's very doable right now um uh yeah so so I think it actually fits in very well to the current current technology so very nice so when you're developing a model for try to evate that and I think what you describe is basically if your model actually aligns better with construction better which doesn't mean that it'll actually better when you put it on like mation so I think there's like and I mean how should we interpret all those trust like from the yeah yeah so I don't have a great answer so so in other words what you're saying is it could be that the stakeholders are wrong um because ultimately what you really want is to I don't know in the perspective of a self-driving car I want to build a self-driving car I don't want to just make someone happy that has labeled I think a bicycle should include a person um it's kind of an arbitrary distinction so I think uh I don't have a great answer for you there the the the the angle that I want to take is that at some level when it comes to uh perhaps these these uh um high-risk applications um we need to have some visibility into what we think the model can do this is sort of the transparency and Trust argument for AI you know the that the the the the the instruct you know the the White House dictate um and so for that uh I think it's almost like instead of trying trying to build trust in the model we almost build trust in the in the instructions given to the model um so I don't know how to ensure that but but I think that's going to be part of the part of the equation and right now it seems to not be like right now there's a lot of the instructions are are sort of Lost in in the history of time and I like what you said too like sometimes I this may be a bold claim I almost feel like their their ads are more important than the actual data annotation themselves um process of passing on that information is actually much more critical than the which is fascinating because that's not how we talk about data as all we talk about data and labels we don't talk about this other thing um go for it IDE instructions also the combination of well the descriptions in that and the got it but the question I wanted to ask actually was about so I guess if I was to go back to the original question of like open world yeah you still think um that there will be a discussion when weting between sort of Concepts that the model SE in some way training not is that something that is worth testing or it's yeah okay I can tell you and this may be the wrong way but this is the best way I would think about this now is that I want to come up with a framing of it where I don't have to know what it was trained on because I want to let things that have been trained privately to enter in the conversation so if if that's the case then I don't want to Shackle handcuffs on what you're allowed to train on but then I want to argue that it's my due diligence as a as a as a benchmarker or evaluator to make sure that for example I can go out and take pictures right now and if I go and take pictures right now I know those don't exist on the on the internet there might be very similar ones but at least those don't because I just took them right now I can take it I can put them on a computer that is you know insulated from the internet and then I can say this is my validation set and so I would rather sort of take it uh put the onus on the the the collection of the validation rather than the way that it's trained go for it um so I guess this is also a benchmarking question but you know suppose I want to set up a benchmark with a multimodal prompt is that do I take the prompt that's been engineered for human communication on a particular uh task or do I take one that you've done hill climbing with respect a particular model so ideally I would like this interactive setting but of course that that I also don't know how to set up a consistent kind of evaluation that keep interactive short of just ask lot of humans to evaluate totally exactly the right question I don't have any answer to that but that's exactly what we sort of wrestling with now what is the right way to formalize it or or set it I mean one one nice thing that I do like is that it's it seems interesting that like instructions we have right now are already engineered so might as well like start with that um it's been engineered so that we in a way that we think helps communicate to each other what we mean by something and then um uh we can see if we could do better um so so it could be that like maybe they haven't been engineered well uh maybe if we had just said the the right thing to the annotator we would have gotten even better quality than than what we did historically like I remember this this this this artifact when we were looking at CoCo annotation um there's like this cultural issue we had a lot of annotators from uh China and India and uh they didn't quite know if a hot dog was a sandwich or not um and so that was one of those things that that came out of this conversational process where we had to refine the instruction to say that um and so my guess is probably if we worded things a different way and gave different visual examples we would have gotten higher quality and um so you know so hopefully we can start with what exists there and then actually see if we can optimize but then in order to optimize you need to have some data I guess that would be the like the few shot examples like that that's that's what we technically H climb down yeah go for it you definitely see the value there's T of stuff with images's Stu with text but it seems like most the power that we come from is the stuff that's unique to what audio that isn't in the scene by how do you see ath yeah yeah so I I don't have a great example for the audio setting but like um let me give you example from from the one shot setting so if I give you an image of a uh a purple cowboy hat and I say find me one shot examples that look like this and I just give you the visual visual image I don't think you know what I'm asking I might be asking you to look for hats I might be asking you look for cowboy hats may be asking you look for purple cowboy hats and just a little bit of text that's a dramatic uh is dramatically helpful in resolving that am it I think that's one of those reasons why when we sort of explain things having the different modalities is often times helpful so it's definitely the case like sometimes like in that case that bit of text is even more useful than another visual example um because it totally explains what it is you're trying to find um so my my intuition is that things like this would exist for audio and other modalities as well I haven't really thought about that um but but I think that'd be cool if somehow like the sum of the modalities is more than the than the parts can you put it this way that it's easier to actually State a rule or state a exception in text rather than in images right you can label an image but you can't really Express whether so the bicycle exam what to exclude what to include or similarly the Hat exam you have to label thousand images yeah to basically kind of make an an understanding that you are supposed to only label cowboy hats or purple hats whatever but you could just write it in a text stating roots in text yeah so so I could imagine like I think an interesting space Maybe to answer Charles's question as well is thinking about this maybe not justly with text but a bit more programmatically uh jerck was saying this so for example uh like finding not purple cowboy hats if you just put that in text like there's a lot of text encoders that get confused by that one word not but I could easily kind of algorithmically code something like how purple is it and then downweight the score based on that like I almost algorithmically change uh uh sort of clip cosine similarity and so so I think Pro programming some of this is is really natural and so that's one of the things I also was super excited about the instructions that they naturally include negative examples like in fot we normally don't doesn't enter into the story but once we think about how we talk to each other it's it's it's very much there okay than e e e e e e e Beau ISO is everything okay on the zoom side great done hey how's it going think no yeah pleas oh I I did yeah thanks a lot thank you so much my student um for folks over Zoom uh can you did this fix the problem are you seeing the right thing now ah great okay thank you thank you for letting me know yeah sure sure and it's I think it's a 40 minute Vlog okay great actually let me just check you to go until 1050 okay great great I'll try to for that all right thank you everyone for joining us let's get started uh our next speaker is Andrew Owens from University of Michigan is uh and he's done a ton of cool work in multimodal learning learning from video Eng brand some you some of you may be familiar with his long running Workshop P sound which has been seventh year and if you're frequenting on on academic machine learning Twitter maybe you've seen some visuals of visual anagrams all these projects but today uh Andrew will be talking about learning multi models of the physical world and I'm very excited to see what beening please thank you thank you so much the very kind introduction I'm really excited to be here and thank you so much for inviting me so we still get a lot of our supervision today from humans uh in the past some of the information we get be from uh really specific things like uh labels or semantic segmentations or scene categories but as we've seen in the last few talks increasingly we're relying on language based supervision there's this been this move toward learning from correspondences between vision and text and while it might feel like this data is really abundant and it's free at the end of the day you still need humans to provide you with this data and there's a lot of limitations to the information these humans might provide uh for example it can be um relatively high level information that that in some ways may be imprecise and maybe more subtly you can't learn from this kind of data autonomously like if you imagine a robot that learns from language and the only way that it can learn about the world is as a phone home and ask a human to provide it with instructions it's really going to be a very limited system and so if we're building these so-called uh Foundation models that um we're using to build for other computer vision tasks you might ask whether we're building them on um shaky foundations uh on the one hand you have these sort of high level forms of supervision like labels and language and the other um major form of supervision is just the raw images themselves that you learn from self supervision I think there's kind of this missing middle of um supervision that comes from other sensors uh like daa mentioned liar as one but there are also others that are maybe more familiar to us like like touch and and so the work I'm going to talk today is about learning from this sensory supervision uh that comes from other sensors and in particular I'll be talking about um two different ways that you can um you can learn from this kinds of data um using 3D constraints to learn from tactile signals and using a kind of um analogy reasoning uh to learn from audio so one of the con traits um that that I think these other that that we have on these existing Vision systems is they like have a lot of trouble um understanding like the nuances of the physical world um if you consider like our very best reconstructions of a scene um we might have this kind of shell of the scene we don't we can't tell that uh beneath the surface of this concrete it's actually this really rough complex micro geometry or that it's a rigid surface if I put my hand on it it's not going to move um it's we just have this kind of like empty shell that we get from from say a Nerf um and also what's missing uh Beyond micro geometry is uh this notion of physical interaction like if we see this dirt uh we as a humans know what will happen if you manipulate it it's soft it's crumbly uh if you put your hand in it it will deform um but it's challenging to to learn that information except in a very high level way from signals like language right now and so how do you connect Vision uh with um this this sense of touch uh and we wanted to create models that connect images with our corresponding uh tactile signals um but unlike in other modalities it's a little bit less clear how you do this with touch uh the sensors are less standardized it's not as obvious how you collect the data um and not clear how you define the models and so we start out with this um this very so our group has been working on this for several years and as just a point of background we we relying on of these recent um touch sensors called Vision based touch sensors that you might have seen before um that that are really enabling some really interesting um applications of touch and if you haven't seen these before they sensors like gel site or digit that essentially converts touch sensing into a tactial sensing problem um you're going to create an image like this one here that captures what you're touching so in this case um we're touching an avocado and you can you can see The Ridges of the avocado in this video and the hardness of the avocado is apparent in its uh the amount of deoration that you're seeing in the video um in fact you could like predict if it's ripe or uh uh pre- ripened before rpe avocado this way and here's how these work you're just going to take a commodity camera uh put a piece of gel on it on the lens and then when you push the sensor into an object the object deforms the gel and then you see a video of the deforming gel in the camera and to make this um image a little bit easier to interpret we can add colored lights that illuminate the gel from many different directions the same trick you might have seen in photometric Stereo and this is idea that that's from again from originated from Ted adon's group and has been really carried a long way by engine you want at uui and so we've essentially convert this um this touch signal into this video of an illuminated gel that captures shape over time and that you can process with off-the-shelf video understanding and in the work I'm going to present we're going to use these touch sensors and we're going to exploit the fact that it's under the hood a camera okay so how do you actually collect paired vision and touch data um when I was starting out I thought a really good way to do this would be to use robots you just take a robot in a room you give it all these objects and you have it repeatedly touch them and record video at the same time and well you could do some interesting things with this unfortunately it's um very hard to get diverse data um the robot really only sees one thing the the table that it's sitting on and there's almost this like chicken and egg problem like if the robot is able to successfully perceive an object with touch it probably is already capable of manipulating it to some extent and so it's hard to learn um really like diverse uh um uh objects uh and that that would be difficult for a robot to uh autonomously manage and so there there is something though that's much better uh than than robots for this and that's undergrads so we can um we have this uh our later work we we got this team of amazing undergrads uh some of whom are are now PhD students uh presenting work at this conference and we just had them they just went out and they just collected everything they could uh with a touch sensor well simultaneously recording egocentric video uh and they they really you took this really far uh they went all all over um they went to Parks uh they went to offices at one point they got politely told to leave a museum and you can see that these videos here they have a egocentric video with the corresponding tactile signal and what you're seeing this video of this deforming gel and the um the changes in the colors convey the micro geometry and over time you see how it changes and really you know went all over an arbor captured uh you roughly 14,000 different tactile signals and 4,000 different object instances um and um one of the these um students uh Fred young is uh here um presenting a paper on Friday on sensing as well and once you have this data now you can use it uh in much the same way you can in other areas of multimodal learning you can do things like learn a joint embedding between vision and touch and distinguish between real uh visual tactile pairs like this and fake visual tactile pairs like this that are mismatched and one thing that one preliminary experiment with this that I thought was really interesting is that even though we trained this embedding on entirely real data collected by humans we were able to take the features that we learned and use them for a robotic uh tactile sensing task namely um some grasp prediction work that we had um previously studied and we found the features we learned outperformed imag net features for this task um on on the on the touch representation and well you know one preliminary experiment I think it it shows that you can because you can scale up this procedure and collect more diverse data more adventurous um kinds of uh data in the world uh you can potentially um you know compensate for the the mismatch and domain between the the robot and the human to some extent and there other things you can do like uh we you can uh the same students Fred and J Ching they uh they tried to predict images from touch so you can train the fusion model uh to condition it on a sequence of touch signals and generate a corresponding image uh that that might have gone with it and we have some tricks for getting rid of the hand here so you can see the real image is in the middle it's got a hand in it the generator is on the right it looks like other grass um with no hand um here's another example if you're touching this wood uh this is what the model thinks you're you're you're uh you're pressing and you can even do other things that have been considered in the uh the language domain like you can stylize images using touch so let's say you have this input image of these bumpy rocks you can restyle it using this touch signal that comes from the smooth surface using SD edit and we get this result like kind of smooth out some of the rough parts that you can even try crazier things like make this carpet feel more like this tree and this is what you get um clearly it knows like what's being pressed but yeah I'm not really sure what what what it should do in that case but it has kind of an interesting result okay so you know as ambitious as was great as uh undergrads uh are at this task unfortunately there's a lot of limitations to this approach and your Dava talked about how liar uh data collection will never you know be quite as large scale as Vision this is even more extreme uh uh because every time you there's not like some device that's capturing tons of data passively and this you have to actually physically probe everything that you want to um that you want to record you have to manually you know press all these different locations see there's also this this more subtle issue of occlusion uh if you're touching something uh the visual signal won't see exactly what you're touching because the touch sensor will olude it and there's also this other issue that it's um at least the way we capture this is very object Centric there's nothing that tells you how the touch signal relates in two different parts of the scene and so we wanted to see if we could could uh reduce some of these limitations by putting vision and touch in a shared 3D space u so we'd like to create 3D reconstructions where you can select different locations and estimate what the corresponding touch signal is and I think these these uh multimodal um 3D reconstructions are potentially very useful because they can give models a sense of touch you can you can just act directly access what the touch sign will be at different locations they're also potentially useful sources of paired training data because if you have paired uh site and touch you can sample arbitrary viewpoints for each touch signal and I think ultimately what makes me excited about this is the potential to create multimodal environments where you can imagine um learning an agent creating an agent that learns in a visual tactile reconstruction both what things look like and how they feel and so this is some work that's being presented here at CPR on Friday led by eing da um also with uh fr um elu and Antonio laio who who's starting at pen in the fall and we want to see if we could create called tactile augmented Radiance so it's going to be a NERF where you have a sense of touch and in many of the locations of the Nerf and so here's how this worked we we wanted to register the touch in 3D uh and find where it lies within a 3D construction so we took the way we did this we took a selfie stick and put a camera on one end and a touch sensor on the other and then eing just went around and he just pressed the selfie stick uh on all these different objects and and recorded simultaneously recorded both a touch signal and also a visual signal and now that we have both of these signals we have a way that we can register the touch in 3D because if we can register the location of the visual signal and we know the relative pose between the vision and touch then we can chain these two Transformations together to figure out where we're pressing things in 3D but you know there's also this question how you get this this signal uh how do you how do you get this transformation how do you get the relative pose between vision and touch well we can take advantage of this this fact I mentioned before which is that these vision-based touch sensors are cameras in Disguise uh you have this camera and it has some gel on it and so we can just use tricks for multi geometry to find the relative pose between um real cameras and touch cameras so here's how this works we'd um press the touch sensor on a braille board and each scene uh you me with then manually annotate correspondences between the bumps that we're pressing and both signals and then we' use Simple camera resectioning techniques uh just solve for a relative pose uh between the uh touch sensor and the visual signal and we have to do this in each scene because you know the calibration between them would change over time and so I like that you can kind of take advantage of the fact that these are at the end of the day just ordinary cameras that are being reused for other purposes and and use all the tools we've seen in computer vision and um after creating this capturing procedure we went out and captured a large um many of these different scenes with something like 19,000 different touches total but there's one uh limitation here which is that these touch signals are still very sparse we only have these the touch signals for a subset of the scene and so we wanted to see if we could fill in the touch signals in other parts of the scene that we didn't directly touch and so here and here's how we'll do this we'll reuse this idea of uh using a diffusion model to estimate touch from images we'll take the Nerf that we captured we'll for we'll take a given pose RT of where we want to estimate the touch for and we just train a latent diffusion model to estimate the corresponding touch um from an image and depth map that we're extracting from the Nerf at that location and so this will be a way to kind of take the touch signals that we do have and try to use them to fill in uh the touch signals uh that we don't have in the rest of the scene it's a little bit like like in context learning and I should mention there's some really interesting related work in robotics that has a similar idea they um Corral this year they uh have small smaller objects they use robotic proprioception for and they use a gan to like us to impute uh touch so it's a very nice work that I recommend checking out and okay so then this is the result of this procedure we have a bunch of training probes you can see in this overhead view different locations that we probed and then on the right we're going to show generated touch for a selection of different locations in the scene Each of which is close to the places we touch um but not that close so it's about at least 10 cmers away each one and you can see this is we can estimate here what this would feel like uh you can see that it it estimates that it's an edge um you can tell this is kind of a softer surface that's uh this is a flat surface and we've seen enough carpet that we know that just like the texture of this service should look like that and even though we've never directly touched this table we can infer something from other tables or from other flat surfaces and here are some other examples of this we also have outdoor scenes we even have some scenes that were collected in snow more challenging and you can see the model is learned to propagate this touch in kind this quasi dense way and one of the things you can do with uh this model is you can learn joint embeddings between vision and touch and use them to localize tactile signals in a scene so you give me a touch signal I'll tell you where it's located in the scene uh and just to visualize this we can just take a bunch of crops from this image the true tactile signal is located at the center and I'm highlighting all the different locations uh that it's um that it's attending to and you can see the model it knows that this is an edge it doesn't know which one though it's it's one of these different edges and it's it's unsure which um here's another example where we press this Fabric and the model um lights up the whole area that has um Fabric and one thing that I thought was interesting about this approach was that the imputed touch is good enough that you can use it for training data so if we train our embedding with the densified you know estimated touch uh versus just using the raw samples we collected then we get a slight boost in performance on the tactile localization task which suggests that LE a touch signal we're get ating is useful enough that um the simulated data may be useful for um uh for other know Downstream tasks and likewise um in uh representation learning you can um you can take the features you learn and apply them to a material recognition task that we we used we developed in our previous work and you can see you again get a boost from using our data set and and maybe like a small Boost from um including the both the real and estimated data okay so just just summize part of of of the of the work this these tactile a mitted Radiance Fields um I I think you're exciting for the sort of open world setting because you can use these 3D constraints to uh collect um multimodal data um in a in a shared 3D space you get paired vision and touch for free there's no occlusion and it lets you kind of impute um touch in other locations and you can in principle collect this autonomously either uh with a robot but in our case we used uh hum do it I think what makes it exciting is that you could potentially um eventually have these sort of models that use 3D to learn about the world in a tonomous way okay so just to the rest I want to talk about some of our other work on um learning from audiovisual signals uh and we're gonna have a different uh way of learning about the world that doesn't exploit geometry instead it's going to use this idea uh of an audio visual analogy so one of the assumptions behind a lot of audio visual models and a lot of multimodal learning in general is you have some paired data somebody gives you an image and audio or image and text and there's a a known correspondence between them your task is to predict one from the other uh this audio goes with this image this text go with this image so on but in a lot of applications actually isn't quite what you want to do uh for example in the task of um visual to audio Generation Um merely predicting the corresponding soundtrack for a video isn't always what you want um the sometimes the the actual sound in a video is is actually not what a user would want to hear and uh just as an example of this the the names for the the artists that creates these sound effects are named fully artist and this is how they work so uh if they're if you um hear a sound in a video It's Made on a sound stage and the way that the fully artist produces the sound can be quite different from what's actually happening in the scene so here's an example walking on leaves or in a forest that's what you're hearing be walking for snow what we normally use is regular play sand and then to add to that sometimes you hear that familiar crunch which we use cornstarch your ear hears snow but in actuality it's sand what you're hearing in movies and and TV is really this very artificial sound and so if you actually correctly produce the sound that you want in the video it might not actually be um what you need for this Downstream application and so we took inspiration from this work uh the classic work from Aon herzman on image analogies um for those um who haven't seen the idea is given some input image b and a pair a a prime we want to create a new image B Prime uh that relates in the same way that to to B as a prime relates to a like take the change from a to a prime and apply it from B to B Prime and um we can try to do something similar here we could take a silent input video uh and our goal will be to take um some conditional audio visual example and use it to um create a corresponding soundtrack that input sound video so we're going to make this Silent Video sound like this other um video that does have sound so this is an appealing idea but how do we actually get the supervision for this so this is some work uh that was in collaboration with some folks at Adobe led by former undergrad Yushi do and Z chin as well as Justin Solomon and Brian Russell um we wanted to we found that we could create we can automatically create these an analogies by taking Audio Visual data and creating creating analogies from it as follows uh we will take one subsequence of the clip we'll call that a conditional example and then we'll take another sequence in the clip taken from a few seconds before or after and our goal will be to estimate the audio from that second clip using the conditional clip as a kind of a hint and this takes advantage of the fact that audio tends to be repetitive a lot of times if you hear an action you you probably hear a similar action some other time during the scene and so the model can exploit this conditional audio visual example as a hint to allow it to fill in audio that um it hasn't been provided with and again you think again as another kind of kind of in context learning um but it's one where we have the special Tim locked uh multimodal signal we have vision and sound that correspond in time every frame has a corresponding sound it's like a very particular type of in content learning uh that we're we're training our model to solve and so the actual model itself looks like a very standard um Vision to Audio model we adapted from this very nice work from G and Ru on um basically applying VQ Gans to sound generation we'll just tokenize the input video we'll tokenize the conditional video and audio and then we'll generate from all three signals together the um the code of a corresponding audio signal and then we can convert that then into a wave and one thing we found that was interesting here is we could also um improve the synchronization of the audio with the video um by using U what are they called synchronization models these are models that take a given visual signal take a given audio signal and they tell you whether they're in sync or not and we can use reranking just like clip reranking but with a synchronization signal to um rank the audio Generations that have the highest degree of um synchronization with the input video so we'll just generate like a hundred different audio tracks pick the one that's most synchronized and uh just produce that and so this lets us do things like take this input sound video of somebody hitting hitting things with a drumstick and we can um take a conditional example of somebody hitting a phone or a plastic surface and transfer that sound uh to the um to the Silent Video and we can even you swap out the conditional signal like what if we make it sound like this instead slightly different surface get this resolve and we can take also more kind of real world videos uh we can take a silent video of somebody juggling this um soccer ball condition it on this video of uh dropping ping pong ball and generate this bit another example sound video of this ball bouncing condition it on this drum and uh generate this um corresponding sound one more chopping a vegetable again make this sound like the dra okay so one and then finally one other um project that kind of puts both of these threads together uses 3D constraints and uses analogies together uh involves um sound sound localization so one one thing that we we we notice that or that I guess is well known is that sound conveys changes in camera pose like as you move your head around um the direction of um of this sound source that change both the visual and audio louder in your left or right GE depending on where you're looking and the visual signal will change in a corresponding way and so we wanted to see if we could use this idea to self-supervised uh visual um camera rotation estimation systems and audio localization systems and again this is work for my students Chen and collaboration with sing Shian um part of David fo's group grou and we wanted to again use this idea like if you're staring at um Straight Ahead at a Sound Source like a ringing phone and you rotate your head to the right it's gonna get louder in your left ear so this binaural sound uh in your your head will that you're perceiving will get louder in the left ear and softer in the right ear and then simultaneously there will be this motion the sound will move to the left and so if we can try to use uh use these these simultaneous changes in vision and sound to um to to train models that can solve sound localization and Camera pose estimation and one one way we can kind of learn one way to do this just to learn features using the signals we can train a model to take all these inputs a a prime and B and try to estimate the corresponding binal sound B Prime and again it's like a little analogy a is to a prime as B is to B Prime can you figure out what the corresponding sound and B Prime is given these other inputs it's a self super learning test that kind of forces you to understand the relative pose and from Vis from visual data and the change in the sound direction from audio and likewise you could also kind of just fill in one of the two channels and we we find that you can learn features this way you can of train a um kind of a cross view binarization model of condition on video and uh learn features that were helpful for camera pose estimation and uh sound localization but we also wanted to see if we can use these features for slightly crazier Downstream tasks so what if we just use um these two signals to just jointly train a a model that can estimate sound directions and a model that can estimate camera rotation so we want can we like do this without having any supervision no labels on sound directions or camera rotation we'll just force these two models to agree with one another we'll take we'll train the model in particular such that the estimated visual rotation between a pair of frames s& T is corresponds to the change in the estimated change in audio uh sound Direction the predicted sound Direction at time T and Time s um when you subtract those should correspond to the uh visual change and Camera rotation and you can like then kind of jointly train both these models the sound Direction estimation method and the camera rotation estimation method there's some subtlety here there's like ambiguities you can't tell if this town is in front of you or behind you you have to add some constraints to avoid generate Solutions um but you can you can actually train these models and and we train them in a simulated environment uh from Tong on chin called sound spaces and what's cool is even though it's train and simulation you can General you can apply it to off-the-shelf um real data sets for example you can um take just the camera uh rotation estimation model and plug it into this uh Stanford 2D 3D uh Benchmark and you can see that the model can estimate camera rotation pretty well not as well as like the sift plus ransac uh but U actually pretty well you get like something like a mean error of one degree a rotation error on our Benchmark um and we can also use the sound localization model to solve sound localization benchmarks uh where again it's you know not the state-of-the-art method but it's it's uh surprisingly good on a lot of tasks and we can even apply them both real video we can we recorded this video where we rotated the camera while sound was playing and sound Direction corresponds to the change in the camera rotates sound Direction C rotation all right so just to summarize uh I I I think these um these sensory modalities like uh touch and sound are going to be increasingly useful learning about the physical world and giving you information that' be hard to learn from Vision alone with vision and language which I think makes it really exciting for open world uh settings and the in particular there's two ways that we can learn from these signals uh 3D constraints that allow us to um impute missing observations and sparse signals like touch and we can use this idea of analogies to um do self-supervision and solve a variety of um Downstream tests I want to thank all my collaborators and also thank you yeah if it's too far out it it will probably just ignore it um but it's it's like really incentivized to use use anything it can because during training it always sees similar data but yeah if it's too far out it just fails it I should say there's the one Li limitation there is we could only really deal with these sort of percussive sounds due to the generation method but I think it' be interesting to try that with larger scale you know training and more diverse data I suspect that if you add camera cuts it also might break for the reason it might you might not have examples where it learns to ignore it yeah say uh Works what would happen if you you appli this to like truly deformable because a lot of the examples you showed were rigid or things which don't really visibly change when you touch that's a great question yeah and I think we um I think what's interesting about touch isn't so much the geometry part of it it's the uh the deoration the material properties um that's something you can't really get from like lidar or any depth sensor and and I think that's what makes touch unique um and there's a little bit of that in these as like the grass for example and then in the touch and go data set with where we the undergrads capture everything we do have some of that um uh and but I think that we need to look at a little bit more carefully because I I think I think one of the main issues we're just generating a single touch example here single frame and it doesn't really capture the motion and so I think we really need to apply like video synthesis to this to to Really analyze it uh more carefully I agree we don't we didn't explore that as much for this work as we should have yeah um for the the touch imputation I guess you're you're imputing this sort of raw sensor data yes um but if you know I build my own gel yes sensor presumably it's raw sensor data will look much different than what you're doing here so what what's the right way to kind of measure I think it's a great question because the touch signal sensors are really very holistic yeah yeah I think it's it's a little bit frustrating that touch is so uh uh so diverse like it's not like microphones where they're pretty much the same or other other lar where it's like somewhat unified so um I think there's a few different ways to address this we have a paper on Friday led by uh Fred Yang who um uh in collaboration with Alex Wong at Yale that's about learning embeddings uh by just taking all of the vision touch data from all the different sensors we could find on the internet and just learning um a single embedding for all those and I I suspect that that sort of approach might be one way to address this you just collect a lot of touch data and there's enough similarities in it as a human that you can kind of tell what's going on maybe a a computer will also eventually learn that even though One sensor is gel s sites another is digits you know different sensor it learns what's important there um another answer that that we're exploring is uh translating between touch sensors and it's it it looks as though you might be able to um just take the touch sensors two different sensors and predict one from the other but I do think that it's like an issue that that is open problem in this field yeah so my question what do you think is like the most natural Way Forward like scale of this touch and vision data build a multi Foundation model for touch and just collection yeah that's a really good question I think I think I think there's a lot of different approaches one that I didn't talk about is synthetic data people often generate touch but I think that's it's really hard to model like the micro geometry and for the deformation I don't think there's very actually very good uh physical models there uh it's actually a very difficult thing to simulate um so that but I think that is like a possible source of getting at least some data that you could like pre-train a model with um the other source I do think that just like the hiring a bunch of people to go out and gra and collect sensors is actually a pretty good way to acquire a data set that you might be able to then adapt to other things but the but later I think the ultimate approach would be like to actually have a robot um uh collecting the data but so I kind of view that this work as a way to just kind of get start get started with pre-training the same way you might have a a foundation model with vision language is be equ with vision touch I'll be with much less data uh than than than the entire internet but um it's just a way to kind of get started ultimately for robotics thank you yeah I'm M make sure volume I have my video turn up post no start your video there we go that's good thanks sh take this and maybe minimize it or put it somewhere on the photo the videos oh yeah there we go okay uh don't know how do I make a full screen here like that but uh do I have it my presenter scen no um wait how I show my present you know how to do that that's a little rough though uh proba this is a way to go this is probably okay okay all right uh our last session before we break to lunch is uh is a session by who's going to be talking about challenge we have little Challenge on instin protection um which uh which we we'll hear more about so please take away all right thank you uh hi everyone my name is Jan and it's my great pleasure to present our challenge uh instead uh object instant detection Challenge on behalf the organizing committee including Chen Y and also Shu and uh all right without further Ado let's get started uh so first uh what's instead and why we care about this problem so uh instead uh is uh basically try to detecting a very specific instance uh from a 2D image in the format of a bounding box and the instant detection is actually one of the fundamental capability of humans for example in the airport like when the passenger is trying to find their luggage from 100 similar like instance and they'll be able to quickly and accurately identify which one is their luggages even though they probably under heavy occlusions or maybe under different lighting conditions or maybe have slightly different poles from what they see at their homes and uh people mostly able to quickly identify them and that's one of the fundamental capabilities and uh in this case we try to study in that and to bring this like capability into machines and also Foster like the research into other directions so in one line summary basically instead uh instead will be try to locate in The Wanted object at distance and the instant detection is also being uh significantly important in the field of Robotics uh for example in a elderly assist robot if they want to fetch some like object from the kitchens and the they need to First be able to locate the object instance at distance before they make any following operations for example the pass planning or when they get close enough they do the grasping so instead is actually one the very first step before the robot takes any moves so that's why we believe instead is a very important problem um so yeah um so here we present the protocols uh for the instant detections uh so instant detections like uh normally they take the two type of input one will be the the testing image uh commonly known as like the scene image or the some people also call them query images and they also take a bunch of like uh uh qu profile image or so-called reference basically specify the visual appearance of the object instance of Interest so for example here uh the object instance is this is provided as object Centric multiv view images with a Q plac on the on the tabletop and by feeding both of these two parts into the instant detection models and we can see the model outputs the three bounding boxes which is like represent like where's the location of the these instance in the 2D images basically the chip the mouse and also this dinosaur so that's the general protocol for the instant detection problem uh so comparing to the well studied object detection uh in the literature so it's very CL instant detection is very closely related to the object detection but it comes in with a fundamental differences so object detection aims to detect all object that belong to some predefined classes so I think like in the very well studied or like very like popular in the previous years autonomous driving like applications some of the most common classes will be some like vehicles or pedestrians or maybe cyclist while instant detections actually try to detect a very specific object instance and they Define by some visual examples uh in this case each of the instance will be treated as a separate class for example uh taking the coffee beans as one of the example if we want to detect the coffee beans in the object detection then all the three coffee beans showing on the top should be detected while if we try to detect the coffee bean in the instant detection then we first want to specify which uh instance or which like coffee bean back we want to detect obviously there is three right and only one of them we want to detect and that's the fundamental difference between the object detection and the instant detection so in the past object instant detection problem mostly developed in the Clos world uh by seeing that basically uh we feeding the uh the profile image of the of the instance and also the some like uh background image randomly sample the uh and uh ma majorly follow the cacer learn uh method uh basically what cacer learn do is that they cut the foreground of the object instance and then try to paste them onto the random uh backgrounds and then because like the since like the code actually controls like where the C like the foreground ping onto the background so this actually produce the fre annotations and then by doing this uh the mod uh this like a strategy is able to produce or synthesize a large amount of training data and then following this uh the pipeline will train a Detector by treating each of the instance as a different class and then uh with with the fre Comon free annotations uh that's the so-call the closer World setup uh basically the by train after the Train the detectors uh they will try to detect these like predefined uh object instance and then try to detect with with the incoming quy image during the testing time uh however we want to point out the by sampling this random background it is already a open world uh uh uh strategy in terms that since this randomly sample background uh is coming from the internet and uh so uh it could be any images like uh during the uh data synthesize process therefore we believe uh we want to just point out developing so the previous CAC learn strategy is one of the insufficiently way to use the uh background image in the open world so right now we want to ADV uh Advocate developing instant detection problem in the open world so basically not only we want to utilize the open world uh sample background images we want to also uh Leverage The open world trained Foundation models so as we can see both of the background image and also the models are learned on to the uh open world data which is like at the internet scale and the these Foundation models are generally trained for the general purposes and we adapt these Foundation models as a specific like model for the instant detection task and U so these are adaptive Foundation models we will be used for the testing and we still follow the same protocols at the testing time we'll be feeding these profile imag to specify the object instance for interest and uh of course we'll feed the scene images and together these adapted Foundation model will output the bounding box of the object instance of Interest so on the by the way this is also the general pipeline for the uh challenge winner this year so um um then let's dive deep into the challenge um so beginning with the challenge data set so our challenge data set come with 100 object instance and the each them come with 24 profile images and they capture with object Centric uh uh way manner basically we capture at every 15 degrees in atus and around 45 degrees in the elevation View and uh we also place the QR code on the tabletop this is like for the uh potentially falling like to computer camera Poe uh with the com map for example with comap and these profile images come with a high resolution basically around 3,000 by 3,000 pixel resolution and uh for our testing scenes we come with a diverse like different indoor scenarios and we have uh 160 validation images and 320 testing images and the all these like testing image are extremely high resolution basically around 6K by 8K in pixel resolution and they cover diverse scenarios and uh also they come with like very challenging cases for example many of the inst Maybe heavily occluded or maybe they are extremely cluttered for example in the kitchen scenario on the bottom right you can see like many of the instance are cled together and the model need to be able to detect each one of them in this very clut scene and uh also in the maybe the top left one as we can see like for many of the instance they are extremely small and also far and they are partially included by the shelf and these are all the challenging cases that the previous data set doesn't come with and uh some dos and don'ts for the for our challenge uh basically we try to Advocate doing the challenge in the open world so we basically allow the challenge participant to pre-train on anything uh basically we encourage like them to uh pre-rain on the open world data or create any of their own training data set and uh basically leveraging any like powerful Foundation models to for the purpose of the instant detections except they are not allowed to train on our provided validation and testing images and other than this they are allowed to do anything to basically detect all these instance we also provide 200 indoor background images which we randomly sample from the background that shares a little bit similar layout or office side Lings uh to these validation and testing images uh a little bit comparison against the existing data set so in the past most of the instant detection models are mainly developed on the GMU data set or ABD data set as we can see they only come with 23 or 33 inance which is not that many uh in terms of the morning scales and their resolution also a little bit lower uh compar in comparison our uh data set come with 100 instance which are features these multiv view object centry views and it covers 14 or more diversity indoor scenes covering for example office like kitchens and different diverse indoor scenes and it's also publicly available um and we believe this is a a better um data set right now to Foster the problem to develop the problem of the instant detection in the open world uh in terms of the challenge uh evaluation matric we following the uh the spirit of the Coco data set adopting the average proced akap and they are over different IOU threshold with 0.5 to .95 with a step size like 05 and similar for the average recall AK AR uh we also tag the scenes with easy and hard depends on their clutter level and also the occlusion levels uh this is kind of like a uh based on the human like uh judgment for example if we see the object instance in the scenes are mostly verse small and uh they may be mostly heavily included then we just manually tack them as the hard scenes and if the object are sparse and they are fully visible and also big enough then we just attack them as a easy SC of course also uh uh evaluate them across different scales uh this is uh determined by the by the size of the object instance shown in the in the testing images uh by plotting the statistics of the data data set we determine the areas which is less than 200 pixel squares will be determined at small and between 200 to 400 will be medium and over 400 will be determined as the large and uh so this is uh we follow the same like uh um labels for the AR and uh okay now it's time for the exciting leaderboards so uh this year we have uh 133 team and the 30 submissions and many of them many of them are being very competitive um so uh we talked to uh to we have just basically in depth conversation with all these team and they try to summarize their methods and also uh collecting what type of data they are using and we summarize these methods and uh um also results and presented in the following slides uh so before we uh goes into the sophisticated like approach developed by the challenge participant we want to begin the M reviews from the old days which is the very beginning of like how people do the instant detection before uh this deep learning eror so in in the past most of people uh do this like instant detection problem for uh by using this keypoint matching based approach as you can see uh given some profile image and they compute these like uh descriptors for example like C and then they try to match these key points or doing some template matchings uh to detect the object instance in the in the images and uh of course there we also want to mention the the the strong strong Baseline which is the cace alarn which we have already mentioned before and here is like a general pipeline for how c p learn Works uh given uh a bunch of like uh visual uh description or profile image of each of the object instance of course I cut it like already cut it with a foreground and by randomly sample some uh background images like for example this case will be indoor scene and uh the uh c p learn strategy just to try to randomly paste these object instance onto the um onto the background image without quite following any physical uh surfaces uh but you actually Tred to synthesize very uh many diverse appearance try to simulate the um the potential diverse changes in the testing cases uh even though there looks this like approach looks a little bit naive but this has been proven to be very effective and this is the very strong Baseline and has been dominated for years until the very recent before the emerging um Foundation models like shows up um so some remarks in terms like the C learn strategy first generates these fre bounding boxes uh so one of the major um challenge of adapting the major uh object detection models uh for the instant detection will be lack of large scale um annotated data set while CAC learn enable us to synthesize almost infinite amount of numbers uh training data to train the detector Side by treating each of the instance at a separate class and yeah as we said before you can generate almost infinite numbers and synthesize any appearance with this side from this uh profile image maybe we can just by rotate or like U uh by uh yeah by 2D rotation uh to synthesize a little bit different appearances uh as we pointed before it's insufficient insufficiently use of the open world background images uh since you only randomly sample some background image uh open world background image but without leveraging the uh other open world models and open world data uh so nowadays we leverage open world Foundation models like to approach the instant detection uh in the of course open world and uh so let me walk through the pipeline here uh the very first step will be the proposal generation uh this step we leverage the S which is a while the foundation model it can basically detect exhaust detect all object instance uh in the uh in the scene as you can show that's one of the Illustrated figures like uh showing on the bottom left uh so Sam here basically serve as a purpose of the uh just the foreground instant detection and they just need to generate exhaust generate all the instance they see in the images and uh uh in the Second Step the all the uh all the generated proposal from Sam will be uh will be fited into another uh Foundation model which is dino2 this is self-supervised internet scale uh models which will be attracted very meaningful features uh for all the proposal as you can see that's like all the proposals from this like testing image and uh by extracting the uh features of the profile images and also the proposals uh we have a collection of the instant features and also the proposal features uh next we try to do a coside similarity matching between the uh proposal um features and also the uh the profile features and uh for each of the proposal we'll try to uh Rank and select the most uh similar looking um instance and then be basically assign that as the final prediction of the that instance so as you can see eventually uh we use the stable matching and uh so that uh uh that a small dock yellow dock or like that yellow object in the middle will be uh in this case will be uh matched to the to the green uh object on showing on the right even though it's WR but it does show how this like matching step here is done uh some remarks in terms like this like S Plus dyo we2 features uh it first explore the open world model uh the Sam and Dino wi2 these two models are both trained with open world data and it's on the internet scales uh uh but uh it's one thing to not is that this is a non-learned method so both Sam and dino2 we can see the snowflake signal over there it seems that these models are frozen and N of them the waste is not being tked so this non learn meod actually achieved the new state VR approach and the Sam Actually achieved a very high recall basically it just means that you can uh detect almost all the object instance but we just might not be able to match it correctly and the D we2 is also a powerful representation uh motivated from this like many of the challenge participants see the high recall from s so they are thinking about finding the dyo we2 to achieve like more accurate uh feature matching step and uh that's actually what the uh leading uh participant uh do in the in the leaderboard so uh basically they try to learn some like feature representations and uh from the experimental results it shows like a little bit learning to adapt these Foundation models uh for the specific task of the instant detection actually make it uh uh actually improve the performance and uh we have seen submissions that try to fine tuning the dino W2s with a cross entropies basically for each of the these instance they try to treat it as a as a separate class and they do the cross entropy for the classification and we also see some teams using metric learning uh the intution behind this will be they just try to naturally grouping the uh profile image belong to same instance uh closer in the feature space and uh we' seen like many teams that try to find you different uh top layers of the Dino wi2 and the uh in the just like the key takeaway in this uh slide is uh a little bit of learning uh for the foundation models or like Foundation model adaptation uh is actually very helpful in terms of approaching the instant detection in the open world uh next line of strategy is try to augment the profile images in both the training and the testing so in terms of the training will be uh just as uh as the previous slide shows uh by augmenting a little bit more um profile images and they could still following the cross entropy or metric learning to fune the weight of the dyo V2 and in terms of adopting these strategies during the testing then uh most of the team try to adopt this follow still following the previous state of VR which is S Plus D V2 basically using these additional profile images for the for the matching so these like additional U profile image I provide more diverse so they expected to getting from these additional profile images they try to capture more diverse features and to achieve like more accurate matching uh uh results so of course the first uh the first live thought we see will be using standard data augmentation uh here we see mostly will be some like f and transformation something like steel rotations and or like even some standard crop and uh um so another line of s we have seen will be using comic uh so the idea behind the com mix will be try to blending two uh the patches from two object and merging them into a new object and uh this is try to hallucinate some different uh visual appearances um and try to augment the profile images we have also seen people using Nerf to render novel views uh to augment uh uh the profile images this a pretty natural consider we already plac a QR code on the people estimating the Cod map and by perturbing the camera post they're able to render some novel views uh some quantitative results like by using learnable features we have seen uh right now uh most the team still uh developing these in the Clos world uh performing the best and uh by by using some like open world approaches that is performing the second uh and then the with the Baseline here will be the non-learned method which using S Plus dino2 uh but we have seen some like more robustness coming from by using the uh the open world models which we will be showing in the visualization later uh this results actually highlights the potential of like developing the models in the open world and we're expecting to see more sophisticate methods come in and uh eventually we expect that the open world will be uh our performing or performing the best uh a bit quantitative results on the augmented line of approaches uh we have seen right now the center uh transformation performing the best uh so our uh our we believe the standard transformation actually is done on the Fly and uh they're able to syze very diverse views in both training and testing uh so that's why they are performing The Best of course uh Nerf render novel views they actually being very high quality and also very realis looking but they are kind of like come with a fixed views so basically we see submissions that they generate a fixed number of views and this view will be fixed in the training and the testing so the model might be overfitted to these views so that's why they perform the second and uh among all these approaches C Mix performed the worst and uh this is probably uh not very surprising considered the quality of the generated uh images so it might be too trivial to finding a better m with a c mix but uh it is maybe Worth to investigate using C miix in the future space so right now we have seen most of these are do are done in the RGB levels which is a bit trivial Maybe by bringing this strategy into the feature space we might see some more like performance Improvement and here are some visualizations and the the uh the yellow box showing on the left will be the uh the ground shoes and of course the number there will be show the instant detect the instance index and the the blue shade is just representing the predictions and uh now we have seen like uh the closer World model perform the best in this case like the S Plus D2 and the open world model both making some predictions while the uh closer World models actually uh accurately detect all the instance in this in this images uh in this cycle we write ation we have seen uh on the right most bottom we can see open world model actually find this uh object that is uh uh that has been making mistake on some on some closer World models which is like this uh of course on on the average the closer World model performed the best but in many cases we have seen these challenging uh uh instance are being detected robustly by the open world model which is still highlighting the potential of like uh modeling the problem in the open world uh of course the non-learned approach uh they they just using the fixed ways so in comparing to these like fine tuning or using additional like profile image they perform the worst um but still like these like are already way better than the previous method using the um C alarn which is like developing the models in the closer world so uh from this perspect perspective we still encourage like or we expect to see to see more methods uh trying to develop the uh trying to approach the instant detection in the open world uh so looking to the Futures uh we have a few like suggested like future directions first we'll be try to better uh exploiting the open world data so uh right now we have we don't see many of the challenge participants trying to collect a large scale of the open world data and the none of them actually uh try to do uh try to use like synthetic data in the open world uh in the large amount scales and uh so we feel this could be one of the very promising directions if we could carefully curate a a data set um so that's one promising direction we think about and the second interesting thing to discuss will be uh the novel instant detection this is one of the future Focus as well so right now uh for the instant detection uh all the object instance we try to detect at testing time are all available for the model to train during the training time so basically uh the the collection of the instance uh during training and testing are all the same uh while we have seen uh some related work focus on the novel instance detection and in that case uh the object instance trying to detect at testing time are not available at training so basic basically we train a model and at testing time we provide some profile images and some testing images the model should be able to handle these unseen object instance and be still be able to detect this object instance uh at testing time so that's still one of the interesting problem and it could be go parallel with our instant detection problem and uh lastly uh it will be interesting to uh explore some uh efficiency in Computing so right now while the problem with these Foundation model is that they could run very slow while most the instant detection application could be on robotics which they are actually require some real time uh processing time uh it might be interesting to try to uh explore some knowledge distillation or trying to explore some lightweight Foundation models to make the model run faster and uh you will be also interesting to explore some like adapters uh between the uh Foundation models and Al their output to make the uh to make the model runs more efficiently so these are all the three suggested directions we uh summarize uh for the future research and uh that'll be all thank you so much for listening so so yeah feel free to ask any questions like uh if you guys have all right cool okay thank you so much thank you uh um is uh it's on the calendar because this is recording in progress well you then in that case you can turn this off do I have to stay connect through audio uh no okay otherwise there'll be an ech so we can check the present sure hopefully this works beautiful okay I think we're good to go we will get started again yeah so Yan here will introduce you start okay sounds good thanks good um do you mind not recording yeah yes um so the meeting is being recorded maybe we just try to turn off the recording option is it possible for me stop recording neur symbolic uh kind of assistance uh another part is as with this endent is VM and after building this uh AI ststem of course we want to benchmarking to make sure it's robust and it's also can act in open world can also reasoning so we also have this all a lot of like benchmarking works and finally want to also try to combine with like previous shon's work so whether our as assistance can be to this imported sessions that is imported assist uh of course this work collaborated with amazing students in MML okay so you can imagine there's scenarios that is uh you are going to a campaign and you are maybe wearing a smart BL and you want to try to finding a seat right so you can ask like so many people can you find a seat then this assistant may try to localize some positions for you it also wants to end understand relationships right so who is your friend and maybe giveway to another like pedestrian over there right so your seat is mar green but please give away to approaching person first so to accomplish this kind of task you have to first pass the scene and also understand both the visual entities and relationships inside so that's our uh actually previous Works lot of works on this uh P up Sy graph so building Sy graph from image step image to videos and fin to this 40 this 3 vide because now there is a lot of sensors they have this like dep sensing so you can get a lot of like Point Cloud as inut so uh of course we first want to uh Advocate this Beyond object recognition so what is inside the image I think is already well solved right so currently the detection mation and also all the basic Vision task are considered solof so we have like two persons two bench tree and pavement but what happened in the image right so beside the what we also want to understand like well and how right so maybe the story is like the woman man P looking at each other the woman is sitting on the bench on the left the man is sitting on the bench on the right they in front of menes so this kind of capability is still lagging behind for C models even for the very powerful Vision models they don't have a very good sense of these visual relationships and also the special combinations out there so here we want to Advocate test that tries to give one image as input then we want to Output the S graph so the S graph I think is uh pretty much different from the original s graph because we want to uh grounded the whole uh symbolic things on the masks instead of a box right the Bond box are still cost it's not very good for a lot of manipulation test such as in robotics or in AI you actually want these pixel level and standings so we have like a person the bench sitting on looking at in front of right so we want to this all these spal relations out there and using the same graph we can also of QA so actually this test for this data set have helped us PVE the way to tr more powerful Vision L model out there for example we can asking what happened image while is the man sitting on what is man and woman doing right so you can see that's a typical questions for even for the GB gbd 4B right so we can generate training data as well as the benchmarks from this symbolic approach that using the Sy graph so a formal definition is given an image which very complex since our output is a Sy graph as well as the phobic segments we also tries to ground it all the visual entities and relations to the segments out here of course here we want to uh kind of enable a lot of properties that is previously not a CH like accurate grounding proper class grity and also able to invol background because you know the off detection only TS about objects they're not talking about backgrounds or stuff AIO and I think has already been uh open sourc a lot of people have used that you can also try yourself if you interested we have now much more than this 50k and I think the object classes has also increased so at that time we have tried like two approach so why is the two stage so F through the segment then we do the sing gra prediction on all the tokens from the segments right so each of the regions we can have certain tokens out there then we try to pull the features do the relations right so it's pretty good it's easy to use it's also support a lot of classic methods if you have your own favorite segmentation models you can use that but it very heavily rely on this secondment model or the Detectors of course another way is more like this one stage approach so here uh we uh kind of try to train the whole model and Trend this focus on vision and it's direct training but I think downsiders need a long time to to learn whole pipeline because everything is uh tokenized and just use the Transformers to predict both like the predictions for mask prediction for the class and prediction for the relations out here so it's also may have like the conflict with current pipeline of this pan so combining these two we have uh this another transform uh Transformer uh Network so it's good we have both the advantages of the uh direct modeling as well as quick convergence because you can leverage some of the pain features but the time limit I will not uh for details here so here we can actually run some like life demos if you are interested you can do the registration out there or you like listening to some workshops you can try to run our uh kind of demo that well man with a fancy back standing with very in photos something that so after that I think now there's more of people they are like interested in these videos especially for the long videos right so you want to understand what's happening in the video and also tries to ground elements inside video to do the tracking along long time as well as the relations out there if there's very complex videos involving multiple persons M objects as well as very complex things so that's why we uh try to starty this uh tbsg that is called Panic video uh sing graph so I think the program is like this so given one mon video so can quite long me level long then we'll try to predict both the mass this P mask as well as the relation so one thing to know is here the relation is dynamic instead of static right because maybe from frame one to frame 10 person a is kind of in from person B but after FR 10 10 it goes to behind right so they tries to predict this all the dynamic relations out there so it's actually a very Ching task because you have to do these groundings pixel level groundings on the videos as well as as this Dynamic relation predictions so uh that's why we propose this called Dynamics in graph so I think it can be very useful for two uh sub subfields so one is definitely this uh kind of import AI so when the robot is working outside if you can understand word in this way it has more understanding of the relations and context information another I think is uh on this uh more like VN so we are train like gbd4 right if you want to also generate of qls so this is a very good way it can help you not just understanding the static space information but also this dxing so you can see it's actually ding graph so the relation changes across time we also there are some new relations uh will happens afterwards so to enable all these like applications uh have mentioned so we actually build this data set so you have very long videos and our also we contain both entric motions and also third person contain both and for the uh different scenarios and for the uh kind of multiple perspectives we have included uh inside so you can use this one for any task from low level middle level pation to like captioning QA to the kind of embod AI right so here is uh one of the examples here you can see there's a boy right is kind of receiving giving and andw wrapping the this on holiday right so there are some dance scription of the earth that is from spr zero to 18 that the little boy is passed through the television and to pick cut G right so that is very important comp actions also contains all the spine gr relations so maybe you can also ask some questions so why did the little boy give the gift to the woman right so you can also try case to the gp4 O something that so current vrm actually is not quite good at the symbolic reasoning so the answer is actually it might be a gift exchange moment and the gift is for the woman right so you have to understand the interpersonal relations and also the rationale behind that that is reasoning across across the long of course for this Ecentric we're building upon this x kitchen and eal for the so this one is very useful for like the import AI uh patients so we also make a life demo here you can see this one so that is we actually implemented on some of the first person class so when we go home and tries to also processing around this it can provide this Frame level and op compation for the long video you can see when you open this refrigerator it tries to part all the things out there and all relations you can have your tables photos and tries to predict the kind of action relations at one holding something and something's on something and he's doing something else like like try to clean in the uh that one and open another part so I think it's quite valuable beyond the test that is the symbolic test and we can also do some QA so what we are the man do next right say try to do the action anticipation and also you can do other things right comp both third person view and first person Vios of course the model I don't want to emphasiz more so it's uh much more like what have been built but here the features is a tube instead of just a token on St regions because we are operating on the videos right if it's a long video you actually want to do the check in and also uh make the agregate features across the two so stage two is tries to predict the spal temper relations across time so of course we have some demo RS it's also online open source if you're interested you can check out and try to play on your own own videos so after that uh noce uh we not just have molecular videos we also have 3D sensors everywhere right so that's why we try to adding this uh pbsg this P graph with videos with 3D then we got this P PSD 4D that is we want to par inside this this 40 word so I think the concept is just that we have some visual input from this 40 is dynamic word it can be Co Cloud it can be like BS can be anything else but contain this three informations so you try to aggregate this information to Parts into this 40 anding graph so you not just have the positions X Y you also have the Z there so that's you know like how far is the object right so it's that very important for the robotics so after that you also try to figure out the relations what the what is the different object they are interacting with each other what the person doing so what you should do next so that we can accomplish some of tasks right so you can also do the reason and planning AIO so of course we uh have included like two part the first is more like stive set so building on GTA so that contains like six seven videos and 35 different classes for three relations and we also build another real world data set is called H H4 so pipeline we actually included like two different op options so option one if your camer is rgbd then you can use this rgd rgbd based method and if your sensor is outputting point is also okay so we try to agregate the two different uh input modalities together of course the major thing here is also to do the tracking there's also very active research here that is how to track within the 4D word right so after tracking we try to inference and also learn the relations out there so here we also have imprint on very cute robots out there to help us to clean up the the graph so we find that it's also very accurate if you just Implement on some like very cute robots so it's first try to segment SC then tries to pick up and also can also have some interactions with the VM right it can be integrated with the gbt so the gbt has the relation have the reasoning and this one can help the gbt with some intermediate steps so give your gbt with some relations with some the paring rounding s right so after we building this kind of AI system with SE graph so we thinking of we are not just want this symbolic things right neuros symbolic things we also want to goes to just LM props ENT based approach so we want to go from this mod input to singra to L based method so that is the second part is AI assist with vrms so actually we tries to follow the same path the uh domains that is from this language models to language assistance we can see from tb2 tb3 to 3.54 so they actually following the same path you first have the capability of zero short learning then you will have want to add in some Inc Learning so that is very important and finally you want to align with the human structure so that's why you want this ability of instruction following and alignments so in V we should also do the same right so like two or three years ago everyone is working on clip so clip is good because is zero shot right zero shot learning and then we have like Flamingo models or lava models so they are doing this incon learning so however they still not a very good assistant because not tries to very friendly follow instructions so that's why we want to build something that can do the instruction following is auor model so at that time we are building about flamingos of course we have doing some upgrades uh up that so flaming models I think everyone is very familiar so very quickly go through so you can do the instruction in context learning given some uh in text PS you can also do the inference compilation you can also operate on videos so that's a good thing because can operate on image mod image and also the videos it's a very unified models so even though it's a very good uh model for zero shot but it's not a good model for assistance because it's not trying to follow instructions we training objectives just to complete the next reasonable sentence right it's training this way so it's just a pre model instead of instruction following models so that's why we want to build something more than this mmc4 so there's a lot of people building on very fine green accurate data set know is for uh this m models pre-training and we argue that Beyond this we also want something that is have this capability of instruction tuning right that's why we have Bu The M that so for this one you can see we tries to uh find a clever way to automatically finding this interlift uh datas right from PR tun to instruction tuns of course the model itself I think is more or less the same to what we have seen for this mle models we have Vision modules have language modules we have some res Samplers MLPs connect together so I think the major part Ence is on this data sets so it tries to enable both the perception reasoning and plannings out there so here I will give some examples of course we also try to include this MTI lingal sense this comprises of eight languages English Chinese Korean Japanese German French Spanish and Arabic right we Tres to be more inclusive this contains lot TV captions out there we can also en this visual storytelling and even do the planning so this thing you can see can you suggest any recreational Pursuit that can be enjoyed within fol right so it's definitely certain degrees of reasoning out here and also is there enough space in the room to set up a home gr St so you have to also have some this spatial understandings out there and for this one we also have this Ecentric be standing so this is also important it's safe to work while the woman is cleaning like you should have some common sense to to do the reg a and of course it's some capabilties a there you can try our demos it's all open source and I think we also doing some like benchmarking so you know it's very hard to uh to keep the to on The Benchmark every day there's new models comes out we have the gbd4 the judge also evaluate on some objective benchmarks the ad bench something that that time it's good it's good so after that we also build another model it's called autd because if you wants really accurate grounding but really um fine green reasonings you have to see the image with every details right so if the current like vrm can only support like 256 by 250 as inputs that's definitely something that can't can't be recovered because when you do the pooling when do this kind of embedding you already lost lot of informations so here actually our Al HD is building from this F based models so it tries to operate originally on this highr visual inputs so we don't do any of the pullings any of the encodings out there we just use the row visual inputs so that every thing that's is uh of the information is is inside and the good thing is also supports flexible input sites so test time you don't have to use the same size of the image dur a training you can use it like flexible image inference it also eliminates the need for vision encoder right that's a good thing is encoder free approach so find that this kind of Auto HD is very good as uh kind of the tables or the fine green figures you can see this whole approach is very simple it's a simplified version of the F you just fit your input patch using a very simple linear projections to do the Transformers then your outputs we find that in this way you can also do this V like it so we also have some benchmarks next five Benchmark to test where you can see all the timy things inside image right so sometimes you can some 10 timy things as relationships out there so of course for this one I would like to skip you can like spot all the numbers or you can see all the text it has some OCR C out there and this year we are collaborating with the lava team so we are trying to combine the strength of Al HD to this lava so we have proposed lava L I think the major lessons we learn is of course the stronger LM can supercharge the mod capabilities in the well so we just extend our previous lava next to the three times larger so now we have two models one is 72 billions another is 110 billions so you can see it's still following the scaling law on the on the gra right so on this MML score when model is larger we find this constant achieving better performance it's still falling this on this lock cell is still still St right so we find that under the same training strategies stronger L with larger model size larger uh training data bring stronger Mar modality performance so good thing is that our lav X now already on certain metric certain metric it reaches it gp4 level M model performance right I think it's one of the most powerful open source model now this so our largest model this lav 110 billion models it's actually can be trained on this 100 uh gpus for 18 hours so we also open source all the training scripts also some of the configurations so you can try to uh reproduce yourself so here is some the results out there of course I will try to skip it can be very fine green captions with no hallucination so you can also understand this multilingual right uh like just write the Chinese po to to this image you can see it's it's kind of kind of G kind of G it's also understanding this uh basic of the P from this m mingal perspective so here is our link we also have set of online demos you can try you can use your own photos to convert to some code to understand it to get some caption that's that's definitely good so of course if we have a very powerful image models next steps always have a very powerful video models right so op already goes from gb4 to gb40 so now we also want want like video model that can have the same or similar capability that's our project for L next videos I think the major lessons are just two FS so first is just using the current pre-rain image image level models can really achieve strong performance on video level the second is that the suitable or efficient representations in the video is just a bunch of patch across time so you can try to think of our input is just a bunch of mar resolution like Mar aspiral patches across times that can gather then fit into the the Transformers right so you can see here is one examples we use the Sora Dem videos to to do this caption in you can see the results are actually pretty good so we also Pro another version that is using the using the dpos so it's a kind of re reward tuning so we have the DPO you can see uh the content they are more or less similar but it's more uh relates to our human human perceptions out there and here is the benchmarking results so you can see it's currently the most powerful video models noway so this one is the video lava video next so first models they are more like 30 40 this kind of accuracies so we push it to 66 something that now here is the gbd4 G4 V so they are just 70 so we still have like 10 percent of the Gap but we still now is the the most powerful video model now since our group is also working on generate models so we find that is a very good tool to generate all the fine GRE captions for like video generation am generations for 3D Generations out there okay so after talking about the this AI assistant with the V this entn assistance next I will briefly talk about our efforts to benchmarking it right so I think uh we propose an effort it's called Ms so everyone have faced a problem so every day there's new bench marks comes out new models come out there are some numbers on the papers so you can't reproduce it you can't compare with them right so how do you can do a very efficient comparisons across like 20 to 30 benchmarks so that's our initial motivations we want to solve the problem for you so if you just install our tour box you can run the evation with just one L code so can try to uh align all the performances across the 20 30 benchmarks and help you get results compar with all the pur models both open source models and close Source models right so you can see now this we can we we have to compare across large different benchmarks like in Flamingo right so they compare to this 20 different benchmarks now this so sometimes even for the evaluation itself it will take two days because you have to run evaluations on lots of benchmarks lot of samples so here we give you a unified interface so here is the something we have included we have both the Image level Benchmark video level Benchmark like EOS schema activity net then we have this Evolution tool case right so you can choose your Tas which kind Benchmark you want you choose the models then you also tries to uh select the natur wantes then with just one code we are trying to align all them for you we also provide some uh n tools to give you a sense of uh what which part of of this Benchmark that is model is very good ads or it's very bad ads so that you can debu your models to finding shortcomings right so it has a lot of this lock tools for TBT for TS you can try out so now this we they have like a supports uh 50 plus image image test and 10 plus video test and also include right include like 10 set of Arts Mar models inside so if you have interest you are more than welcome to contribute right so open source project and just PR and your models or your data sets will be inide and all this the others this kind of uh kind of like video data sets they are focusing on more like the uh very ordinary things like the action recognition or like the anticipations but there are very profound things that is the kind of how humans uh tries to understand humor right so that's why we propose dat fate so we find is very important to understanding the fun part of the videos like this one right so if our humans find is very funny so how will the machines feel about that right so like the Q is like localize this musing part of the video so why do you think the video is is very very kind of a com right and for this one is the same right so humans find is very surprising and how the machines for this Mar models will feel about that so it provides a lot ofq PS to to help you to evaluate so it have U three part humor QA creative QA match QA and how the kind of Mark models understand the Magics for this part is just the magic right so how will this kind of AI perceive all the magic tricks of ofs so I think I will just skip this part and here is overall like the uh kind of the uh distributions across different benchmarks and we find that current V is not very good so they are not understanding our human humor out there so they are kind of tries to understand the funny part or the counterintuitive part of the videos you can see we have turn with authors or lava lava videos or even like gbt models so you can see the results here so they are not quite so current models only like achieve like 20 to 40% right there still a long way to go for vrm to truly understand in this conter intuitive and surprising videos so another part very important part of the video is this about word knowledge right so everyone is talking about word models so what is word models so word models is tries to understand kind of uh different aspects like the common sense like the toour use like the conceptual information sare so we here we provide the Benchmark for us to understand all the work knowledge inside this V whether your VM has some something inside so it proposed something called reason steps so I think all of you have heard about the channel SS right so you have a long chain then this means this test involves much more buiness so we find that our Benchmark is four to five steps of the chain of s to solve compared to other like exting C test only one or two steps so this one actually need very deep thoughts about the word knowledge inside so here's distributions I think I just show the video demos of this data set so have a very intuitive sense so the question is where the lady go when she was absent from the video so you can see she's making coffee read news papers and goes to the clock she look at clock and she realize she go then she picked up her suit then she go then how do you answer this where did the lady go when she was absent from the videos right this thing is very in for hum but for V we find this very sh very hard because it have to agrad information from the visual domains from frame one frame 100 and also some of other cues like why she look at the clock and why she kind of put up Pursuit right something like that so you have to understanding both the actions the reasonings also the song So of this data sets containing the song trck so your model should be Mark enough to take both visual input and audio input together right so I think finally your model has to aggregate all these information scatter you should have some common sense you should understand the tour use you should also have this multi model associations out there so you can try yourself it's also open source and finally is the unsolvable program detection so that is a lot of people tries to trick the current VR right so using something that is uh definitely not uh kind of uh uh should be answered for example for this one so given this document kind of image that is what color is a CL right this definitely a irant question but current vrms will try to answer because you have asked questions you also give ABC like gbt the answer or a or b or c something that for example lava we answer I will choose a but the right answer is actually your question and your input they are irrelevant you should not answer questions right so that's we want to uh equip the current VM with the abilities to reject some of the questions right so you should have a sense whether the question is sensible or not so we also have this three different types so absent answer detection incompatible answer detection also is in incompatible visual question detections so I think the observation is C model is not good at so they are always tries to answer from GP 4B to Lava series to G to like to others so they are still very weak so current model don't have this mechanisms to handle unreasonable inputs or out of distribution inut so that's a very important lessons for us and du time limit I very quickly goes through the F part that is inol a system so essentially we try to uh combining the VMS we build with the kind of the Sy graph spal uh perceptions passing to import environments that is we want to go from building multimodel AI system to building embod multimodel AI system so I think for this one we just SK of course we try to generate some data dat we try to op we have some like reinforc learning inside so it's just like the reinforce learning from Human feedback but here our concept is reinforc learning from environment feedback because when you do an AI what we are facing is a dynamic changing world right so that's the open word so how can you learn from this open word we just use this uh Arrow based method to updating some of the parameters in the in the VR so for this one we give some of demos so we are trying to generate executable code code out here so he actually want to gra the path then all models tries to generate code the code will tries to complete test inside the gbd inside the this one the input is a visual input the video and output is actually an executable code the code can be executed inside this environment so if it is a import environment or it's a game engine we can just follow the code you can do all these actions okay so for more details we can yeah you can just look at this uh this one yeah there our demos and project page I think that's all for my talk and thank you yeah oh I think I I should have time for take one or two questions yeah okay so what do you think bottom is it because the training data does not social medas or it's because the architecture of the models and what do you think are the ways to improve to go beyond okay I think that's a good question uh I think the reason comes from like three different FS so first it def from the pin data so current P data don't don't have this long chain reasoning so it's more like one or two steps so that's Laing and the other side is on the architecture side so current architecture lava series or others they just have a very simple like bridge between this visual input and the language input I think that's it's not enough to have a very deeper understanding of the visual inputs and the third is I think it's on the post training side so a lot of models you know during post training they using RL HF or something like that to improve or bpos so that part I think still need some improvement to enable the truly reasoning abilities now currently this post training only devels very fluent like this incorrects but no reason is inside thank you okay thank you let's go for my e e e e e e e e e e e e e yeah still Yes slide it's right right be good yeah yeah they can hear the sound now okay okay okay I'm sorry about that yeah all right so cool yeah thanks so as we know um generative AI has emerged as a new wave following the discriminative AI the community has developed various type of powerful gener models such as large language models uh diffusion models neuron reading field and video generated models to name a few they are able to create very impressive text code images 3D things and even vales but creating a law isn't the automate goal what we need is to understand and take actions in the work through the generation process so what is missing when integrating generative AI in the real world which is inherently open and dynamic first our word is 3D plus time only synthesizing 2D images is not enough the generation should be rotating 3D and even 4D more importantly we need to bridge generative models and discriminative models so that they cooperate with each other and improve the perception in the open world going Beyond mirror generation this alone is insufficient for navigating the complexity of the real world we aim to push the boundary of Genting models empowering them to address desired decision making and problem solving task effectively function as autonomous agent timately considering the variety of generative models available ranging from large language models to visual gen models they potentially encode complimentary aspect of the underlying work World it will be exciting to compose all these General models to create a somewhat a comprehensive super model enabling us to address even more complex tasks that were prly deemed unattainable in this talk I will discuss some of our group's effort in Pursuit Of This research objective unifying Genting and discriminative model as well as their diverse capabilities and the different environments to this end we first need to advance gen models to perform High FAL generation in the 4D physical world the first step is to live the successful 2D image generation models like diffusion model to synthesize objects or s in 3D and 4D not only do we want to precisely Rec construct the 3D object for six we would also like to manipulate the general models to addit the objects or things through language based instruction as here in the welln instruct Nerf to Nerf however unlike 2D image generation the fundamental difficulty here is a lack of 3D training that means so far we still don't have strong generalizable 3D diffusion model as Illustrated here current 3D diffusion can only generate simple 3D object from specific classes so the widely adopted solution is to somehow distill the 3D generation or acing signal from pre- generalizable 2D models like 2D diffusion models such as using score designation sampling proposing dream Fusion the basic pipeline such as in instr Nerf to Nerf is formulated as follows so Nerf is the first trend on the original images to reconstruct the object in 3D and then a 2d diffusion model like instruct PS to PS is used to addit images in 2D with a language based instruction and the Nerf is tuned to reconstruct the object in from this generated images now we can render the added objects in while Nerf tries to produce sty consistent Reconstruction from the given images unfortunately the 2D diffusion model cannot provide multi view consistent images during the editing process for Nerf to operate this leads to fied adding result in 3D and becomes one of the major challenges of this widely adopted solution so we have developed a series of work to overcome is this issue for example our Von Nerf generates 3D consistent editing or sell transfer result across a wide range of s and instructions we significantly improved multiv view consistency AC controllability efficiency our considered dreamer further enhances the 3D awareness and 3D consistency achieving High Fidelity high resolution editing or style transfer in complicated since particular in large scale indoor cats we significally improved sharpness and fineen texture our key Insight underlying this approaches is to not solely rely on Nerf but explicitly propagate acting information across different views so as to ensure multiview consistency so here I will mainly talk about one approach consist dreamer which introduces three synergistic strategies to explicit propagate the adding information across the different views structur noise surrounding views and consistent enforce in training with this strategies we designed the framework of consist streamer to achieve 3D consistence in 2D fion intuitively consistent adding result should be also Den nois from consistent Noise We therefore con construct the structure Noise by putting noise on the S so that we can render St consistent noise in map so in this example we can observe that the noise also moving together with the object so the structure noise set up the foundation of the consistent adding result inable and consistent adding from beginning to the end only structure noise is insufficient we also observe that like contact in some views lead to unstable or even abnormal editing result as Illustrated here so to this end we construct surrounding views by strring a lar Central View with we use them as the input to the 2D diffusion with the sufficient context implicit reasen by the self attention modeles in 2D diffusion model we now achieve consistent and desired ad result we further propose a multi-gpu framework to addit and surrounding views in parallel on NG gpus so we apply self supervisor training on the diffusion model by constructing the target through warping and outraging all the something and Views which are used to self supervise the diffusion model we also similarly self-supervised the predicted noise at each Den noising step to ensure a consistent Den noising procedure let's start from consistent noise and ends with consistent add images so with the framework composing all these strategies our con dreamer achieved High Fidelity scene Ting result compared with the Baseline in shock Nerf to Nerf our consist dreamer generates much sharper result with higher instruction Fidelity in a variety of tasks so for example in the adding results of the top left which is the F our consider dreamer consistently generally result with cleaner more detailed texture than instruct Nerf to Nerf and in The indorsing Ting result at the bottom the thing is a bit more complicated large scale s so the Baseline instruct nerve to nerve actually fs and generate very result while our cons dreamer add result has bright color and Shining appearance as show so we also conduct an abolition study or four abolition tasks to quantify the Fidelity of the disting from 2D to 3D we propose a normal matric disting Fidelity score DFS which is defined as FID between the diffusion generated images and the rendered images from add sys so a lower DFS indicate better sing consistency and improved adding result so we can see removing any of these three components result in Blurred result and the significant increase in the this indicate that all these three synergistic strategies are crucial to our high quality uh editing results our most recent instruct ner uh instruct 40 to 40 work further extend this success from 3D to 40 achieving style or object specific Ting of challenging 4D Dynamic SC so in addition to style editing we also would like to addit the 3D Shape of the S our neural work integrates the explicit representation of Point Cloud into the implicit Sy representation of Nerf therefore achieving ship editing by directly manipulating the underlying Point Cloud so our approach achieve high quality result inv both ship information and more to process so we have introduced geometry awareness into the general models when extending them into the 4D World another important property is the gered model should be also aware of physics when integrated into the physical world while this can be achieved let's say by considering physics simulator our oral work shows a simp strategy in the context of the diffusion models for the task of 3D human object interaction synthesize and prediction so given the pass sequences of human object interaction as unrated by Green mches our approach predicts the plausible future interaction sequence as illustrated by the color mes so here the physics means that there should be no penetration between the synthesized the human body and object and their content should be modeled reasonably okay so far we have discussed how we advance the capability of generic models in the 4D work so they are geometry aware and physics aware now we consider how these gen models can facilitate discriminative tasks this REM this remains an open question in the community so here I will briefly show some example as you may Imagine One straightforward strategy is to treat a gener model as a data engine and use the generated data for data argumentation this strategy has been show great success in simple task like uh image classification object detection so here is an illustration from little more complicated task open work Hier object detection so we can see the successful segmented object together we uh together with their con part sub parts our work further demonstrate the effectivess of these strategies in dealing with a more challenging pixel level dance prediction tasks for which the model need to estimate semantic labels depths surface normal Edge and key points so we achieve this by extending either nerve or diffusion model to synthesize not only RGB inies but simultaneous also s surface Shing p and key points these are crucial for comprehensive 3D understanding so experimental result in this board shows that our synthesized data greatly boost the performance on top of a state of art baselines for various dense uh pixel level prediction tasks so our recent work iton de moves St fter and shows that synthetic data are also helpful for more challenge reinforcement learning tasks so we introduce a diffusion model to generate full senstive trajectory based on the initial State and the T as Illustrated in this figure given the initial State and the task turn on the light our synthesizer gener the corresponding um trajectory up turning the light intuitively we can decouple ourl method into two parts the replay buffer and the learning algorithm so we modify the original replay buffer with ion diff so you our modified replay buffer we store the original sample in one replay buffer and we store the synthesized sample in another replay buffer and we random get samples from both buffers since our it only affects the replay buffer without the need for changing the r learning algorithm so our approach can be applied to or ourl me with a replay buff so experimental result validate that the generated trajectory significantly and consistently accelerate a variety of online RL me right so the right plot here actually shows noticiable performance by integrating H with a variety of RM Rd and the left figure further shows the strength of our method especially in a little bit more complicated environment where R PD actually feels in way but our approach with cized data substantially improve the performance okay so we have show the effess of using the visual J models like diffusion models to synthesize additional useful data and to facilitate the learning of the discriminative uh discriminative learn if you think a little bit further this actually indicate that the Genting models have in fact learned useful feature representations therefore an even simpler approach emerges where gen models are directly used to extract pre-learned representation without the need for additional data synthesiz procedure we investigate this in the context of textual video diffusion models and find that they provide a semantics aware and temporally consistent representation that can be used for a wide range of Downstream CL including V object Discovery post acation and video objects so again here the most interesting observation is we can directly use this kind of representation with very light decoder to deal with this complicated radio perception tasks while it is intutive to assume a visual Genera model would like to offer valuable representation for visual perception past our recent work devs into a more interesting exploration can linguistic Gena models like LMS provide useful representation for visual perception since the emergency of LMS they are mainly used as encoders for language or decoders for tokenized output on the other hand due to their training on massive data om encodes a vas amount of knowledge capturing high level and Abstract understanding of the world so our intuition is that the knowledge within our M can directly benefit the representation learning of another modality for example refining and enhancing the representation of images between Cloud time series data along this line we discover that a single Frozen Transformer from LM Can Be an Effective coder layer for visual task our approach is quite simple directly appending a Transformer later after the find import layers of any visual backb because the mismatch of dimensionality we introduce additional um I two two linear layers we can see the approach is basically as simple as it despite this Simplicity this Paradigm is actually gen generalizable across a wide range of modalities including 2D images 3D points radi and non semantic time series and be effective across a wide range of task like image classification Point Cloud classification abute recognition U and motion for when combined with language modality it is also useful for 2D and 3D uh Vision language task by default here we use a l Transformer layer from uh llama as for our design choices a crucial analysis is that we first remove the LM Transformer while keeping the same number of trainable parameters which is basically um plus MLP as sh so the result shows that the performance Improvement is not as large as using Frozen pre-train uh LM block so this observed obser observed Improvement for visual encoding is not just a result of a larger Capac capability or large capacity but more like The Leverage of this additional LM we further analyze if fine-tuning the Frozen Transformer could lead to better result so so this is basically U show us L from this result we can see that fine tuning full fine tuning actually leads to a performance job because of over fitting to the training set um so in the following experiment we mainly use this Frozen uh um layers we further found that our Discovery is applicable to various LMS and layers for example here we use another LM opt we can observe that uh again there is kind of a consistent Improvement not just the last layer of llama can improve the performance but also the last few layers from this show that this kind of behavior is somewhat generic so from the experiment you can also observe that often the highest performance Improvement comes from the last of few layers of but the beginning few layers you can say it's actually not so this is um reasonable from um kind of inunity perspectives the first few layers are basically converting words to EMB bandings while the later layers conduct high level recognition and Rising which might be the best beneficial to the other tasks so addition consider different skills opts and um um with their final Transformer blocks so as show in this table we can see that the Improvement of LM for visual encoder only emerges with LM with sufficient lale so now we would like to ask this question right why pre-rain Transformers in RM have such an effect on visual recognition task what is the underlying mechanism our idea is this kind of information filtering hypothesis which is derived by our observation of the feature activations actually so we can say from this kind of feature visualization it explains how a frozen LM Transformer actually improve the representation ability on visual tasks specifically we visualize the the feature activation either in magnitude or frequency surprisingly the model trained with a frozen RM Transformer show the emerging Tendencies of segmenting the forground objects which is not observed in the original EV for example you can say right so the bird is basically clear show on the activation in E frequency or magnetude domain so this is somewhat surprising because this kind of segmentation ability was only demonstrated on self-supervised viit like Dino before so encouraged by this Behavior we formulate our hypothesis as LM benefit from distinguishing and amplifying the informative tokens so uh as a sign note here we are not totally clear about the which layer actually is LM attention or FM or we are not clear is most notable in the magnitude or the frequency this needs some further exploration uh but what we show as a consistent behavior is that the magnitude after output by the final layer clear Illustrated the target objects again this phenomena is only is not only limited to classification so other task like action recognition right so this kind of information filtering hypothesis also supported and observed so here we also visualize the tokens um actual um action recognition so different from the uh image task so video tokens are activated jointed in all the frames so every uh video token is a cube covering two frames and image patches of 16 by 16 so as show in the low Str show row right so the model with llama Focus to the forr with less noise and furthermore the High stret shows that the most Focus frame to indicate the temporal selection of the um temporal selection of the models so after ending llama we can see the model significantly introduce a larger activation on the frames with actual manipulation movement or the hand object interaction we also consider another task um so this is basically um a even more challenging task a 3D Vision language task for situation of QA where an embodied agent is given position and then question to answer so in this example the situation is basically I'm sitting I'm standing in front of the table and facing trash can and the agent has to understand its location and facing like the the Green Arrow and the question is how many uh chaires are at the table behind me so what we visualize here is basically this kind of activation uh of the SE point from the point Cloud where a brighter color indicates actually cor color indicates a larger acation so we can clear observe that ending Lama enables a sharper activation especially for locations uh behind the behind the agent which is consistent with the the situation okay so I have shown the remarkable effess of utilizing generative models such as LM to perceive the word not only the linguistic word but also the visual word however this alone is insufficient to address the complexity of the real world recently there has been a growing interest in extending generative models particular our MS um as agent for decision making and problem solving as I discussed earlier right so this is promising for two key reasons so first LM can perceive the work through natural language and with vision language model it can also integrate image-based perception and more importantly LM acts with common sense and high level task comprehension so which is difficult for traditional control Sur and um our agent particularly in task transfer scenarios and with highly complex task to achieve such operational agent several essential capability are required such as internal reasoning and external decision making or acting planning there are existing me that have focus on specific capability but constraining to specific tasks so for example as a foundation to many followup work CH of s cot is introduced for reasing which guides basically LM to think a step by step and provide a justification for the answer so this means a unified framework that is simly integrates all these capabilities is essential and much needed for Effective problem solving so our recent work language agent research large fulfills this need at the first Me by unifying reasoning acting and the planning into a general prompting framework based on the molor research and therefore I'll perform prior Master From Any individual domains so how we integrate all this capability into one system well to achieve this we build upon the tree of thought to which addresses Planning by combining class research algorithms with RM so we improve upon with the four step and here we use an example of buying an apple as illustration the first step is prospective evaluation which is kind of a smart initialization position of value functions suppose I want to buy an apple right so I want to um make an my our agent to buy a Fuji apple on Amazon given the XM file of the website given such website the LM agent has many possible choices for example extract the keyword and Adder Apple in the search box or enter Fuji in the search box or even simply lock out as it feels that it shouldn't buy Apple here if we use a search algorithm such as BFS uh DFS of vanilla M CTS we will have to evaluate each Cho many times before we figure out which action is good which is very inefficient so here if we larage the common sense encoding the prein LM which is the knowledge that traditional planning algorithm always dreamed for so we can actually let LM to perform prospective evaluation and to figure out which action might be the most promising this will save many CHS for us the second step is basic heris selection where we sample multiple next step and select the most promising one with MCTS um rules right so we select such a step by generally considering the novelty and LM evaluative value function of the candidate next step and the thir St is feedback update so if we s we succeed on the first trial we can simply end the search and output on however our agent can make mistakes from time to time right for example it might mix apple pencils with real apples and purchase a wrong item so to learn from such failure cases we use a feedback update that updates the value function along the pass trajectory so the last step is self-reflection where we fully leverage lm's comprehension ability to let itself generate instruction on how to do better for the next TR so for example it might figure out right it shouldn't mix up Apple electronics with real apples so which will go to the next Tri as part of PR to sum up in large right so an external environment enhances both reasoning and planning by grounding it to the real work condition this also reduces some kind of hallucination and reasoning improve acting by organizing feedback and context and planning as a search overacting trajectory introduces Global trajectories and Global decision makings so notably our approach is applicable to a lot of tasks for both reasoning and decision making this is benefited from the share the state space of action thoughts and observation so from this um different T including listening programming question answering and the real world but navigation we can see there is a consistent performance Improvement um from Lots making it as a leading approach in ourm agent learn okay so so far we have mainly focused on the exploration of individual models each a powerful tomb in its own however the potential of gener models not lie in just isolation but in their collab collaborative Synergy so can we compose and synergize heterogeneous generative models like LMS and other gen models while each model illuminate different aspect of the world we consider this Synergy of generative modeling in the context of generating human object interaction hoi given text description by Fusion of lm's motion gen models and interaction price so here basically LM understand the semantics of human object interaction motion models understand the Dynamics of the physical movement and additional interaction prior like physics can be also integrated by Fusion all them together this enable us to bridge the gap between actual comprehension and the physical movement so as to allow us to gener handle 3D human object interaction particularly in a zero sh matter so more specifically our task is defined as given a task desri uh description of human object interaction we want to generate this kind of interaction sequence as shown here that aligns with the texal uh guidance so but this task pH is several challenges so the first is the complex property of object including various shapes as well as diverse motion pattern subject to internal physical characteristic objects and also the intertwining of the Dynamics of the both whole body and the the object and their shipes so for example the body needs to fit on the surface of the object during the interaction and driving the object to move but these are not the main difficulty here and not the main FAL issues here The crucial thing is that we only have a limited amount of 3D H hoi data and we don't even have any paired actual description together with this kind of hoi data so this leads out to question whether it is feasible to learn P condition hoi generation in a zero shot bner without a paent supervised data to address this challenge right so our key insight as discussed earlier is to leverage a variety of knowledge through gener models not dependent on tax interaction par data and importantly such knowledge can be individually is Ultimate compos due to that semantics and Dynamics can be decoupled in Hoi that is semantics can be shifted by human motion and the initial object post meanwhile dynamics that in subsequent Behavior object is driven by forces applied by the human under the law of the physics so the semantics of interaction can be harnessed from LM right so which is with a prr text to motion model but such large models lack crucial lowlevel uh Dynamics so we address this by developing word model that predicts the subsequent state of an object affected by the interaction so more specifically we propose inter dreamer formulated as an m and DP we basically token tokenize human and object Dynamics into actions and States under such an mmdp our framework start with high level planning which basically given the text description deciphers a detailed text as show in this example right we refer to the answers provided bym uh for extracting object categories and body parts and uh um we also use that to provide a mod defined description so it sort of narrow down the V range of possible interactions into a more manageable distribution within capability of our framework so we utilize fot prompting and the chain of assault prompting to enhance the Inc context learning capability of the RM so the lowlevel control aims to create the initial State and the sequence of human actions such that they correspond with the information provided from the high level planning and here the um interaction retrieval is basically set up an initial object State based on the rules we discussed so the key to the retrieval is information provided by the high level planning in the previous step and the output is basically a contact map that can lead an object um of its initial post so we then have this kind of text to motion model which is prein to generate the actions based on the refr Tex and finally with the initial State and it ative generated action from the lowlevel control so the word model or the dynamic model simulate State trans transitions affected by this kind of Applied actions so um a naive me would be directly taking the wrong action right as a condition which suffers from this kind of severe or fitting issue generation issue right so during um uh during inference so the dynamic model is like to encounter some kind of actions that don't exist in the training data so we basically model the interaction from the context vertices on the object this kind this kind of locality um ensure that the Dynamics models focus on only the interactions in the contact region without being distracted by the motion of body parts that are not relevant irrelevant to the object and the uh most important thing so the dynamic model is the only component that need to be trained in our framework all other components are just used as of the Sha models so we conduct a quantitative comparison of our approach with the existing uh text to hoi work with the framework of supervisor learning again we I would like to emphasize our is complete unsupervised zero shot manner so this comparison is a sort of unfair so we directly evaluate um uh our Master you um by testing our Trend models on their annotated data available from their website specifically retrieving their generated uh videos for direct comparisons so remarkably right so even with training on this kind of data set our approach generates high quality interactions and more impressive our approach is able to generate viid interaction sequences given this even longer and more complex text descriptions and effective in synthesizing complex interactions involving Dynamic changing objects um Dynamic changing objects Dynamic changing content such as handle okay so as a summary in this talk I have shown how we bridge and unify gener models and this discriminative models and further compose heterogenous and S further compose and synergize heterogenous J models to create a somewhat super model so this Integrations in power AI system and agent to advance their comprehensive perception understanding reasoning acting and planning capability within the open Real World potentially in a zero sh again this a very initial step towards a bit long um long-term Vision um that's all thank you for your attention yes please yes so as a as a um you mean the last part of the work right again so here um um uh that's a good question I would like to address from two aspects so first here we want to show that it is possible to learn this generally this kind of plausible interaction sequence without any additional tools for physic simulation or something like that so of course because you don't have this additional guarantee right so the the result is not perfect but again it's kind of comparable and even outperform the existing approach with the supervis data okay that's a main investigation in that line of work meanwhile I agree with you if you want to fully model the physics you probably need some kind of physical simulator you can integrate that we have some ongoing work actually use this physical simulator to guarantee the physics and integrated into this kind of thanks for question yes please okay yeah I think that's a great questions um my again this is a kind of open question right so people are trying to allow these different modalities to align them and generate a stronger representation for different modalities my two sents are first um there's somewhat alignment between this uh representation learn for different modalities even when you don't learn them in the cross model scenarios that's what we show in that kind of um um the the piece of work right so even you train the our with pure language you don't have any access of visual data it can help with the visual task uh yes we we showed uh yes we showed a variety of tasks including um 2D image classification uh 3D Point CL classification action recognition and motion forecasting and even 2D and 3D U Vision language reasoning so across the board right so they have consistent benefit consistent Improvement but the biggest Improvement still comes from let's say semantic task for example image classification uh video recognition um there is some benefit for this kind of motion for C that's a kind of announ T um but the the game is sort of um much less noticeable uh yeah coming back to the question so what what I mean is while this kind of representation alerted for different modality there is still some kind of sh share me S some kind of alignment between that because essentially they are learn from the the same word right same underlying word so there is some kind of alignment that means you can sort of compose that with minimal let's say tweaking or minimal fine tuning you can actually have a a good model but mean meanwhile they also have some kind of U complimentary aspect this is what I show in this kind of the last piece of work with human object interaction generation LM basically understand the sematics but it cannot understand the lowlevel Dynamics and physics then you need this kind of additional model that is good at understand this kind of low-level Dynamics physics but once you compose them like like this kind of high level planning and lowlevel uh control right so you can sort of address task that might be very challenging thank you yes please classification pass so what how Ure the generated content is correct I say also what they false how that yeah so that's a good question and also a slightly different aspect right so what what we show here is basically they are helpful but of course there are successful cases right but there's also some misting and um let's say failure cases my sense is probably we need kind of some additional layer that that sort of tells you um maybe can able to predict this capability in the on right so um if it is helpful you should use that if it is not helpful you probably don't want to use that so you can have a little bit more hybrid strategy to some extent but I I guess the uh the most important is first to understand again I think that's an open open question in the community right so um under what setting this different modality can help with the target modality why it helps right so what we show is this LM can actually help you to identify the desired forground objects right that it helps but um there is a need U we need some additional more in depth and uh exploration analysis under what sment actually can can do so in what setting we cannot do so and then we can probably have a little bit highy strategies to further improve thank you thank youone Che e e I'll introduce myself no worries uh what's the when when does this technically end 4:15 okay half an hour that's plenty of time all right uh good afternoon folks uh today I'll be talking about the foundational fuch shop object detection challenge so my name is Niar Perry I'm the guy on the right the second guy over there uh and uh today we'll be discussing uh the inaugural Challenge and uh highlighting some of the the insights from our community I think the first thing to call out here is that uh uh we've seen remarkable progress in zero shot open vocabulary detection performance and uh by simply typing in uh a text string you're able to detect basically anything that you can imagine so by simply typing the keywords laptop you're able to detect laptops in any arbitrary image uh so this raises a natural question it seems like given this impressive performance it seems like 2D object detection is a solved problem should we be able to use these VMS as automatic annotators uh and can we simply use this to annotate large scale data and uh um uh and be able to um scale up our our vision uh language models and I think the answer as of right now is is not quite so these VMS are not quite there as automatic annotators and I think we need to uh observe this this problem a little bit deeper um and it it really turns out that the the key reason why we can't necessarily use VMS out out of the box is because there's poor concept alignment between uh the data that the VMS are pre-trained on and the target domains that we actually care about here so in this example on our left uh we can take an off the shelf state-of-the-art VM we can type in the Tex string truck and uh you know our VM will detect the entire truck whereas The annotation is just the the front cab similarly for bicycle we find that uh when we type in the Tex string bicycle VMS will only detect the the part of the the um uh the wheels of the bicycle but according to The annotation instructions in new images um this this includes both the bicycle and the and the rider so importantly there's this disconnect between the data which we use to uh pre-train our models and the web scale uh foundational knowledge and what we actually care about in Target domains so importantly the concepts uh for particular data sets are misaligned um and at first glance this this seems like uh it should be pretty easy to solve um and this issue itself is not all that surprising because human annotators themselves need multimodal concept alignment so when we want to uh collect a new data set and when we want to uh annotate uh uh this data set uh we usually need to describe with a few visual examples some rich textual examples and often times uh some examples of what we don't want them to annotate to get these human annotators to understand what we actually mean when we uh Define some of our Target Concepts so uh one natural question is because we want to be able to turn VMS into automatic annotators and we actually align our foundational models to work more like how we align human annotators and the answer is obviously yes so ideally we'd like to be able to fine-tune these models with a few multimodal uh Visual and textual examples um and for those of you familiar with with the the task of f shot object detection what I just described using a few examples of uh uh text and and images sounds a lot like this F sh F shot object detection problem but importantly the existing setup is incredibly limited so the the existing fuch shot object detection setup has a set of Base classes uh so something like uh Coco that you can train on some subset of Coco that you can train on and uh they they evaluate on some other subset of um Coco which they call the novel set of classes uh and importantly there should be no overlap between these two sets um and given that we want to be able to Leverage The the foundational uh uh VMS and uh given their their strong performance uh it seems a little bit strange that we can't even use these VMS in the current Paradigm so foundational VMS violate current fsod benchmarking protocols uh so importantly as I mentioned um the standard setup has the split between Bas and novel classes uh but VMS like clip are trained on private data sets so we don't even know what what it means to be a base class uh how there there's no way to identify whether or not something was uh included in clip pre-training and we also have this this other concept uh called concept leakage which is something which existing fsod benchmarks try incredibly hard to avoid by having disjoint sets between their pre-training and and fine-tuning sets uh and importantly uh for common fsod benchmarks like Coco uh some of these uh novel categories which they want to avoid training on are are things like cars cats and people so these are the things which are novel in Coco but if you really think about it uh these are things which we see all over the Internet so their foundational Vision language models have seen uh many examples of these in practice so concept leakage is uh sort of Impractical and uh maybe even impossible to avoid if we want to use blms for this task uh the other sort of surprising observation here is that zero shot VMS beat state-of-the-art fuse shot object detectors uh so if we think about that statement for a second this is really not all that surprising because zero shot vlms are trained on internet scale data whereas existing few shot object detectors are basically only trained on Coco so a claim here is that if we really want to uh Embrace this uh uh this foundational view of the world pre-training on large scale data will be a key enabler for generalization so importantly if we really want to buy into this notion of foundational F shot object detection we really need to make these foundational models of first class citizen so if we want these foundational VMS to enter the conversation of uh F shot object detection uh we really need to embrace the concept of pre-training and we need to embrace the concept of concept leakage and this really requires that we reframe what we mean when we say base and novel classes so to uh make an analogy to the standard um if you shut object detection setup uh so in the standard setup your base class classes are some subset of Coco but in our proposed setup this is really data in the open world so this is all of the internet pre-training data that you can get your hands on and in the existing setup your set of new classes will be a few shot set of examples uh from some subset of Coco uh but rather than doing this we we propose that the novel set or the the fuch shot classes should be whatever classes you actually care about uh in your target data set so so given this this uh uh problem formulation given the motivation of trying to use foundational VMS to um bridge the gap between human annotation and automatic annotation uh We've set up a challenge called the foundational F shot object detection challenge so first I'll I'll talk through the challenge setup highlight a couple of the baselines which uh we we propose and then uh disc discuss some of the community insights and interestingly uh even in the the few short months which this challenge has been live the community has more than doubled our uh strongest Baseline performance so I think this is in and of itself a very exciting result so first the challenge setup uh unlike the traditional few shot object detection task we repurpose a data set called new images and this is a little bit different from the standard setups which use coco or Pascal this is actually a 2d autonomous vehicle data set and this has many challenging categories like pushable pullable and debris so these are things that just given the textual description it's not quite obvious what we're trying to detect so this further motivates the need for multimodal examples uh and why do we choose new images over some of the more traditional data sets the answer is because new images provides multimodal an annotator instructions so this is unlike many of the the Contemporary data sets we actually know what uh the new images curators uh instructed their human annotators to uh to label whenever they constructed this data set and unfortunately new images is one of the few data sets which we were able to find which includes uh such Rich multimodal annotator instructions so I think you know moving forward given that we find these multimodal instructions are such a key asset for aligning foundational Vision models with uh um with Target domains uh especially in the context of this challenge we hope that future data sets will actually also release these uh so since this is an object detection task we we do evaluate using mean average precision and it's nothing fancy here we simply repurpose Coco style evaluation for the 18 classes within new images and we we outline a couple of boundaries for this uh challenge uh so whenever we talk about found foundational models one of the challenges is understanding what we can and can't train on so the first thing is we allow competitors to pre-train on anything except two data sets one is new images and the other is new scenes because these two data sets are basically the exact test domain which we're trying to evaluate on so it doesn't really make sense to let people pre-train on these and competitors can fine-tune on 10 shot examples uh within the new scenes uh or new images data set so for each of the 18 classes we provide uh competitors with 10 examples per class and this is what uh what they can train on so now that we understand the challenge setup we Define three baselines so first we uh Define a Federated fine-tuning Baseline and this uh comes about from a rather interesting Insight that Fus shot object detection benchmarks are actually Federated data sets and what do I mean when I say Federated data sets uh a Federated data set is a colle a collection of smaller subd dat sets where each image in this smaller subd data set is exhaustively annotated for a single category so here on on on the left we can actually see two examples of although they're a single image these are technically Federated data sets so for the example on the far left with the bus uh within this image all bus examples are exhaustively annotated there's only one bus and uh we don't guarantee that anything else in the image is annotated same thing for the example with the motorcycle so given this example it intuitively may may seem pretty straightforward why standard fine-tuning doesn't work because if we apply standard fine tuning techniques all unannotated classes are considered negatives and this will negatively impact your fine treating performance okay so how do we solve this well we leverage the zero shot capabilities of these foundational VMS to uh generate pseudo labels uh but uh trusting these pseudo labels uh doesn't necess neily get us the the performance that we want because we can't necessarily trust that the VM is aligned to our Target data set so rather than trusting the the predictions themselves we can actually use these predictions to identify what classes are not in the uh Target images so we can run our VM and our VM uh tells us that there are no examples of strollers in these uh in these two examples so when we actually fine tune fine tune our data set we can apply a penalty uh such that our detector doesn't predict that that there are any strollers in this data set so the second Baseline uses multimodal in context prompting so the previous example is a more traditional uh uh detector-based Baseline but it turns out that in recent years there's been tons of work in multimodal prompting so uh here we can we we have a a model called mq Deb which takes as input uh a few shot image queries and some uh textual descriptions so we can given these training image queries and this text description uh we can use the VM which is pre-trained on uh a large scale amount of data to detect this uh this instance of a stroller in a new test image so importantly we don't use any gradient-based finetuning here we're simply uh tokenizing the uh few shot examples from our annotator instructions and using this as context to detect um what we mean when we say stroller in a new data set uh in in a new image and we can actually look at some of the incredible uh performance from uh state-of-the-art multimodal queried uh VMS so given a simple image prompt or a text prompt we can uh learn to we can without any fine tuning detect all examples that are similar to The the query uh with in the image itself so that this sort of highlights the the power of scale and the power of um multimodal query input uh so the third Baseline which we want to highlight is uh using multimodal chat assistant like chat GPT um to improve the the equality of f shot object detection so one key Insight is that whenever we uh actually want to uh build a data set uh there's a an iterative process between stakeholders and human annotators um when constructing the annotator guidelines so human annotators often need many rounds of clarifications and iterative feedback to fully grasp the target Concepts and we can even see this within the new images uh guidelines themselves so let me read a portion of this guideline to you uh so new images defines uh if an object has extremities the bounding box should include all the extremities and in parentheses exceptions are the side view mirrors and antennas and vehicles so even in that particular example you can almost see how they're defining what what to do and what not to do and this this probably came about through multiple rounds of iterations where a human an annotator probably annotated the wrong thing at first and uh tried to fix this uh in subsequent iterations so we actually uh construct a baseline using chat GPT 40 uh and the first thing we try is to ask chat GPT given the list of new images classes classify the given image props and output a confidence score and surprisingly it does something reasonable once in a while but more often than not chat gbt will give you highly correct yet incon uh yet incorrect labels uh so another thing which we can try is uh rather than asking chat GPT to solve the classification problem for us uh let's rather try and get chat gbt to give us more creative uh ways of of prompting so using chat GPT in the loop as a black blackbox Optimizer to improve the the quality of our our prompts so given this visual example of a TR of a debris we can ask chat GPT what it thinks it should call these things so for example it tells us maybe we should call it a traffic control sign temporary warning sign road sign or warning boards and given all of these uh riched textual synonyms uh we can actually use these to prompt a VM and we find that this improves uh performance uh so of all the three baselines we find that uh multimodal prompting so the the second Baseline which I which I highlighted performs the best uh but despite the strong performance which we got with our baselines uh we were kind of Blown Away by the response from the community uh so we ran this challenge for just a few few short uh few short months and we had sub over 50 submissions from eight teams and notably the top team beat our strong Baseline which is mq glip by more than double so the top Baseline achieves 45% accuracy uh overall versus our Baseline was just 21.5% uh so unfortunately the top performing team uh did not provide us any details about uh how their method works but even without this we can analyze some of the qualitative and quantitative performance so first we note that uh this top performing model model has provided significant Improvement in performance across both common and rare categories uh so you may be thinking to yourself uh I would have thought car detect detection accuracy would have been really high right because there are tons of examples of cars on the internet but it turns out that because of the specific quirks of the the new images annotation guideline uh for example cars that are not in the drivable area should not be annotated and should not be detected right so being able to specify this through few shot uh finetuning actually significantly improves car performance and as you may expect uh having a few shot examples of these rare and Ambiguously defined classes like pushable pullable and debris also significantly improves performance and we can see this uh play out uh in the qualitative examples so our Baseline incorrectly uh uh predicts that barriers in traffic cones are considered debris uh but through uh the the fine-tuning that this method performs they're able to uh correctly identify the one example of debris in this image and they don't uh incorrectly classify anything else as debris so I think this is this is a pretty cool result uh but uh so I I'd like to also highlight two more methods and uh importantly these we do have details about these two methods so we can kind of dive a little bit deeper into the uh the specifics so the second place team uh achieves 32.5% accuracy and the way in which they do this is a little bit unsurprising so they pre-train on basically every data set known to man uh and the the the large scale pre-training is is one of the key reasons why this method performs significantly better uh than our strong Baseline uh but they they have an interesting approach uh Beyond just pre training on on tons and tons of data so similar to our multimodal chat agent Baseline uh they first uh try to prompt chat GPT with the few shot examples uh and ask for descriptions uh so given this uh this image crop what are the referential descriptions which might be able to describe this particular instance uh so given these uh these referential descriptions uh we can then uh uh prompt a VM with these text descriptions and image prompts uh and match uh the specific random combination of text descriptions that best aligns with the ground truth so what this means is this uh simple heuristic identifies which particular uh ground Truth uh which particular random combination of referential descriptions best matches the ground truth descriptions of the uh um uh with within the data set so once we have the set of referential text descriptions and images we can pseudo label the rest of the data set so importantly uh as as we show showed earlier the new images fot data set is Federated in nature and this means that it's not exhaustively annotated so uh this method uses the uh textual descriptions to generate pseudo labels and given uh the images textual descriptions and pseudo labels we can then fine-tune uh the BLM and specifically this this paper uses grounding dyo as their as their backbone model um so uh as I mentioned the the key to addressing Federated labels or or one of the key ways this paper addresses Federated labels is through pseudo labeling so given an input image here with the uh uh single motorcycle that that's annotated we can generate pseudo labels and feed this back into the VM uh for self-training and this paper finds that this is incredibly effective so qualitatively we find that uh performance on rare categories like barrier is significantly improved and uh their uh one one of the key insights is that uh aligning The Prompt um and importantly aligning the the textual description um to what chat GPT thinks is a better description of this particular image uh significantly improves performance from 11% to 46% so notably in the Baseline you can prompt with just the the word barrier but barrier is very Ambiguously defined um and this uh sort of iterative process finds that single short tarp barrier is a much more descriptive uh way of of uh identifying this particular instance okay so we can look at some some visual examples here and uh you can see that uh mq glip uh which is just uh prompted with barrier identifies that some debris is is uh mislabeled as barrier but uh this method finds that they're able to remove these false positives okay and our uh third team here which we want to highlight uh does something a little bit differently uh so importantly although they they also do pre-train on basically every uh detection data set um they have a very different fine tuning strategy so instead of using uh just positive pseudo labels for uh addressing the Federated nature of foundational F shot uh data sets they use both positive and negative pseudo labels so again they use a off-the-shelf VMS like uh clip and uh um and and and others to uh identify examples um that are likely to be uh within the federative fuchia data set so um the the examples on the left here are things that we know are potential uh object categories that are within the data set because these are the 18 class classes which we care about and we can mine a set of negative pseudo labels by simply querying large text corpuses of uh nouns and using both positive and negative pseudo labels uh we can then fine-tune uh these object detectors uh and importantly differently from the previous method uh this approach uses is prompt tuning so given an input text string called stroller uh we can encode the class name with the text encoder and we can throw away the text encoder uh and we can fine-tune the embedding uh just the embedding to better align with uh the um target class or or the visual example here uh and importantly this uh doesn't require that we have uh interpretable pokin uh so if you take this fine-tuned embedding and look for the nearest neighbors uh you'll find that sometimes these don't really make sense um and this is one of the benefits of using um these implicit uh encodings okay so we can look at some qualitative results and we find that mq glip or Baseline is basically unable to detect emergency vehicles but this Basel this method is able to significantly improve uh detection of emergency vehicles and one hypothesis here is that because we're able to use negative prompts uh so we're able to uh you know train the detector to explicitly identify that the car on the left is not a police car um we we think that this is what uh really contributes to the improved performance okay so a retrospective on on the first uh challenge here is that we outlined some limitations of what data people can and can't train on so although it makes sense that people should not train on new and new scenes when we evaluate on uh the new images foundational object detection challenge does it make sense to allow people to train on other indain data sets other uh autonomous driving data sets and our answer is yes um because in in a foundational world uh in the foundational Paradigm which we which we want uh people to to live in it really doesn't make sense to uh allow people to uh train on some data and not others unless it's absolutely necessary um such that you know we're not training on the test data set so that seems to be the only limitation uh in terms of what people can and can't do um another perspective here is rather than limiting the data that people can train on maybe we should be testing on totally out of distribution data so although many of the pre-training data sets uh like coco contain similar images to Urban scenes so although Coco contains s um you know maybe uh the right answer here is that we should be training on data sets or we should be evaluating on data sets that are totally out of distribution so something like medical images or or aerial photography which is probably unlikely to be captured in standard pre-training data sets uh and yet another uh potential solution is to uh collect uh totally sequester data sets so if I were to go outside and take a picture uh right now I can guarantee that it will have never been in any of the pre-training of these foundational models uh so collecting and curating a data set that will never be released onto the internet may be one way in which we can uh evaluate such foundational models in the future okay so with that I want to highlight a couple of future directions for uh this foundational fsod challenge uh so it there uh our top competitors seem to have two different ways of dealing with alignment so we saw examples of explicit alignment where people can explicitly mine different prompts and and examples uh versus implicit alignment through prompt prompt tuning where we didn't necessarily need a interpretable set of tokens uh so identifying which uh is actually the the right strategy remains to be seen uh there's also uh scope for improvement between textual versus image alignment so a lot of the top methods this year uh focused on improving prompting uh textual prompting uh but there is scope for using generative models uh for example although we don't see many examples of strollers in our data set we can query Dolly 3 uh for examples of strollers and maybe we can use uh use this to augment our training data sets uh as I mentioned previously out of distribution data sets uh is another is another area of of interest and lastly although we really focus on 2D foundational detection in this challenge uh there's a very natural extension to uh 3D as well so with that I'd like to thank the organizers of of this Challenge and in particular I'd like to uh highlight the the efforts of lead author here Anish uh so Shameless plug Anish is applying for PHD programs he's an expert in VMS so if this stuff is something you're working on go talk to him and with that I'd like to uh thank the audience and uh I'm happy to take any questions thank uh who's next yeah yeah so my sens is uh whenever you try and crop um a bounding box and pass it to um foundational model for classification you lose out on a ton of context and I think that context is really what what allows detectors to identify different plasms um so I think that that would be my bet is because mq glip is trained actually as a detector uh it's able to Leverage The Sanic context and do a better job at classification so the the real difference there isn't the foundational level of pre-training but rather sort of the the alignment to the tests mhm yeah absolutely so I think uh the way in which mq glip uh uses the the the few shot examples is through in context pre-training or in context learning so I think this is uh important to distinguish between the um the the interface of the challenge itself and and sort of uh the way in which the the model itself operates so if we think about this as a blackbox the method is still taking his input the training examples and whatever they're doing with it is sort of uh totally something which people can play around with so it turns out that you know if we compare Baseline two with Baseline one Baseline one does more traditional fine tuning but that performs worse so clearly the difference isn't do we do gradient based fine tuning versus in context prompting so I think this is another area of interest for future exploration cool thank you very much yeah can you hear me uh yes but let's wait and just want to make sure that the sharing okay uh guys uh so this is our next challenge uh is about open vocabulary part segmentation um so Professor Wong uh Professor Mo she will present remotely and also we have another student over here to present in person so uh Professor uh would you please check if you can uh if can talk to see if we can hear you please yeah can you share can you see my uh screen sharing yes we can see you uh yes okay okay okay let's go get started um hello everyone um my name is mway and I'm a PhD student from the University of Hong Kong um um um uh this is the presentation of our open cabulary passation challenge I'm your host uh today um first of all I uh thanks to all my co-organizers uh from Shanghai air lab the University of Hong Kong the University of syy and the University of maau okay um in this presentation um I will first um introduce the backround and the motivations of this overpass Challenge and then I will walk you through the challenge set up uh explain our objectives and then I will introduce three outstanding teams with who will share their Innovative approaches okay finally uh I will give a brief um summary and uh discuss some future works okay um uh every day we come across a diverse range of visual and textual content um this open world is full of abundant and constantly involving information uh which presents both challenges and also huge opportunities for artificial intelligence um recently there have been significant improvements in how we can use this open world data to develop large Foundation models with um Advanced pre-training Techniques such as uh the contrastive learning with which we have seen in the groundbreaking models like clip from open AI um this are Foundation models gave us the key to unlock the full potential of um the open world data um allowing us to uh explore New Frontiers in our AI research um so far with these Foundation models the uh the research field of open vocabulary learning has shown um impressive results across a wide range of object Central applications like um object class uh classification um detection segmentation Lang language grounding and so on um but compared to what has been achieved in the object level open world understanding the exploration of power level understanding has not received the equal attention um actually this um detailed level of comprehension is much more practical for real world applications such as in autonomous driving and in robotics um like object affordance learning and object manipulation um we identify two primary challenges in adapting uh uh adapting these Foundation models to a fine grind understanding uh personally they uh these Foundation models have limited par level recognition abilities as they were not uh explicitly trained on on abandoned par level data uh and in the analysis of several state of the art Vision Lun Foundation models we can observe the the there are are consistent struggles to recognize the object parts or fine grind object fences uh which we think uh is resulting from the scarcity of the relevant training data and secondly uh there is the open grity problem uh which uh in Real uh real world applications we usually expect the expect the models to dynamically adjust their granularity to segment Parts at different levels of detail um to fit various scenarios okay uh so we set up this over pass challenge uh our first objective is to learn a model to um handle the parts of um the relevant noal objects without extensive retraining uh for example if we train a model to segment the defined Paths of a cat and the motor should also has um the zero sh gener generalization ability to segment the the corresponding parts of a dog so in the first track of our challenge we first we focus on the generalize D pass segmentation task uh first way provide the Pasco part6 data set uh this data set is uh is improved from the original Pasco VC 201 data set um uh it has um a431 training images and A5 Z testing images covering 116 object par classes across 17 uh uh object classes and way split uh the object part classes into thing and unseen class L uh the same class is um includes the parts of uh 12 objects and uh uh the aning classes contains the part of the remaining five objects um each aning object has has a has a corresponding relevant uh object in the same class split um the models should be trained only on the same classes and will be tested on both the syn and un syn classes and for evaluation we um we C we uh the metric is calculated uh with the main IOU across both the S and un classes and we choose the ammonic M IOU as the primary Matrix also we assume that the Oracle object maskus are known okay um the the second um objective is we want to explore the model's ability to gen generalize across different Parr narratives uh which means the train models can also um handle uh customized user prompt which make require Parts defined in different level of details so in our second track we first we Fus on the cross data set passation task um we also provide um a2k uh uh 24 uh 234 the set which um uh has uh 73 uh 7347 training images and one16 testing images covering uh two three4 object par classes across 44 object classes um but um Although our B our provided basine is trained on this uh data set um we also allow usage of other data set excluding the in domain uh test set from Pasco part 116 uh and um this we make this track we make this track more flexible uh if you if the participants want to use pre-rain Foundation models which already can segment the paths of objects this is also permitted uh but the foundation models should not be trained on the Parco p16 um our Pro our Baseline is um is a fight is uh we F some uh um ways of uh the CP to uh CP paper in CIP and um uh this uh the the evaluation matri of this track is the main IOU across all object classes also we assume Oracle object masks and this is the challenge results uh in in check one we receive uh submissions from a teams and uh uh submissions from Seven teams uh in track two uh some teams uh submit both uh subit results to both tracks and uh we are denied to invite the teams which uh submit the top two results in track one and track two uh for uh sharing their Solutions um the first team is from shidan University China they they won the first place in track one and the place in track two um the the second team is from cast uh South Korea uh they won the second place in both track one and and the track two and the final team is from the University of Hong Kong they won the first place in track two um congratulations for the winning teams and thanks for their outstanding efforts okay the uh for the next part let's move to the team presentations I think I will stop my sharing now hi Shan can you share your screen I will show the recording of our mask can I hear yes okay good afternoon everyone I'm honored to have the opportunity to share with you the methods I use used in the open vocabulary part segmentation challenge sorry sorry hi uh I can I cannot see your screen okay thank you okay okay okay good afternoon everyone I'm honored to have the opportunity to share with you the methods I used in the open vocabulary part segmentation challenge my team is from the school of artificial intelligence zidian University in China and I am team leader shangu I am currently a master's degree student at the school of artificial intelligence cidian University my research interests include machine learning deep learning and multimodal learning team members include shinyang Wang and kexing Jang we were supervised by professors fanglu and Ling Ling Lee the two data sets used during the competition are first described as Pascal part 116 and AD 20K part 234 Pascal part data set is an extension of the Pascal VOC 2010 data set further annotating objects part masks Pascal part 1 16 is a more practical version of Pascal part where some over segmentation parts are merged Pascal part 116 includes a total of 116 object part classes across 20 object classes a2k data set provides open-ended annotations of 847 objects and 1,000 plus parts following the word net hierarchy ad 20K part 234 only keeps the objects which have more than one frequently annotated part over 100 occurrences and the parts which have more than 10 occurrences to reduce noise in the annotations duplicated Parts have been manually merged our team participated in track one and track two in this competition according to the requirements of the competition given a natural image and a set of candidate object Parts described by text labels accurately assigning a label to each pixel in track one we train the OV parts model on the training set of Pascal part 116 and test it on the test set of Pascal part 11 16 in track two we train the OV parts model on the ad20 K 2234 training set and test it on the Pascal part 116 test set our solution to the whole task is shown in figure for the segmentation results of the original Baseline model the results are first visualized and analyzed then pre-processed for the background classes and finally model Fusion is performed according to the segmentation effects of the different models to achieve complimentary details and obtain the final results the key to this task is to exploit the potential of the existing base model in solving OV parts and thus accurately assign a label to each pixel in addition to this the segmentation model should be able to handle classes outside of the train vocabulary at object level and part level during inference therefore we first choose to train the existing model under different configurations to obtain different learning results under different configuration adjustments analyze the details of different models and training results through visualization merge the segmentation results of different objects by further pre-processing the background classes to integrate the results and based on the TR tradeoffs between different segmentation effects further highlight or weaken the details of the fights segmentation effects and their final effect merging realizing the purpose of complimenting the advantages and disadvantages between different models and finally obtaining excellent segmentation results based on the method in the existing Baseline model OV parts for inferential segmentation of the race test set according according to the segmentation results showed that the zero shot setting in the clip SEC model has the best segmentation results so we fine-tuned the model and trained it on the Nvidia GeForce RTX 490 and by using different training configurations we obtained segmentation results of two different result areas the results were tested only 33.9 and 3438 scores did not reach the original segmentation scores in AGG pus analyzed may be due to the existence of the number of graphics cards and batch size and LR differences however the results were further visualized subsequently and the segmentation was better on some of the images so it was also put into use subsequently according to the existing segmentation results of different models and different training parameters of the same model we visualize the different categories separately and find that the background category exists in the same image according to different objects divided into multiple background category annotations so we firstly postprocess the segmentation visualization results and merge the background category annotations of the same image as shown in this figure according to the existing postprocessing segmentation results of different models and the same model with different training parameters we visualize the results for different categories separately and the obtained visualization results are shown in figure and immediately after that we analyze the segmentation advantages and disadvantages of the different models although the segmentation category coverage of zss EGA plus is the highest but due to having a low detail correct rate it will be in part of the categories with the clip egg has moderate segmentation category coverage and high recognition accuracy during category segmentation so we use the category segmentation results of this model as the final category segmentation results for the existing categories the categ model segmentation has a high degree of boundary Clarity and Target Integrity as well as a certain degree of recognition therefore we weigh the existing segmentation results for different needs set four different weights fuse the results and finally add the fused results for different needs to get get the final results we used the above method to get first place in track one and third in track two thanks for your listening oh okay I think um J can you share your screen oh yes okay uh can you see my screen yes yes uh yes okay uh good afternoon everyone I'm Gio Choy from kaist uh this work was done with another co- first contributor son Lee and our teammates first of all thank you for thank you all organizer for this challenge and thank you for inviting us to this presentation we appreciate this opportunity to share our walk uh in this talk I'm going to briefly introduce our team solution for the challenge uh to begin with our team solution achieved second place in both track one and track two of this challenge which are the cross category settings and cross data set settings open voal open vocabulary part segmentation is a challenging task since parts are of often smaller and more diverse which requires a broader spectrum of information and accurate identification to address this challenge the Baseline method clip set has leveraged a vision language model such as such as clip uh despite the progress current state of the art ofps method struggle with generalization of object and often fail to predict less common or smaller part uh these are some failure cases of V parts and clip EG of OB part uh where ship's ears are located inside a dog object and missing smaller parts like tail our approach a to overcome this issue by utilizing object level context with generalized part our method interpret a part as a common structural component across different object categories and also by integrating the context of the object we can achieve more accurate segmentation result for instance if we train our model on cat torso and disjointly learn object level instance cat and part level instance torso the model can better influence the Torso of a unseen object which um by applying the generalized torso information trained with the base categor now this is the overview of of our method uh we use the modified clip architecture to parse object specific part category names into object level and part level component similar to clip set this category name embeddings are then transformed into clip embeddings and combined with the image feature using film we reconstruct the object specific part embedding from this generalized part and object embedding by refining them we can produce final segmentation prediction of object specific part uh our approach ensure comprehensive guidance by leveraging supervision of objects specific part the entire object and generalized part uh the multi-level supervision enhance our model capability to distinguish and generalize various parts leading to improve segmentation accuracy for unseen categories we compareed the mou and harmonic IOU of the proposed method with the provided Baseline clipse in track one and two the proposed method uh part clip set show the significant improvement over the Baseline which is 10.7% and 12.2% impr Improvement respectively uh in addition to quantitative anal uh assessment we also explore qualitative different difference between the two method the visualization displays uh displayed here show segmentation output from both Cliff set and part clip set compared with the ground Truth uh from the image uh you may see that part clip set captures more diverse labels such as dog's ear and dog's eye however it it is noticed that there are some samples where part clipx still miss the parts such as muzzle uh and yes uh there are some more quality qualitative result with object object like cars par CP significantly reduce the misclassification of class label across different parts of of the vehicles resulting in more accurate and consistent segmentation uh in conclusion part clipse SEC is a noble framework designed for open vocabulary part segmentation by leveraging generalized part and object level guidance we address generalization issue and improve segmentation accuracy for unseen Parts our approach uh achieved noticeable Improvement and achiev second place in both TR one and track two of this challenge uh and we actually recently released a preprint on the archive please feel free to refer to this for more information and thank you for your attention and for further information and details please please feel free to reach us by email uh thank you very much okay thank you um thank okay um the 13 uh in hand can you share oh here your screen oh okay H yes I can I can I can see this okay maybe should another one because they cannot see full screen so we first make sorry not not loading correctly so do not share the Microsoft try to share screen okay I just sh yeah okay it is an honor to give a presentation here I'm Newan from the University of Hong Kong I will first introduce our method for OB Parts Challenge and then talk about our further extension to 3D space for 3D part segmentation uh as we know segment anything model is able to produce high quality object masks from input prompts such as points or boxes and it can be used to generate musks for all objects in an image so the first stage of our work is to generate part segment uh part musk proposals using segment anything model for an image in the test site we first crop the object from the image with the musk and then use same musk generator which inputs stce points prompts to sign and then filter fing part musks with confidential value then we will get uh several high quality fun green part musks we then use some postprocessing methods to filter and merge these musks we first remove inviolate musks which have no overl with VI regions or have more than 10% overlap background regions next uh then we remove redent musks if the IOU with any existing musk is greater than 97% the musk is considered redundant next we sort the Musk from C green to fine green assigning overlaping pixels to smaller regions and remove them from larger regions after this each pixel will be assigned a single part index after obtaining part segmentation of um object we further utilize GPT 4 to generate SE semantic labels for each proposed part mask we highlight it with a right Circle and input it into gbtf with text prompts and then we can get the label for that part after performing uh this option on on each part of that object we can obtain the semantic segmentation result of the entire object here we show the final quantitative results on track two which uh we achieve an improved performance by around 9% compared to F model on on data sit other than has coart 116 uh but there's still a performance Gap compared to F change models on Pasco part one6 uh we further extend this method to 3D space for 3D part segmentation uh because 3D part segmentation uh is an important task which is essential for robotics controllable or computational 3D generation and 3D editing we aim to figure out whether can we utilize Sam and multimod to facilitate 3D part segmentation there are three main uh challenges of 3D part segmentation the first one is how to generaliz to open world 3D objects without uh 3D part annotations because collecting 3D part annotations is time consuming and costly we don't have enough 3D part annotations data to train a big 3D part segmentation model second one is how to leverage 3D priors from unlabeled 3D ships because we have large scale unlabeled 3D data sets such as observers observers Excel and Omni Omni object 3D the third challenge is how to define the ambiguity of 3D Parts it is challenging to have a clear definition of parts of an object so how to control the granularity of segmentation is also a challenge to tackle these challenges we propose 3D set segment and part in 3D objects we can segment 3D object Parts at multi granularities with zero short generalization at shown it can also handle complex 3D objects we also try some simple editing applications with our part segmentation results and we will further explore more applications for 3D generation this is the pipeline of 3D side mainly divided into three parts large scale pre trining simple specific F training and semantic quing with multimodal LMS the first step is large scale paining should distill 2D V features to 3D by4 we sample the mches in large scale 3D data site into Point Claws and encode them with the 3D backbone to get 3D features the data set we use is observers and we modify and effectively 3D backbone Point trans V3 as our encoder to adapt to object level we then encode multiv view renderings with dyo V2 to get 2D ver features with rendering depth and Camera poses we can calculate the mapping between 3D Point cloths and 2D pixels so that we can distill 2D video features to 3D by B then for each 3D object we can utilize our pre-train 3D backb to encode pwn claws effectively and only need to train lightweight MPS to generate segmentation or 3D features we use Sam to generate multiscale 2D segmentation musks from multiv view Runing and assign each musk a 3D space scale which is also a control value inputed into the model then we sample pixels on 2D segmentation masks and use a contrastive RWS to train lightweight MLPs then we can get 3D part segmentation results which are multiv consistent and multi-granularity controlled by 3D space control after training we can obtain segmentation a wi 3D features conditioned on a scale value and then use CL clustering algorithm to segment point CLA into several parts and can also get the mesh segmentation result result voted by Pawn claw then for each 3D part we highlight it in the 2D Runner and generate semantic description with multimodal RM for example the multimodal RM generates the description like the right fot or the right sh there are some visualization of multigranularity 3D part segmentation results of our model with smaller scales value input the segmentation results become more funr so we can control the granularity of part segmentation there are more results to demonstrate the performance of our model on different 3D representations including P Claws and measures there are some uh editing applications including part material editing we can change the material of car windows and part shap editing and animation we can segment the chimney and modify its position or Le find rotate and click based hierarchical segmentation by clicking on the object we can obtain the hierarchical segmentation result controlled by the scale we will further explore more applications in different fields our CS will be coming soon at this giab link and thank you for listening thank you okay thank you okay okay thanks for the insights and Innovations from our participant teams uh finally I want to uh summarize two key findings from these submissions uh firstly um as uh object uh provide important context four parts and have much more annotated data than Parts uh I think it's important to use the narn the representations from pre-rain object segmentation models and to effective uh effectively adapt the object models for p segmentation um appropriate fing techniques are also very important and the second thing is uh we can say that um even using the most advanced Foundation models like segment anything model and gb4 there's still a significant performance Gap compared to the supervisor models I think um it's mainly because um that although the V Vision Foundation models like them can provide very accurate and fine Grand masks but um achieving part segmentation with controlled gr Nar um by in incorporating language context is still an an unsolved problem so for future walk I think um bridging bridging this Gap will need more research and development but I still still think it is a very promising direction to further explore and optimize the integration of foundation models with multi multimodel lar language models to achieve more robust and the contest of world pass segmentation and to enhance controlled grarate um I wish to make over Parts more um adaptable to various applications and extensions such as 3 okay thank you I think that's all thank you for joining today's ration for our oi challenge okay thank you Mon and thanks all the team for sharing the results okay all right let's move on to the SE challenge e e e e e oh sure okay over detection is a Cornerstone for wearing real world applications and has been extensively exped for years uh it's gradually uh goes from a CL set fully superise setting to a more apply appliable and challenging open world scenarios and with continuous development object detection data set undergo iterations uh examples including Pasco Coco open images LS and more each designed to meet the our updating requirements however compared to image cation object detection data set have has have a much smaller number of classes for example always has around 1,000 classes in contrast the cation data set with largest vocabulary the bambo data set has a vocabulary that is 100 times larger so the data set with limited classes are insufficient for training and evaluating detectors in the open world so accordingly we propose with three that was vocabulary we show detection data set with three appealing properties the first one is what the vocabulary it contains over 1,000 categories on real world images and the second is hierarchical category organization the was vocabulary of V that is organized by a category tree and the third one is Rich annotations it's uh it is composed of Pressly annotated boxes and category descriptions this table presents a detailed comparison of various data sets notably our data set has much more categories extending others by more than one order of magnitude res that has a suitable amount of images and boxes sufficient to ensure the reliability of experiments without huge training cost furthermore the C the classes of W3 death are organized by a welld designed category tree and this tree offers insights for recognizing uh numerous classes and exploring category relationships we also offer detailed information for each class including class name object Centric example image and description crafted by GT4 and GT4 Vision this Vision language knowledge is crucial for open world perception notably visual prior is crucial for accurate category descriptions for sample with the example image the gp4 is CB Spring as a SE well with the example image gp4 Vision can uh correctly describe it as a mechanical device finally we speed the images into three sets for training validating and the testing and the test site annotations are uh hold out for our with3 that challenge there are some visualizations of annotations in complex SC and funing categories based on our data set we with that challenge inv wies we show perceptional experts to advance the bound R of object detection there are two tracks in our challenge the first track is the was vocabulary object detection which provides an arena for different uh super learning techniques in extremely large langu uh large number of classes and in this track one the model can be trained on the train set of w that and other public academic data sets but the submitted technical report should clarify the use data sets the final performance is evaluated on the test set or wi that with the AP metric uh as in Coco Benchmark and the second track is open vocabulary object detection this track splits all classes into Base Class and noval class detectors are expected to detect objects of both base and Noel classes for the base split the complete annotations are given during the training and for the Noel site only class information is given including a name a class description and an object Centric example image and we prepare uh we pre prepare a subset of TN speed for this OED check and with only Base Class annotations the evaluation is for performed on both base and no classes and the final AP is a with sum of AP base and AP Noel finally we have 20 registed teams and 68 wet submissions those two tables show the entries with performance higher than the provided host Bas lines the top three teams are winners in each track for the track one the first place is the team CZ six uh czm 36 369 from the chinai University and the second place is the team LC M from fan University and the Microsoft research and the third place is the team TCS VT from the University of Science and Technology of China Jan University uh Northwest Northwestern Pol Technical University and Northwest University congratulations and for the track two the first place is team Alim from F University and Microsoft research the second place is the TC SBT from the University of Science and Technology of China Jan University Northwestern Poly Technical University and the West Northwest University and the third place is the team Innovation from chai University congratulations we invite the winners to introduce their Solutions the first uh one is the team Alo from F University and Microsoft research they win the first place in track two and second place in track one welcome okay thanks that's I'm excited to share our solution Rec Dino for for three that and this work is collaborated with and you can tell our solution built upon our PR work reset and achieved the first uh first place in the ovd track and the second place in the supervisor track here is our here is the outline uh it includes a data set overview of our solution and experiment results and the main challenge SL line the main challenges SL in object classification for vocabulary and pro and proposal regression for high resolution image uh to address this challenge we propos our method richo and focon net uh we developed a strong Baseline and extend to open vocabulary detection by aligning the prototype medings for classification the first feature of V three that is its vast vocabulary and specifically it has more than 10 thousand categories with more than which is more than 100 times category in and 10 times cat in a due to this was vocabulary we found many PR training techniques used in such as RFS and the feder laws do not fit well for this R and as show in this T we conduct appliation study on the uh threshold of RFS um but it result only a minor Improvement of even performance drops um uh uh uh we also conduct appliation on F a common use trick of f fed loss but unfortunately it's unfilled in the vory um um another challenge is the B regession of high resolution inputs uh it's challenged to cover dense object and achieve the higher recall rate uh okay um basic this challenge we de developed our framework namely reach D focal net given a classic detector peline they utilize Dino and focal n in our framework and we apply object 365 P training for improved performance uh additionally we propos a simple but effective prototype alignment vensor to convert close set classification into open set classification here we our experiments on on model evolution of a data set using d as a strong detector U the the backbone place an important role in the final performance uh when with s from ret to SW tiny and compar comparable flops and increase the motor size of Swing the performance consistently improved and as the large modor size we replace as one larg with the focal net large resting a further performance Improvement uh finally after scaling up the focal net size to huge uh to to to the huge and introduced object three uh 365 pre training and the model achieved the best performance on L therefore we directly apply the focon KN huge as backbone and object 65 P Rich time d as our detector to convert the tra uh to further convert the traditional clid clation into open side classification we propos a simple but effective alignment based classification method that includes both region language prototypes for computation specifically wecode the given image uh the given uh the given object Cent IM example image to obtain the the visual ining uh we further increase the number of imag by introducing CR the image from the tram side and the whole image from the image uh we include the gp4 way description as a language prototypes and the maintain of Cl side classifier for supervis supervis ass finally we align the object features with this prototype in to obtain classification Logics here is the detail uh with utilize openi clip large for vision encoding and a long clip large for the tax encoding we also find that increase uh we also found that increase the number of prototype image improves the performance uh in our final summation we we utilize prly 100 image per category using only the pro providing the ex example image and IM image for not for the no categories uh here are our final result without soptic training stre we achieves APL 50 53.1 under the supervisor setting with with aoly uh 80% record for the open vocabulary detection um we achieved a noral APO 22.9 and a normal a 15 we use the T TI to to K to analyze our DET detection errors in the supervis second as shown in the figure and the errors mainly stem from the uh object CL object classification test and this further highlight the challenge of classifying object M vocabulary uh okay uh inclusion uh it tried with a strong detector uh a strong detector and strong backbone and extensive for training our rich side D fet achieved that outstanding performance on W dat with a high proposal recall however the M detection error in the classifications of test underscoring the challenges of uh classifying U with a mass vocabulary looking forward uh leveraging class uh L classification data set to enhance uh detection performance especially the object classification for both the open vocabulary detection and the supervisor detection is a larest step they have already explored the P plan and that uh Incorporated inject as data for L longtail detection and this approach should be further developed for this R act and thanks for your listening we will release that the training code and checkpoint in our recent record free feel free to ask me any questions any questions thank you okay sure uh the team CDM 369 recorded a video to explain their solution they win the first place uh in the track one hello everyone my name is z I graduate with a master degree of computer technology from chin University and my advisor is associate professor Jan our first place solution is called mix mixed sud labels based on COD as we all know we three dat significantly increased the number of categories before then then previous data set uh but generally more categories requires more samples per category to ensure performance but we can see the average number of images and annotations for category in recent dat is too slow so it's increase the difficulty to trade a detector on recent dat uh as Coco is a famous Benchmark on detection uh in fact method achieving over 60 M on Coco you use object 365 PR but let's show limited benefits on the W that set onek difference is that the form F turns from 665 to 18 classes well the letter is from 365 to 13,29 classes Anze of data set and exciting method reveal the r three dat LS sufficient sample per category and fing with object 365 that not well with with that detailed category space so Sur surprisingly semi surprise could solve this problem we use R three dat as label the DAT set and object 365 as unlb for sem training on the pre of 365 model the model generates sud labels for object 365 using W 3 D category space enhancing sample availability for each category and Ling it with the pre trade with visual representations uh the method we use is MPL MPL is a universal semi surprise object detection framework so we can choose the cod as a detector which is the S detector on Coco Benchmark but first we we duplicate the code which has already been pred on object 365 into two copies serving as the teacher model and the student model we use the teacher model to predict s labels on weekly augmented unlabeled images and then we appli dra argumentations on the same and label images and mix the images and their label to get mixed Lab images we us this mixed sud label images from object 365 and label the images from dat to trat the student model and then apply the students parameter the tools teacher model so the teacher could be optimiz with student uh in addition because the in addition the cod with s large backbone and the f scale LPN is to huge and semise L also require to calculat labeled and unlabeled loss within a bench so we use gradient accumulation we calculat loss in three iterations for labeled images and mix up images and mic images and we just y frequence to keep consistency with the student gradient update uh let's uh let me explain why the mixp work uh besides the relationship between labels and the samples we can identify TP FP T and iPhone true positive and the false negative uh using a fixed High stretch hold most IP sample can be filled out uh look at the the picture the X exercise represents gradient norm and the Y exercise represents the number of samples we can see the blue is about yellow which means uh mix up could provide more High gr gradient nor more TP samples which is benefit to model uh as for the I sample we can see the yellow is about blue which means the mix up could reduce the number of high gray gradient normal I samples which means reduce the negative effective of missing detection and in addition M provides more box for small objects increasing the number of TP sample for small objects where also reducing the size of Miss detection because the image is small so the M object is also small thereby decreasing the number of I samples uh in visualization we can see there is z uh the the green box means it's being it's detected correctly it represents TV sample we can see stronger augmentation uh results in a higher gradient response and and the mix up could increase the response further uh meanwhile uh we can see the picture C there is a dog behind the raring the Red Box means missing detection so we can see the strong augmentation could reduce the Ione samples uh gradient response and the mix up could reduce the response further at seems that to totally disappeared uh in conclusion we implo the the semi supppr object detection framework M PL and the S detect code to Leverage The complementary of we3 dat and the object file and uh this approach this approach utilize both the detailed cat grade space of R dat and the large scale an T the previous of object SP f for Jo model training and and get a great score over 54.5 M but since we did not utilize the class Tre of w dat and not address the long tailes issue so there is substantial room for performance Improvement and research potential on this task uh we hope our approach could inspire you thank you for listening hello everyone okay and the team C uh TC SVT also recorded a video for to uh explain their solution and they win the third place in the track one and the second place in the track two there's the their video hello everyone I'm very pleased to share our achievement in the challenge of was vocabulary virtual detection at this cvpr Workshop our team achieved third place in the supervis track one and the second place in the OV track 2 I'm boson chai and it is my honor to represent our team to present and share our work in the following presentation I will cover four main aspect first I will provide Bic information briefly introducing our team and background secondly I will explain the significant of this competition and its importance in the field of virtual detection finally I will provide a detailed introduction and analyze our method and the experimental results in track one and track two first let me introduce our team members I'm bosai currently a PhD student at the College of computer science Jang University applicated with the state key lab of bring machine intelligence my research focused on Brin Spar Computing and computer vision p who is currently studying at the department of electronic engineering and information Science University of Science and Technology of China our team also includes several excellent PhD student and boning Wong in this competition Professor Shen provide us with meticulous guidance his expensive experience is crucial to Our Success he's a a social professor at the school of software Northwestern Poly Technical University with research interests in computer vision and remote sensing with the continuous development of the field of computer vision object detection as a crucial component has made a significant progress traditional object detection task usually focus on predefined set of categories such as Pasco VC and Ms Coco data sites although these data sites provide Rich aned data the number and the types of categories are still limited these data sites fail to Encompass the complex and the diverse object categories found in the real world thereby restricting the generalization of detection system in real world scenario the V3 that challenge aims to overcome a limitation of traditional object detection data sites by constructing a large scale data site with over 13,000 C categories this initiative not only better reflects the diversity of objects in the real world but also provides an important experimental platform for the research and development of open vocabulary object detection it is organized using a heroical category tree structure detailing the inclusion relationship between categories the data side includes precisely aned object bonding box in 245,000 images and each category is accomplished by expert description written by human Specialists and chat gbt P one primarily evaluates supervised learning based object detection models trained and tested on the entire V3 that data side consisting of 13,000 204 categories participants can use public available academic data set but are prohibited from using vory that validation and the test sets for training try to divide V three dat into base categories and novel categories requiring detectors to accelerate identify objects from both segments Bas category comes from complete anation information where novel categories provide only C category name description and ex example image of objects the official Benchmark models for this competition are listed in the table below the first two models are based on the mm detection framework while the last two are based on detection to we select the two best performing models for the sub subsequent testing and competition first we conduct various conventional supervised object detection improvements for track one these include adjustments to the model structure such as replacing fpn with PA fpn pass aggregation fpn enhance the interaction and integration all multikill feature by intoducing a pass aggregation mechanism within the feature parameter Network additionally we imple Implement various data mentation strategy to inhance the generalization capability of the model adjusting the loss function was also a key part of optimization efforts particularly through the introduction of generalized focal loss and the distance in IOU loss this no novel loss function better balance the weight between positive and the negative symol when handling object detection task as shown in table two after trying various methods we find that while some improvements seem efficient and they don't achieve the expected results to further enhance the model's performance we turn to the Eva model based on detection to this model is based on clip and impl mask imaged modeling to reconstruct image test aligned visual feature generated by clip achieving strong generalization performance clip outstanding performance STS from its Training Method which integrates Visual and language information enables the model to efficiently understand and infer image content the design of Eva model relies not only on adjustment to its structure but more important to an Innovative Training Method and the effective utilization of between models in construct our attempt to achieve significant in the V3 that challenge by simply modifi and the design model structure are un successful this funding suggest that feature research and Improvement direction should focus more on Innovation training strategy and deep integration of model pin and we submit a Bas Baseline based Eva and achieved a third place on track one as shown in the figure we visualize the distribution of class and simples count in the DAT side revealing an uneven distribution among classes this obviously help explain the efficiency of gfl loss following the requirements of track two from the competition organizers we train our improved K RC on ovd based class data we approach the track two as a supervised object detection task without extensive experiments achieving a second place result for further tasks we plan to employ the large language model to enhance model's ability to learn texture features in a multimodal contest our success can be attribute to the Reg variety of classes and heroic anotations in the Orion V three that decides which provide diverse schematic information coupled with excellent detectors and de sides our model demonstrate considerable generalization capability Implement our model using pator 2.1.0 and conduct our experiments on a system with four h100 gpus using a batch size of 40 eight we use Adam W with a learning rate of 0.001 we sincerly thank the ccnt laboratory at jjan University for their strong support in this competition we appreciate all teammates for their hard work and contribution special thank to Professor Shen for his careful guidance and support throughout the competition the following archive link provide a detailed presentation for our EXP experimental details we hope our experience and fundings can provide some insight for you thank you all for listening hello everyone uh in summary uh first the open vocabulary object detection uh is still far from being installed there we can see here there uh is a significant gap between the supervise performance and the open vocabulary object detection and the winner of we with that track one achieved an aage average position of 154 while the performance in the track two uh for novel classes was only 16 and the second is L both both Visual and language priers is a promising approach for open vocabulary object detection uh I believe that eling the generaliz ability of multimodel Life language model will further Advance this field and and I will thank all organizers in the V3 that challenge we show Perception one learning in our open world workshop and our sponsors Shanghai a laboratory the Chinese University of Hong Kong and Sh Shanghai gelon University and the technical report of winner winning entries is now available on account you can also scan the QR code of we homepage for more information about the challenge and the data set uh thank you for your attention yes it's all right I know it's it's getting late uh but there is just one more thing left to do before we wrap up uh and uh that that is to thank all the people that that made this Workshop possible so uh you know in in just one day we've heard from six of the uh leaders in the field of open World Vision and we've heard from uh some of the four challenges and and the competitors uh and some of the innovative solutions they have to a variety of problems within uh open world perception uh I'd first like to thank all of the speakers uh that uh spent time uh to uh help uh lead the discussion on open world perception uh the goals of this Workshop were pretty simple there are four things we wanted to to accomplish we wanted to connect people identify New Opportunities exchange ideas and understand existing challenges and by all metrics I think we solved all of these these four goals uh there are many organizers involved in in making this happen but I'd specifically like to highlight Shu Kong who unfortunately could not be here uh he is sort of the uh the the the leader of of this Workshop he's been hosting this for four consecutive years uh and this Workshop would not have been possible without him uh more importantly uh we also have a number of challenge organizers and there are simply too many to to highlight individually but I think this should show you that it really takes uh uh a the computer vision Village to to put the show on uh and lastly I'd like to highlight our three on-site coordinator coordinators who throughout the day their their tius tireless work is what really made this Workshop what it is uh so with that I'd like to conclude by uh letting everyone know that a recording of This Workshop will be available on YouTube and uh we have a link to the Workshop website where you can uh take a look at uh today's program and and uh revisit uh the recording so we'll have a a link to the recording there and with that I'd like to conclude by thanking everyone for attending today's workshop and I hope to see everyone again at cvpr 2025 in Nashville e e e for

