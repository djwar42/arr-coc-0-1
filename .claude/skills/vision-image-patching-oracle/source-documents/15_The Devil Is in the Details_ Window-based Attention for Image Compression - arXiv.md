---
sourceFile: "The Devil Is in the Details: Window-based Attention for Image Compression - arXiv"
exportedBy: "Kortex"
exportDate: "2025-10-28T18:42:19.600Z"
---

# The Devil Is in the Details: Window-based Attention for Image Compression - arXiv

a5188d54-8118-459b-b914-5f66c77596fe

The Devil Is in the Details: Window-based Attention for Image Compression - arXiv

e77c2446-e9fa-4528-87e4-ad89ff5f98ec

https://arxiv.org/pdf/2203.08450

https://lh3.googleusercontent.com/notebooklm/AG60hOpbCcCs4oxueOa06qwVajRogrgBxqLilETh4mSCmuunqJGQmyGJzBpmQnvHbFPGgyGnZFhkGBpZiPjncM61IMCw5d43gNhgLiBCSpS7aFjfpKPvOXU2Tlg0GvOCcQtXCF8s3KLP=w226-h156-v0

b29283d7-f87f-4928-9d62-6bca06035ba0

https://lh3.googleusercontent.com/notebooklm/AG60hOodOLJEmCljSPJJdlP2NjzMdVdQ5Qo6nGcx_EEKE6oZHCT4JnwHVEcKu2eBb5r2cbzbqxkE9g6y-ijsCuwxRz8gGsw5XwypGurLQITsdOQQFdU-K4G9FUkbdihvi5XCDB3DB02NGQ=w80-h45-v0

8ed7aca1-c7b4-406b-af7b-33dcf7cc5b82

https://lh3.googleusercontent.com/notebooklm/AG60hOqSafXOjCgdXs2D2bxdghBWAeZBRYPImDDf9hAgRIf2lS8bLfqZoPdHSfhccBO0qayeRs48NHXeZc_Tpvcjdi7bdzoScEP5PISQnEr6Uts8zPm64jP1Scnp_zYWJVkTZgk_xaGywQ=w150-h70-v0

c4b58883-bcc4-451b-9edc-1a9e12d04975

https://lh3.googleusercontent.com/notebooklm/AG60hOrjH1Qr-XFkeiPL1gZ6ec8IeMWCZ0eE7f-cW8xpD9bmM9OIs-mq1R1Jb1eqYJbp2tKPhOymSqzzjee0li7Mka2vWlVC-QTl5YAyZ4Cwo7a3lZpgk7ilpdaXjYHNTOI5yAl3k6Cv=w80-h45-v0

4ca7b04d-894f-48ea-b32e-e1e42e66959a

https://lh3.googleusercontent.com/notebooklm/AG60hOrfRa80cKXy_GgLn2RWRO7-7Ob_YfzSFThXJwFgHlppBNEy6R_Oue4e127DxawIVvLqtLTFnFWoY7FkZOHWtZo73qyOqiWgOvuUCcKm_gFUcX50RHu0TAmBE5yNGQgxZ5MRR5EbeQ=w150-h70-v0

5abd2698-0d73-49d9-80b4-b7f24aa49af9

https://lh3.googleusercontent.com/notebooklm/AG60hOq7PQKityeC8_veZD0e7gCVy1WZvAH2jBh7krsLsTAJVTdpLVg-YiT6l0xjd0CFHHU9SMTDnoQo_YUwfbI-5Q5nDhfKu_UbFNblaMu5fprqpF5BCdlKMafQCWoEBxDPLkcLumjUOA=w80-h45-v0

f0e7bd3c-ae59-46a0-acf5-510ba924c38c

https://lh3.googleusercontent.com/notebooklm/AG60hOqcS-xnAXp_jlUhV-INZBU-b7neOEMrY-pRbFDOYVpWHCgznMwiJFblDZLqHGmW86nI4dzW2U3HYdAL9faLXNRMZ969KadEpBa03KP_Chos_GAGovSGl8pHk6XyKsu9ZCrnGvFWFg=w150-h70-v0

5024ec37-5477-45de-8a2a-fba0d74a0560

https://lh3.googleusercontent.com/notebooklm/AG60hOq6VA5wFrPe02ELnJKA3GblfvBnOTofnIc0g86meuGBytx1vwhXQfVnpsbK-sR64qMBt1gi905K4sPvvcgToRDtf6luxFkr1gBRbO9UHRcIgnGDTZDT_T6hJwvt2pTZb8JthsvSyg=w80-h45-v0

6af126c1-d468-4a1a-b724-495121e6ce36

https://lh3.googleusercontent.com/notebooklm/AG60hOqKJOULL7sDa4pAPDJeJ-YOyN94bfcCcOLulRys_ttcrUxGm_QPx4S0kx-x9OSOAnFRkoN-ShddPPxcB3tfm3hQkldv-OtylozynZEEnC6xZeidNFjU29tR812Yn_b52u5amlFEzA=w150-h70-v0

1e85e7ab-ae38-4a14-a9a0-6cf29d9191fd

https://lh3.googleusercontent.com/notebooklm/AG60hOpXTgPRejUFg6g8mmufJuQ52rVjKro09jD1o3FR91OaHIZihvSugquDbN1UNIpX0FiTE0wsWshPAaJab7ACPY9LWbFTaubz__5SIa2-Cq6Sx9R5nm58pVLub93LwQZj_IIP0gtC2g=w80-h45-v0

fff5b903-22b6-4665-addd-4250aaf0fd29

https://lh3.googleusercontent.com/notebooklm/AG60hOonTao4nQXJwZzp2znLu0-26Uxuraoti6FjW7A0F9EPKZF7UB31Tz7bHuflZdpj-mIuvDtzj8NADKRut2BiCI55BdN8CeOGaSxhJJAUZxEDxw8Cgyq-dNf5wmctC9yuPFKK0N3HFw=w150-h70-v0

53d24235-e937-4db1-80a8-43e380102c5e

https://lh3.googleusercontent.com/notebooklm/AG60hOrkgxbN8JAHl1ArUqoRrubgKzmx4FOf2pse2G0N6rPdOwzWh2VW3xTwfAoSg9O9ZzZFTE6BDgcWe9QvcB2rGCz9vSKNe1_R_mfbtwhHpcBiF8u1h13AvoxB0exndVVoIUaol-q7=w80-h45-v0

e78b51b8-63fd-490b-ba72-ab1382a47b45

https://lh3.googleusercontent.com/notebooklm/AG60hOpzpPW5WbUlOS3lyhzrBB_2ayrI8Ee6tRX9XR7BvYWU4WBb2ylJ2lyNelV61YtlHSKe32ZEnuJ7hlYKBv3fz-qwpA4I4ECHNosxsJE7zNTF-9rEybce46MFwUvp90tHVdu50zimiw=w150-h70-v0

f8d85b2c-8bdf-484a-b113-9a8a2b12be84

The Devil Is in the Details: Window-based Attention for Image Compression

Renjie Zou 1 Chunfeng Song 1 Zhaoxiang Zhang 1,2,‚àó

1 National Laboratory of Pattern Recognition (NLPR) of Institute of Automation, Chinese Academy of Sciences (CASIA) &

University of Chinese Academy of Sciences (UCAS) 2 Centre for Artificial Intelligence and Robotics, HKISI CAS

{zourenjie2020, chunfeng.song, zhaoxiang.zhang }@ia.ac.cn

Learned image compression methods have exhibited su-perior rate-distortion performance than classical image compression standards. Most existing learned image com-pression models are based on Convolutional Neural Net-works (CNNs). Despite great contributions, a main draw-back of CNN based model is that its structure is not de-signed for capturing local redundancy, especially the non-repetitive textures, which severely affects the reconstruction quality. Therefore, how to make full use of both global structure and local texture becomes the core problem for learning-based image compression. Inspired by recent pro-gresses of Vision Transformer (ViT) and Swin Transformer, we found that combining the local-aware attention mech-anism with the global-related feature learning could meet the expectation in image compression. In this paper, we first extensively study the effects of multiple kinds of atten-tion mechanisms for local features learning, then introduce a more straightforward yet effective window-based local at-tention block. The proposed window-based attention is very flexible which could work as a plug-and-play component to enhance CNN and Transformer models. Moreover, we pro-pose a novel Symmetrical TransFormer (STF) framework with absolute transformer blocks in the down-sampling en-coder and up-sampling decoder. Extensive experimental evaluations have shown that the proposed method is ef-fective and outperforms the state-of-the-art methods. The code is publicly available at https://github.com/ Googolxx/STF.

### 1. Introduction Image compression is a fundamental and long-standing

research topic in image processing field. With the ever in-creasing visual applications, lossy image compression is a

* Corresponding Author
[0.264bpp/28.96/0.945] [0.255bpp/28.79/0.942]

[0.247bpp/26.60/0.961] [0.232bpp/26.71/0.959] [0.240bpp/26.48/0.958]

[0.261bpp/28.74/0.942]

CNN-att (Ours) STF-att (Ours) Minnen2020 (SOTA CNN)

Figure 1. Comparison of image reconstruction by different meth-ods, including CNN + window-attention (CNN-att), Symmetrical TransFormer + window-attention (STF-att), and previous SOTA CNN model (MBT). The upper shows results optimized for MSE, while the lower is optimized for MS-SSIM. It is obvious that the proposed window-based attention is effective for both supervi-sion, especially with the STF model, showing that the proposed window-based attention model could internally allocate more bits on high contrast areas and achieve better visual quality. The met-rics are [bpp‚Üì/PNSR‚Üë/MS-SSIM‚Üë].

vital technique for storing images and videos efficiently in limited hardware resources. Classical lossy image compres-sion standards including JPEG [44], JPEG2000 [40], BPG [8], and VVC [9] follow a similar coding scheme: trans-forming, quantization, and entropy coding. However, these image compression standards rely heavily on the hand-crafted rules, which means they are not expected to be of the ultimate solution for image compression.

In recent years, the learned image compression based on variational auto-encoder (VAE) [24] has achieved bet-ter rate-distortion [41] performance than conventional lossy image compression methods on metrics of Signal-to-Noise Ratio (PSNR) and Multi-Scale-Structural Similarity Index Measure (MS-SSIM) [46], showing great potential for prac-tical compression use. Here, we would briefly introduce the general pipeline [5] of the VAE-based methods. For en-coding, the VAE-based image compression methods use a linear and nonlinear parametric analysis transform to map the images to a latent code space. After quantization, en-tropy estimation modules predict the distributions of latents, then the lossless Context-based Adaptive Binary Arithmetic Coding (CABAC) [30] or Range Coder [31] compresses the latents into the bit stream. Meanwhile, hyper-prior [6], auto-regressive [34] priors and Gaussian Mixture Model (GMM) [15] allow the entropy estimation modules to more precisely predict distributions of latents, and achieve bet-ter rate-distortion (RD) performance. For decoding, loss-less CABAC or Range Coder decompresses the bit stream, then the decompressed latents are mapped to reconstructed images by a linear and nonlinear parametric synthesis trans-form. Further, there are also some works [21,27,48] design-ing post-processing networks for better quality of recon-struction. Combining above sequential units, those mod-els could be trained end-to-end. Although great progresses have been achieved, one core problem of above CNN-based model is that the original convolutional layer is designed for the high-level global feature distillation, rather than the low-level local detail restoration. As shown in the right side of Fig.1, even the SOTA CNN model is still affected by the weak local detail learning ability which would inevitably limit further performance improvement.

Inspired by the success of attention mechanism in nat-ural language processing (NLP) and computer vision tasks such as the image classification and semantic segmentation, many researchers apply the non-local attention mechanism to guide the adaptive processing of latent features, which could help the compression algorithm allocate more bits to challenging areas (i.e., edges, textures) for better RD perfor-mance. However, such non-local attentions still not change the intrinsic global-aware character of the CNN structure. Recent studies [13,18,29,42] have demonstrated that trans-former [43] can be successfully applied to vision tasks with competitive even better performance compared with con-volutional neural networks (CNNs). Those attention-based networks, such as Vision Transformer [18] and Swin Trans-former [29] take the advantages of attention mechanism to capture global dependency. However, we intuitively find that the global semantic information in image compression is not as effective as in other computer vision tasks. Instead, spatially neighboring elements have stronger correlation.

Following above discussions, this paper explores to ad-

dress the detail missing problem from two aspects, i.e., studying the local-aware attention mechanism and intro-ducing a novel transformer-based framework. First, we comprehensively study how to combine the neural networks with the attention mechanism for designing local-aware lossy image compression architectures. Through conduct-ing a set of comparative experiments based on a global at-tention mechanism and a local attention mechanism, we have verified our aforementioned guess, that the local at-tention is more suitable for the local texture reconstruction. Afterward, we present a flexible attention module combined with neural networks to capture correlations among spa-tially neighboring elements, namely window-attention. As shown in Fig.1, the proposed attention module could work as a plug-and-play component to enhance CNN and Trans-former models. Second, although the transformer-based models have achieved great success in a variety of computer vision tasks, there are still great challenges applying trans-former model in image compression, e.g., no up-sampling units, fixed attention model. To this end, we propose a novel Symmetrical TransFormer (STF) framework with ab-solute transformer blocks in down-sampling encoder and up-sampling decoder, which may be the first exploration of designing up-sampling transformer, especially for the im-age compression task. Extensive experimental results show that our methods are superior to the state-of-the-art (SOTA) image compression methods in essential metrics. The main contributions of this paper are summarized as follows:

We extensively study the local-aware attention mech-anism, and find that it is crucial to combine the global structure learned by neural networks and the local tex-ture mined by the attention units.

We present a flexible window-based attention module to capture correlations among spatially neighboring el-ements, which could work as a plug-and-play compo-nent to enhance CNN or Transformer models.

We design a novel Symmetrical TransFormer (STF) framework with absolute transformer blocks in both down-sampling encoder and up-sampling decoder.

Extensive experimental evaluations have shown that the proposed methods are effective and outperform the SOTA image compression methods.

####### 2. Related Works Learned Image Compression. Recently, learned image compression models [5,6,15‚Äì17,19,27,32,34,35] based on CNNs exhibit a fast development trend and achieve signifi-cant breakthroughs. For VAE-based architectures, [5] firstly proposes an end-to-end optimized model for image com-pression. [6] incorporates a hyper-prior to effectively cap-ture spatial dependencies in the latent representation based on [5]. Inspired by the success of auto-regressive priors in probabilistic generative models, [34] further enhances the

entropy model in [6] by adding an auto-regressive com-ponent. In addition to this, [15] enhances the network ar-chitecture by using residual blocks and adding a simpli-fied attention module with the Gaussian Mixture Model (GMM) replacing the mostly used Single Gaussian Model (SGM). Though the GMM-based entropy model performs better in RD performance, the adopted probability distri-bution function (PDF) and cumulative distribution function (CDF) have to be dynamically generated for every single el-ement in encoding and decoding, thereby introducing much redundancy and making it time-consuming. In contrast, the SGM-based entropy model can build fixed PDF and CDF tables for entropy coding, which is less computationally expensive. To minimize serial processing steps in auto-regressive context models, [35] proposes a channel-wise auto-regressive entropy model. Some approaches [2,33] use the Generative Adversarial Networks (GANs) to directly learn the distribution of images and prevent compression artifacts. For GAN-based architectures, image compression is a rate-distortion-perception trade-off task. Attention Mechanism. Attention mechanism mimics the internal process of biological observation, devoting more attention resources to the key regions to obtain more details and suppress other useless information. Non-local attention mechanism [45] has been proven beneficial in various vi-sion tasks. For learned image compression, [28] applies the non-local attention to generate implicit importance masks to guide the adaptive processing of latent features, while [15] simplifies the attention mechanism by removing the non-local block. Transformer-based Model. Inspired by the success of Transformer architectures [43] in the natural language pro-cessing, there are many works exploring the potentiality of Transformer in computer vision tasks. [18] conducts im-age classification with transformer architecture. [13] im-plements transformer-based models in detection. [14] pro-poses a universal pre-training approach for image process-ing tasks. [29] proposes a hierarchical Transformer com-puted with shifted windows and achieves SOTA perfor-mance in image classification, semantic segmentation, and object detection. However, as far as our knowledge, there is no transformer related work for image compression. In this paper, we explore how to design a transformer-based archi-tecture for learned image compression to achieve compara-ble RD performance. Normalization in Neural Networks. Generalized divi-sive normalization (GDN) [4] is a milestone in learned im-age compression. GDN is highly efficient in Gaussianizing the local joint statistics of natural images. We have con-ducted some comparative experiments by replacing GDN with batch normalization [22], layer normalization [3], and channel normalization [33]. We found that replacing GDN in CNN would result in a huge drop in RD performance.

However, we also found that GDN is unstable in a deep Transformer-based architecture. In addition, GDN is not compatible with the attention mechanism in Transformer blocks.

##### 3. Method 3.1. Formulation

Since our method is built upon the hyper-prior architec-ture [6,34] and channel-wise auto-regressive entropy model [35], we would briefly introduce the basic pipeline for better understanding.

The encoder E maps a given image x to a latent y. After quantization Q, ≈∑ is the discrete representation of the latent y. Then ≈∑ is mapped back to the reconstructed image xÃÇ using the decoder D. The main process is formulated as:

y = E(x;œï)

xÃÇ = D(≈∑; Œ∏)

where œï and Œ∏ are trainable parameters of the encoder E and decoder D. Quantization Q inevitably introduces clip-ping errors of the latent (error = y‚àíQ(y)), which leads to distortion of the reconstructed image. Following previous work [35], in the training phase, we also modify the quanti-zation error by rounding and adding the predicted quantiza-tion error.

We model each element ≈∑i as a single Gaussian distri-bution with its standard deviation œÉi and mean ¬µi by intro-ducing a side information ·∫ëi. The distribution p≈∑i|·∫ëi of ≈∑i is modeled by a SGM-based entropy model:

p≈∑i|·∫ëi(≈∑i|·∫ëi) = N (¬µi, œÉ 2 i ) (2)

The loss function of image compression model is:

L =R+ Œª ¬∑D =Ex‚àºpx

[‚àí log2 p≈∑|·∫ë(≈∑|·∫ë)‚àí log2 p·∫ë(·∫ë)]

+ Œª ¬∑ Ex‚àºpx [d(x, xÃÇ)]

where Œª controls the trade-off between rate and distortion, R is the bit rate of latents ≈∑ and ·∫ë, d(x, xÃÇ) is the distortion between the raw image x and reconstructed image xÃÇ.

#### 3.2. Window-based Attention

Most of previous approaches [15, 28, 49] apply the at-tention mechanism to generate attention masks based on a global receptive field. Attention mechanism has also been successfully adopted in many computer vision tasks, such as the image classification, semantic segmentation and ob-ject detection. However, intuitively, global semantic infor-mation in image compression is not as practical as in those computer vision tasks.

https://lh3.googleusercontent.com/notebooklm/AG60hOp3VIqKiTLxitOtQGSi8ikytYd6K6FaLZm4Gz_BX9mlFBOHIgDPmBdAOLn6aOBIUv5N7bgm9NzBBTTGfIprWyWWmC-MfFoP9XCXZtnp9i7yaYWKWTNcwQz_uuV5MZwADNWeD5a6eQ=w384-h603-v0

3db1d977-4cb2-4f9e-9290-bc3c370c6c21

https://lh3.googleusercontent.com/notebooklm/AG60hOoZqrpr0Ob59J-xDJDDxrmjVNXtI5dOuWKNrPJsJ7smjmF52YY2uASpJGpOHXhoX41msfNbD-hIkXzWquJQ6rvryGF95y63YrVgGN30PBcxGrQvLGcG5dxSUZMjCkz7u6UtWZFCoA=w110-h110-v0

e377d251-208b-4ad8-b570-2fad1f165580

https://lh3.googleusercontent.com/notebooklm/AG60hOrFs2Vs0yXBj6cAsJygjC378097ky93lwxNu4PWpOu8mrJXYojx3SC_DvwdpqoxT3IAiXqiQR5cM_a7oXUtfE08cn-hflgERGeSdyNAshnwLImUFjiSe2fN3XJHZ2JgjgSvqfKOUQ=w387-h463-v0

2208a45b-c550-4fff-b6b4-9ff5ba371210

https://lh3.googleusercontent.com/notebooklm/AG60hOpvrUSzKFyqDsjpxriMxWMqoBVY-aQ9Kc_bkWNvnIOoOlwOcSUmHSK2QXw0NERBKR0cz0Wqzn71xsFaxivYoE1ddPnr0TvPriGyUo_elQzHf98QbRtd17KVmLixG1_ISCPxlVXuzg=w377-h434-v0

d8ff7c35-98d7-4716-8545-3154fad01cdb

https://lh3.googleusercontent.com/notebooklm/AG60hOrYtKCbEdNIXHXMC3pki9ybAe1nvta65LmhJyLDAlNRfAB1ZuuJNp3Mx_2R3mUGqoZTfEg_DVCclt41mpIz6wRm5ohI0X0CDyRYbJtQ2P_YqA4BLoNqu34DOaAYuMmmgMDJ2kerbw=w377-h434-v0

f368c5e0-de16-4b5d-8924-b622e936f2cc

https://lh3.googleusercontent.com/notebooklm/AG60hOqrq1j9BPvfh2zaVYpTd3kPTLG_GKSZ9X7MmNeYF9EgxFcop49RDXitrHclfU6G0SgeLHkz6E3CL5vhcBVsI6iRpUs27DgqXyxKFiuvTBiVcbuQtuagCVtK2gTg4wZyLJfXJ8oIzw=w400-h457-v0

62b78999-5b09-4334-add3-efbfdf528095

https://lh3.googleusercontent.com/notebooklm/AG60hOpMkS2q2ufFPzxkkBZ0lfcq08sRtgibcrL4C0J8yXfPgAJnmms5oim9iNZLYsu75jNdKRczycKyfbVIsy2TpfzFYmVmpnn4yUktJ6iNAU8DaAiflRqXxBCxxqcZ_6OaDnND6qFq=w392-h458-v0

14e2b0f1-fea9-4d41-9669-27ef6de19fd2

https://lh3.googleusercontent.com/notebooklm/AG60hOpFrvfczJx4jkUC6n5BmcXMux3YBkAqDBgbheeguwagpbmeJAue5JAZRybktUHEf7uAhbOF2BmMCH6ZM6vH7S5eZor1abo8_fsv0ABAs1zapH59gcVAVnT-2-e1JLMj6XusD9uqOw=w379-h450-v0

b4f8366e-4fcb-495c-a8e4-69aec0a837dc

https://lh3.googleusercontent.com/notebooklm/AG60hOpDrHUVux-WNGJELITdzs-CSZCDZsLYaV4-9LHuiaEDXXK9ijOnNuWXlz640VMCGhXiFIE73idt_mRhjwbgSpNwTW_7VuSgYN2erLqXHIifnfvsDZMvLL-1HshL7CSe-r_24gA5=w512-h768-v0

633939e7-866b-43dd-b671-e1c06a898f70

https://lh3.googleusercontent.com/notebooklm/AG60hOo21NF9JnJZyXSZhsawjO_G9vgKdc_IYjErC8ZoY3_geTq3RBw0niyaqTEIpwfx2bj8bf2j3ADcC-Zj8rt59-kJfgk1yMdncMtNZ_wFTI7lmNxHRQgRnZGywFTmRQnSboHFu2HPDg=w60-h80-v0

26aa8fdb-04da-4449-87c5-4674d2b94f64

https://lh3.googleusercontent.com/notebooklm/AG60hOqWNrsXY97YTUfcGFq14UaAb8x9rphdzdqOYUh5kvkAOiKDHU_mhDzf71oBysM7QpgX6yCz_xlQtPPZiv4hKv_Ab7DLuFSkjgO3h90QTdscUv6i_VojYnpqnXDiJT_mgMAdYzNTiQ=w60-h80-v0

6b5fe7f8-84ef-44f4-a289-991e6f0778cf

https://lh3.googleusercontent.com/notebooklm/AG60hOpRK_XxI8lFv5nldEQubgZlf8sHbvIsc1iinca6BG67LcdylAWR5_k7q_WQxG1c3glPrbo3ejTO46cs_CvdvwTDoLy4OO8cZJnjhDabjenB3p5UKqaF4plFcVKy_pLFQZORJZK5HA=w60-h80-v0

2d17c18b-143a-4ea7-bbd2-01d829620edf

## A local window

## Feature Map

ùúÉ:1√ó1 ùúô:1√ó1 ùóÄ:1√ó1

## Attention Computing Unit

Figure 2. An illustration of the window-based attention. Attention masks are computed in a local window.

Attention in Non-overlapped Windows. We notice that generating attention masks based on spatially neighboring elements can improve RD performance with less comput-ing cost. For efficient modeling and focusing on spatially neighboring elements, we propose the window-based atten-tion. As illustrated in Fig. 2, the feature map is divided into windows of M√óM in a non-overlapping manner. We com-pute the attention map in each window separately, Xk

j are i-th and j-th elements in k-th window, as below:

f(Xk i , X

with f(Xk i , X

k j ) = eŒ∏(X

Tœï(Xk j ),

C(Xk) = ‚àë ‚àÄj

f(Xk i , X

g(Xk j ) = WgX

Here, Œ∏(Xk i ) = WŒ∏X and œï(Xk

i ) = WœïX , where WŒ∏

and Wœï are cross-channel transforms. f(¬∑) is an embed-ded Gaussian function. Ck(X) is a normalizing factor. For given i and k, 1

Ck(X) f(Xk

i , X k j ) is the softmax computa-

tion along the dimension j in k-th window. Residual con-nection is necessary for such attention mechanism, the out-put is as follows:

Zk i = WzY

where Wz is a weight matrix for computing the position-wise embedding on Y k

i , given in Eq.4. Window Attention Module. Liu et al. [28] propose the Non-Local Attention Module (NLAM), which consists of a cascade of a Non-local Block and regular convolutional lay-ers. We replace the Non-local Block with Window Block for focusing on regions with high contrast. Fig. 4 (b) shows our Window Attention Module (WAM). We visualize the channel with the highest entropy for WAM, NLAM, and w/o attention module, as shown in Fig. 3. In the 3-rd col-umn, it‚Äôs obviously observed that WAM can allocate more

## Original Image

## Latents Allocated bits Details

Figure 3. Visualization of WAM, NLAM, and w/o attention mod-ule for the channel with maximal entropy. It shows that our WAM focuses on high contrast regions (sailboats) and allocates more bits on those areas while fewer bits on low contrast regions (sky and clouds). Instead, the bits are allocated evenly in the w/o attention module and NLAM.

bits in complex regions (high contrast) and fewer bits in simple regions (low contrast). In addition, the WAM re-constructed image has sharper texture details. NLAM with global receptive field often leads to evenly allocation of bits on different regions, which is not consistent with the expec-tation in [28].

#### 3.3. CNN-based Architecture

As shown in Fig. 4, our CNN-based architecture is es-tablished upon [35]. We enhance the encoder and decoder by inserting the proposed WAMs, respectively. The WAMs help allocate bits on different areas more rationally and in-ternally with negligible computation overhead. It is simply yet could significantly improve RD performance.

#### 3.4. Transformer-based Architecture

Inspired by the success of Transformer architectures [18, 29] in computer vision and our aforementioned exper-imental result that the local attention helps allocate bits ra-tionally and improve RD performance, we further propose a novel Transformer architecture for learned image compres-sion, as shown in Fig. 5. Rethinking the Transformer. Since our goal is to ver-ify whether combining self-attention layers and MLPs can achieve comparable performance with original CNN-based architectures, we design a novel Symmetrical Transformer (STF) framework without convolution layers in the encoder and decoder. The difficulties of designing the Transformer model for learned image compression are the following:

Previous works are mostly based on the CNNs to elim-inate the spatial redundancy and capture the spatial structure. Directly splitting image to patches may re-sult in space redundancy within each patch.

https://lh3.googleusercontent.com/notebooklm/AG60hOrr3FuR7asn7TYNX_15uqpUQCMGsrW-xnOdf2lxJw_-AojOCCQ0Kww74ti6MCx6jr0q_11UxatUoVS9ZAi0eh0GM0nH4x2ctXcfi-j5I482Keo2fUk6agu0MZRz9iujCTJOpHUGrQ=w354-h236-v0

64c93fe3-f060-4152-b730-aad8de9787c2

https://lh3.googleusercontent.com/notebooklm/AG60hOo1xHHKp0ItB8y7of0QHC5FeNgUrHFvHIxbqWuh-t523jHTDF-MOXaavzxOxGQiGecGxwFPUQ_Xc65284AQ2DyWXyalAyeNdMmvaXh3EnnbnYSV4ulLS9s9mxYkpHK2Ys0buiqA3Q=w354-h236-v0

a9e75fa7-077e-4cce-a8a7-6d0fbd337b73

## C o n v  N

x 5 x 5  / 2 ‚Üì

## Original Image

## Reconstruction

(a) The Architecture of Our CNN-based Model

## Window Block

(b) Window Attention Module

Figure 4. (a) The architecture of our proposed CNN-based model. We adopt the architecture of [35] for the sake of subsequent comparison. IGDN is the inverse GDN. (b) RB is the residual block which consists of 1√ó 1 and 3√ó 3 convolutional layers.

GDN is the most commonly used normalization and nonlinear activation function in image compression. However, GDN is unstable in the deep Transformer-based architecture. Furthermore, we have found GDN and the attention mechanism in the transformer are in-compatible.

Based on our previous analysis and experimental re-sults, calculating the attention map on a large field is not optimal.

To address above concerns, we choose a small patch size to avoid the space redundancy within each patch. We use the LN for normalization, which is most common used in Transformer. GELU is adopted as the nonlinear activation function in our Transformer architecture. Inspired by [29], we compute the attention map within local windows. The advantage of our Transformer architecture is that it could focus on spatially neighboring patches while gradually ex-panding the receptive field, with acceptable computational complexity. Transformer-based Encoder. We split the raw image x ‚àà R3√óH√óW into patches with patch size N . A linear embed-ding layer is applied on the raw patches to generate a feature map fp ‚àà RC√óH

N √óW N with C channels. The feature map

fp ‚àà RC√óH N √óW

N is reshaped into a sequence fs ‚àà RP 2√óC , where P = HW

N2 is the number of patches. Then the se-quence fs will be input into Transformer blocks and patch merging layers. Following the structure in [29], the former calculate attention masks in a window for feed-forwarding. At the same time, the latter down-sample the resolution of features and doubles channels of features. Transformer-based Decoder. We design a symmetrical decoder consisting of multiple Transformer blocks, patch splitting layers and a de-embedding layer. The patch split-ting layers up-sample the resolution of features and halve

channels of features. The de-embedding layer maps the fea-ture map to the reconstructed image xÃÇ. Entropy Model. To more effectively and efficiently predict the probability distribution of the latent. We use an SGM-based channel-wise auto-regressive entropy model [35].

##### 4. Experiments 4.1. Experimental Setup

Training. We implement our proposed CNN-based ar-chitecture and Transformer-based architecture in Compres-sAI platform [7]. For training, we randomly choose 300k images from the OpenImages dataset [26], and randomly crop them with the size of 256√ó256. All models are trained for 1.8M steps using the Adam optimizer [23] with a batch size of 16. The initial learning rate is set to 1 √ó 10‚àí4 for 120k iterations, and drops to 3√ó 10‚àí5 for another 30k iter-ations, 1√ó 10‚àí5 for the last 30k iterations.

Our models are optimized using two quality metrics (MSE and MS-SSIM) as supervisions. Following the same settings in [7], when the model is optimized for MSE, the lambda values Œª belongs to {0.0018, 0.0035, 0.0067, 0.0130, 0.025, 0.0483}. When the model is optimized for MS-SSIM, the lambda values Œª belongs to {2.4, 4.58, 8.73, 16.64, 31.73, 60.50}.

For our CNN-based models, the channel number of la-tent and hyper latent are set to 320 and 192, respectively. For our Transformer-based models, patch size is 2, window size is 4, and channel number C is 48. Typically, our models have same hyper-parameters for different Œª.

Evaluation. We evaluate our CNN-based models and STF models by calculating the average RD performance (PSNR and MS-SSIM) on the commonly used Kodak im-age set [25] and CLIC professional validation dataset [1].

https://lh3.googleusercontent.com/notebooklm/AG60hOpfUkt1eWWmRE5ybkpf8vGdHTSgI6ydfZojEeQZQWFTLYSxO49dszwQjtcNbwCg-6_eXGxAOCemmVx7otiLBSF1MoNGZvzNhr4Q0kAAW6a5_IKztuK7bP_TJU8ghmmPI63dFSXC=w354-h236-v0

3c665b69-4fa3-49aa-ba55-84f0e1c89f6f

https://lh3.googleusercontent.com/notebooklm/AG60hOohlfE1Sxz-w9hrEBj-7ZsDdaH1wdaXhb0os843xMgdOm89XaJP3BSfqXV9Z_WKh9dyRnyn9E08QB2xFiV3UcOmaaJojvn6a42Vu5FtF8ytrJnfxtxgjkZ9RC4IBUO4FEmWg1lKUA=w354-h235-v0

0f623a93-e5ca-4661-bd7f-04a00528f63c

to Patches

√ó2 √ó2 √ó6 √ó2

## Patches to

Image ùëü‡¥§ùë¶

(a) STF Architecture (b) Transformer Block

√ó2 √ó2 √ó6 √ó2

## Original Image

## Reconstruction

Figure 5. (a) The architecture of our proposed Transformer-based model. The patch merging layer and patch splitting layer consist of linear layers and LN layers. (b) W-MSA [29] is the window-based multi-head self-attention, and SW-MSA [29] is the shifted window-based self-attention, respectively.

We compare our methods with influential learned compres-sion methods, including the context-free hyperprior model (Ball√©2018) [6], auto-regressive hyper-prior model (Min-nen2018) [34], and auto-regressive hyper-prior model with GMM and simplified attention (Cheng2020) [15]. Refer to appendix for RD curves covering a wide range of conven-tional and ANN-based compression methods.

#### 4.2. Comparison with the SOTA Methods

RD Performance. Fig. 6 shows the comparison results on Kodak dataset. When trained for MSE and measured by PSNR, the performances of our CNN-based models and STF models are very close, and could outperform other learned compression methods. In contrast, when trained and measured by MS-SSIM, our CNN-based models and STF models only have slight improvements. As mentioned in [6], MS-SSIM has the effect of attenuating the error in regions with high contrast, and boosting the error in regions with low contrast. But the fact is not as expected, it fre-quently assigns more details to the regions with low con-trast (e.g., grass and hair) and removes details from regions with high contrast (e.g., text and salient objects). Atten-tion mechanism focuses more on high contrast regions, and consequently allocates more bits on them. The contradic-tion may result in an inconspicuous improvement for our attention-based models when optimized for MS-SSIM.

As illustrated in Fig. 7, the comparison results on the CLIC professional validation dataset indicate the same con-clusion. It shows the robustness of our CNN-based models and STF models.

Visual Quality. Fig. 8 shows the example of recon-structed images (kodim07.png) by our methods and the compression standards JEPG, BPG, and VVC (VTM 9.1). Our reconstructed images retain more details with approxi-

Method Enc (s) Dec (s) PSNR‚Üë bpp ‚Üì Cheng2020 [15] 8.49 14.49 35.12 0.595 Minnen2018 [34] 16.19 21.16 35.09 0.640 Our CNN 0.12 0.12 35.91 0.650 Our STF 0.15 0.15 35.82 0.651

Table 1. Comparison of the averaged encoding and decoding time with Minnen2018 [34] and Cheng2020 [15] on Kodak dataset us-ing GPUs (TITAN V). Note that the results of Cheng2020 are based on a light implementation (without attention module and Gaussian mixture likelihoods) in CompressAI [7].

mate bpp. When optimized for MS-SSIM, our CNN-based models and STF models yield significant improvements in visual qualities.

#### 4.3. Codec Efficiency Analysis

Spatially Auto-regressive (AR) context model is effec-tive and follow-up studies often use it to enhance the RD performance. However, the AR model sequentially encodes and decodes each spatial symbol, which significantly slows down the codec efficiency on GPUs and TPUs. GMM-based entopy model has the same defect. Although it can more precisely estimate PDFs and CDFs of latents, generat-ing CDFs and PDFs dynamically would sacrifice the coding efficiency, while SGM-based entopy model has fixed PDFs and CDFs tables. Therefore, we use channel-conditional (CC) models [35] as the auto-regressive context model along the channel dimension for better parallel processing, and adopt the SGM-based entopy model for more efficient coding efficiency.

In Table 1, we evaluate the inference latency of our meth-ods and those time-consuming models [15,34] on the Kodak

0.2 0.4 0.6 0.8 1.0 bpp

Our STF Our CNN Cheng2020 Minnen2018 Ball√©2018

0.2 0.4 0.6 0.8 1.0 bpp

Our STF Our CNN Cheng2020 Minnen2018 Ball√©2018

Figure 6. RD Performance evaluation on the Kodak dataset, which contains 24 high quality images.

0.1 0.2 0.3 0.4 0.5 0.6 0.7 bpp

Our STF Our CNN Cheng2020

0.1 0.2 0.3 0.4 0.5 0.6 0.7 bpp

Our STF Our CNN Cheng2020

Figure 7. RD Performance evaluation on CLIC Professional Validation dataset, which contains 41 high resolution and high quality images.

Method Œª PSNR ‚Üë bpp ‚Üì Baseline 0.0035 30.32 0.198 Baseline + NLAM 0.0035 30.42 0.202 Baseline + WAM 0.0035 30.58 0.199

Baseline 0.0130 33.87 0.454 Baseline + NLAM 0.0130 33.95 0.467 Baseline + WAM 0.0130 34.10 0.448

Table 2. Contrast Experiment. Baseline is Minnen2020 (SOTA CNN-based model) [35].

#### 4.4. Evaluating the Effects of Window Attention

To prove our conclusion that focusing on spatially neigh-boring elements can achieve better RD performance, we present contrast experiments by removing the WAMs or us-ing NLAMs, as shown in Table.2. Ablation study in Fig. 9 shows that the proposed window-based attention is effective and could enhance current SOTA CNN-based model [35].

#### 4.5. Discussion

More Efficient Window Attention Designing. Our CNN-based architecture still have room for improvement,

because the window-based attention is not powerful enough for capturing the structural information. Besides, our exper-imental results show that GDN and attention mechanism are incompatible. We guess that GDN is more of a nonlinear ac-tivation function than a normalization block. Our follow-up experiments show that directly using convolution layers to generate attention masks can also achieve comparable RD performance.

Compatible Normalization for Transformer. In our Transformer-based architecture, we use the layer normal-ization (LN) as default. The drawback is that LN rescales the responses of linear filters in the network so as to keep it in a reasonable operating range with an identical rescaling factor across all spatial locations, which may destroy the Gaussian distributions of elements. When computing the attention map, the LN is necessary to rescale the response range. It seems contradictory but implies that Transformer-based architecture has more potential in learned image com-pression. Moreover, we are looking forward to combining the Transformer and convolution blocks, our results so far show that they are complementary.

Human Perception. Learned image compression mod-els directly optimize metrics such as PSNR or MS-SSIM and achieve high RD performance. However, some works [11,12,36‚Äì39] propose neither of PSNR and MS-SSIM cor-

https://lh3.googleusercontent.com/notebooklm/AG60hOrOVhBrIVDASWrQB6P6amv0o5OxglEyWHT0kFE_FruD2WhchGFzp7Y5F3XdbNLAW6YQ7fRRT1UYfZkGEtKZ-4ePwbe8z7deduazYMw3rv3kGH9VRTZSabYjFWabe9-22zRuG5Yw2w=w436-h291-v0

536e3382-24a7-4752-ae94-5d525a9c611a

https://lh3.googleusercontent.com/notebooklm/AG60hOpkr1odBJvtrVq_AkGDHZqyudozIy7NJhxdCCHwe_jRBrm7oMSk9-ti3gOhMJ-PQKqJMvlvn1w4SH0xvyGa_YXmPVYHDUV5AY5YG_6e-vBT5i6SWhr4DYu6Dfa3WLB1UyvzAQK_Qg=w437-h291-v0

eca52775-d2e3-46a8-89fe-2e5772d4fb34

https://lh3.googleusercontent.com/notebooklm/AG60hOr5CIlsVwURFXMbONDWk3pX2yBCuXgYjDgbXoHxRyBbgPeftW3jVtTdfj-9Sb8J7-wcIyhnnSs3ocmRfqPSs5bmNSrptNsJfTOtM5ilE6aL8YLepZO-yxYonFANvLWSfzrTDvgr=w436-h291-v0

65cc1f95-6621-408a-8b12-6a248db0ce02

https://lh3.googleusercontent.com/notebooklm/AG60hOrB4WkqUQ6bOYgJ9aiUQwMxc59sMtaxiTxTfm3Q61ijavRrc5Z6TgnMbkQuzo9XT65K2IFSIG74MO6iID3vTYwEnYw9LOzcPQfTOhuBM2qQ2c-SLqV77STN5JoVi8Q7GFTF_tmYKA=w437-h291-v0

0c712853-8201-412e-ba7b-bb33a33999db

https://lh3.googleusercontent.com/notebooklm/AG60hOqchYaGGoYLxBKU5P9Y9V3Sd_2-1Z4O4ETSScumstlIHAQh8NMCYAWYpvVtMRKPmPp1-F1I04NkJeL-ayS8xlK_MXOiMIlUBs7Ls2WAH9l5krMzkSm5JBF2WPX7D9JJtU88uX5lUQ=w436-h291-v0

4d92268d-42bb-4eed-8392-dbd8a3e74d29

https://lh3.googleusercontent.com/notebooklm/AG60hOoqGU9n_s_I8zqyxyle0gZPumHp4mqrwG1AC4UxtCTJgb6TP3j8PKuhJl_Au_2ZIximcTwaye9c_UmZ9zelVauf4aBWH6zqVn9isK61tg9RrCWbbq3WWxMWeqTxkceHxFmrOnnB=w436-h291-v0

551c5156-5731-45dd-8a49-3f10b1b202e2

https://lh3.googleusercontent.com/notebooklm/AG60hOoYw0PSEwbv2X78zNlrXV526VD54f_ekMfYThBxpDMgCxyQQz4WKiGRj4h_7hr8iqFt6wa7CYdROrTEZkYDuxjUMuUZo8s7o-CPNEcUGS7sZcQWI43bpwtWuTzLVYxLATvupyTs=w436-h291-v0

15c472e7-0773-43b6-a8f6-5d4651c6c3b4

https://lh3.googleusercontent.com/notebooklm/AG60hOrCW2idX4LxZVkosT91yv-3NcnV_bdyn_eu5o71JJDvruIT_hmtmt0y8ua_oRHYlIpEHANqStiQrv4bmjtJJz-Km0AXhbyf7SuJbJG0u_mSUnHopQlam4Ml0LeOusQ-ACaWMZaoxg=w436-h291-v0

9b24bfc8-a620-4c8b-8236-125977268174

https://lh3.googleusercontent.com/notebooklm/AG60hOpxXJkDQoh5rhEi4F-xLv6fsceIh7MISlM0eO6vTmrW4C0UmASpPNv40OzlCMAaYxCQJGcfbo1rikAIuCN-roKphUFP91Y0Zzd1yD7vMrnO6pnCLsvHYvjnbAM7PNdYU0wZfsEj=w150-h100-v0

f00d2c4c-9a85-425d-9b35-c897a18cccb1

https://lh3.googleusercontent.com/notebooklm/AG60hOq-hQDVYTFZg7eB5qp8iWHNCt-Hd50g9wwE9FIH4zYiidLuwgNb1JTul3e89R04T6XFL6jfZofbAI_P-jiMAdj9jtQs8t-6JVmHDOKKkdxwI8Yv-jjfhN4KlL4bpNef_AHO9_Rg_g=w150-h100-v0

995012da-f4a4-4c81-8312-e1f8f010845b

https://lh3.googleusercontent.com/notebooklm/AG60hOq2iaMwdxlOm4C183IkdasW9nvh2jVH6UaCP4x35phwrwK8J6cWNDKOreBT1F0_LsVJzGhHeG0BmUe0i9_1voBVM2HsydmfVC3EiZ8Qs01ZjFvamW3dpYpkvHb5LxuYC4G0wkHNpg=w150-h100-v0

e5e9f6e5-517e-4cd9-bc01-09939e135dcd

https://lh3.googleusercontent.com/notebooklm/AG60hOr4NCz4alCcRfM8nOCi1b9qxOZWMK3lp0XMUCyxXtoSnqnYaUKHlhS7ej56ZjkQNgUUxaug9W98WQUeG8Pra4piWdywUohgJ2MRpSk63GGSY-Am5nTji5uYiKDVU7br_d8digjbew=w150-h100-v0

fd3c4c97-6353-44d2-b70f-654a67ab2199

https://lh3.googleusercontent.com/notebooklm/AG60hOo-VAnS6rRelVF-GP03ov6oowfTFuGU1GMOgHlQuyMzCGq-YUhACIGXyhPPFMcUG2ZihClzlzv2X3MT4k7nPj2g3Tab7-L5Z4kvv-8TP3_ui1h8GkG3-_yqW-hNQXt__kVuh4M2=w150-h100-v0

a8c1d31a-1d0f-4015-9842-3baadcc8e9ce

https://lh3.googleusercontent.com/notebooklm/AG60hOqqpJVJeWEbf_7XpbT6lRVwUkuP13BeBYDRPWt9JHXSnrrYwObxx0fEgeM6ph7OFN2_kDp79V1jUq8y-ahGYAvARlqHUwpEyXEN1KjJ39YqEiQKWrfIdDGc2yS4rm8ItiRIqcSyyw=w150-h100-v0

eb4213e7-33e3-49af-935c-8b664a962d1b

https://lh3.googleusercontent.com/notebooklm/AG60hOocdInN2EIqa0-53XfJs-0jO8gwcRTYmSNeMaKAkw8RT0Pi-3aBidSt64Z219Xwx-TlwWwPAL23E_EnDCgIq78kXD3VNT6vrWRiQADg2AfX4-AHduxchslWD-0Hz6DDNiU5aNo4gg=w150-h100-v0

a45b85e1-35e5-43d3-82fb-16877ed98c0c

https://lh3.googleusercontent.com/notebooklm/AG60hOrRzTBkBXZmIfNky3TZOeL-XB2zeXrOvnCdwerarh5hw2XGLCg_uvY6o1aK1nmxgR4Wf__5BFotY6TwVsQA2zDI3MdSclLuXSaU8RPj058zUXaW8b_Bl_yEBCPGwL3sj9JRBR1Qww=w150-h100-v0

80a18186-6790-4076-9fc8-7a9f21471365

https://lh3.googleusercontent.com/notebooklm/AG60hOpCaZpqFSNSTHahxTVMe41whWLV9XrpsWNWyZdHeKrenQB1MaOfplSNVRBFbg9FgxfIDnQLAkzT0kTK0EckPQyCEzF3KlZlXyK4lpMMEFwi4otWHVBgAGvSGT9VtZyLUNie_r3_TQ=w150-h100-v0

46549f9e-726d-4010-8d29-c14ac81c15f8

https://lh3.googleusercontent.com/notebooklm/AG60hOo8_eBboR0PVxv2p4P-B4fbB9cLPUwbHc6JvU2xWFX2tNxc4KnisDQtMfdwEqr4EcPF9paqvT0aMRCNtkFV_OacafH2tohU1kfWVEW0fvqi2FCxXlkBBpi6USq5V0UDPOboJEvuVw=w150-h100-v0

92f40950-2271-4787-96ee-776bfc5892ce

https://lh3.googleusercontent.com/notebooklm/AG60hOovJar2Rm3WjNb1nfVQvMDL0JCBM5GZ4ujiWOlkGJ0Fv2A_YgAUP_C2RIFLpY5WZxQC5TUoMsUVui1w5C2tk_qtozZ9q39VZb1VgOMRCXPKHVOqIkWD6K-PinTaK0tOLPq8DkczFA=w150-h100-v0

9e4ece0a-62ad-4d21-b942-bb8e63dbe2ea

https://lh3.googleusercontent.com/notebooklm/AG60hOpJhk98XCQFzhYeqhC4omaTEjKiNDg_C0DWx3uaUMbM7LM-_b4uWgQgbc0ln-OrVutXnoiG51DOMO-PbhUuMZ_nis9riyIv6dE-YGBXoQtPceAWSzzappfycVFiH_VLcgmp-Qb3=w150-h100-v0

e9fed043-00e7-4cb3-b54b-d5c7a206e50f

https://lh3.googleusercontent.com/notebooklm/AG60hOrIDfVZLV5r1Wxflgbci2gjV1Hcpxld2iKg3dEFxNWMRldOjfslHlbX9s4l9bK1BDERgZ3tKu0eX3XT2KBiJgl5mlhOnFM-vmBmYIa-p_mhBi4zuCatfBdmaQEcGueK7EyJsMP7LA=w150-h100-v0

750f6b16-2783-46c6-ade8-e6aad9a834bf

https://lh3.googleusercontent.com/notebooklm/AG60hOoQ52XDU7lnZTVX4qIN9FWrIzqcCB5o1SUdc_2pjJtdxsGckvteiINbsB9TchyLktmlC3goNoiBM0ltwc3x6BRPJlHJYJEe-B_UNvYS5IEndBnh8wfCttys7fEXl5LhVY9BY0MfiQ=w150-h100-v0

74d0e4e2-8905-45e7-b74a-6abb8f4321b8

https://lh3.googleusercontent.com/notebooklm/AG60hOqZEMcf0H--cUs6vW8xPArx-HHMlyKWwjlhoF_5nALPstW4msJNk-RgoQm6JSenUmhT6jbBl6gZ2f0g5yTK-gKMx-i_ZRwoGYeb_LMUn89hZNYxQ8mDc3Ce1kGwiCfwbgu-Vmk=w150-h100-v0

b1993539-0700-4646-afbf-96a60e23687f

https://lh3.googleusercontent.com/notebooklm/AG60hOr0JKWAk8Z6_7wEgJPYvzeBBH55Hm4YA5QfE6VYVxgprzOw16lYlXnBr2yz5oXZtZrmnx1doRdYPZM-rqV5PZEDw2csQWPB0U_-KSO3-lH_kjw0AvxB_Jr--Mps-ojoATzw5yxYrw=w150-h100-v0

f5524912-80fd-4d2f-b1f7-69933482d302

Original Image Ours-CNN (MS-SSIM) [0.127bpp/28.89/0.980]

Ours-STF (MS-SSIM) [0.120bpp/30.11/0.979]

JPEG [0.170bpp/21.79/0.790]

BPG [0.167bpp/30.73/0.965]

VVC (VTM9.1) [0.158bpp/32.10/0.973]

Ours-CNN (MSE) [0.153bpp/32.78/0.978]

Ours-STF (MSE) [0.146bpp/32.68/0.977]

## JPEG BPG VVC

Ours-CNN (MS-SSIM) Ours-CNN (MSE) Ours-STF (MS-SSIM)

Ours-STF (MSE)

## Original Image

Figure 8. Visualization of the reconstructed image (kodim07.png) from Kodak dataset. The metrics are [bpp‚Üì/PNSR‚Üë/MS-SSIM‚Üë].

0.2 0.4 0.6 0.8 bpp

Our CNN Minnen2020

Figure 9. Ablation study of window-based attention. Our model is based on Minnen2020 with WAM. The RD points of Minnen2020 are obtained from [35], whose models were trained for 5M steps on the dataset consisting of 2M web images.

relate well with human perception. We found that mod-els optimized for MSE would result in blurred image, and models optimized for MS-SSIM would remove de-tails from regions with high contrast (e.g., text and salient objects), as shown in Fig.1. [39] proposed a new metric, which is learned on perceptual similarity data specific to image compression. Inspired by the mathematical defini-tion of perceptual quality in [11], [12] studied the rate-distortion-perception trade-off. For a more impartial eval-uation, perceptual quality measure are vital for practical ap-

plications(e.g.,LPIPS [47], FID [20], KID [10]).

### 5. Conclusion

In this paper, we have extensively studied the local-aware attention mechanism and have found that it is cru-cial to combine the global structure learned by neural net-works and the local texture mined by the attention unit, we have presented a flexible window-based attention module to capture correlations among spatially neighboring elements, which could work as a plug-and-play component to enhance CNN or Transformer models. Furthermore, we have pro-posed a novel Symmetrical TransFormer (STF) framework with absolute transformer blocks in both down-sampling encoder and up-sampling decoder. Extensive experimen-tal results show that the proposed methods are effective and exceed the state-of-the-art (SOTA) RD performance. In fu-ture, we will deeply explore other factors that affect the lo-cal detail reconstruction in image compression, such as the convolution kernel shaping and the normalization mode.

## Acknowledgements

This work was supported in part by the Major Project for New Generation of AI (No.2018AAA0100400) and the Na-tional Natural Science Foundation of China (No.61836014, No.U21B2042, No.62006231, No.62072457). Zou is im-mensely grateful to the comments from Fan Li that greatly improve the manuscript.

References [1] Workshop and challenge on learned image compression,

2020. https://www.compression.cc/. 5 [2] Eirikur Agustsson, Michael Tschannen, Fabian Mentzer,

Radu Timofte, and Luc Van Gool. Generative adversarial networks for extreme learned image compression. In CVPR, 2019. 3

[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-ton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 3

[4] Johannes Ball√©, Valero Laparra, and Eero P Simoncelli. Den-sity modeling of images using a generalized normalization transformation. In ICLR, 2016. 3

[5] Johannes Ball√©, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression. In ICLR, 2017. 2

[6] Johannes Ball√©, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational image compression with a scale hyperprior. In ICLR, 2018. 2, 3, 6

[7] Jean B√©gaint, Fabien Racap√©, Simon Feltman, and Akshay Pushparaja. Compressai: a pytorch library and evalua-tion platform for end-to-end compression research. arXiv preprint arXiv:2011.03029, 2020. 5, 6

[8] Fabrice Bellard. Bpg image format, 2014. https:// bellard.org/bpg/. 1

[9] Bross Benjamin, Chen Jianle, Liu Shan, and Wang Ye-Kui. Versatile video coding. In JVET, 2020. 1

[10] Miko≈Çaj Bi≈Ñkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. In ICLR, 2018. 8

[11] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In CVPR, 2018. 7, 8

[12] Yochai Blau and Tomer Michaeli. Rethinking lossy com-pression: The rate-distortion-perception tradeoff. In ICML, 2019. 7, 8

[13] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020. 2, 3

[14] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yip-ing Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In CVPR, 2021. 3

[15] Zhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro Katto. Learned image compression with discretized gaussian mixture likelihoods and attention modules. In CVPR, 2020. 2, 3, 6

[16] Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Variable rate deep image compression with a conditional autoencoder. In ICCV, 2019. 2

[17] Ze Cui, Jing Wang, Shangyin Gao, Tiansheng Guo, Yihui Feng, and Bo Bai. Asymmetric gained deep image compres-sion with continuous rate adaptation. In CVPR, 2021. 2

[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale. In ICLR, 2020. 2, 3, 4

[19] Dailan He, Yaoyan Zheng, Baocheng Sun, Yan Wang, and Hongwei Qin. Checkerboard context model for efficient learned image compression. In CVPR, 2021. 2

[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilib-rium. In NeurIPS, 2017. 8

[21] Jianhua Hu, Ming Li, Changsheng Xia, and Yundong Zhang. Combine traditional compression method with convolutional neural networks. In CVRP workshops, 2018. 2

[22] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co-variate shift. In ICML, 2015. 3

[23] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 5

[24] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In ICLR, 2014. 2

[25] Eastman Kodak. Kodak lossless true color image suite (pho-tocd pcd0992), 1993. http://r0k.us/graphics/ kodak/. 5

[26] Ivan Krasin, Tom Duerig, Neil Alldrin, Andreas Veit, Sami Abu-El-Haija, Serge Belongie, David Cai, Zheyun Feng, Vittorio Ferrari, Victor Gomes, Abhinav Gupta, Dhyanesh Narayanan, Chen Sun, Gal Chechik, and Kevin Murphy. Openimages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://github.com/openimages, 2016. 5

[27] Jooyoung Lee, Seunghyun Cho, and Munchurl Kim. An end-to-end joint learning scheme of image compression and qual-ity enhancement with improved entropy minimization. arXiv preprint arXiv:1912.12817, 2019. 2

[28] Haojie Liu, Tong Chen, Peiyao Guo, Qiu Shen, Xun Cao, Yao Wang, and Zhan Ma. Non-local attention optimized deep image compression. arXiv preprint arXiv:1904.09757, 2019. 3, 4

[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. 2, 3, 4, 5, 6

[30] Detlev Marpe, Heiko Schwarz, and Thomas Wiegand. Context-based adaptive binary arithmetic coding in the h. 264/avc video compression standard. IEEE Transactions on circuits and systems for video technology, 13(7):620‚Äì636, 2003. 2

[31] G Nigel N Martin. Range encoding: an algorithm for remov-ing redundancy from a digitised message. In Proc. Institution of Electronic and Radio Engineers International Conference on Video and Data Recording, 1979. 2

[32] Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Conditional probability models for deep image compression. In CVPR, 2018. 2

[33] Fabian Mentzer, George Toderici, Michael Tschannen, and Eirikur Agustsson. High-fidelity generative image compres-sion. arXiv preprint arXiv:2006.09965, 2020. 3

[34] David Minnen, Johannes Ball√©, and George D Toderici. Joint autoregressive and hierarchical priors for learned im-age compression. NeurIPS, 2018. 2, 3, 6

[35] David Minnen and Saurabh Singh. Channel-wise autoregres-sive entropy models for learned image compression. In ICIP, 2020. 2, 3, 4, 5, 6, 7, 8

[36] Yash Patel, Srikar Appalaraju, and R Manmatha. Deep perceptual compression. arXiv preprint arXiv:1907.08310, 2019. 7

[37] Yash Patel, Srikar Appalaraju, and R Manmatha. Human per-ceptual evaluations for image compression. arXiv preprint arXiv:1908.04187, 2019. 7

[38] Yash Patel, Srikar Appalaraju, and R Manmatha. Hierarchi-cal auto-regressive model for image compression incorporat-ing object saliency and a deep perceptual loss. arXiv preprint arXiv:2002.04988, 2020. 7

[39] Yash Patel, Srikar Appalaraju, and R Manmatha. Saliency driven perceptual image compression. In Proceedings of the IEEE/CVF WACV, 2021. 7, 8

[40] Majid Rabbani. Jpeg2000: Image compression fundamen-tals, standards and practice. Journal of Electronic Imaging, 11(2):286, 2002. 1

[41] Claude Elwood Shannon. A mathematical theory of commu-nication. ACM SIGMOBILE mobile computing and commu-nications review, 5(1):3‚Äì55, 2001. 2

[42] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv√© J√©gou. Training data-efficient image transformers & distillation through at-tention. In ICML, 2021. 2

[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 2, 3

[44] Gregory K Wallace. The jpeg still picture compression stan-dard. IEEE Transactions on Consumer Electronics, 38(1), 1992. 1

[45] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-ing He. Non-local neural networks. In CVPR, 2018. 3

[46] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Mul-tiscale structural similarity for image quality assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003. 2

[47] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-man, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018. 8

[48] Lei Zhou, Chunlei Cai, Yue Gao, Sanbao Su, and Junmin Wu. Variational autoencoder for low bit-rate image com-pression. In CVRP workshops, 2018. 2

[49] Lei Zhou, Zhenhong Sun, Xiangji Wu, and Junmin Wu. End-to-end optimized image compression with attention mecha-nism. In CVPR workshops, 2019. 3

