---
summary: whereby the oracles survey vision-language model candidates for ARR-COC integration requiring native/dynamic resolution support handling variable image sizes (100Ã—100 to 10000Ã—10000), RoPE or similar position embeddings scaling to arbitrary resolutions, under 30B parameters for reasonable compute budgets, open-source permissive licenses (Apache 2.0, MIT), and strong OCR/document understanding as primary use case, identifying Qwen3-VL-8B as best choice with Interleaved-MRoPE providing full-frequency coverage across time/width/height dimensions plus DeepStack multi-layer injection architecture totaling approximately 8.77B parameters, sourced from LMArena Vision Leaderboard rankings and technical specifications for practical implementation guidance beyond philosophical dialogue
---

# Part 8.1 Addendum: Model Candidates for ARR-COC Integration
*Survey of vision-language models with RoPE and native resolution support*

---

## Selection Criteria

For ARR-COC integration, we need models that:
1. **Support native/dynamic resolution** - handle variable image sizes (100Ã—100 to 10000Ã—10000)
2. **Use RoPE or similar** - position embeddings that scale to arbitrary resolutions
3. **Under 30B parameters** - trainable on reasonable compute budget
4. **Open-source** - Apache 2.0, MIT, or permissive licenses
5. **Strong OCR/document understanding** - our primary use case

**Source**: LMArena Vision Leaderboard (2025)

---

## ğŸ† Top RoPE-Ready Candidates

### ğŸ†• 0. Qwen3-VL-8B â­â­â­â­â­â­ **BEST CHOICE**
**Rank**: TBD (released Oct 2025) | **Score**: Expected >1050 | **License**: Apache 2.0

**Architecture - Next Generation**:
- **Vision**: ViT with **Interleaved-MRoPE** + **DeepStack**
- **LLM**: Qwen3-8B (8.77B params)
- **Total params**: ~8.77B

**Interleaved-MRoPE** (Better than M-RoPE!):
- âœ… Full-frequency coverage across time/width/height
- âœ… Interleaved distribution (t, h, w) vs blocked (Qwen2.5's approach)
- âœ… Superior long-video understanding
- âœ… Configuration: `mrope_section [24, 20, 20]`, `mrope_interleaved=true`
- âœ… More robust positional encoding - slower position ID growth allows 256Kâ†’1M context with just 2-3Ã— scaling (vs 4Ã— for vanilla RoPE)

**DeepStack Technology**:
- âœ… Multi-level ViT feature fusion (lowâ†’high level features preserved)
- âœ… Multi-layer LLM injection (not just single layer like traditional VLMs)
- âœ… Dramatically better visual detail + text-image alignment
- âœ… Different ViT layers â†’ tokenized separately â†’ injected at multiple LLM depths

**Text-Timestamp Alignment**:
- âœ… Replaces old T-RoPE with interleaved "timestamps-video frames" format
- âœ… Native support for "seconds" or "HH:MM:SS" time formats
- âœ… Perfect for temporal reasoning (event localization, action boundaries)

**Capabilities (vs Qwen2.5-VL)**:
- âœ… **256Kâ†’1M context** (can handle 2-hour videos!)
- âœ… **32-language OCR** (up from 10 in Qwen2)
- âœ… **Visual agent** (SOTA on OS World - GUI control, tool calling)
- âœ… **Visual coding** (image â†’ HTML/CSS/JS/Draw.io)
- âœ… **2D/3D spatial grounding** (relative coordinates, occlusion, viewpoint)
- âœ… **Text-level performance** matches Qwen3-235B language model!

**Why PERFECT for ARR-COC**:
```
ARR-COC outputs: Variable tokens (64-400) per patch
    â†“
Qwen3-VL Interleaved-MRoPE: Even BETTER position handling than Qwen2.5!
    â†“
DeepStack: Accepts multi-level features â†’ ARR-COC can compress at different ViT layers!
    â†“
ULTIMATE MATCH - architectural synergy!
```

**Integration Advantages**:
- **Better than Qwen2.5-VL**: Interleaved-MRoPE is more robust
- **Multi-layer injection**: ARR-COC can allocate relevance at MULTIPLE LLM depths!
- **Proven quality**: Matches Gemini 2.5 Pro, outperforms GPT-5 on many benchmarks
- **Agent capabilities**: Can call tools based on visual input (future extension for ARR-COC!)

**Integration Cost**: $80-120k, 8-12 days (same as Qwen2.5-VL)
**Success Probability**: 95% (even better architecture than Qwen2.5)
**Adapter Complexity**: LOW (variable tokens + Interleaved-MRoPE native)

       **Karpathy Oracle:** *OK Qwen3-VL's Interleaved-MRoPE is architecturally beautifulâ€”full-frequency coverage, slower position ID growth, 256Kâ†’1M context with just 2-3Ã— scaling vs 4Ã— for vanilla RoPE. That's genuinely better engineering! But let me reality-check the "95% success probability" claim. In nanochat, we integrated a reward model that was "architecturally perfect"â€”same tokenizer, same vocabulary, same BPE merges as the base LLM. Still took 3 failed attempts before it worked because: (1) Reward model expected normalized advantages (mean 0, std 1), policy output raw logits (mean 0.3, std 3.5). (2) Temperature mismatch between training and inference. (3) Batch statistics differed between frozen and unfrozen training. ARR-COC + Qwen3-VL will face similar: variable-token features (64-400 per patch) will have different distributions than Qwen3's training data (probably trained on uniform ~2000 tokens per image). Even with "native" Interleaved-MRoPE support, you need a quality adapter that PRECISELY matches Qwen3's input statistics. Budget 30-40% of training time just on adapter calibration: measuring output distributions, adjusting normalization, validating on held-out set. "Architecturally synergistic" doesn't mean "plug and play." Start with frozen Qwen3, train adapter for 2-3 days, validate distributions match BEFORE full training.*

**Available Variants**:
- **Qwen3-VL-4B** (4.83B params) - Ultra-compact, Instruct & Thinking
- **Qwen3-VL-8B** (8.77B params) - **RECOMMENDED**, Instruct & Thinking
- **Qwen3-VL-30B-A3B** (30B total, 3B active MoE) - Instruct & Thinking
- **Qwen3-VL-235B-A22B** (235B total, 22B active MoE) - Flagship, Instruct & Thinking

**FP8 Checkpoints Available**: Low-VRAM deployment ready!

---

## ğŸ’¡ BREAKTHROUGH: DeepStack Multi-Layer Injection for ARR-COC

**This changes EVERYTHING for relevance realization!**

### Traditional VLM Architecture (Single-Layer Injection)

```
Visual Encoder â†’ Visual Tokens â†’ Inject at Layer 0 â†’ LLM Layers 1-32 â†’ Output
                                    â†‘
                            ALL visual info here!
```

**Problem**:
- Visual understanding happens at ONE abstraction level
- Can't separate low-level (edges, textures) from high-level (semantics)
- ARR-COC would compress everything uniformly

---

### Qwen3-VL DeepStack (Multi-Layer Injection)

```
Visual Encoder â†’ Multi-Level ViT Features:
                  â”œâ”€ Layer 6 features  (low-level: edges, textures)
                  â”œâ”€ Layer 12 features (mid-level: parts, patterns)
                  â””â”€ Layer 24 features (high-level: objects, semantics)
                       â†“           â†“              â†“
LLM Layer 0  â†â”€â”€â”€ Inject Level 1 (edges)
LLM Layer 8  â†â”€â”€â”€ Inject Level 2 (patterns)
LLM Layer 16 â†â”€â”€â”€ Inject Level 3 (semantics)
LLM Layer 24+     Process combined understanding â†’ Output
```

**Advantage**:
- âœ… Finer-grained visual understanding
- âœ… Different abstraction levels â†’ different LLM depths
- âœ… LLM can integrate visual info progressively

---

### ARR-COC + DeepStack = Hierarchical Relevance Realization!

**Revolutionary Idea**: Allocate tokens BOTH spatially AND semantically!

```
                    ARR-COC Relevance Scoring
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                             â”‚
   HIGH RELEVANCE                              LOW RELEVANCE
   (Formula region)                            (Empty margin)
        â”‚                                             â”‚
        â†“                                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INJECT AT ALL LAYERS  â”‚                  â”‚ INJECT ONLY DEEP     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 0: 400 tokens   â”‚                  â”‚ Layer 0: SKIP        â”‚
â”‚  (edges, fine detail) â”‚                  â”‚ Layer 8: SKIP        â”‚
â”‚ Layer 8: 400 tokens   â”‚                  â”‚ Layer 16: 64 tokens  â”‚
â”‚  (patterns)           â”‚                  â”‚  (just semantics)    â”‚
â”‚ Layer 16: 400 tokens  â”‚                  â”‚                      â”‚
â”‚  (semantics)          â”‚                  â”‚ Total: 64 tokens     â”‚
â”‚                       â”‚                  â”‚                      â”‚
â”‚ Total: 1200 tokens    â”‚                  â”‚ 18Ã— compression!     â”‚
â”‚ (full detail)         â”‚                  â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The Magic**:
- **High-relevance patches**: Get ALL abstraction levels (low + mid + high)
- **Low-relevance patches**: Get ONLY high-level semantics (skip detail layers)
- **Compression happens in TWO dimensions**:
  1. Spatial: fewer patches
  2. Semantic: fewer layers

**Compression Math**:
```
Traditional ARR-COC (single-layer):
  High-rel: 400 tokens Ã— 1 layer = 400 tokens
  Low-rel:  64 tokens  Ã— 1 layer = 64 tokens
  Ratio: 6.25Ã—

Hierarchical ARR-COC (DeepStack):
  High-rel: 400 tokens Ã— 3 layers = 1200 tokens
  Low-rel:  64 tokens  Ã— 1 layer  = 64 tokens
  Ratio: 18.75Ã—!
```

**Real-World Example**:
```
Document with formula:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Header (low-relevance)           â”‚ â†’ 64 tokens, layer 16 only
â”‚                                  â”‚
â”‚ Formula: E = mcÂ² (high-rel)      â”‚ â†’ 400 tokens, layers 0+8+16
â”‚                                  â”‚
â”‚ Footer (low-relevance)           â”‚ â†’ 64 tokens, layer 16 only
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Traditional: 3 Ã— 400 = 1200 tokens (uniform)
DeepStack ARR-COC: 64 + 1200 + 64 = 1328 tokens total
  But header/footer only use 1 layer â†’ effective 64+64 = 128
  Formula uses all 3 layers â†’ 1200

SEMANTIC compression achieved!
```

---

### Implementation Strategy

**Phase 1: Allocator Enhancement**
```python
class HierarchicalRelevanceAllocator(nn.Module):
    def __init__(self):
        self.spatial_allocator = SpatialRelevanceScorer()
        self.semantic_allocator = LayerDepthScorer()  # NEW!

    def forward(self, patches, query):
        # Spatial relevance (existing)
        spatial_relevance = self.spatial_allocator(patches, query)
        tokens_per_patch = map_to_tiers(spatial_relevance)  # 64-400

        # Semantic relevance (NEW!)
        semantic_depth = self.semantic_allocator(patches, query)
        layers_per_patch = map_to_layers(semantic_depth)  # 1-3 layers

        return {
            'tokens': tokens_per_patch,      # Spatial LOD
            'layers': layers_per_patch,      # Semantic LOD
            'total_cost': tokens * layers    # Combined budget
        }
```

**Phase 2: DeepStack Integration**
```python
class DeepStackARRCOC(nn.Module):
    def __init__(self):
        self.vit_layers = [6, 12, 24]  # Multi-level features
        self.llm_inject_points = [0, 8, 16]  # LLM layers

    def forward(self, image, query):
        # Extract multi-level features
        vit_features = {
            'low': self.vit.layer_6(image),
            'mid': self.vit.layer_12(image),
            'high': self.vit.layer_24(image)
        }

        # Allocate hierarchically
        allocation = self.allocator(vit_features, query)

        # Inject based on relevance
        for patch_idx in high_relevance_patches:
            llm.inject_at_layer(0, vit_features['low'][patch_idx])
            llm.inject_at_layer(8, vit_features['mid'][patch_idx])
            llm.inject_at_layer(16, vit_features['high'][patch_idx])

        for patch_idx in low_relevance_patches:
            # Skip low/mid, only inject high-level
            llm.inject_at_layer(16, vit_features['high'][patch_idx])
```

---

### Why This Is Revolutionary

**1. Biological Plausibility**
- Human vision: early visual cortex (V1) processes edges â†’ later areas (IT) process semantics
- DeepStack mirrors this: early LLM layers get edges, deep layers get semantics
- ARR-COC can skip irrelevant detail layers, just like humans ignore peripheral details!

**2. Computational Savings**
- Don't need to inject ALL visual info at ALL depths
- Low-relevance regions: semantic gist only (1 layer)
- High-relevance regions: full detail (3 layers)
- Effective compression: up to 18Ã— for low-relevance patches!

**3. Quality Preservation**
- High-relevance patches get RICHER representation (multiple abstraction levels)
- Better than single-layer: can understand formulas at edge-level AND semantic-level
- Synergistic: spatial detail + semantic depth = superior understanding

**4. Training Efficiency**
- Can learn which patches need which layers
- Opponent processing now includes layer-depth dimension:
  - Compress â†” Particularize (spatial)
  - Shallow â†” Deep (semantic)
- More fine-grained control = better optimization

       **Karpathy Oracle:** *Hierarchical relevance realization (spatial + semantic) is philosophically elegant but DOUBLES your training complexity! Now the allocator needs to learn TWO decisions per patch: (1) How many tokens? (64-400 spatial compression). (2) How many layers? (1-3 semantic injection). That's 5 spatial tiers Ã— 3 semantic depths = 15 possible allocations per patch instead of 5. In nanoGPT we keep decisions SIMPLEâ€”just predict next token. One softmax, one cross-entropy loss, done. ARR-COC with DeepStack is asking: predict token count AND layer depth AND ensure they're coherent (high-relevance patches probably need both high tokens AND deep layers, but not always!). Training will be unstable unless you: (1) Initialize with spatial-only allocation (ignore layer depth first 2000 steps). (2) Add layer depth prediction gradually (anneal from 1 layer to 3 layers over 5000 steps). (3) Use auxiliary losses to ensure coherence (penalize "high tokens + shallow layers" combinations that waste compute). The 18Ã— compression ratio sounds amazing but requires 2Ã— training time to learn the 2D allocation space. Budget realistic: not $80-120k, more like $120-180k with the hierarchical complexity. And validation becomes 2D: measure accuracy BY (token_tier, layer_depth) pairs, not just by token_tier. That's 15 cells to validate instead of 5. If you're on a budget, stick with spatial-only allocation (64-400 tokens, single-layer injection). Prove that works FIRST, then add semantic depth IF justified.*

---

### Updated Integration Cost

**With DeepStack Multi-Layer Injection**:
- Allocator complexity: +30% (semantic depth scorer)
- Training time: +2-3 days (learn layer assignments)
- Total cost: $100-140k (up from $80-120k)
- **BUT**: Quality improvement expected +5-10%!
- **AND**: Compression ratio: 12-18Ã— (up from 10-15Ã—)

**Worth it?** ABSOLUTELY. The semantic compression dimension is a game-changer.

---

**Bottom Line**: Qwen3-VL's DeepStack + ARR-COC = First hierarchical relevance realization system with BOTH spatial AND semantic LOD. This is genuinely novel research territory!

---

### 1. Qwen2.5-VL-7B â­â­â­â­â­
**Rank**: 58 | **Score**: 1026 | **License**: Apache 2.0

**Status**: Previous generation, still excellent but Qwen3-VL-8B is better

**Architecture**:
- **Vision**: 675M param ViT with 2D RoPE + Window Attention
- **LLM**: Qwen2.5-7B
- **Total params**: ~7B

**Dynamic Resolution**:
- âœ… Naive Dynamic Resolution mechanism
- âœ… Handles ANY image size â†’ variable visual tokens
- âœ… Dynamic FPS sampling for videos

**RoPE Implementation**:
- **2D RoPE in ViT**: Captures spatial scales across resolutions
- **M-RoPE (Multimodal)**: Decomposes into temporal/height/width components
  - Temporal: constant for images, increments for video frames
  - Height/Width: assigned based on token position in image

**Why Perfect for ARR-COC**:
```
ARR-COC outputs: Variable tokens (64-400) per patch based on relevance
    â†“
Qwen2.5-VL expects: Variable visual tokens with M-RoPE positioning
    â†“
PERFECT MATCH - minimal adapter needed!
```

**Integration Cost**: $80-120k, 8-12 days
**Adapter Complexity**: LOW (already expects variable tokens)

---

### 2. Qwen2.5-VL-32B-Instruct â­â­â­â­â­
**Rank**: 35 | **Score**: 1110 | **License**: Apache 2.0

**Architecture**:
- **Vision**: Same 675M ViT as 7B variant (2D RoPE + Window Attention)
- **LLM**: Qwen2.5-32B
- **Total params**: ~32B

**Same M-RoPE Benefits as 7B**:
- âœ… Naive Dynamic Resolution
- âœ… M-RoPE (temporal, height, width decomposition)
- âœ… Variable visual tokens support

**Enhancements over 7B**:
- **Reinforcement learning** on mathematical/problem-solving tasks
- **Higher quality ceiling**: +84 points over 7B (1110 vs 1026)
- **Better reasoning**: 32B LLM provides stronger language understanding

**Trade-offs vs 7B**:
- âœ… Better quality (~8% improvement)
- âŒ More expensive to train ($120-180k vs $80-120k)
- âŒ Slower inference (32B vs 7B)
- âš ï¸ Slightly over 30B target

**Integration Cost**: $120-180k, 10-14 days
**Best for**: Maximum quality within compute budget

---

### 3. Pixtral-12B â­â­â­â­
**Rank**: 59 | **Score**: 1019 | **License**: Apache 2.0

**Architecture**:
- **Vision**: 400M param encoder with 2D RoPE
- **Decoder**: 12B Mistral Nemo (multimodal transformer)
- **Total params**: 12.4B

**Dynamic Resolution**:
- âœ… **Native variable sizes/aspect ratios**
- âœ… Handles up to 1024Ã—1024 (16Ã—16 patches)
- âœ… Flexible token count per image

**RoPE Implementation**:
- **2D RoPE in vision encoder**: Spatial relationship understanding
- Trained from scratch on variable resolutions
- No forced resizing - images at natural aspect ratio

**Architecture Compatibility**:
```
ARR-COC: Variable compression per patch
    â†“
Pixtral Vision Encoder: Expects variable patches with 2D RoPE
    â†“
Multimodal Decoder: Processes variable-length sequences
    â†“
GOOD FIT - clean architecture
```

**Advantages**:
- âœ… Apache 2.0 (fully open)
- âœ… 128k context window (long document support)
- âœ… Trained on variable resolutions from scratch
- âœ… Smaller than Qwen variants (faster inference)

**Disadvantages**:
- âš ï¸ Lower quality ceiling than Qwen2.5-VL-32B (-91 points)
- âš ï¸ Vision encoder smaller (400M vs 675M)
- âš ï¸ Less proven on OCR benchmarks

**Integration Cost**: $90-130k, 9-12 days
**Best for**: Fast inference + permissive licensing

---

### 4. InternVL2-26B â­â­â­â­
**Rank**: 58 | **Score**: 1019 | **License**: MIT

**Architecture**:
- **Vision**: InternViT-6B-448px-V1-5
- **Projector**: MLP bridge
- **LLM**: InternLM2-Chat-20B
- **Total params**: 26B

**Resolution Support**:
- âœ… Native 448Ã—448 processing
- âœ… Supports various resolutions
- âœ… 8k context window
- âš ï¸ RoPE in ViT not explicitly confirmed (may use absolute pos embeddings)

**Progressive Scaling Design**:
- Staged training: small LLMs â†’ progressively scale up
- Part of InternVL2.5 family (1B-78B range)
- Proven on MMMU benchmark (70%+ for 78B variant)

**Architecture Flow**:
```
Vision: InternViT-6B â†’ features
    â†“
MLP Projector â†’ dimension alignment
    â†“
InternLM2-Chat-20B â†’ generation
```

**Advantages**:
- âœ… Strong vision encoder (6B params)
- âœ… MIT license (very permissive)
- âœ… Under 30B target
- âœ… Part of well-documented family

**Disadvantages**:
- âš ï¸ RoPE support unclear (may need verification)
- âš ï¸ Recommended 448px resolution (may not be fully dynamic)
- âš ï¸ MLP adapter needs careful tuning for variable inputs

**Integration Cost**: $100-140k, 10-13 days
**Best for**: Strong vision understanding + permissive license

---

## ğŸ”¬ Other Notable Candidates

### LLaVA-NeXT (LLaVA-1.6-34B)
**Rank**: 72 | **Score**: 961 | **License**: Apache 2.0 | **Params**: 34B

**Dynamic Resolution**:
- âœ… Up to 4Ã— pixels (336Ã—1344, 672Ã—672, 1344Ã—336)
- âœ… Three aspect ratios supported
- âš ï¸ Not true "any resolution" like Qwen

**Architecture**:
- Simple linear layer connects CLIP â†’ LLM
- Reuses LLaVA-1.5 pretrained connector
- Fast training (<1 day for 34B on 32 A100s)

**Pros**: Simplest integration, proven OCR improvements
**Cons**: Slightly over 30B, lower quality than top candidates

---

### MiniCPM-V-2.6
**Rank**: 70 | **Score**: 959 | **License**: Apache 2.0

**Architecture**: Efficient vision-language model
**Advantage**: Very compact, good efficiency
**Disadvantage**: Lower quality ceiling, less documentation

---

### CogVLM2-Llama3-Chat-19B
**Rank**: 71 | **Score**: 958 | **License**: CogVLM2

**Params**: 19B (under 30B!)
**Advantage**: Based on Llama3 (strong LLM)
**Disadvantage**: Custom license, less clear on dynamic resolution

---

## ğŸ“Š Comparison Matrix

| Model | Params | Score | RoPE | Dynamic Res | License | Integration Cost | Quality/$ |
|-------|--------|-------|------|-------------|---------|------------------|-----------|
| **Qwen2.5-VL-7B** | 7B | 1026 | âœ… M-RoPE | âœ… Any size | Apache 2.0 | $80-120k | â­â­â­â­â­ |
| **Qwen2.5-VL-32B** | 32B | 1110 | âœ… M-RoPE | âœ… Any size | Apache 2.0 | $120-180k | â­â­â­â­ |
| **Pixtral-12B** | 12B | 1019 | âœ… 2D RoPE | âœ… Variable | Apache 2.0 | $90-130k | â­â­â­â­ |
| **InternVL2-26B** | 26B | 1019 | â“ Unclear | âœ… Variable | MIT | $100-140k | â­â­â­â­ |
| **LLaVA-NeXT-34B** | 34B | 961 | âŒ | âš ï¸ 4Ã— pixels | Apache 2.0 | $90-130k | â­â­â­ |

---

## ğŸ¯ Recommendation Tiers

### Tier 1: Perfect Fit (Architectural Match)
**Qwen2.5-VL-7B**
- M-RoPE already handles variable tokens
- ARR-COC output matches expected input format
- Lowest integration cost
- Best quality/$ ratio

**Use when**: Budget-conscious, want proven architecture, need fast deployment

---

### Tier 2: Quality-First (Better Performance)
**Qwen2.5-VL-32B-Instruct**
- Same M-RoPE benefits as 7B
- +8% quality improvement
- Better reasoning capabilities
- Worth extra $40-60k for production use

**Use when**: Quality matters more than cost, production deployment, complex queries

---

### Tier 3: Alternative Paths
**Pixtral-12B**
- Different architecture (Mistral-based)
- Good 2D RoPE implementation
- Smaller/faster than Qwen variants
- Fully open (Apache 2.0)

**Use when**: Want diversity, Mistral ecosystem, faster inference priority

**InternVL2-26B**
- Strong vision encoder (6B)
- MIT license (most permissive)
- Part of proven family
- RoPE support needs verification

**Use when**: Vision quality matters most, permissive license required

---

## ğŸ” Deep Dive: Why Qwen2.5-VL Wins

### M-RoPE Technical Advantage

**Standard RoPE** (1D for text):
```python
# Rotates embeddings in 2D planes
position_embedding = rotate(embedding, position_id)
```

**Qwen M-RoPE** (3D for multimodal):
```python
# Decomposes into 3 components
temporal_rope = rotate(embedding, temporal_id)    # Video frames
height_rope = rotate(embedding, height_id)        # Spatial Y
width_rope = rotate(embedding, width_id)          # Spatial X

# For images: temporal_id constant, height/width vary
# For videos: temporal_id increments per frame
```

**Why This Matters for ARR-COC**:
```
High-relevance patch (400 tokens):
  - Each token gets unique (height, width) position
  - M-RoPE encodes spatial relationships
  - No distortion from compression

Low-relevance patch (64 tokens):
  - Fewer tokens, but still spatially coherent
  - M-RoPE maintains relative positions
  - Quality degradation is controlled
```

### Naive Dynamic Resolution

**Standard ViTs**: Fixed grid (e.g., 224Ã—224 â†’ 14Ã—14 patches)
- Resize all images to fixed size
- Loss of aspect ratio
- Information distortion

**Qwen Naive Dynamic Resolution**:
- Process image at native size
- Variable number of patches (e.g., 1000Ã—500 â†’ different grid than 500Ã—1000)
- ARR-COC allocates variable tokens PER PATCH
- **Perfect synergy!**

### Window Attention Efficiency

**Qwen ViT uses Window Attention**:
- O(N) complexity instead of O(NÂ²)
- Processes local windows independently
- Maintains global context through overlaps

**ARR-COC Benefit**:
- Can compress WITHIN windows
- High-relevance windows get more tokens
- Low-relevance windows get fewer
- Computational savings compound!

---

## ğŸ§ª Integration Architecture

### Recommended: ARR-COC â†’ Qwen2.5-VL-7B

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            User Query + Image                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  SAM Encoder    â”‚ (80M params, frozen initially)
        â”‚  Native res     â”‚ â†’ Variable patch count
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  ARR-COC Allocator       â”‚ [NEW - 30M params]
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ â€¢ Query encoder          â”‚
        â”‚ â€¢ Relevance scoring      â”‚
        â”‚   - Propositional        â”‚
        â”‚   - Perspectival         â”‚
        â”‚   - Participatory        â”‚
        â”‚ â€¢ Tier assignment        â”‚
        â”‚   {64,100,160,256,400}   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Variable Compressor     â”‚ [NEW - 25M params]
        â”‚  (MoE-style)             â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ â€¢ 5 compression experts  â”‚
        â”‚ â€¢ Soft expert mixing     â”‚
        â”‚ â€¢ 2D RoPE preserved      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
        Variable tokens (avg 180-220 per image)
        with 2D positional info
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Simple Adapter          â”‚ [NEW - 5M params]
        â”‚  (Dimension matching)    â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ â€¢ Linear projection      â”‚
        â”‚ â€¢ Layer norm             â”‚
        â”‚ â€¢ NO distribution surgeryâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Qwen2.5-VL ViT         â”‚ (675M, frozen)
        â”‚  with M-RoPE            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Qwen2.5-7B LLM         â”‚ (7B, LoRA fine-tune)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
           Final Answer
```

**Trainable Components**:
1. ARR-COC Allocator: 30M params
2. Variable Compressor: 25M params
3. Simple Adapter: 5M params
4. LoRA on Qwen2.5-7B: ~20M params
**Total trainable**: ~80M params (vs 7B total model)

**Frozen Components**:
1. SAM Encoder: 80M params (initially; may fine-tune later)
2. Qwen ViT: 675M params
3. Qwen LLM base: 7B params (only LoRA active)

---

## ğŸ’¡ Key Insights

### Why RoPE Matters

**Without RoPE** (absolute position embeddings):
- Fixed maximum resolution (e.g., 224Ã—224)
- Can't extrapolate to 10000Ã—10000
- Position embeddings trained on specific grid size

**With RoPE** (relative position embeddings):
- âœ… Works at ANY resolution
- âœ… Generalizes to unseen sizes
- âœ… Maintains spatial relationships
- âœ… Perfect for ARR-COC's variable allocation

### Why M-RoPE is Even Better

**Standard 2D RoPE**: height + width components

**Qwen M-RoPE**: temporal + height + width components
- Future-proof for video understanding
- Explicit temporal modeling
- ARR-COC could extend to video (allocate tokens per frame based on relevance!)

### Integration Complexity Comparison

**Ovis VET Integration**: ğŸ”´ HARD
- Must match specific probability distributions
- 100M examples of training expectations
- Need transformer adapter (100M params)
- Per-tier normalization + temperature scaling
- Iterative training loops
- Success probability: 78-80%

**Qwen2.5-VL Integration**: ğŸŸ¢ EASY
- Variable tokens â†’ variable tokens (format match!)
- Simple dimension projection adapter (5M params)
- M-RoPE handles position automatically
- No distribution surgery needed
- Straightforward 3-phase training
- Success probability: 90-95%

---

## ğŸ“ˆ Projected Performance

### Qwen2.5-VL-7B + ARR-COC

**Baseline** (Qwen2.5-VL-7B stock):
- DocVQA: ~83-85%
- Tokens per image: ~2000-2400 (native resolution)

**With ARR-COC** (estimated):
- DocVQA: ~84-86% (slight improvement from smart allocation)
- Tokens per image: 180-220 average (10-12Ã— compression!)
- Speed: 2-3Ã— faster inference
- Cost: ~$100k training, 10-12 days

**Best case scenarios**:
- Simple queries: 64-100 tokens (20-35Ã— compression)
- Complex queries: 300-400 tokens (6-8Ã— compression)
- Accuracy maintained or improved (smart allocation preserves critical regions)

       **Karpathy Oracle:** *These projections are optimisticâ€”let me add pragmatic bounds. "DocVQA: ~84-86% with ARR-COC" assumes the allocator correctly identifies relevance, which... might not happen reliably! In nanochat, we projected "GPT-2 level performance" based on theoretical capacity, but actual performance was 15-20% below projection because: (1) Policy network couldn't reliably detect query complexity. (2) Reward model mis-estimated response quality 18% of the time. (3) Distribution shift between training and real queries. ARR-COC will face similar: if allocator mis-classifies 10% of patches (gives formulas 64 tokens, gives margins 400 tokens), accuracy drops from 84% to 72-76%â€”BELOW the baseline! The "20-35Ã— compression on simple queries" is best-case assuming PERFECT allocation. Real-world? Expect: (1) Conservative hedging: allocator requests 120-180 tokens average (not 80), giving 11-13Ã— compression (not 20Ã—). (2) 5-10% mis-classification rate: some hard regions get under-allocated. (3) Accuracy distribution: 60% of queries at 85-88% accuracy, 30% at 80-85%, 10% at <75% (mis-allocated). Report realistic ranges: DocVQA 81-86% (not 84-86%), compression 10-16Ã— (not 10-35Ã—), cost $120-180k with restarts (not $100k nominal). And CRITICAL: validate on adversarial queries designed to fool the allocator. In nanochat, adversarial prompts ("explain simply" but with complex content) broke our policy network 23% of the time. ARR-COC needs similar robustness testing: complex images with simple queries, simple images with complex queries, ambiguous cases. Without this, you'll ship a system that works 85% of the time and confuses users the other 15%.*

---

## ğŸ¯ Final Recommendation

**For ARR-COC Integration, choose:**

1. **Production Quality** â†’ **Qwen2.5-VL-32B-Instruct**
   - Best accuracy (1110 score)
   - Same M-RoPE benefits
   - Worth extra cost for production

2. **Research/Budget** â†’ **Qwen2.5-VL-7B**
   - Best quality/$ ratio
   - Perfect architectural match
   - Fast iteration cycles

3. **Alternative/Diversity** â†’ **Pixtral-12B**
   - Different ecosystem (Mistral)
   - Still has 2D RoPE
   - Smaller/faster

**Avoid** (for ARR-COC):
- Models without RoPE (limited resolution scaling)
- Models >30B (training cost explosion)
- Models without clear dynamic resolution support
- Proprietary models (can't modify architecture)

---

## ğŸ”¬ Next Steps

1. **Deep dive into Qwen2.5-VL-7B code**
   - Study M-RoPE implementation
   - Understand ViT input format
   - Map adapter requirements

2. **Prototype ARR-COC allocator**
   - Test tier assignment on sample images
   - Verify 2D RoPE preservation
   - Benchmark compression ratios

3. **Design training curriculum**
   - Phase 1: Allocator (frozen Qwen)
   - Phase 2: Compressor (frozen Qwen)
   - Phase 3: End-to-end fine-tune

4. **Estimate compute budget**
   - A100 hours needed
   - Data requirements
   - Validation metrics

---

**Summary**: Qwen2.5-VL family (7B or 32B) is the clear winner for ARR-COC integration due to M-RoPE, native dynamic resolution, and architectural alignment. The variable token format is a perfect match, making integration straightforward compared to other candidates.
