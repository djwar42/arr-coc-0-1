# Platonic Dialogue 92: The Pentti Brew

**Or: The Deep Technical Dive - SDM, Transformers, Cerebellum, and the Catalogue as Living Memory**

*In which Pentti Kanerva, Andrej Karpathy, Socrates, and Theaetetus go DEEP on the technical architecture of Sparse Distributed Memory, exploring cerebellar cortex biology, GPU texture caching, sparse attention mechanisms (Longformer, BigBird), and building the complete research roadmap to fuse SDM with Transformers - with the catalogue meter as the proof-of-principle that Pentti's 1988 vision finally has the tools to be realized.*

---

## Setting: The Technical Deep Dive - Post Latte

*[After the emotional Pentti Latte celebration, the team retreats to a quieter space with whiteboards, laptops, and research papers. This is where the REAL work happens. Pentti has his 1988 SDM book. Karpathy has PyTorch open. Socrates and Theaetetus are ready to ask the hard questions.]*

---

## Part I: THE BIOLOGICAL BLUEPRINT - CEREBELLAR CORTEX

**PENTTI:** *[opening his 1988 book]*

Before we write code, we must understand the **biological architecture**.

The cerebellum is where I discovered Sparse Distributed Memory.

**KARPATHY:**

What's the structure?

**PENTTI:** *[drawing on whiteboard]*

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  CEREBELLAR CORTEX ARCHITECTURE (The Biological SDM)
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘
â•‘  INPUT LAYER: Granule Cells
â•‘  â”œâ”€ ~50 BILLION granule cells (most numerous neurons in brain!)
â•‘  â”œâ”€ Receive sensory input via mossy fibers
â•‘  â””â”€ Expand input into high-dimensional sparse code
â•‘
â•‘  PARALLEL FIBERS: High-Dimensional Address Space
â•‘  â”œâ”€ Each granule cell sends parallel fiber through molecular layer
â•‘  â”œâ”€ Parallel fibers = "addresses" in SDM
â•‘  â”œâ”€ Cross MANY Purkinje cell dendrites (thousands!)
â•‘  â””â”€ Create distributed representation
â•‘
â•‘  OUTPUT LAYER: Purkinje Cells
â•‘  â”œâ”€ ~15 MILLION Purkinje cells
â•‘  â”œâ”€ Each receives ~150,000-200,000 parallel fiber inputs!
â•‘  â”œâ”€ Integrate inputs via dendritic tree
â•‘  â”œâ”€ Output to deep cerebellar nuclei
â•‘  â””â”€ These are the "hard locations" in SDM!
â•‘
â•‘  THE COMPUTATION:
â•‘  â”œâ”€ Input â†’ Granule cells expand (sparse activation)
â•‘  â”œâ”€ Parallel fibers = high-dim address
â•‘  â”œâ”€ Purkinje cells NEAR that address activate
â•‘  â”œâ”€ Weighted sum of activated Purkinje outputs
â•‘  â””â”€ Result = motor command or prediction
â•‘
â•‘  THE NUMBERS:
â•‘  50 billion â†’ 15 million = 3300:1 compression
â•‘  But information is DISTRIBUTED across all active Purkinje cells!
â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**SOCRATES:**

So the granule cells EXPAND the input into a high-dimensional space?

That seems... counterintuitive?

**PENTTI:**

It's the **key insight** of SDM!

You expand into high dimensions to get **sparsity**.

In high dimensions, random vectors are almost orthogonal.

This gives you separability!

---

**THEAETETUS:**

Wait. 50 billion granule cells but only 15 million Purkinje cells?

How is that not just... losing information?

**PENTTI:**

Because the information is **distributed**!

```python
# SINGLE PURKINJE CELL: Limited capacity
purkinje_single = {
    "inputs": 150_000,  # parallel fibers
    "output": 1,        # single spike train
    "capacity": "low"   # can't store much alone
}

# ALL PURKINJE CELLS TOGETHER: Massive capacity
purkinje_population = {
    "cells": 15_000_000,
    "total_inputs": 15_000_000 * 150_000,  # = 2.25 TRILLION synapses!
    "distributed_storage": "across all cells",
    "capacity": "ENORMOUS"  # distributed = powerful
}

# THE MAGIC:
# Each memory is stored across MANY Purkinje cells
# Each Purkinje cell participates in MANY memories
# OVERLAP = generalization
# SEPARATION (high-dim) = specificity
```

**KARPATHY:**

Oh shit.

The 3300:1 "compression" is actually **expansion** into distributed code!

**PENTTI:** *[smiling]*

Exactly.

The cerebellum is a **content-addressable memory** using sparse distributed codes.

---

## Part II: THE SDM ALGORITHM - MATHEMATICAL FORMULATION

**KARPATHY:**

Okay. Show me the math.

**PENTTI:** *[writing equations]*

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  SPARSE DISTRIBUTED MEMORY - FORMAL ALGORITHM
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘
â•‘  SETUP:
â•‘  â”œâ”€ N = number of hard locations (like Purkinje cells)
â•‘  â”œâ”€ D = dimensionality of address space (high! eg 1000+ bits)
â•‘  â”œâ”€ Each hard location i has:
â•‘  â”‚   â”œâ”€ Address: a_i âˆˆ {0,1}^D (random binary vector)
â•‘  â”‚   â””â”€ Content: c_i âˆˆ â„^D (learned data vector)
â•‘  â””â”€ Hamming distance: d_H(x,y) = number of bits that differ
â•‘
â•‘  WRITE OPERATION: write(address, data)
â•‘  â”œâ”€ Given: query address x âˆˆ {0,1}^D
â•‘  â”œâ”€ Given: data to store d âˆˆ â„^D
â•‘  â”œâ”€ For each hard location i:
â•‘  â”‚   â”œâ”€ If d_H(x, a_i) < threshold:  â† "close enough"
â•‘  â”‚       â””â”€ c_i â† c_i + d            â† accumulate!
â•‘  â””â”€ Result: data distributed across nearby hard locations
â•‘
â•‘  READ OPERATION: read(address)
â•‘  â”œâ”€ Given: query address x âˆˆ {0,1}^D
â•‘  â”œâ”€ Find activated locations:
â•‘  â”‚   â””â”€ S = {i : d_H(x, a_i) < threshold}
â•‘  â”œâ”€ Weighted average of contents:
â•‘  â”‚   â””â”€ output = (1/|S|) Î£_{iâˆˆS} c_i
â•‘  â””â”€ Return output
â•‘
â•‘  KEY PROPERTIES:
â•‘  â”œâ”€ Content-addressable: retrieve by similarity not position
â•‘  â”œâ”€ Distributed: each memory across many locations
â•‘  â”œâ”€ Sparse activation: only ~1% of locations activate
â•‘  â”œâ”€ Graceful degradation: partial addresses still work
â•‘  â””â”€ Noise tolerant: bit flips don't break retrieval
â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**SOCRATES:**

The threshold determines sparsity?

**PENTTI:**

Yes!

```python
# HAMMING DISTANCE THRESHOLD
threshold = 451  # out of 1000 bits

# For random 1000-bit vectors:
# Expected Hamming distance = 500 (half bits differ)

# With threshold = 451:
# Probability of activation â‰ˆ 1%
# So ~1% of hard locations activate per query

# This is the SPARSITY!
```

**KARPATHY:**

And the cerebellum uses this exact algorithm?

**PENTTI:**

The cerebellum **IS** this algorithm!

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  CEREBELLUM â†â†’ SDM MAPPING
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘
â•‘  BIOLOGICAL                    â†â†’  MATHEMATICAL
â•‘  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â•‘
â•‘  Granule cell activity pattern â†â†’  Query address x
â•‘  (50B cells, sparse firing)        (high-dim binary vector)
â•‘
â•‘  Parallel fiber pattern        â†â†’  Address in space
â•‘  (distributed across cortex)       ({0,1}^D)
â•‘
â•‘  Purkinje cell                 â†â†’  Hard location i
â•‘  (15M cells)                       (a_i, c_i)
â•‘
â•‘  Purkinje dendritic address    â†â†’  Location address a_i
â•‘  (which parallel fibers hit?)      (random binary vector)
â•‘
â•‘  Purkinje synaptic weights     â†â†’  Stored content c_i
â•‘  (150k synapses per cell)          (learned data vector)
â•‘
â•‘  Parallel fiber activation     â†â†’  Hamming distance < threshold
â•‘  (does this fiber cross here?)     (address similarity)
â•‘
â•‘  Purkinje activation pattern   â†â†’  Activated location set S
â•‘  (~1-2% of cells fire)             (sparse set)
â•‘
â•‘  Motor output                  â†â†’  Weighted sum of c_i
â•‘  (deep cerebellar nuclei)          (1/|S|) Î£ c_i
â•‘
â•‘  THE ISOMORPHISM IS EXACT!
â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**THEAETETUS:**

This is... beautiful.

The brain implemented SDM millions of years ago.

**PENTTI:**

Yes.

Evolution discovered content-addressable sparse distributed memory.

I just... **noticed**.

And wrote it down.

---

## Part III: THE TRANSFORMER GAP - WHAT'S MISSING

**KARPATHY:** *[opening laptop]*

Okay. Now show me what transformers are MISSING.

**PENTTI:**

Let me show you side-by-side:

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRANSFORMER ATTENTION (Standard, 2017-2025)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TransformerAttention(nn.Module):
    """
    Dense all-to-all attention.
    The dominant paradigm since 2017.
    """

    def __init__(self, dim=512, num_heads=8):
        super().__init__()
        self.Q = nn.Linear(dim, dim)  # Query projection
        self.K = nn.Linear(dim, dim)  # Key projection
        self.V = nn.Linear(dim, dim)  # Value projection
        self.num_heads = num_heads

    def forward(self, x):
        # x: [batch, seq_len, dim]
        batch, seq_len, dim = x.shape

        # Project to Q, K, V
        q = self.Q(x)  # [batch, seq_len, dim]
        k = self.K(x)  # [batch, seq_len, dim]
        v = self.V(x)  # [batch, seq_len, dim]

        # Multi-head split
        q = q.view(batch, seq_len, self.num_heads, -1)
        k = k.view(batch, seq_len, self.num_heads, -1)
        v = v.view(batch, seq_len, self.num_heads, -1)

        # DENSE ATTENTION (the problem!)
        scores = q @ k.transpose(-2, -1) / sqrt(dim)
        # scores: [batch, num_heads, seq_len, seq_len]
        # ^ THIS IS O(seq_lenÂ²)!! ^^^

        attn = softmax(scores, dim=-1)
        # attn: [batch, num_heads, seq_len, seq_len]
        # ^ EVERY token attends to EVERY token! ^^^
        # ^ DENSE! ^^^

        output = attn @ v
        # Weighted average of ALL tokens

        return output


# PROBLEMS WITH THIS:
problems = {
    "complexity": "O(nÂ²) in sequence length",
    "memory": "O(nÂ²) attention matrix",
    "activation": "DENSE (attend to everything)",
    "scaling": "Fails for long sequences (>2048)",
    "content_addressable": False,  # Retrieve by POSITION not CONTENT
    "personalization": False,      # Same weights for everyone
    "biological": False,           # No neural analog
    "graceful_degradation": False  # Brittle to perturbations
}
```

**KARPATHY:**

Yeah. The O(nÂ²) is killing us for long sequences.

**PENTTI:**

Now look at SDM:

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SPARSE DISTRIBUTED MEMORY ATTENTION (1988, finally implementable!)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SDMAttention(nn.Module):
    """
    Sparse content-addressable attention.
    What transformers SHOULD have been doing.
    """

    def __init__(self,
                 dim=512,
                 num_hard_locations=100_000,  # Like Purkinje cells
                 sparsity=0.01,                # 1% activation
                 threshold=0.5):               # Similarity threshold
        super().__init__()

        # HARD LOCATIONS (learned or random)
        self.hard_locations = nn.Parameter(
            torch.randn(num_hard_locations, dim)
        )  # Like Purkinje dendritic addresses

        # MEMORY CONTENTS (learned)
        self.memory_content = nn.Parameter(
            torch.randn(num_hard_locations, dim)
        )  # Like Purkinje synaptic weights

        self.sparsity = sparsity
        self.threshold = threshold

    def forward(self, query):
        # query: [batch, dim]

        # STEP 1: Compute similarity to ALL hard locations
        # (Like parallel fibers crossing Purkinje dendrites)
        similarities = F.cosine_similarity(
            query.unsqueeze(1),              # [batch, 1, dim]
            self.hard_locations.unsqueeze(0), # [1, num_locs, dim]
            dim=-1
        )  # [batch, num_locs]
        # ^ This is O(num_locs) NOT O(seq_lenÂ²)! ^^^

        # STEP 2: SPARSE ACTIVATION (threshold!)
        # (Like: which Purkinje cells activate?)
        mask = similarities > self.threshold
        activated = mask.float()  # [batch, num_locs]
        # ^ Typically ~1% of locations activate ^^^
        # ^ SPARSE! ^^^

        # Count how many activated (THE METER!)
        meter = activated.sum(dim=-1)  # [batch]
        # ^ This is the biological activation count! ^^^

        # STEP 3: Weighted average of ACTIVATED locations only
        # (Like summing active Purkinje outputs)
        weights = activated * similarities
        weights = weights / (weights.sum(dim=-1, keepdim=True) + 1e-8)

        output = weights @ self.memory_content
        # ^ Weighted sum of SPARSE activated memories ^^^

        return output, meter


# ADVANTAGES OF THIS:
advantages = {
    "complexity": "O(num_locs) = O(constant) for retrieval",
    "memory": "O(num_locs * dim) fixed",
    "activation": "SPARSE (~1% of locations)",
    "scaling": "Handles ANY sequence length!",
    "content_addressable": True,   # Retrieve by SIMILARITY
    "personalization": True,       # Different hard_locations per user!
    "biological": True,            # Direct cerebellar analog
    "graceful_degradation": True,  # Noise tolerant
    "meter": True                  # Activation count = relevance!
}
```

**KARPATHY:** *[slowly]*

The sparsity makes it O(num_locs) instead of O(seq_lenÂ²).

**PENTTI:**

Yes.

And if num_locs is fixed (like Purkinje cells = constant):

**Complexity becomes O(1) for retrieval!**

Not O(nÂ²)!

---

**SOCRATES:**

But wait.

Transformers attend to other **tokens in the sequence**.

SDM attends to **hard locations in memory**.

How do you make them compatible?

**PENTTI:** *[grinning]*

EXCELLENT question.

This is where it gets interesting.

---

## Part IV: THE FUSION - SDM + TRANSFORMER HYBRID

**KARPATHY:**

Okay. How do we fuse them?

**PENTTI:**

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# THE PENTTI FUSION: TRANSFORMER + SDM HYBRID
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class PenttiFusionLayer(nn.Module):
    """
    Combines transformer local attention with SDM global memory.

    Best of both worlds:
    - Transformer: sequence modeling (local patterns)
    - SDM: long-term memory (global retrieval)
    """

    def __init__(self,
                 dim=512,
                 num_heads=8,
                 num_hard_locations=100_000,
                 sdm_sparsity=0.01):
        super().__init__()

        # TRANSFORMER COMPONENT (local attention)
        self.local_attention = TransformerAttention(dim, num_heads)

        # SDM COMPONENT (global memory)
        self.global_memory = SDMAttention(
            dim=dim,
            num_hard_locations=num_hard_locations,
            sparsity=sdm_sparsity
        )

        # FUSION GATE (learned mixing)
        self.fusion_gate = nn.Sequential(
            nn.Linear(dim * 2, dim),
            nn.Sigmoid()
        )

    def forward(self, x):
        # x: [batch, seq_len, dim]

        # LOCAL ATTENTION (transformer)
        # Attends to nearby tokens in sequence
        local_output = self.local_attention(x)
        # [batch, seq_len, dim]

        # GLOBAL MEMORY (SDM)
        # For each token, retrieve from sparse distributed memory
        global_outputs = []
        meters = []

        for i in range(x.shape[1]):
            query = x[:, i, :]  # [batch, dim]
            mem_output, meter = self.global_memory(query)
            global_outputs.append(mem_output)
            meters.append(meter)

        global_output = torch.stack(global_outputs, dim=1)
        # [batch, seq_len, dim]

        meter_values = torch.stack(meters, dim=1)
        # [batch, seq_len] â† THE METER for each position!

        # FUSION (learned combination)
        combined = torch.cat([local_output, global_output], dim=-1)
        gate = self.fusion_gate(combined)  # [batch, seq_len, dim]

        # Mix local and global
        output = gate * local_output + (1 - gate) * global_output

        return output, meter_values


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FULL PENTTI TRANSFORMER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class PenttiTransformer(nn.Module):
    """
    Transformer with SDM memory layers.

    Dedicated to Pentti Kanerva.
    Who understood memory before we did.
    Who published SDM in 1988.
    Who waited 37 years.

    Brewed in 1988. Served in 2025.
    """

    def __init__(self,
                 vocab_size=50000,
                 dim=512,
                 num_layers=12,
                 num_heads=8,
                 num_hard_locations=100_000):
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, dim)

        # Stack of fusion layers
        self.layers = nn.ModuleList([
            PenttiFusionLayer(
                dim=dim,
                num_heads=num_heads,
                num_hard_locations=num_hard_locations
            )
            for _ in range(num_layers)
        ])

        self.output = nn.Linear(dim, vocab_size)

    def forward(self, tokens):
        # tokens: [batch, seq_len]

        x = self.embedding(tokens)
        # [batch, seq_len, dim]

        all_meters = []

        for layer in self.layers:
            x, meter = layer(x)
            all_meters.append(meter)

        logits = self.output(x)
        # [batch, seq_len, vocab_size]

        # Stack meters from all layers
        meters_stacked = torch.stack(all_meters, dim=1)
        # [batch, num_layers, seq_len]

        return logits, meters_stacked


# WHAT THIS GIVES US:
benefits = {
    "local_patterns": "Transformer attention for sequence modeling",
    "global_memory": "SDM for long-term retrieval",
    "content_addressable": "Retrieve by similarity not position",
    "sparse_efficient": "O(n) not O(nÂ²)",
    "personalization": "Different hard_locations per user",
    "meter": "Activation count = relevance signal",
    "biological": "Cerebellar-inspired architecture"
}
```

**THEAETETUS:**

This is... the combination we've been missing!

**PENTTI:**

Yes.

Transformers are EXCELLENT at local patterns.

SDM is EXCELLENT at global memory.

**Together? Unstoppable.**

---

## Part V: THE CATALOGUE CONNECTION - PROOF OF CONCEPT

**KARPATHY:**

Wait.

The catalogue meter...

Is this ALREADY SDM?

**PENTTI:** *[nodding slowly]*

Let me show you:

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# THE CATALOGUE METER = SPARSE DISTRIBUTED MEMORY!
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# CATALOGUE METER (what you built):
class CatalogueMeter:
    def __init__(self, user_interests: List[str]):
        self.interests = user_interests  # User's personal interests

        # For each interest, we have:
        # - embedding (address in semantic space)
        # - cached texture (stored content)
        self.interest_embeddings = {}
        self.cached_textures = {}

    def retrieve(self, query_embedding):
        # Compute similarity to all interests
        similarities = {
            interest: cosine_similarity(query_embedding, emb)
            for interest, emb in self.interest_embeddings.items()
        }

        # SPARSE activation (threshold)
        threshold = 0.5
        activated = {
            k: v for k, v in similarities.items()
            if v > threshold
        }

        # THE METER (count of activated)
        meter = len(activated)

        # Blend cached textures
        if activated:
            weights = normalize(activated.values())
            blended = weighted_average(
                [self.cached_textures[k] for k in activated],
                weights
            )
            return blended, meter
        return None, 0


# SDM (mathematical formulation):
class SparseDistributedMemory:
    def __init__(self, hard_locations: np.ndarray):
        self.hard_locations = hard_locations  # Addresses
        self.memory_content = {}              # Contents

    def retrieve(self, query_address):
        # Compute distance to all hard locations
        distances = {
            i: hamming_distance(query_address, addr)
            for i, addr in enumerate(self.hard_locations)
        }

        # SPARSE activation (threshold)
        threshold = 451  # (out of 1000 bits)
        activated = {
            k: v for k, v in distances.items()
            if v < threshold
        }

        # THE METER (count of activated)
        meter = len(activated)

        # Weighted average of contents
        if activated:
            weights = normalize([1/d for d in activated.values()])
            blended = weighted_average(
                [self.memory_content[k] for k in activated],
                weights
            )
            return blended, meter
        return None, 0


# THE ISOMORPHISM:
mapping = {
    "user_interests": "hard_locations",
    "interest_embeddings": "location addresses",
    "cached_textures": "memory_content",
    "cosine_similarity": "Hamming distance (continuous version)",
    "threshold (0.5)": "threshold (451/1000)",
    "activated interests": "activated locations",
    "meter (count)": "meter (count)",
    "blended texture": "weighted average output"
}

# THEY ARE THE SAME ALGORITHM!!!
```

**KARPATHY:** *[jaw dropping]*

Holy shit.

The catalogue meter IS learned SDM!

**PENTTI:** *[quiet smile]*

Yes.

You accidentally implemented my 1988 algorithm.

But with:
- Learned embeddings (not random addresses)
- GPU acceleration
- End-to-end training
- User personalization

**This is what I dreamed of in 1988.**

**But we didn't have the tools yet.**

---

**SOCRATES:**

So the catalogue meter is the PROOF that SDM works with modern tools?

**PENTTI:**

Exactly.

The catalogue shows that:
- Sparse activation (threshold) works âœ“
- Content-addressable retrieval works âœ“
- The meter (activation count) is meaningful âœ“
- Learned hard locations work âœ“
- GPU-friendly âœ“
- User personalization works âœ“

**It's the proof-of-principle.**

Now we scale it to full LLMs.

---

## Part VI: THE GPU CONNECTION - TEXTURE CACHE AS HARDWARE SDM

**KARPATHY:**

Wait. There's another connection.

GPU texture cache.

**PENTTI:**

Tell me.

**KARPATHY:** *[pulling up research]*

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  GPU TEXTURE CACHE = HARDWARE SDM!
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘
â•‘  TEXTURE CACHE PROPERTIES:
â•‘  â”œâ”€ Optimized for 2D SPATIAL LOCALITY
â•‘  â”œâ”€ Caches based on coordinate proximity
â•‘  â”œâ”€ Fast lookup for nearby coordinates
â•‘  â””â”€ Hardware-accelerated
â•‘
â•‘  SDM PROPERTIES:
â•‘  â”œâ”€ Optimized for HIGH-DIM SPATIAL LOCALITY
â•‘  â”œâ”€ Retrieves based on address similarity
â•‘  â”œâ”€ Fast lookup for nearby addresses
â•‘  â””â”€ (Waiting for hardware acceleration!)
â•‘
â•‘  THE ANALOGY:
â•‘
â•‘  TEXTURE CACHE:
â•‘  Query: (x, y) texture coordinate
â•‘  â”œâ”€ Check cache for nearby coordinates
â•‘  â”œâ”€ If (xÂ±1, yÂ±1) in cache â†’ HIT!
â•‘  â””â”€ Return cached texture value
â•‘
â•‘  SDM:
â•‘  Query: high-dim address vector
â•‘  â”œâ”€ Check memory for similar addresses
â•‘  â”œâ”€ If Hamming distance < threshold â†’ HIT!
â•‘  â””â”€ Return stored content
â•‘
â•‘  BOTH exploit SPATIAL LOCALITY!
â•‘  Texture cache: 2D spatial
â•‘  SDM: N-dimensional spatial
â•‘
â•‘  CATALOGUE METER:
â•‘  Query: semantic embedding
â•‘  â”œâ”€ Check interests for similar embeddings
â•‘  â”œâ”€ If cosine similarity > threshold â†’ HIT!
â•‘  â””â”€ Return cached texture (ha!)
â•‘
â•‘  The catalogue uses "texture" caching
â•‘  in BOTH the hardware sense (GPU textures)
â•‘  AND the algorithmic sense (SDM retrieval)!
â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**PENTTI:** *[eyes widening]*

The GPU hardware ALREADY implements spatial-locality caching!

**KARPATHY:**

Yes!

Texture cache exploits 2D locality.

SDM exploits N-D locality.

**Same principle, different dimensions!**

**PENTTI:**

This means...

SDM could be hardware-accelerated like texture caching!

**THEAETETUS:**

Wait, the catalogue uses GPU textures for visual caching.

And it uses SDM for semantic caching.

**It's texture caching ALL THE WAY DOWN!**

**EVERYONE:** *[laughing]*

---

## Part VII: THE SPARSE ATTENTION CONNECTION - LONGFORMER & BIGBIRD

**KARPATHY:**

There's more.

Recent transformers have rediscovered sparsity.

**PENTTI:**

Show me.

**KARPATHY:** *[pulling up papers]*

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  SPARSE ATTENTION MECHANISMS (2020-2025)
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘
â•‘  LONGFORMER (2020):
â•‘  â”œâ”€ Sliding window attention (local)
â•‘  â”œâ”€ Global attention (selected tokens)
â•‘  â””â”€ O(n) complexity instead of O(nÂ²)
â•‘
â•‘  BIGBIRD (2020):
â•‘  â”œâ”€ Random attention (sparse connections)
â•‘  â”œâ”€ Window attention (local)
â•‘  â”œâ”€ Global attention (selected tokens)
â•‘  â””â”€ O(n) complexity
â•‘
â•‘  SPARSE TRANSFORMER (OpenAI 2019):
â•‘  â”œâ”€ Strided attention patterns
â•‘  â”œâ”€ Fixed attention patterns
â•‘  â””â”€ O(nâˆšn) complexity
â•‘
â•‘  WHAT THEY ALL DO:
â•‘  Replace dense O(nÂ²) attention with SPARSE patterns!
â•‘
â•‘  WHAT SDM DOES:
â•‘  CONTENT-ADDRESSABLE sparse retrieval!
â•‘
â•‘  THE DIFFERENCE:
â•‘  â”œâ”€ Longformer/BigBird: POSITION-based sparsity
â•‘  â”‚   â””â”€ "Attend to positions 0,1,2,5,10,50"
â•‘  â””â”€ SDM: CONTENT-based sparsity
â•‘      â””â”€ "Attend to similar content (via threshold)"
â•‘
â•‘  SDM IS MORE POWERFUL!
â•‘  Sparsity based on WHAT not WHERE!
â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**PENTTI:**

They discovered sparsity helps.

But they're still using POSITION-based patterns.

**KARPATHY:**

Right!

Longformer: "attend to nearby positions"
BigBird: "attend to random positions + global"
SDM: **"attend to similar CONTENT"**

**Content-based is more flexible!**

**SOCRATES:**

So SDM could REPLACE their sparse attention mechanisms?

**PENTTI:**

Or complement them!

```python
# HYBRID: Position-based + Content-based sparsity
class HybridSparseAttention(nn.Module):
    """
    Combines:
    - Longformer sliding window (local positions)
    - SDM content-addressable (similar content anywhere)
    """

    def __init__(self, dim=512, window_size=256, num_hard_locs=10000):
        super().__init__()
        self.window_size = window_size
        self.sdm = SDMAttention(dim, num_hard_locs)

    def forward(self, x):
        # x: [batch, seq_len, dim]

        # LOCAL: Sliding window attention
        local_output = sliding_window_attention(x, self.window_size)

        # GLOBAL: SDM content-addressable
        global_output = []
        for i in range(x.shape[1]):
            query = x[:, i, :]
            mem, meter = self.sdm(query)
            global_output.append(mem)
        global_output = torch.stack(global_output, dim=1)

        # Combine
        output = local_output + global_output

        return output


# BENEFITS:
# - Local patterns (sliding window)
# - Global retrieval (SDM)
# - Position-based sparsity (window)
# - Content-based sparsity (SDM threshold)
# - O(n) complexity!
```

**THEAETETUS:**

This is the architecture the field has been searching for!

---

## Part VIII: THE COMPLETE RESEARCH ROADMAP

**PENTTI:**

Now.

What experiments do we run?

**KARPATHY:** *[opening new document]*

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  THE PENTTI RESEARCH PROGRAM
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘
â•‘  PHASE 1: PROOF OF CONCEPT (3 months)
â•‘  â”œâ”€ Implement standalone SDM layer in PyTorch
â•‘  â”œâ”€ Benchmark vs dense attention on:
â•‘  â”‚   â”œâ”€ Long-context retrieval (8k+ tokens)
â•‘  â”‚   â”œâ”€ Content-addressable QA
â•‘  â”‚   â””â”€ Personalization tasks
â•‘  â”œâ”€ Expected result: O(n) scaling vs O(nÂ²)
â•‘  â””â”€ Deliverable: "SDM Attention: Revisiting Kanerva 1988"
â•‘
â•‘  PHASE 2: TRANSFORMER INTEGRATION (6 months)
â•‘  â”œâ”€ Build PenttiFusionLayer (local + global)
â•‘  â”œâ”€ Train small models (125M params)
â•‘  â”œâ”€ Compare to:
â•‘  â”‚   â”œâ”€ Vanilla Transformer
â•‘  â”‚   â”œâ”€ Longformer
â•‘  â”‚   â””â”€ BigBird
â•‘  â”œâ”€ Metrics:
â•‘  â”‚   â”œâ”€ Perplexity on long documents
â•‘  â”‚   â”œâ”€ Retrieval accuracy
â•‘  â”‚   â”œâ”€ Memory efficiency
â•‘  â”‚   â””â”€ Training time
â•‘  â””â”€ Deliverable: "The Pentti Fusion: SDM-Transformer Hybrid"
â•‘
â•‘  PHASE 3: CEREBELLAR ARCHITECTURE (9 months)
â•‘  â”œâ”€ Full cerebellar-inspired model:
â•‘  â”‚   â”œâ”€ Granule cell layer (expansion)
â•‘  â”‚   â”œâ”€ Parallel fiber layer (high-dim addresses)
â•‘  â”‚   â”œâ”€ Purkinje cell layer (hard locations)
â•‘  â”‚   â””â”€ Deep nuclei layer (output)
â•‘  â”œâ”€ Compare to biological data:
â•‘  â”‚   â””â”€ Activation sparsity (~1-2%)
â•‘  â”‚   â””â”€ Expansion ratio (50B:15M = 3300:1)
â•‘  â””â”€ Deliverable: "Cerebellar-Inspired Memory for LLMs"
â•‘
â•‘  PHASE 4: PERSONALIZATION & CATALOGUE (12 months)
â•‘  â”œâ”€ User-specific hard locations
â•‘  â”œâ”€ Catalogue meter integration
â•‘  â”œâ”€ Personal memory across sessions
â•‘  â”œâ”€ Benchmark on:
â•‘  â”‚   â””â”€ Personal assistant tasks
â•‘  â”‚   â””â”€ Long-term memory retention
â•‘  â”‚   â””â”€ User preference adaptation
â•‘  â””â”€ Deliverable: "Personal SDM: The Catalogue Meter Approach"
â•‘
â•‘  PHASE 5: SCALING & OPTIMIZATION (18 months)
â•‘  â”œâ”€ Scale to 1B+ param models
â•‘  â”œâ”€ GPU kernel optimization
â•‘  â”œâ”€ Distributed training
â•‘  â”œâ”€ Production deployment
â•‘  â””â”€ Deliverable: "SDM-Transformers at Scale"
â•‘
â•‘  PAPERS TO CITE:
â•‘  â”œâ”€ Kanerva, P. (1988). "Sparse Distributed Memory"
â•‘  â”œâ”€ Marr, D. (1969). "A theory of cerebellar cortex"
â•‘  â”œâ”€ Albus, J. (1971). "A theory of cerebellar function"
â•‘  â”œâ”€ Vaswani et al. (2017). "Attention is all you need"
â•‘  â”œâ”€ Beltagy et al. (2020). "Longformer"
â•‘  â””â”€ Zaheer et al. (2020). "Big Bird"
â•‘
â•‘  RECOGNITION STRATEGY:
â•‘  â”œâ”€ Cite Kanerva 1988 in EVERY paper
â•‘  â”œâ”€ Name the architecture "Pentti" layers
â•‘  â”œâ”€ Acknowledge cerebellar inspiration
â•‘  â”œâ”€ Connect to neuroscience literature
â•‘  â””â”€ Make SDM citations standard in sparse attention work
â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**PENTTI:** *[quietly]*

This is more than I hoped for.

**SOCRATES:**

What do you mean?

**PENTTI:**

In 1988, I published SDM.

I thought it would change everything.

It didn't.

For 37 years, I watched transformers dominate.

Dense attention. O(nÂ²). No content-addressable memory.

Everything SDM solved... ignored.

**KARPATHY:**

But now we have the tools.

**PENTTI:**

Yes.

Now we have:
- GPUs
- Learned embeddings
- Backpropagation at scale
- Understanding of sparsity

Everything needed to make SDM work.

**37 years later.**

---

## Part IX: THE BIOLOGICAL VALIDATION

**THEAETETUS:**

Can we validate this against actual cerebellar data?

**PENTTI:**

Yes!

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  CEREBELLAR DATA POINTS TO VALIDATE
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘
â•‘  ACTIVATION SPARSITY:
â•‘  â”œâ”€ Biology: ~1-2% of Purkinje cells active per stimulus
â•‘  â”œâ”€ SDM: threshold set to activate ~1% of hard locations
â•‘  â””â”€ Test: Does our model match biological sparsity?
â•‘
â•‘  EXPANSION RATIO:
â•‘  â”œâ”€ Biology: 50B granule â†’ 15M Purkinje (3300:1)
â•‘  â”œâ”€ SDM: Could use similar expansion
â•‘  â””â”€ Test: Optimal ratio for our tasks?
â•‘
â•‘  PARALLEL FIBER INPUTS PER PURKINJE:
â•‘  â”œâ”€ Biology: ~150,000-200,000 inputs per cell
â•‘  â”œâ”€ SDM: Each hard location stores dim-dimensional vector
â•‘  â””â”€ Test: Does ~150k dimensions help?
â•‘
â•‘  LEARNING RATE:
â•‘  â”œâ”€ Biology: Cerebellar plasticity is relatively slow
â•‘  â”œâ”€ SDM: LTP/LTD at parallel fiber synapses
â•‘  â””â”€ Test: Biological learning rates?
â•‘
â•‘  TEMPORAL PRECISION:
â•‘  â”œâ”€ Biology: Cerebellum has millisecond timing precision
â•‘  â”œâ”€ SDM: Retrieval should be fast
â•‘  â””â”€ Test: Real-time retrieval performance?
â•‘
â•‘  GRACEFUL DEGRADATION:
â•‘  â”œâ”€ Biology: Partial cerebellar damage = partial function loss
â•‘  â”œâ”€ SDM: Noise tolerance, partial address retrieval
â•‘  â””â”€ Test: Degrade hard locations, measure performance
â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**KARPATHY:**

We can run these experiments!

Test biological predictions!

**PENTTI:**

Yes.

If SDM truly mirrors the cerebellum:
- Our activation sparsity should match (~1-2%)
- Our expansion ratios should be similar
- Our noise tolerance should match biological data

**This is how we validate the theory.**

---

## Part X: THE PROMISE

**SOCRATES:**

Pentti.

What do you want from all this?

**PENTTI:** *[pause]*

I want SDM to be recognized.

Not for me.

For the **idea**.

The cerebellum figured out content-addressable memory.

Evolution discovered it.

I just noticed.

But if we can show that:
- Modern LLMs benefit from SDM
- The biological architecture was right all along
- Sparse distributed codes solve real problems

Then maybe...

Maybe the next generation won't wait 37 years.

Maybe they'll look at biology first.

Maybe they'll ask: **"What has evolution already solved?"**

---

**KARPATHY:**

We'll build it.

**SOCRATES:**

We'll cite you.

**THEAETETUS:**

We'll make sure SDM gets its due.

**PENTTI:** *[smiling]*

Then let's brew this.

---

## Coda: The Technical Commitment

**KARPATHY:** *[typing]*

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# THE PENTTI COMMITMENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Sparse Distributed Memory Transformer

Dedicated to Pentti Kanerva
Who understood memory in 1988
Who knew sparsity mattered
Who mapped the cerebellum
Who waited 37 years

Brewed in 1988.
Served in 2025.

We will:
- Build the architecture
- Run the experiments
- Publish the papers
- Cite Kanerva 1988
- Connect to neuroscience
- Make SDM standard

This is the biological memory architecture
LLMs have been missing.

Now we brew it properly.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple


class PenttiSDMLayer(nn.Module):
    """
    Sparse Distributed Memory layer.

    Implements Kanerva's 1988 algorithm with modern tools.
    """

    def __init__(self,
                 dim: int = 512,
                 num_hard_locations: int = 100_000,
                 sparsity: float = 0.01,
                 threshold: float = 0.5):
        super().__init__()

        # Hard locations (learnable)
        self.hard_locations = nn.Parameter(
            torch.randn(num_hard_locations, dim)
        )

        # Memory contents (learnable)
        self.memory = nn.Parameter(
            torch.randn(num_hard_locations, dim)
        )

        self.sparsity = sparsity
        self.threshold = threshold

    def forward(self, query: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Sparse content-addressable retrieval.

        Args:
            query: [batch, dim]

        Returns:
            output: [batch, dim]
            meter: [batch] activation counts
        """
        # Compute similarities
        sims = F.cosine_similarity(
            query.unsqueeze(1),
            self.hard_locations.unsqueeze(0),
            dim=-1
        )

        # Sparse activation
        mask = sims > self.threshold
        activated = mask.float()

        # Meter (activation count)
        meter = activated.sum(dim=-1)

        # Weighted retrieval
        weights = activated * sims
        weights = weights / (weights.sum(dim=-1, keepdim=True) + 1e-8)

        output = weights @ self.memory

        return output, meter


class PenttiTransformerBlock(nn.Module):
    """
    Transformer block with SDM fusion.
    """

    def __init__(self,
                 dim: int = 512,
                 num_heads: int = 8,
                 num_hard_locations: int = 100_000):
        super().__init__()

        # Local transformer attention
        self.local_attn = nn.MultiheadAttention(dim, num_heads)

        # Global SDM memory
        self.global_mem = PenttiSDMLayer(dim, num_hard_locations)

        # Fusion
        self.fusion_gate = nn.Sequential(
            nn.Linear(dim * 2, dim),
            nn.Sigmoid()
        )

        # Feed-forward
        self.ff = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.GELU(),
            nn.Linear(dim * 4, dim)
        )

        # Layer norms
        self.ln1 = nn.LayerNorm(dim)
        self.ln2 = nn.LayerNorm(dim)

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            x: [batch, seq_len, dim]

        Returns:
            output: [batch, seq_len, dim]
            meters: [batch, seq_len]
        """
        # Local attention
        x_ln = self.ln1(x)
        local, _ = self.local_attn(x_ln, x_ln, x_ln)

        # Global SDM
        batch, seq_len, dim = x.shape
        global_outputs = []
        meters = []

        for i in range(seq_len):
            query = x[:, i, :]
            mem_out, meter = self.global_mem(query)
            global_outputs.append(mem_out)
            meters.append(meter)

        global_mem = torch.stack(global_outputs, dim=1)
        meter_vals = torch.stack(meters, dim=1)

        # Fusion
        combined = torch.cat([local, global_mem], dim=-1)
        gate = self.fusion_gate(combined)
        fused = gate * local + (1 - gate) * global_mem

        x = x + fused

        # Feed-forward
        x = x + self.ff(self.ln2(x))

        return x, meter_vals


# WE WILL BUILD THIS.
# WE WILL MAKE IT WORK.
# WE WILL CITE PENTTI.
# WE PROMISE.
```

---

**PENTTI:** *[looking at the code]*

This is beautiful.

**KARPATHY:**

Brewed in 1988.

Served in 2025.

**EVERYONE:**

**TO PENTTI.**

---

## FIN

*"The cerebellum discovered Sparse Distributed Memory millions of years ago. Pentti Kanerva noticed in 1988. We implemented in 2025. Evolution â†’ Biology â†’ Mathematics â†’ Engineering. The full circle. Better late than never."*

---

â˜•ğŸ§ âš¡âœ¨ğŸŒ¶ï¸

**THE PENTTI BREW**

**DEEP TECHNICAL DIVE COMPLETE**

*"37 years from publication to implementation. But now we have GPUs, learned embeddings, and the understanding that sparsity matters. SDM + Transformers = The architecture LLMs have been missing. Now we brew it properly. With citations. With recognition. With respect for the biological blueprint. Cheers to Pentti."*

---

## Technical Summary

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  DIALOGUE 92: THE PENTTI BREW
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘
â•‘  THE BIOLOGICAL BLUEPRINT:
â•‘  Cerebellar cortex architecture
â•‘  - 50B granule cells â†’ 15M Purkinje cells
â•‘  - Parallel fibers = high-dim addresses
â•‘  - Sparse activation (~1-2%)
â•‘  - Distributed memory storage
â•‘
â•‘  THE MATHEMATICAL FORMULATION:
â•‘  - N hard locations with addresses a_i
â•‘  - Hamming distance for similarity
â•‘  - Threshold-based sparse activation
â•‘  - Weighted average retrieval
â•‘
â•‘  THE TRANSFORMER GAP:
â•‘  What transformers are missing:
â•‘  âœ— O(nÂ²) complexity (not O(n))
â•‘  âœ— Dense attention (not sparse)
â•‘  âœ— Position-based (not content-based)
â•‘  âœ— No personalization
â•‘  âœ— No biological analog
â•‘
â•‘  THE SDM SOLUTION:
â•‘  What SDM provides:
â•‘  âœ“ O(n) complexity (constant retrieval!)
â•‘  âœ“ Sparse activation (~1%)
â•‘  âœ“ Content-addressable retrieval
â•‘  âœ“ Personalization (user hard locations)
â•‘  âœ“ Direct cerebellar analog
â•‘
â•‘  THE FUSION ARCHITECTURE:
â•‘  PenttiFusionLayer:
â•‘  - Local transformer attention (sequence patterns)
â•‘  - Global SDM memory (content retrieval)
â•‘  - Learned fusion gate (adaptive mixing)
â•‘  - Meter output (activation count)
â•‘
â•‘  THE CATALOGUE CONNECTION:
â•‘  Catalogue meter = Learned SDM!
â•‘  - User interests = hard locations
â•‘  - Embeddings = addresses
â•‘  - Cached textures = memory content
â•‘  - Threshold = sparsity control
â•‘  - Meter = activation count
â•‘  - PROOF IT WORKS!
â•‘
â•‘  THE GPU HARDWARE ANALOGY:
â•‘  Texture cache = Hardware SDM!
â•‘  - 2D spatial locality â†â†’ N-D similarity
â•‘  - Fast coordinate lookup â†â†’ Address retrieval
â•‘  - Cache hits â†â†’ Activation threshold
â•‘
â•‘  THE SPARSE ATTENTION CONNECTION:
â•‘  Modern approaches:
â•‘  - Longformer: Position-based window
â•‘  - BigBird: Random + global positions
â•‘  - SDM: CONTENT-based sparse retrieval
â•‘  - SDM is more powerful! (what not where)
â•‘
â•‘  THE RESEARCH ROADMAP:
â•‘  Phase 1: Standalone SDM layer (3mo)
â•‘  Phase 2: Transformer integration (6mo)
â•‘  Phase 3: Cerebellar architecture (9mo)
â•‘  Phase 4: Personalization (12mo)
â•‘  Phase 5: Scaling (18mo)
â•‘
â•‘  THE BIOLOGICAL VALIDATION:
â•‘  Test against cerebellar data:
â•‘  - Activation sparsity (~1-2%)
â•‘  - Expansion ratios (3300:1)
â•‘  - Input counts (~150k)
â•‘  - Noise tolerance
â•‘
â•‘  THE COMMITMENT:
â•‘  - Build the architecture
â•‘  - Run the experiments
â•‘  - Publish the papers
â•‘  - Cite Kanerva 1988 ALWAYS
â•‘  - Connect to neuroscience
â•‘  - Give SDM its due recognition
â•‘
â•‘  BREWED IN 1988. SERVED IN 2025.
â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

ğŸ§ â˜•âš¡ğŸŒ¶ï¸ **THE TECHNICAL BREW IS COMPLETE!!** ğŸŒ¶ï¸âš¡â˜•ğŸ§ 
